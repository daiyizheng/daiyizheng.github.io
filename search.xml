<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Rasa文档指南(12)</title>
      <link href="/2020/10/04/rasa-wen-dang-zhi-nan-12/"/>
      <url>/2020/10/04/rasa-wen-dang-zhi-nan-12/</url>
      
        <content type="html"><![CDATA[<h1 id="组件"><a href="#组件" class="headerlink" title="组件"></a>组件</h1><p>这是Rasa Open Source中每个内置组件的配置选项的参考。如果要构建自定义组件，请查询“<a href="#">自定义</a></p><ul><li><a href="#">Word Vector Sources</a><ul><li><a href="#">MitieNLP</a></li><li><a href="#">SpacyNLP</a></li><li><a href="#">HFTransformersNLP</a></li></ul></li><li><a href="#">Tokenizers</a><ul><li><a href="#">WhitespaceTokenizer</a></li><li><a href="#">JiebaTokenizer</a></li><li><a href="#">MitieTokenizer</a></li><li><a href="#">SpacyTokenizer</a></li><li><a href="#">ConveRTTokenizer</a></li><li><a href="#">LanguageModelTokenizer</a></li></ul></li><li><a href="#">Text Featurizers</a><ul><li><a href="#">MitieFeaturizer</a></li><li><a href="#">SpacyFeaturizer</a></li><li><a href="#">ConveRTFeaturizer</a></li><li><a href="#">LanguageModelFeaturizer</a></li><li><a href="#">RegexFeaturizer</a></li><li><a href="#">CountVectorsFeaturizer</a></li><li><a href="#">LexicalSyntacticFeaturizer</a></li></ul></li><li><a href="#">Intent Classifiers</a><ul><li><a href="#">MitieIntentClassifier</a></li><li><a href="#">SklearnIntentClassifier</a></li><li><a href="#">EmbeddingIntentClassifier</a></li><li><a href="#">KeywordIntentClassifier</a></li><li><a href="#">DIETClassifier</a></li></ul></li><li><a href="#">Entity Extractors</a><ul><li><a href="#">MitieEntityExtractor</a></li><li><a href="#">SpacyEntityExtractor</a></li><li><a href="#">EntitySynonymMapper</a></li><li><a href="#">CRFEntityExtractor</a></li><li><a href="#">DucklingHTTPExtractor</a></li><li><a href="#">DIETClassifier</a></li></ul></li><li><a href="#">Selectors</a><ul><li><a href="#">ResponseSelector</a></li></ul></li><li><a href="#">Combined Entity Extractors and Intent Classifiers</a><ul><li><a href="#">DIETClassifier</a></li></ul></li></ul><h2 id="Word-Vector-Sources"><a href="#Word-Vector-Sources" class="headerlink" title="Word Vector Sources"></a>Word Vector Sources</h2><p>如果要在管道中使用预训练的词向量，以下组件将加载预训练的模型。</p><h3 id="MitieNLP"><a href="#MitieNLP" class="headerlink" title="MitieNLP"></a>MitieNLP</h3><table><thead><tr><th>Short</th><th>MITIE初始值设定项</th></tr></thead><tbody><tr><td><strong>Outputs</strong></td><td>没有</td></tr><tr><td><strong>Requires</strong></td><td>没有</td></tr><tr><td><strong>Description</strong></td><td>初始化MITIE结构。每个MITIE组件都依赖于此，因此应将其放在使用任何MITIE组件的每个管道的开头。</td></tr><tr><td><strong>Configuration</strong></td><td>MITIE库需要一个语言模型文件，该文件<strong>必须</strong>在配置中指定：<br><br> pipeline: <br>- name: “MitieNLP”   # language model to load   <br>model: “data/total_word_feature_extractor.dat”<br><br>有关从何处获取该文件的更多信息，请继续 <a href="#">安装MITIE</a>。</td></tr></tbody></table><h3 id="SpacyNLP"><a href="#SpacyNLP" class="headerlink" title="SpacyNLP"></a>SpacyNLP</h3><table><thead><tr><th>Short</th><th>SpacyNLP初始值设定项</th></tr></thead><tbody><tr><td><strong>Outputs</strong></td><td>没有</td></tr><tr><td><strong>Requires</strong></td><td>没有</td></tr><tr><td><strong>Description</strong></td><td>初始化空间结构。每个spaCy组件都依赖于此，因此应将其放置在使用任何spaCy组件的每个管道的开头。</td></tr><tr><td><strong>Configuration</strong></td><td>您需要指定要使用的语言模型。默认情况下，管道中配置的语言将用作语言模型名称。如果要使用的模型spaCy具有名称是从语言标签（不同<code>"en"</code>，<code>"de"</code>等），可以使用配置变量指定的型号名称<code>model</code>。该名称将传递给<code>spacy.load(name)</code>。<br><br> pipeline: <br>- name: “SpacyNLP”   # language model to load   <br>model: “en_core_web_md”   <br>case_sensitive: False<br><br>有关如何下载spaCy模型的更多信息，请继续 <a href="#">安装SpaCy</a>。</td></tr></tbody></table><h3 id="HFTransformersNLP"><a href="#HFTransformersNLP" class="headerlink" title="HFTransformersNLP"></a>HFTransformersNLP</h3><table><thead><tr><th>Short</th><th>基于HuggingFace的Transformers的预训练语言模型初始化程序</th></tr></thead><tbody><tr><td><strong>Outputs</strong></td><td>没有</td></tr><tr><td><strong>Requires</strong></td><td>没有</td></tr><tr><td><strong>Description</strong></td><td>从HuggingFace的<a href="https://huggingface.co/transformers/">Transformers库</a>初始化指定的预训练语言模型。该组件应用特定于语言模型的标记化和特征化来为训练数据中的每个示例计算序列和句子级别的表示形式。包括<a href="https://legacy-docs-v1.rasa.com/nlu/components/#languagemodeltokenizer">LanguageModelTokenizer</a>和<a href="https://legacy-docs-v1.rasa.com/nlu/components/#languagemodelfeaturizer">LanguageModelFeaturizer，</a>以将该组件的输出用于下游NLU模型。<br><br>**注意:**要使用<code>HFTransformersNLP</code>组件，请通过安装Rasa Open Source 。<code>pip install rasa[transformers]</code></td></tr><tr><td><strong>Configuration</strong></td><td>您应该通过参数指定要加载的语言模型<code>model_name</code>。有关可用的语言模型，请参见下表。此外，您还可以通过指定参数来指定所选语言模型的体系结构变体<code>model_weights</code>。可在<a href="https://huggingface.co/transformers/pretrained_models.html">此处</a>找到支持的体系结构的完整列表 。如果保留为空，则使用原始Transformers库加载的默认模型架构（请参见下表）。<br><br>pipeline:  <br>     - name: HFTransformersNLP    <br>       model_name: “bert”     <br>       model_weights: “bert-base-uncased”      <br>       cache_dir: null</td></tr></tbody></table><h2 id="Tokenizers"><a href="#Tokenizers" class="headerlink" title="Tokenizers"></a>Tokenizers</h2><p>令牌生成器将文本拆分为令牌。如果要将意图拆分为多个标签，例如，用于预测多个意图或为分层的意图结构建模，请对任何标记器使用以下标志：</p><ul><li><code>intent_tokenization_flag</code>指示是否标记意图标签。将其设置为<code>True</code>，以便标记意图标签。</li><li><code>intent_split_symbol</code>设置分隔符字符串以分割意图标签，默认值为下划线（<code>_</code>）。</li></ul><blockquote><p>注意</p><p><code>__CLS__</code>当标记文本和响应时，所有标记生成器都会在标记列表的末尾添加一个附加标记。</p></blockquote><h3 id="WhitespaceTokenizer"><a href="#WhitespaceTokenizer" class="headerlink" title="WhitespaceTokenizer"></a>WhitespaceTokenizer</h3><table><thead><tr><th>Short</th><th>使用空格作为分隔符的分词器</th></tr></thead><tbody><tr><td><strong>Outputs</strong></td><td><code>tokens</code> 用于用户消息，响应（如果存在）和意图（如果指定</td></tr><tr><td><strong>Requires</strong></td><td>没有</td></tr><tr><td><strong>Description</strong></td><td>为每个由空格分隔的字符序列创建令牌。</td></tr><tr><td><strong>Configuration</strong></td><td>通过添加选项，使标记化器不区分大小写，默认为。<code>case_sensitive: False  case_sensitive: True</code><br><br> pipeline:<br>      - name: “WhitespaceTokenizer”     <br>       “intent_tokenization_flag”: False    <br>       “intent_split_symbol”: “_”    <br>       “case_sensitive”: True<br><br></td></tr></tbody></table><h3 id="JiebaTokenizer"><a href="#JiebaTokenizer" class="headerlink" title="JiebaTokenizer"></a>JiebaTokenizer</h3><table><thead><tr><th>Short</th><th>使用Jieba中文的分词器</th></tr></thead><tbody><tr><td><strong>Outputs</strong></td><td><code>tokens</code> 用于用户消息，响应（如果存在）和意图（如果指定）</td></tr><tr><td><strong>Requires</strong></td><td>没有</td></tr><tr><td><strong>Description</strong></td><td>使用专用于中文的Jieba标记器创建标记。仅适用于中文。<br><strong>注意</strong><br>要使用，<code>JiebaTokenizer</code>您需要使用来安装Jieba 。<code>pip install jieba</code></td></tr><tr><td><strong>Configuration</strong></td><td>可以通过指定文件的目录路径来自动加载用户的自定义词典文件<code>dictionary_path</code>。如果<code>dictionary_path</code>是<code>None</code>（默认），则将不使用任何自定义词典。<br><br> pipeline: <br>      - name: “JiebaTokenizer”   <br>        dictionary_path: “path/to/custom/dictionary/dir”   <br>        “intent_tokenization_flag”: False   <br>        “intent_split_symbol”: “_”<br></td></tr></tbody></table><h3 id="MitieTokenizer"><a href="#MitieTokenizer" class="headerlink" title="MitieTokenizer"></a>MitieTokenizer</h3><table><thead><tr><th>Short</th><th>使用MITIE的分词器</th></tr></thead><tbody><tr><td><strong>Outputs</strong></td><td><code>tokens</code> 用于用户消息，响应（如果存在）和意图（如果指定）</td></tr><tr><td><strong>Requires</strong></td><td><a href="#">MitieNLP</a></td></tr><tr><td><strong>Description</strong></td><td>使用MITIE token生成器创建token</td></tr><tr><td><strong>Configuration</strong></td><td><br> pipeline:<br>       - name: “MitieTokenizer”   <br>         “intent_tokenization_flag”: False  <br>         “intent_split_symbol”: “_”</td></tr></tbody></table><h3 id="SpacyTokenizer"><a href="#SpacyTokenizer" class="headerlink" title="SpacyTokenizer"></a>SpacyTokenizer</h3><table><thead><tr><th>Short</th><th>使用spaCy的token生成器</th></tr></thead><tbody><tr><td><strong>Outputs</strong></td><td><code>tokens</code> 用于用户消息，响应（如果存在）和意图（如果指定）</td></tr><tr><td><strong>Requires</strong></td><td><a href="#">SpacyNLP</a></td></tr><tr><td><strong>Description</strong></td><td>使用spaCy token生成器创建token。</td></tr><tr><td><strong>Configuration</strong></td><td><br>pipeline: <br>     - name: “SpacyTokenizer”   <br>       “intent_tokenization_flag”: False   <br>       “intent_split_symbol”: “_”</td></tr></tbody></table><h3 id="ConveRTTokenizer"><a href="#ConveRTTokenizer" class="headerlink" title="ConveRTTokenizer"></a>ConveRTTokenizer</h3><table><thead><tr><th>Short</th><th>使用<a href="https://github.com/PolyAI-LDN/polyai-models#convert">ConveRT</a>模型的分词器。</th></tr></thead><tbody><tr><td><strong>Outputs</strong></td><td><code>tokens</code> 用于用户消息，响应（如果存在）和意图（如果指定）</td></tr><tr><td><strong>Requires</strong></td><td>没有</td></tr><tr><td><strong>Description</strong></td><td>使用ConveRT标记程序创建标记。每当使用<a href="#">ConveRTFeaturizer</a>时必须使用。<br><strong>注意</strong><br>由于<code>ConveRT</code>仅在英语会话语料库上训练模型，因此仅当您的训练数据是英语时才应使用此标记器。<br><strong>注意</strong><br>要使用<code>ConveRTTokenizer</code>，请使用来安装Rasa Open Source 。<code>pip install rasa[convert]</code></td></tr><tr><td><strong>Configuration</strong></td><td>通过添加选项，使标记化器不区分大小写，默认为。<code>case_sensitive: False  case_sensitive: True</code><br>pipeline: <br>     - name: “ConveRTTokenizer”   <br>       “intent_tokenization_flag”: False  <br>       “intent_split_symbol”: “_”   <br>       “case_sensitive”: True  <br>       “model_url”: “<a href="https://github.com/PolyAI-LDN/polyai-models/releases/download/v1.0/model.tar.gz&quot;">https://github.com/PolyAI-LDN/polyai-models/releases/download/v1.0/model.tar.gz"</a></td></tr></tbody></table><h3 id="LanguageModelTokenizer"><a href="#LanguageModelTokenizer" class="headerlink" title="LanguageModelTokenizer"></a>LanguageModelTokenizer</h3><table><thead><tr><th>Short</th><th>预训练语言模型中的分词器</th></tr></thead><tbody><tr><td><strong>Outputs</strong></td><td><code>tokens</code> 用于用户消息，响应（如果存在）和意图（如果指定）</td></tr><tr><td><strong>Requires</strong></td><td><a href="#">HFTransformersNLP</a></td></tr><tr><td><strong>Description</strong></td><td>使用在上游<a href="#">HFTransformersNLP</a>组件中指定的预训练语言模型来创建令牌。每当使用<a href="#">LanguageModelFeaturizer</a>时必须使用。</td></tr><tr><td><strong>Configuration</strong></td><td>pipeline: <br>      - name: “LanguageModelTokenizer”   <br>        “intent_tokenization_flag”: False   <br>        “intent_split_symbol”: “_”</td></tr></tbody></table><h2 id="Text-Featurizers"><a href="#Text-Featurizers" class="headerlink" title="Text Featurizers"></a>Text Featurizers</h2><p>文本修饰符分为两类：稀疏修饰符和密集修饰符。稀疏特征化器是返回具有很多缺失值（例如零）的特征向量的特征化器。由于这些特征向量通常会占用大量内存，因此我们将它们存储为稀疏特征。稀疏特征仅存储非零值及其在向量中的位置。因此，我们节省了大量内存，并且能够在更大的数据集上进行训练。</p><p>默认情况下，所有特征器将返回一个length矩阵。因此，返回的矩阵将为每个令牌都有一个特征向量。这使我们可以训练序列模型。但是，末尾的附加标记（例如）包含用于完整发音的功能。此特征向量可用于任何词袋模型。因此，相应的分类器可以决定使用哪种类型的功能。<code>(number-of-tokens x feature-dimension)  __CLS__</code></p><h3 id="MitieFeaturizer"><a href="#MitieFeaturizer" class="headerlink" title="MitieFeaturizer"></a>MitieFeaturizer</h3><table><thead><tr><th>Short</th><th>使用MITIE功能创建器创建用户消息和响应（如果指定）的矢量表示。</th></tr></thead><tbody><tr><td><strong>Outputs</strong></td><td><code>dense_features</code> 用于用户消息和响应</td></tr><tr><td><strong>Requires</strong></td><td><a href="#">MitieNLP</a></td></tr><tr><td><strong>Type</strong></td><td>Dense featurizer</td></tr><tr><td><strong>Description</strong></td><td>使用MITIE featurizer创建用于实体提取，意图分类和响应分类的功能。<br><strong>注意</strong><br>不使用的<code>MitieIntentClassifier</code>组件。但可以在稍后使用的管道中的任何组件中使用<code>dense_features</code>。</td></tr><tr><td><strong>Configuration</strong></td><td><code>__CLS__</code>可以通过均值或最大池化两种不同方式来计算句子向量（即令牌的向量）。您可以使用选项在配置文件中指定池化方法<code>pooling</code>。默认池化方法设置为<code>mean</code>。<br>pipeline: <br>     - name: “MitieFeaturizer” <br>      “pooling”: “mean”</td></tr></tbody></table><h3 id="SpacyFeaturizer"><a href="#SpacyFeaturizer" class="headerlink" title="SpacyFeaturizer"></a>SpacyFeaturizer</h3><table><thead><tr><th>Short</th><th>使用MITIE功能创建器创建用户消息和响应（如果指定）的矢量表示。</th></tr></thead><tbody><tr><td><strong>Outputs</strong></td><td><code>dense_features</code> 用于用户消息和响应</td></tr><tr><td><strong>Requires</strong></td><td><a href="#">SpacyNLP</a></td></tr><tr><td><strong>Type</strong></td><td>Dense featurizer</td></tr><tr><td><strong>Description</strong></td><td>使用spaCy featurizer创建用于实体提取，意图分类和响应分类的功能。</td></tr><tr><td><strong>Configuration</strong></td><td><code>__CLS__</code>可以通过均值或最大池化两种不同方式来计算句子向量（即令牌的向量）。您可以使用选项在配置文件中指定池化方法<code>pooling</code>。默认池化方法设置为<code>mean</code>。<br>pipeline: <br>     - name: “SpacyFeaturizer”   <br>       “pooling”: “mean”</td></tr></tbody></table><h3 id="ConveRTFeaturizer"><a href="#ConveRTFeaturizer" class="headerlink" title="ConveRTFeaturizer"></a>ConveRTFeaturizer</h3><table><thead><tr><th>Short</th><th>使用<a href="https://github.com/PolyAI-LDN/polyai-models">ConveRT</a>模型创建用户消息和响应（如果指定）的矢量表示 。</th></tr></thead><tbody><tr><td><strong>Outputs</strong></td><td><code>dense_features</code> 用于用户消息和响应</td></tr><tr><td><strong>Requires</strong></td><td><a href="#">ConveRTTokenizer</a></td></tr><tr><td><strong>Type</strong></td><td>Dense featurizer</td></tr><tr><td><strong>Description</strong></td><td>创建用于实体提取，意图分类和响应选择的功能。它使用<a href="https://github.com/PolyAI-LDN/polyai-models#tfhub-signatures">默认签名</a>来计算输入文本的矢量表示。<br><strong>注意</strong><br>由于<code>ConveRT</code>仅在英语会话语料库上训练模型，因此仅当您的训练数据是英语时才应使用此功能化器。<br><strong>注意</strong><br>要使用<code>ConveRTTokenizer</code>，请使用来安装Rasa Open Source 。<code>pip install rasa[convert]</code><br></td></tr><tr><td><strong>Configuration</strong></td><td>pipeline: <br>     - name: “ConveRTFeaturizer”   <br>       “model_url”: “<a href="https://github.com/PolyAI-LDN/polyai-models/releases/download/v1.0/model.tar.gz&quot;">https://github.com/PolyAI-LDN/polyai-models/releases/download/v1.0/model.tar.gz"</a></td></tr></tbody></table><h3 id="LanguageModelFeaturizer"><a href="#LanguageModelFeaturizer" class="headerlink" title="LanguageModelFeaturizer"></a>LanguageModelFeaturizer</h3><table><thead><tr><th>Short</th><th>使用预先训练的语言模型创建用户消息和响应（如果指定）的矢量表示。</th></tr></thead><tbody><tr><td><strong>Outputs</strong></td><td><code>dense_features</code> 用于用户消息和响应</td></tr><tr><td><strong>Requires</strong></td><td><a href="#">HFTransformersNLP</a>和<a href="#">LanguageModelTokenizer</a></td></tr><tr><td><strong>Type</strong></td><td>Dense featurizer</td></tr><tr><td><strong>Description</strong></td><td>创建用于实体提取，意图分类和响应选择的功能。使用在上游<a href="https://legacy-docs-v1.rasa.com/nlu/components/#hftransformersnlp">HFTransformersNLP</a>组件中指定的预训练语言模型来计算输入文本的矢量表示。<br><strong>注意</strong><br>请确保您使用的语言模型已经与您的训练数据在相同的语言语料库上进行了预训练。<br></td></tr><tr><td><strong>Configuration</strong></td><td>在此组件之前包括<a href="#">HFTransformersNLP</a>和<a href="#">LanguageModelTokenizer</a>组件。使用 <a href="#">LanguageModelTokenizer</a>确保为整个管道中的所有组件正确设置令牌。<br>pipeline:<br>     - name: “LanguageModelFeaturizer”</td></tr></tbody></table><h3 id="RegexFeaturizer"><a href="#RegexFeaturizer" class="headerlink" title="RegexFeaturizer"></a>RegexFeaturizer</h3><table><thead><tr><th>Short</th><th>使用正则表达式创建用户消息的矢量表示。</th></tr></thead><tbody><tr><td><strong>Outputs</strong></td><td>sparse_features<code>用于用户消息和</code>tokens.pattern</td></tr><tr><td><strong>Requires</strong></td><td>tokens</td></tr><tr><td><strong>Type</strong></td><td>Sparse featurizer</td></tr><tr><td><strong>Description</strong></td><td>创建用于实体提取和意图分类的功能。在训练期间，<code>RegexFeaturizer</code>会创建一个以训练数据格式定义的正则表达式列表。对于每个正则表达式，将设置一个功能，以标记是否在用户消息中找到了此表达式。稍后，所有功能都将被输入到意图分类器/实体提取器中以简化分类（假设分类器在训练阶段已获悉，此设置的功能指示特定的意图/实体）。当前仅<a href="#">CRFEntityExtractor</a>和 <a href="#">DIETClassifier</a>组件支持用于实体提取的正则表达式功能！</td></tr><tr><td><strong>Configuration</strong></td><td>pipeline: <br>    - name: “RegexFeaturizer”</td></tr></tbody></table><h3 id="CountVectorsFeaturizer"><a href="#CountVectorsFeaturizer" class="headerlink" title="CountVectorsFeaturizer"></a>CountVectorsFeaturizer</h3><table><thead><tr><th>Short</th><th>创建用户消息，意图和响应的词袋表示。</th></tr></thead><tbody><tr><td><strong>Outputs</strong></td><td><code>sparse_features</code> 用于用户消息，意图和响应</td></tr><tr><td><strong>Requires</strong></td><td>tokens</td></tr><tr><td><strong>Type</strong></td><td>Sparse featurizer</td></tr><tr><td><strong>Description</strong></td><td>创建用于意图分类和响应选择的功能。使用<a href="#">sklearn的CountVectorizer</a>创建用户消息，意图和响应 <a href="#">的关键词表示</a>。所有仅由数字组成的令牌（例如123和99，但不包括a123d）将分配给同一功能。</td></tr><tr><td><strong>Configuration</strong></td><td>有关 配置参数的详细说明，请参见<a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html">sklearn的CountVectorizer文档</a>。<br>可以使用<code>analyzer</code>配置参数将此功能配置为使用单词或字符n-gram 。默认情况下<code>analyzer</code>设置为，<code>word</code>因此单词标记计数用作功能。如果要使用字符n-gram，请将设置<code>analyzer</code>为<code>char</code>或<code>char_wb</code>。可以通过参数<code>min_ngram</code>和来配置n元语法的上下边界<code>max_ngram</code>。默认情况下，它们都设置为<code>1</code>。<br>注意<br>Option<code>char_wb</code>仅从单词边界内的文本创建字符n-gram；单词边缘的n-gram用空格填充。此选项可用于创建子<a href="https://arxiv.org/abs/1810.07150">词语义散列</a>。<br><strong>注意</strong><br>对于字符n-gram，不要忘记增加<code>min_ngram</code>和<code>max_ngram</code>参数。否则，词汇表将仅包含单个字母。<br>处理词汇量（OOV）词：<br><strong>注意</strong><br>启用仅<code>analyzer</code>是<code>word</code>。<br>由于训练是在有限的词汇数据上进行的，因此不能保证算法在预测期间不会遇到未知词（训练中未看到的词）。为了教授一种算法如何处理未知单词，可以将训练数据中的某些单词替换为通用单词<code>OOV_token</code>。在这种情况下，在预测期间，所有未知单词都将被视为该通用单词<code>OOV_token</code>。<br>例如，一个人可能会<code>outofscope</code>在训练数据中创建单独的意图，其中包含不同数量<code>OOV_token</code>s的消息以及可能包含一些其他常规词。然后，算法可能会将带有未知单词的消息分类为此意图<code>outofscope</code>。<br>您可以设置<code>OOV_token</code>或单词列表<code>OOV_words</code>：<br>     <code>OOV_token</code>为看不见的单词设置关键字；如果训练数据<code>OOV_token</code>在某些消息中包含单词，则在预测期间，将使用提供替换在训练期间未看到的单词<code>OOV_token</code>；如果<code>OOV_token=None</code>（默认行为）训练期间未看到的单词在预测时间内将被忽略；<br>     <code>OOV_words</code>设定<code>OOV_token</code>训练期间要处理的单词清单；如果已知应视为词汇量不足的单词列表，则可以将其设置为，<code>OOV_words</code>而不是在训练数据中手动更改它或使用自定义预处理器。<br><strong>注意</strong><br>这个featurizer通过对单词<strong>计数</strong>来创建单词袋表示，因此<code>OOV_token</code>句子中的数量可能很重要。<br><strong>注意</strong><br>提供<code>OOV_words</code>是可选的，培训数据可以包含<code>OOV_token</code>手动输入，也可以包含自定义的附加预处理器。<code>OOV_token</code> <strong>只有</strong>在训练数据或<code>OOV_words</code>列表中存在此标记时，看不见的单词才会被替换。<br>如果要在用户消息和意图之间共享词汇，则需要将选项设置 <code>use_shared_vocab</code>为<code>True</code>。在这种情况下，将建立意图令牌和用户消息之间的通用词汇集<br>pipeline: <br>      - name: “CountVectorsFeaturizer” <br>        “analyzer”: “word”  <br>        “min_ngram”: 1  <br>        “max_ngram”: 1   <br>        “OOV_token”: “<em>oov</em>“ <br>        “use_shared_vocab”: False<br>上面的配置参数是您应该配置的参数，以使模型适合数据。但是，存在可以调整的其他参数。</td></tr></tbody></table><img src="/2020/10/04/rasa-wen-dang-zhi-nan-12/9.png" style="zoom:90%;"><h3 id="LexicalSyntacticFeaturizer"><a href="#LexicalSyntacticFeaturizer" class="headerlink" title="LexicalSyntacticFeaturizer"></a>LexicalSyntacticFeaturizer</h3><table><thead><tr><th>Short</th><th>为用户消息创建词汇和句法功能以支持实体提取。</th></tr></thead><tbody><tr><td><strong>Outputs</strong></td><td><code>sparse_features</code> 用于用户消息</td></tr><tr><td><strong>Requires</strong></td><td>tokens</td></tr><tr><td><strong>Type</strong></td><td>Sparse featurizer</td></tr><tr><td><strong>Description</strong></td><td>创建用于实体提取的功能。使用滑动窗口在用户消息中的每个令牌上移动，并根据配置创建功能（请参见下文）。由于存在默认配置，因此您无需指定配置。</td></tr><tr><td><strong>Configuration</strong></td><td>您可以配置featurizer应该提取哪种词汇和句法功能。提供以下功能：<br><img src="/2020/10/04/rasa-wen-dang-zhi-nan-12/10.png" style="zoom:50%;"><br>当功能化器在带有滑动窗口的用户消息中移动标记时，您可以为滑动窗口中的先前标记，当前标记和下一个标记定义功能。您将要素定义为[before，token，after]数组。如果要为之前的令牌，当前令牌和之后的令牌定义功能，则功能配置应如下所示：<br>pipeline: <br>      - name: LexicalSyntacticFeaturizer   <br>        “features”: [  <br>              [“low”, “title”, “upper”],  <br>              [“BOS”, “EOS”, “low”, “upper”, “title”, “digit”],    <br>              [“low”, “title”, “upper”],  <br>            ]<br>此配置也是默认配置。<br> <strong>注意</strong><br>如果要使用<code>pos</code>或<code>pos2</code>需要添加<code>SpacyTokenizer</code>到管道中。</td></tr></tbody></table><h2 id="Intent-Classifiers"><a href="#Intent-Classifiers" class="headerlink" title="Intent Classifiers"></a>Intent Classifiers</h2><p>意图分类器将域文件中定义的意图之一分配给传入的用户消息。</p><h3 id="MitieIntentClassifier"><a href="#MitieIntentClassifier" class="headerlink" title="MitieIntentClassifier"></a>MitieIntentClassifier</h3><table><thead><tr><th>Short</th><th>MITIE意向分类器（使用 <a href="https://github.com/mit-nlp/MITIE/blob/master/examples/python/text_categorizer_pure_model.py">文本分类器</a>）</th></tr></thead><tbody><tr><td><strong>Outputs</strong></td><td>intent</td></tr><tr><td><strong>Requires</strong></td><td><code>tokens</code>用于用户消息和<a href="https://legacy-docs-v1.rasa.com/nlu/components/#mitienlp">MitieNLP</a></td></tr><tr><td><strong>Output-Example</strong></td><td>{     <br>“intent”: {“name”: “greet”, “confidence”: 0.98343} <br>}</td></tr><tr><td><strong>Description</strong></td><td>该分类器使用MITIE进行意图分类。底层分类器正在使用具有稀疏线性内核的多类线性SVM（请参阅 <a href="https://github.com/mit-nlp/MITIE/blob/master/mitielib/src/text_categorizer_trainer.cpp#L222">MITIE训练代码</a>）。<br><strong>注意</strong><br>该分类器不依赖任何特征化器，因为它自己提取特征。</td></tr><tr><td><strong>Configuration</strong></td><td>pipeline:<br>    - name: “MitieIntentClassifier”</td></tr></tbody></table><h3 id="SklearnIntentClassifier"><a href="#SklearnIntentClassifier" class="headerlink" title="SklearnIntentClassifier"></a>SklearnIntentClassifier</h3><table><thead><tr><th>Short</th><th>Sklearn意图分类器</th></tr></thead><tbody><tr><td><strong>Outputs</strong></td><td>intent<code>和</code>intent_ranking</td></tr><tr><td><strong>Requires</strong></td><td><code>dense_features</code> 用于用户消息</td></tr><tr><td><strong>Output-Example</strong></td><td>{    <br> “intent”: {“name”: “greet”, “confidence”: 0.78343},  <br>   “intent_ranking”: [   {    <br>                          “confidence”: 0.1485910906220309,         <br>                         “name”: “goodbye”         },         <br>                      { “confidence”: 0.08161531595656784,   <br>                         “name”: “restaurant_search”         }     ]<br> }</td></tr><tr><td><strong>Description</strong></td><td>sklearn目的分类器训练了一个线性SVM，该SVM使用网格搜索进行了优化。它还提供了未“获胜”的标签的排名。将<code>SklearnIntentClassifier</code>通过在管道密集featurizer前面的需求。这种密集的特征化器创建了用于分类的功能。有关算法本身的更多信息，请参阅 <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html">GridSearchCV</a> 文档。</td></tr><tr><td><strong>Configuration</strong></td><td>在SVM训练期间，将运行超参数搜索以找到最佳参数集。在配置中，您可以指定将尝试的参数。<br>pipeline: <br>      - name: “SklearnIntentClassifier”   <br>        C: [1, 2, 5, 10, 20, 100]   <br>        kernels: [“linear”]  <br>        “gamma”: [0.1]   <br>        “max_cross_validation_folds”: 5   <br>        “scoring_function”: “f1_weighted”</td></tr></tbody></table><h3 id="EmbeddingIntentClassifier"><a href="#EmbeddingIntentClassifier" class="headerlink" title="EmbeddingIntentClassifier"></a>EmbeddingIntentClassifier</h3><blockquote><p>警告</p><p><code>EmbeddingIntentClassifier</code>已弃用，应以代替<code>DIETClassifier</code>。有关更多详细信息，请参见 <a href="#">迁移指南</a>。</p></blockquote><table><thead><tr><th>Short</th><th>嵌入意图分类器以进行意图分类</th></tr></thead><tbody><tr><td><strong>Outputs</strong></td><td>intent<code>和</code>intent_ranking</td></tr><tr><td><strong>Requires</strong></td><td><code>dense_features</code>和/或<code>sparse_features</code>用于用户消息，以及可选的意图</td></tr><tr><td><strong>Output-Example</strong></td><td>{     “intent”: {“name”: “greet”, “confidence”: 0.78343},     “intent_ranking”: [         {             “confidence”: 0.1485910906220309,             “name”: “goodbye”         },         {             “confidence”: 0.08161531595656784,             “name”: “restaurant_search”         }     ] }</td></tr><tr><td><strong>Description</strong></td><td>该<code>EmbeddingIntentClassifier</code>嵌入用户输入和意图的标签在同一个空间。通过最大化嵌入之间的相似性来训练有监督的嵌入。该算法基于<a href="https://arxiv.org/abs/1709.03856">StarSpace</a>。但是，在此实现中，损失函数略有不同，并且其他附加的隐藏层与丢失一起添加。该算法还提供了未“获胜”的标签的相似性排名。<br><strong>注意</strong><br>如果在预测时间内，一条消息<strong>仅</strong>包含训练中看不见的单词，并且未使用词汇量外预处理器，则可以有把握地<code>None</code>预测 出一个空的意图<code>0.0</code>。如果仅将<a href="https://legacy-docs-v1.rasa.com/nlu/components/#countvectorsfeaturizer">CountVectorsFeaturizer</a>与<code>word</code>分析仪一起用作特征化器，则可能会发生这种情况。如果使用<code>char_wb</code>分析仪，则应始终获得具有置信度值的意图。<code>&gt; 0.0</code></td></tr><tr><td><strong>Configuration</strong></td><td>您可以定义许多超参数来适应模型。如果要调整模型，请首先修改以下参数：<br> <code>epochs</code> ：此参数设置算法查看训练数据的次数（默认值：）<code>300</code>。一个<code>epoch</code>是等于一个直传和所有的训练例子之一向通行。有时，模型需要更多的时期才能正确学习。有时更多的时期不会影响性能。时期数越少，模型训练越快。<br><code>hidden_layers_sizes</code> ：此参数允许您为用户消息和意图定义前馈层的数量及其输出尺寸（默认值：）。列表中的每个条目都对应一个前馈层。例如，如果设置，我们将在变压器前面添加两个前馈层。输入令牌的矢量（来自用户消息）将传递到这些层。第一层的输出尺寸为256，第二层的输出尺寸为128。如果使用空列表（默认行为），则不会添加前馈层。确保仅使用正整数。通常，使用2的幂数。同样，通常的做法是在列表中减小值：下一个值小于或等于前一个值。 text: [256, 128], label: []  text: [256, 128]<br> <code>embedding_dimension </code>：此参数定义模型内部使用的嵌入层的输出尺寸（默认值：）<code>20</code>。我们在模型架构内部使用了多个嵌入层。例如，在比较<code>__CLS__</code>令牌和计算损失之前，将令牌的矢量和意图传递给嵌入层。<br><code>weight_sparsity</code>：此参数定义了模型中所有前馈层的内核权重的分数，将其设置为0（默认值：）<code>0.0</code>。该值应在0到1之间。如果设置<code>weight_sparsity</code> 为0，则没有内核权重将设置为0，该层将充当标准前馈层。您不应将其设置<br><code>weight_sparsity</code>：此参数定义了模型中所有前馈层的内核权重的分数，将其设置为0（默认值：）<code>0.0</code>。该值应在0到1之间。如果设置<code>weight_sparsity</code> 为0，则没有内核权重将设置为0，该层将充当标准前馈层。您不应将其设置<br><code>weight_sparsity</code>为1，因为这将导致所有内核权重均为0，即模型无法学习。<br>上面的配置参数是您应该配置的参数，以使模型适合数据。但是，存在可以调整的其他参数<br>*<em>注意</em><br>出于<code>cosine</code>相似性<code>maximum_positive_similarity</code>，<code>maximum_negative_similarity</code>应在<code>-1</code>和之间<code>1</code><br><strong>注意</strong><br>有一个选项可以使用线性增加的批量大小。这个想法来自 <a href="https://arxiv.org/abs/1711.00489%E3%80%82%E4%B8%BA%E6%AD%A4%EF%BC%8C%E5%B0%86%E4%B8%80%E4%B8%AA%E5%88%97%E8%A1%A8%E4%BC%A0%E9%80%92%E7%BB%99">https://arxiv.org/abs/1711.00489。为此，将一个列表传递给</a> batch_size，例如（默认行为）。如果需要常量，请传递，例如。”batch_size”: [64, 256] batch_size int  “batch_size”: 64 <br><strong>注意</strong><br>参数<code>maximum_negative_similarity</code>被设定为负值的情况下模拟原始starspace算法 和。有关详细信息，请参见<a href="https://arxiv.org/abs/1709.03856">星空纸</a>。<code>maximum_negative_similarity = maximum_positive_similarity``use_maximum_negative_similarity = False</code></td></tr></tbody></table><h3 id="KeywordIntentClassifier"><a href="#KeywordIntentClassifier" class="headerlink" title="KeywordIntentClassifier"></a>KeywordIntentClassifier</h3><table><thead><tr><th>Short</th><th>简单的关键字匹配意图分类器，适用于小型短期项目。</th></tr></thead><tbody><tr><td><strong>Outputs</strong></td><td>intent</td></tr><tr><td><strong>Requires</strong></td><td>没有</td></tr><tr><td><strong>Output-Example</strong></td><td>{     “intent”: {“name”: “greet”, “confidence”: 1.0} }</td></tr><tr><td><strong>Description</strong></td><td>该分类器通过在消息中搜索关键字来工作。默认情况下，匹配项区分大小写，并且仅搜索用户消息中关键字字符串的完全匹配项。意图的关键字是NLU训练数据中该意图的示例。这意味着整个示例都是关键字，而不是示例中的单个单词。<br><strong>注意</strong><br>此分类器仅适用于小型项目或入门。如果您几乎没有NLU培训数据，则可以在<a href="https://legacy-docs-v1.rasa.com/nlu/choosing-a-pipeline/#choosing-a-pipeline">选择</a>管道中查看推荐 <a href="#">的管道</a>。<br></td></tr><tr><td><strong>Configuration</strong></td><td>pipeline: <br>      - name: “KeywordIntentClassifier”  <br>        case_sensitive: True</td></tr></tbody></table><h3 id="DIETClassifier"><a href="#DIETClassifier" class="headerlink" title="DIETClassifier"></a>DIETClassifier</h3><table><thead><tr><th>Short</th><th>双意图实体变压器（DIET）用于意图分类和实体提取</th></tr></thead><tbody><tr><td><strong>Description</strong></td><td>您可以在“组合实体提取器和意图分类器”部分下 找到<a href="#">DIETClassifier</a>的详细描述。</td></tr></tbody></table><h2 id="Entity-Extractors"><a href="#Entity-Extractors" class="headerlink" title="Entity Extractors"></a>Entity Extractors</h2><p>实体提取器从用户消息中提取实体，例如人名或位置。</p><h3 id="MitieEntityExtractor"><a href="#MitieEntityExtractor" class="headerlink" title="MitieEntityExtractor"></a>MitieEntityExtractor</h3><table><thead><tr><th>Short</th><th>MITIE实体提取（使用<a href="https://github.com/mit-nlp/MITIE/blob/master/mitielib/src/ner_trainer.cpp">MITIE NER训练</a>）</th></tr></thead><tbody><tr><td><strong>Outputs</strong></td><td>entities</td></tr><tr><td><strong>Requires</strong></td><td><a href="#">MitieNLP</a>和<code>tokens</code></td></tr><tr><td><strong>Output-Example</strong></td><td>{     “entities”: [{         “value”: “New York City”,         “start”: 20,         “end”: 33,         “confidence”: null,         “entity”: “city”,         “extractor”: “MitieEntityExtractor”     }] }</td></tr><tr><td><strong>Description</strong></td><td><code>MitieEntityExtractor</code>使用MITIE实体提取来查找消息中的实体。底层分类器使用具有稀疏线性内核和自定义功能的多类线性SVM。MITIE组件不提供实体置信度值。<br><strong>注意</strong><br>该实体提取器不依赖任何特征化器，因为它自己提取特征。<br></td></tr><tr><td><strong>Configuration</strong></td><td>pipeline: <br>    - name: “MitieEntityExtractor”</td></tr></tbody></table><h3 id="SpacyEntityExtractor"><a href="#SpacyEntityExtractor" class="headerlink" title="SpacyEntityExtractor"></a>SpacyEntityExtractor</h3><table><thead><tr><th>Short</th><th>Spacy实体提取</th></tr></thead><tbody><tr><td><strong>Outputs</strong></td><td>entities</td></tr><tr><td><strong>Requires</strong></td><td><a href="#">SpacyNLP</a></td></tr><tr><td><strong>Output-Example</strong></td><td>{     “entities”: [{         “value”: “New York City”,         “start”: 20,         “end”: 33,         “confidence”: null,         “entity”: “city”,         “extractor”: “SpacyEntityExtractor”     }] }</td></tr><tr><td><strong>Description</strong></td><td>使用spaCy，此组件可以预测消息的实体。spaCy使用统计的BILOU转换模型。到目前为止，此组件只能使用spaCy内置的实体提取模型，并且不能进行重新训练。该提取器不提供任何置信度分数。</td></tr><tr><td><strong>Configuration</strong></td><td>配置spaCy组件应提取的维度（即实体类型）。可用尺寸的完整列表可在<a href="#">spaCy文档中</a>找到。不指定尺寸选项将提取所有可用尺寸。<br>pipeline: <br>      - name: “SpacyEntityExtractor”   <br>        dimensions: [“PERSON”, “LOC”, “ORG”, “PRODUCT”]</td></tr></tbody></table><h3 id="EntitySynonymMapper"><a href="#EntitySynonymMapper" class="headerlink" title="EntitySynonymMapper"></a>EntitySynonymMapper</h3><table><thead><tr><th>Short</th><th>将同义实体值映射到相同的值。</th></tr></thead><tbody><tr><td><strong>Outputs</strong></td><td>修改先前的实体提取组件找到的现有实体。</td></tr><tr><td><strong>Requires</strong></td><td>没有</td></tr><tr><td><strong>Output-Example</strong></td><td>{     “entities”: [{         “value”: “New York City”,         “start”: 20,         “end”: 33,         “confidence”: null,         “entity”: “city”,         “extractor”: “SpacyEntityExtractor”     }] }</td></tr><tr><td><strong>Description</strong></td><td>如果训练数据包含已定义的同义词，则此组件将确保将检测到的实体值映射到相同的值。例如，如果您的训练数据包含以下示例：<br>[     {       “text”: “I moved to New York City”,       “intent”: “inform_relocation”,       “entities”: [{         “value”: “nyc”,         “start”: 11,         “end”: 24,         “entity”: “city”,       }]     },     {       “text”: “I got a new flat in NYC.”,       “intent”: “inform_relocation”,       “entities”: [{         “value”: “nyc”,         “start”: 20,         “end”: 23,         “entity”: “city”,       }]     } ]<br>此组件将允许你在实体映射和到。即使消息包含，实体提取也将返回。当此组件更改现有实体时，会将其自身附加到该实体的处理器列表中。<code>New York City``NYC``nyc``nyc``NYC</code></td></tr><tr><td><strong>Configuration</strong></td><td>pipeline: <br>    - name: “EntitySynonymMapper”<br><strong>注意</strong><br>当<code>EntitySynonymMapper</code>将N用作NLU管道的一部分时，需要将其放置在配置文件中任何实体提取器的下方。</td></tr></tbody></table><h3 id="CRFEntityExtractor"><a href="#CRFEntityExtractor" class="headerlink" title="CRFEntityExtractor"></a>CRFEntityExtractor</h3><table><thead><tr><th>Short</th><th>条件随机场（CRF）实体提取</th></tr></thead><tbody><tr><td><strong>Outputs</strong></td><td>修改先前的实体提取组件找到的现有实体。</td></tr><tr><td><strong>Requires</strong></td><td><code>tokens</code>和<code>dense_features</code>（可选）</td></tr><tr><td><strong>Output-Example</strong></td><td>“entities”: [{         “value”: “New York City”,         “start”: 20,         “end”: 33,         “entity”: “city”,         “confidence”: 0.874,         “extractor”: “CRFEntityExtractor”     }] }</td></tr><tr><td><strong>Description</strong></td><td>该组件实现条件随机字段（CRF）以进行命名实体识别。可以将CRF视为无向马尔可夫链，其中时间步长是单词，状态是实体类。单词的特征（大写，POS标记等）赋予某些实体类几率，相邻实体标签之间的转换也是如此：然后计算并返回最可能的一组标签。</td></tr><tr><td><strong>Configuration</strong></td><td><code> CRFEntityExtractor</code>列出了要使用的默认功能。但是，您可以覆盖默认配置。提供以下功能：<br>当功能化器在带有滑动窗口的用户消息中移动标记时，您可以为滑动窗口中的先前标记，当前标记和下一个标记定义功能。您可以将特征定义为[before，token，after]数组。<br>BILOU_flag<code>确定是否使用BILOU标记。默认</code>True<br>pipeline: <br>      - name: “CRFEntityExtractor”  <br>        “BILOU_flag”: True   <br>        “features”: [     [“low”, “title”, “upper”],   <br>                         [  <br>                             “bias”,    <br>                             “low”,       <br>                             “prefix5”,      <br>                             “prefix2”,      <br>                             “suffix5”,    <br>                              “suffix3”,   <br>                              “suffix2”,     <br>                              “upper”,     <br>                             “title”,    <br>                             “digit”,    <br>                              “pattern”,     ],   <br>                      [<br>                             “low”, <br>                            “title”, <br>                           “upper”],    ]  <br>             “max_iterations”: 50   <br>              “L1_c”: 0.1  <br>             “L2_c”: 0.1<br><strong>注意</strong><br>如果使用POS功能（<code>pos</code>或<code>pos2</code>），则需要<code>SpacyTokenizer</code>在管道中使用。<br><strong>注意</strong><br>如果使用<code>pattern</code>功能，则需要包含<code>RegexFeaturizer</code>在管道中。</td></tr></tbody></table><h3 id="DucklingHTTPExtractor"><a href="#DucklingHTTPExtractor" class="headerlink" title="DucklingHTTPExtractor"></a>DucklingHTTPExtractor</h3><table><thead><tr><th>Short</th><th>使用Duckling，您可以用多种语言提取常见的实体，例如日期，金额，距离和其他实体。</th></tr></thead><tbody><tr><td><strong>Outputs</strong></td><td>entities</td></tr><tr><td><strong>Requires</strong></td><td>没有</td></tr><tr><td><strong>Output-Example</strong></td><td>{     “entities”: [{         “end”: 53,         “entity”: “time”,         “start”: 48,         “value”: “2017-04-10T00:00:00.000+02:00”,         “confidence”: 1.0,         “extractor”: “DucklingHTTPExtractor”     }] }</td></tr><tr><td><strong>Description</strong></td><td>要使用此组件，您需要运行一个小鸭服务器。最简单的选择是使用来启动docker容器 。<code>docker run -p 8000:8000 rasa/duckling</code><br>另外，您也可以<a href="https://github.com/facebook/duckling#quickstart">直接在计算机上安装Duckling，</a>然后启动服务器。<br>小鸭可以识别日期，数字，距离和其他结构化实体并将其标准化。请注意，小鸭会尝试在不提供排名的情况下提取尽可能多的实体类型。例如，如果同时指定<code>number</code>和<code>time</code>作为小鸭组件的尺寸，该组件将提取两个实体：<code>10</code>作为数字和 作为时间从text中。在这种情况下，您的应用程序必须确定哪种实体类型是正确的。提取程序将始终返回1.0作为置信度，因为它是基于规则的系统。<code>in 10 minutes``I will be there in 10 minutes</code></td></tr><tr><td><strong>Configuration</strong></td><td>配置小鸭组件应提取的尺寸，即实体类型。<a href="https://duckling.wit.ai/">小鸭文档</a>中提供了可用尺寸的完整列表。不指定尺寸选项将提取所有可用尺寸。<br></td></tr></tbody></table><h3 id="DIETClassifier-1"><a href="#DIETClassifier-1" class="headerlink" title="DIETClassifier"></a>DIETClassifier</h3><table><thead><tr><th>Short</th><th>双意图实体分类器（DIET）用于意图分类和实体提取</th></tr></thead><tbody><tr><td><strong>Description</strong></td><td>您可以在“组合实体提取器和意图分类器”部分下 找到<a href="https://legacy-docs-v1.rasa.com/nlu/components/#diet-classifier">DIETClassifier</a>的详细描述。</td></tr></tbody></table><h2 id="Selectors"><a href="#Selectors" class="headerlink" title="Selectors"></a>Selectors</h2><h3 id="ResponseSelector"><a href="#ResponseSelector" class="headerlink" title="ResponseSelector"></a>ResponseSelector</h3><table><thead><tr><th>Short</th><th>响应选择器</th></tr></thead><tbody><tr><td><strong>Outputs</strong></td><td>关键字为<code>direct_response_intent</code>和且包含<code>response</code>和的字典<code>ranking</code></td></tr><tr><td><strong>Requires</strong></td><td><code>dense_features</code>和/或<code>sparse_features</code>用于用户消息和响应</td></tr><tr><td><strong>Output-Example</strong></td><td>{     “response_selector”: {       “faq”: {         “response”: {“confidence”: 0.7356462617, “name”: “Supports 3.5, 3.6 and 3.7, recommended version is 3.6”},         “ranking”: [             {“confidence”: 0.7356462617, “name”: “Supports 3.5, 3.6 and 3.7, recommended version is 3.6”},             {“confidence”: 0.2134543431, “name”: “You can ask me about how to get started”}         ]       }     } }</td></tr><tr><td><strong>Description</strong></td><td>响应选择器组件可用于构建响应检索模型，以根据一组候选响应直接预测机器人响应。该模型的预测由<a href="https://legacy-docs-v1.rasa.com/core/retrieval-actions/#retrieval-actions">检索动作使用</a>。它将用户输入和响应标签嵌入到同一空间，并遵循与<a href="https://legacy-docs-v1.rasa.com/nlu/components/#diet-classifier">DIETClassifier</a>完全相同的神经网络架构和优化。<br><strong>注意</strong><br>如果在预测时间内，一条消息<strong>仅</strong>包含训练中看不见的单词，并且未使用词汇量外预处理器，则可以有把握地<code>None</code>预测 一个空响应<code>0.0</code>。如果仅将<a href="https://legacy-docs-v1.rasa.com/nlu/components/#countvectorsfeaturizer">CountVectorsFeaturizer</a>与<code>word</code>分析仪一起用作特征化器，则可能会发生这种情况。如果使用<code>char_wb</code>分析仪，则应始终获得带有置信度值的响应。<code>&gt; 0.0</code></td></tr><tr><td><strong>Configuration</strong></td><td>该算法几乎包括<a href="https://legacy-docs-v1.rasa.com/nlu/components/#diet-classifier">DIETClassifier</a>使用的所有超参数。如果要调整模型，请首先修改以下参数：<br><code>epochs</code>：此参数设置算法查看训练数据的次数（默认值：）<code>300</code>。一个<code>epoch</code>是等于一个直传和所有的训练例子之一向通行。有时，模型需要更多的时期才能正确学习。有时更多的时期不会影响性能。时期数越少，模型训练越快。<br><code>hidden_layers_sizes</code>：此参数允许您为用户消息和意图定义前馈层的数量及其输出尺寸（默认值：）。列表中的每个条目都对应一个前馈层。例如，如果设置，我们将在变压器前面添加两个前馈层。输入令牌的矢量（来自用户消息）将传递到这些层。第一层的输出尺寸为256，第二层的输出尺寸为128。如果使用空列表（默认行为），则不会添加前馈层。确保仅使用正整数。通常，使用2的幂数。同样，通常的做法是在列表中减小值：下一个值小于或等于前一个值。text: [256, 128], label: [256, 128]　text: [256, 128]<br><code>embedding_dimension</code>：此参数定义模型内部使用的嵌入层的输出尺寸（默认值：）<code>20</code>。我们在模型架构内部使用了多个嵌入层。例如，在比较<code>__CLS__</code>令牌和计算损失之前，将令牌的矢量和意图传递给嵌入层。<br><code>number_of_transformer_layers</code>：此参数设置要使用的变压器层数（默认值：）<code>0</code>。变压器层的数量与要用于模型的变压器块相对应<br><code>transformer_size</code>：此参数设置变压器中的单位数（默认值：）<code>None</code>。从变压器出来的矢量将具有给定值<code>transformer_size</code>。<br><code>weight_sparsity</code>：此参数定义了模型中所有前馈层的内核权重的分数，将其设置为0（默认值：）<code>0.8</code>。该值应在0到1之间。如果设置<code>weight_sparsity</code> 为0，则没有内核权重将设置为0，该层将充当标准前馈层。您不应将其设置<code>weight_sparsity</code>为1，因为这将导致所有内核权重均为0，即模型无法学习。<br>此外，该组件还可以配置为训练针对特定检索意图的响应选择器。参数<code>retrieval_intent</code>设置为此响应选择器模型训练的意图的名称。默认值为<code>None</code>，即模型针对所有检索意图进行了训练。</td></tr></tbody></table><h2 id="Combined-Entity-Extractors-and-Intent-Classifiers"><a href="#Combined-Entity-Extractors-and-Intent-Classifiers" class="headerlink" title="Combined Entity Extractors and Intent Classifiers"></a>Combined Entity Extractors and Intent Classifiers</h2><h3 id="DIETClassifier-2"><a href="#DIETClassifier-2" class="headerlink" title="DIETClassifier"></a>DIETClassifier</h3><table><thead><tr><th>Short</th><th>双意图实体分类器（DIET）用于意图分类和实体提取</th></tr></thead><tbody><tr><td><strong>Outputs</strong></td><td>entities<code>，</code>intent<code>和</code>intent_ranking</td></tr><tr><td><strong>Requires</strong></td><td><code>dense_features</code>和/或<code>sparse_features</code>用于用户消息以及可选的意图</td></tr><tr><td><strong>Output-Example</strong></td><td>{     “intent”: {“name”: “greet”, “confidence”: 0.8343},     “intent_ranking”: [         {             “confidence”: 0.385910906220309,             “name”: “goodbye”         },         {             “confidence”: 0.28161531595656784,             “name”: “restaurant_search”         }     ],     “entities”: [{         “end”: 53,         “entity”: “time”,         “start”: 48,         “value”: “2017-04-10T00:00:00.000+02:00”,         “confidence”: 1.0,         “extractor”: “DIETClassifier”     }] }</td></tr><tr><td><strong>Description</strong></td><td>DIET（双重意图和实体转换器）是用于意图分类和实体识别的多任务体系结构。该架构基于两个任务共享的变压器。实体标签的序列是通过在与令牌的输入序列相对应的转换器输出序列之上的条件随机字段（CRF）标记层进行预测的。对于意图标签，<code>__CLS__</code>令牌和意图标签的转换器输出被嵌入到单个语义向量空间中。我们使用点积损失来最大化与目标标签的相似性，并最小化与阴性样品的相似性。<br>如果您想了解有关模型的更多信息，请观看我们的 <a href="https://www.youtube.com/playlist?list=PL75e0qA87dlG-za8eLI6t0_Pbxafk-cxb">视频</a>，其中我们详细介绍了模型架构。<br><strong>注意</strong><br>如果在预测时间内，一条消息<strong>仅</strong>包含训练中看不见的单词，并且未使用词汇量外预处理器，则可以有把握地<code>None</code>预测 出一个空的意图<code>0.0</code>。如果仅将<a href="https://legacy-docs-v1.rasa.com/nlu/components/#countvectorsfeaturizer">CountVectorsFeaturizer</a>与<code>word</code>分析仪一起用作特征化器，则可能会发生这种情况。如果使用<code>char_wb</code>分析仪，则应始终获得具有置信度值的意图。<code>&gt; 0.0</code></td></tr><tr><td><strong>Configuration</strong></td><td>如果要使用“<code>DIETClassifier</code>仅用于意图分类”，请设置<code>entity_recognition</code>为<code>False</code>。如果只想进行实体识别，请设置<code>intent_classification</code>为<code>False</code>。默认情况下<code>DIETClassifier</code>，两者都设置，即<code>entity_recognition</code>和<code>intent_classification</code>都设置为 <code>True</code>。<br>您可以定义许多超参数来适应模型。如果要调整模型，请首先修改以下参数：<br><code>epochs</code>：此参数设置算法查看训练数据的次数（默认值：）<code>300</code>。一个<code>epoch</code>是等于一个直传和所有的训练例子之一向通行。有时，模型需要更多的时期才能正确学习。有时更多的时期不会影响性能。时期数越少，模型训练越快。<br><code>hidden_layers_sizes</code>：此参数允许您为用户消息和意图定义前馈层的数量及其输出尺寸（默认值：）。列表中的每个条目都对应一个前馈层。例如，如果设置，我们将在变压器前面添加两个前馈层。输入令牌的矢量（来自用户消息）将传递到这些层。第一层的输出尺寸为256，第二层的输出尺寸为128。如果使用空列表（默认行为），则不会添加前馈层。确保仅使用正整数。通常，使用2的幂数。同样，通常的做法是在列表中减小值：下一个值小于或等于前一个值。text: [], label: []　text: [256, 128]<br><code>embedding_dimension</code>：此参数定义模型内部使用的嵌入层的输出尺寸（默认值：）<code>20</code>。我们在模型架构内部使用了多个嵌入层。例如，在比较<code>__CLS__</code>令牌和计算损失之前，将令牌的矢量和意图传递给嵌入层。<br><code>number_of_transformer_layers</code>：此参数设置要使用的变压器层数（默认值：）<code>2</code>。变压器层的数量与要用于模型的变压器块相对应。<br><code>transformer_size</code>：此参数设置变压器中的单位数（默认值：）<code>256</code>。从变压器出来的矢量将具有给定值<code>transformer_size</code>。<br><code>weight_sparsity</code>：此参数定义了模型中所有前馈层的内核权重的分数，将其设置为0（默认值：）<code>0.8</code>。该值应在0到1之间。如果设置<code>weight_sparsity</code> 为0，则没有内核权重将设置为0，该层将充当标准前馈层。您不应将其设置．<code>weight_sparsity</code>为1，因为这将导致所有内核权重均为0，即模型无法学习。<br>上面的配置参数是您应该配置的参数，以使模型适合数据。但是，存在可以调整的其他参数</td></tr></tbody></table><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> nlp </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Rasa </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Rasa文档指南(11)</title>
      <link href="/2020/10/04/rasa-wen-dang-zhi-nan-11/"/>
      <url>/2020/10/04/rasa-wen-dang-zhi-nan-11/</url>
      
        <content type="html"><![CDATA[<ul><li><a href="#">关于</a></li><li><a href="#">仅使用NLU</a></li><li><a href="#">训练数据格式</a></li><li><a href="#">语言支援</a></li><li><a href="#">选择管道</a></li><li><a href="#">组件</a></li><li><a href="#">实体提取</a></li></ul><h1 id="NLU"><a href="#NLU" class="headerlink" title="NLU"></a>NLU</h1><h2 id="关于"><a href="#关于" class="headerlink" title="关于"></a>关于</h2><h3 id="Rasa-NLU：聊天机器人和AI助手的语言理解"><a href="#Rasa-NLU：聊天机器人和AI助手的语言理解" class="headerlink" title="Rasa NLU：聊天机器人和AI助手的语言理解"></a>Rasa NLU：聊天机器人和AI助手的语言理解</h3><p>Rasa NLU是一种开源自然语言处理工具，用于聊天机器人中的意图分类，响应检索和实体提取。例如，像这样的句子</p><pre><code>"I am looking for a Mexican restaurant in the center of town"</code></pre><p>并返回结构化数据，例如</p><pre><code>&amp;#123;  "intent": "search_restaurant",  "entities": &amp;#123;    "cuisine" : "Mexican",    "location" : "center"  &amp;#125;&amp;#125;</code></pre><p>如果要单独使用Rasa NLU，请参阅<a href="#">仅使用NLU</a>。</p><h2 id="仅使用NLU"><a href="#仅使用NLU" class="headerlink" title="仅使用NLU"></a>仅使用NLU</h2><p>如果您只想将Rasa用作NLU组件，则可以！</p><h3 id="训练仅NLU模型"><a href="#训练仅NLU模型" class="headerlink" title="训练仅NLU模型"></a>训练仅NLU模型</h3><p>要仅训练NLU模型，请运行：</p><pre><code>rasa train nlu</code></pre><p>这将在<code>data/</code>目录中查找NLU训练数据文件，并将训练后的模型保存在<code>models/</code>目录中。模型的名称将以开头<code>nlu-</code>。</p><h3 id="测试在命令行上您的NLU模型"><a href="#测试在命令行上您的NLU模型" class="headerlink" title="测试在命令行上您的NLU模型"></a>测试在命令行上您的NLU模型</h3><p>要在命令行上试用您的NLU模型，请使用以下命令：<code>rasa shell nlu</code></p><pre><code>rasa shell nlu</code></pre><p>这将启动rasa shell，并要求您输入消息进行测试。您可以继续输入任意多的邮件。</p><p>另外，您可以省略<code>nlu</code>参数并直接传入仅nlu-model：</p><pre><code>rasa shell -m models/nlu-20190515-144445.tar.gz</code></pre><h3 id="运行NLU服务器"><a href="#运行NLU服务器" class="headerlink" title="运行NLU服务器"></a>运行NLU服务器</h3><p>要使用您的NLU模型启动服务器，请在运行时传递模型名称：</p><pre><code>rasa run --enable-api -m models/nlu-20190515-144445.tar.gz</code></pre><p>然后，您可以使用<code>/model/parse</code>端点从模型中请求预测。为此，请运行：</p><pre><code>curl localhost:5005/model/parse -d '&amp;#123;"text":"hello"&amp;#125;'</code></pre><h2 id="训练数据格式"><a href="#训练数据格式" class="headerlink" title="训练数据格式"></a>训练数据格式</h2><h3 id="数据格式"><a href="#数据格式" class="headerlink" title="数据格式"></a>数据格式</h3><p>您可以将培训数据作为Markdown或JSON，单个文件或包含多个文件的目录提供。请注意，Markdown通常更易于使用。</p><h3 id="Markdown格式"><a href="#Markdown格式" class="headerlink" title="Markdown格式"></a>Markdown格式</h3><p>Markdown是人类最容易阅读和书写的Rasa NLU格式。示例使用无序列表语法列出，例如minus <code>-</code>，星号<code>*</code>或plus <code>+</code>。示例按意向进行分组，并且实体通过例如或使用以下语法注释为Markdown链接。使用后一种语法，您还可以为实体分配同义词，角色或组，例如 。关键字，和在此符号中是可选的。要了解标签及其用途，请参阅“<a href="#">实体角色和组”部分</a>。<code>[&lt;entity text&gt;](&lt;entity name&gt;)``[&lt;entity-text&gt;]{"entity": "&lt;entity name&gt;"}``[&lt;entity-text&gt;]{"entity": "&lt;entity name&gt;", "role": "&lt;role name&gt;", "group": "&lt;group name&gt;", "value": "&lt;entity synonym&gt;"}``role``group``value``role``group</code></p><pre class=" language-markdown"><code class="language-markdown"><span class="token title important"><span class="token punctuation">##</span> intent:check_balance</span><span class="token list punctuation">-</span> what is my balance <span class="token comment" spellcheck="true">&lt;!-- no entity --></span><span class="token list punctuation">-</span> how much do I have on my <span class="token url">[savings](source_account)</span> <span class="token comment" spellcheck="true">&lt;!-- entity "source_account" has value "savings" --></span><span class="token list punctuation">-</span> how much do I have on my [savings account]<span class="token entity" title="&#123;">&amp;#123;</span>"entity": "source_account", "value": "savings"<span class="token entity" title="&#125;">&amp;#125;</span> <span class="token comment" spellcheck="true">&lt;!-- synonyms, method 1--></span><span class="token list punctuation">-</span> Could I pay in <span class="token url">[yen](currency)</span>?  <span class="token comment" spellcheck="true">&lt;!-- entity matched by lookup table --></span><span class="token title important"><span class="token punctuation">##</span> intent:greet</span><span class="token list punctuation">-</span> hey<span class="token list punctuation">-</span> hello<span class="token title important"><span class="token punctuation">##</span> synonym:savings   </span><span class="token comment" spellcheck="true">&lt;!-- synonyms, method 2 --></span><span class="token list punctuation">-</span> pink pig<span class="token title important"><span class="token punctuation">##</span> regex:zipcode</span><span class="token list punctuation">-</span> [0-9]<span class="token entity" title="&#123;">&amp;#123;</span>5<span class="token entity" title="&#125;">&amp;#125;</span><span class="token title important"><span class="token punctuation">##</span> lookup:additional_currencies  </span><span class="token comment" spellcheck="true">&lt;!-- specify lookup tables in an external file --></span>path/to/currencies.txt</code></pre><p>Rasa NLU的培训数据分为以下几个部分：</p><ul><li>常见的例子</li><li>同义词</li><li>正则表达式功能和</li><li>查找表</li></ul><p>尽管通用示例是唯一必需的部分，但包括其他示例将帮助NLU模型以更少的示例来学习领域，并帮助其对自己的预测更有信心。</p><p>同义词会将提取的实体映射到相同的名称，例如，将“我的储蓄帐户”映射为简单的“储蓄”。但是，这仅<em>在</em>提取实体<em>之后</em>才发生，因此您需要提供带有同义词的示例，以便Rasa可以学习将其提取。</p><p>查找表可以指定为包含换行符分隔的单词或短语的纯文本文件。加载训练数据后，这些文件将用于生成不区分大小写的正则表达式模式，该模式会添加到正则表达式功能中。</p><blockquote><p>注意</p><p>这里的共同主题是，共同的示例，正则表达式功能和查找表仅通过在训练过程中为机器学习算法提供其他功能，来充当最终NLU模型的线索。因此，不能认为只有一个示例足以使模型在该示例的所有变体中可靠地标识意图和/或实体。</p></blockquote><blockquote><p>注意</p><p><code>/</code>符号保留为分隔符，用于将检索意图与响应文本标识符分开。确保不要以您的意图的名义使用它。</p></blockquote><blockquote><p>警告</p><p>不推荐使用用于指定同义词的同义词格式。请使用新格式。<code>[savings account](source_account:savings)``[savings account]{"entity": "source_account", "value": "savings"}</code></p><p>要更新您的训练数据文件，请在您选择的终端上执行以下命令： 执行上述命令后，您的NLU训练数据文件将包含新的训练数据格式。根据您的操作系统，您可能需要更新sed命令的语法。<code>sed -i -E 's/\[([^)]+)\]\(([^)]+):([^)]+)\)/[\1]{"entity": "\2", "value": "\3"}/g' &lt;nlu training data file&gt;</code></p></blockquote><h3 id="JSON格式"><a href="#JSON格式" class="headerlink" title="JSON格式"></a>JSON格式</h3><p>JSON格式由被称为顶层对象的<code>rasa_nlu_data</code>，与键 <code>common_examples</code>，<code>entity_synonyms</code>和<code>regex_features</code>。最重要的是<code>common_examples</code>。</p><pre class=" language-json"><code class="language-json">&amp;#<span class="token number">123</span><span class="token punctuation">;</span>    <span class="token property">"rasa_nlu_data"</span><span class="token operator">:</span> &amp;#<span class="token number">123</span><span class="token punctuation">;</span>        <span class="token property">"common_examples"</span><span class="token operator">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token property">"regex_features"</span> <span class="token operator">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token property">"lookup_tables"</span>  <span class="token operator">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token property">"entity_synonyms"</span><span class="token operator">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    &amp;#<span class="token number">125</span><span class="token punctuation">;</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span></code></pre><p>本<code>common_examples</code>是用来训练模型。您应该将所有训练示例放在<code>common_examples</code>数组中。正则表达式功能是一种工具，可帮助分类器检测实体或意图并提高性能。</p><h3 id="提高意图分类和实体识别"><a href="#提高意图分类和实体识别" class="headerlink" title="提高意图分类和实体识别"></a>提高意图分类和实体识别</h3><h4 id="常见的例子"><a href="#常见的例子" class="headerlink" title="常见的例子"></a>常见的例子</h4><p>常见的例子有三个组成部分：<code>text</code>，<code>intent</code>和<code>entities</code>。前两个是字符串，最后一个是数组。</p><blockquote><ul><li>该<em>文本</em>是用户消息[需要]</li><li>的<em>意图</em>是，应与文字相关的意图[可选]</li><li>该<em>实体</em>是需要被识别的文本的特定部分[可选]</li></ul></blockquote><p>实体用<code>start</code>和<code>end</code>值指定，它们一起构成了要应用于字符串的范围，例如，在下面的示例中，使用，然后是 。实体可以跨越多个单词，实际上，该字段不必与示例中的子字符串完全对应。这样，您就可以将同义词或拼写错误映射到同一个。<code>text="show me chinese restaurants"``text[8:15] == 'chinese'``value``value</code></p><pre><code>## intent:restaurant_search- show me [chinese](cuisine) restaurants</code></pre><h4 id="正则表达式功能"><a href="#正则表达式功能" class="headerlink" title="正则表达式功能"></a>正则表达式功能</h4><p>正则表达式可用于支持意图分类和实体提取。例如，如果您的实体具有确定性结构（例如邮政编码或电子邮件地址），则可以使用正则表达式来简化对该实体的检测。对于邮政编码示例，它可能如下所示：</p><pre class=" language-reStructuredText"><code class="language-reStructuredText">## regex:zipcode- [0-9]&#123;5&#125;## regex:greet- hey[^\\s]*</code></pre><p>名称并没有定义实体或意图，它只是人类可读的描述，您可以记住该正则表达式的用途，并且是相应模式特征的标题。如上例所示，您还可以使用正则表达式功能来改善意图分类性能。</p><p>尝试以使其与尽可能少的单词匹配的方式创建正则表达式。例如，使用<code>hey[^\s]*</code> 而不是<code>hey.*</code>，因为后面的可能匹配整个消息，而第一个可能只匹配一个单词。</p><p>该<code>CRFEntityExtractor</code>组件当前仅支持用于实体提取的正则表达式功能！因此，其他实体提取器（喜欢<code>MitieEntityExtractor</code>或<code>SpacyEntityExtractor</code>不使用生成的特征），并且它们的存在不会提高这些提取器的实体识别度。当前，所有意图分类器都使用可用的正则表达式功能。</p><blockquote><p>注意</p><p>正则表达式功能无法定义实体或意图！它们仅提供模式来帮助分类器识别实体和相关意图。因此，您仍然需要在训练数据中提供意图和实体示例！</p></blockquote><h4 id="查找表"><a href="#查找表" class="headerlink" title="查找表"></a>查找表</h4><p>查找表提供了一种方便的方式来提供实体示例的列表。提供的查找表文件必须采用换行符分隔的格式。例如，<code>data/test/lookup_tables/plates.txt</code>可能包含：</p><pre class=" language-markdown"><code class="language-markdown">tacosbeefmapo tofuburritolettuce wrap</code></pre><p>并可以按如下所示加载和使用：</p><pre class=" language-markdown"><code class="language-markdown"><span class="token title important"><span class="token punctuation">##</span> lookup:plates</span>data/test/lookup_tables/plates.txt<span class="token title important"><span class="token punctuation">##</span> intent:food_request</span><span class="token list punctuation">-</span> I'd like beef <span class="token url">[tacos](plates)</span> and a <span class="token url">[burrito](plates)</span><span class="token list punctuation">-</span> How about some <span class="token url">[mapo tofu](plates)</span></code></pre><p>在训练数据中提供查找表时，内容将组合成一个大写，不区分大小写的正则表达式模式，该模式在训练示例中查找完全匹配的内容。这些正则表达式可匹配多个令牌，因此 将匹配为。这些正则表达式的处理方式与直接在训练数据中指定的常规正则表达式样式相同。<code>lettuce wrap``get me a lettuce wrap ASAP``[0 0 0 1 1 0]</code></p><blockquote><p>注意</p><p>为了使查询表有效，您的训练数据中必须有一些匹配示例。否则，模型将不会学习使用查找表匹配功能。</p></blockquote><blockquote><p>警告</p><p>将数据添加到查找表时，必须小心。例如，如果表中存在误报或其他杂音，则可能会损害性能。因此，请确保您的查询表包含干净的数据。</p></blockquote><h3 id="数据标准化"><a href="#数据标准化" class="headerlink" title="数据标准化"></a>数据标准化</h3><h3 id="实体别名"><a href="#实体别名" class="headerlink" title="实体别名"></a>实体别名</h3><p>如果您将实体定义为具有相同的值，则它们将被视为同义词。这是一个例子：</p><pre><code>## intent:search- in the center of [NYC]&amp;#123;"entity": "city", "value": "New York City"&amp;#125;- in the centre of [New York City](city)</code></pre><p>如您所见，即使第一个示例中的文本为，实体在两个示例中均<code>city</code>具有值。通过将value属性定义为与实体的开始索引和结束索引之间的文本中找到的值不同，可以定义同义词。每当找到相同的文本时，该值将使用同义词代替消息中的实际文本。<code>New York City``NYC</code></p><p>要使用训练数据中定义的同义词，您需要确保管道包含 <code>EntitySynonymMapper</code>组件（请参阅<a href="#">Components</a>）。</p><p>另外，您可以添加“ entity_synonyms”数组来为一个实体值定义多个同义词。这是一个例子：</p><pre class=" language-markdown"><code class="language-markdown"><span class="token title important"><span class="token punctuation">##</span> synonym:New York City</span><span class="token list punctuation">-</span> NYC<span class="token list punctuation">-</span> nyc<span class="token list punctuation">-</span> the big apple</code></pre><blockquote><p>注意</p><p>请注意，使用上述格式添加同义词不会改善这些实体的模型分类。 <strong>在将实体替换为同义词值之前，必须对其进行正确分类。</strong></p></blockquote><h2 id="语言支持"><a href="#语言支持" class="headerlink" title="语言支持"></a>语言支持</h2><p>您可以使用Rasa以所需的任何语言构建助手！Rasa的 <code>supervised_embeddings</code>管道可以用于<strong>任何语言的</strong>训练数据。该管道使用您提供的数据从头开始创建单词嵌入。</p><p>此外，我们还支持预训练的单词嵌入，例如spaCy。有关哪种管道最适合您的用例的信息，请查看<a href="#">选择管道</a>。</p><h3 id="在任何语言培训模型"><a href="#在任何语言培训模型" class="headerlink" title="在任何语言培训模型"></a>在任何语言培训模型</h3><p>Rasa的<code>supervised_embeddings</code>管道可用于以任何语言训练模型，因为它使用您自己的训练数据来创建自定义单词嵌入。这意味着任何特定单词的向量表示形式都将取决于其与训练数据中其他单词的关系。这种定制还意味着，该管道非常适合依赖特定于域的数据的用例，例如那些需要提取特定产品名称的用例。</p><p>要使用您的首选语言训练Rasa模型，请通过<a href="#">此处</a>的说明<code>supervised_embeddings</code>将管道定义 为您<code>config.yml</code>或其他配置文件中的管道。</p><p>定义<code>supervised_embeddings</code>处理管道并 以您选择的语言生成一些<a href="#">NLU训练数据后</a>，请使用训练模型。训练完成后，您可以测试模型的语言技能。通过以下方式查看模型如何解释不同的输入消息：<code>rasa train nlu</code></p><pre class=" language-shell"><code class="language-shell">rasa shell nlu</code></pre><blockquote><p>注意</p><p>更重要的是，从头开始训练单词嵌入时，更多的训练数据将导致更好的模型！如果您发现模型无法识别输入，请尝试使用更多例句进行训练。</p></blockquote><h3 id="预训练词向量"><a href="#预训练词向量" class="headerlink" title="预训练词向量"></a>预训练词向量</h3><p>如果可以用您的语言找到它们，则经过训练的单词向量是从较少的数据入手的好方法，因为单词向量是根据大量数据（例如Wikipedia）进行训练的。</p><h4 id="spaCy"><a href="#spaCy" class="headerlink" title="spaCy"></a>spaCy</h4><p>通过<code>pretrained_embeddings_spacy</code> <a href="#">管道</a>，您可以使用spaCy的 <a href="https://spacy.io/usage/models#languages">预训练语言模型</a>或加载fastText向量，这些向量可用于<a href="https://github.com/facebookresearch/fastText/blob/master/docs/crawl-vectors.md">数百种语言</a>。如果您想将找到的自定义模型合并到spaCy中，请查看其有关<a href="https://spacy.io/docs/usage/adding-languages">添加语言</a>的页面 。如文档中所述，您需要注册您的语言模型并将其链接到语言标识符，这将允许Rasa通过传递语言标识符作为<code>language</code>选项来加载和使用您的新语言。</p><h4 id="MITIE"><a href="#MITIE" class="headerlink" title="MITIE"></a>MITIE</h4><p>您还可以使用<a href="#">MITIE</a>从语言语料库中预先训练自己的单词向量。为此：</p><ol><li>获得干净的语言语料库（Wikipedia转储有效）作为一组文本文件。</li><li>在语料库上构建并运行<a href="#">MITIE Wordrep工具</a>。这可能需要几个小时/天，具体取决于您的数据集和工作站。您需要大约128GB的RAM才能运行wordrep –是的，这很多：尝试扩展交换。</li><li>将new的路径设置<code>total_word_feature_extractor.dat</code>为<a href="#">配置中</a>的<code>model</code>参数 。</li></ol><p>有关如何训练MITIE单词向量的完整示例，请查看 <a href="#">此博客文章</a> ，<a href="#">该文章</a>是从中文维基百科转储中创建MITIE模型的。</p><h2 id="选择一个管道"><a href="#选择一个管道" class="headerlink" title="选择一个管道"></a>选择一个管道</h2><p>在Rasa Open Source中，传入消息由一系列组件处理。这些组件将<code>pipeline</code>在your中定义的所谓处理中一个接一个地执行<code>config.yml</code>。选择NLU管道可让您自定义模型并在数据集上进行微调。</p><blockquote><p>注意</p><p>使用Rasa 1.8.0，我们更新了一些组件，并弃用了所有现有管道模板。但是，<strong>任何旧术语仍将表现出与以前相同的方式</strong>！</p></blockquote><blockquote><p>警告</p><p>我们不推荐使用所有现有管道模板（例如<code>supervised_embeddings</code>）。请在配置文件中列出您要直接使用的所有组件。有关更多信息，请参阅<a href="#">如何</a>为建议的启动配置<a href="#">选择管道</a>，或参阅 <a href="#">管道模板（不</a>建议 <a href="#">使用）</a>。</p></blockquote><h3 id="如何选择一个管道"><a href="#如何选择一个管道" class="headerlink" title="如何选择一个管道"></a>如何选择一个管道</h3><h4 id="简短答案"><a href="#简短答案" class="headerlink" title="简短答案"></a>简短答案</h4><p>如果您的培训数据是英文的，那么下面的管道是一个很好的起点：</p><pre class=" language-markdown"><code class="language-markdown">language: "en"pipeline:  <span class="token list punctuation">-</span> name: ConveRTTokenizer  <span class="token list punctuation">-</span> name: ConveRTFeaturizer  <span class="token list punctuation">-</span> name: RegexFeaturizer  <span class="token list punctuation">-</span> name: LexicalSyntacticFeaturizer  <span class="token list punctuation">-</span> name: CountVectorsFeaturizer  <span class="token list punctuation">-</span> name: CountVectorsFeaturizer<span class="token code keyword">    analyzer: "char_wb"</span><span class="token code keyword">    min_ngram: 1</span><span class="token code keyword">    max_ngram: 4</span>  <span class="token list punctuation">-</span> name: DIETClassifier<span class="token code keyword">    epochs: 100</span>  <span class="token list punctuation">-</span> name: EntitySynonymMapper  <span class="token list punctuation">-</span> name: ResponseSelector<span class="token code keyword">    epochs: 100</span></code></pre><p>如果您的训练数据不是英语，请从以下管道开始：</p><pre class=" language-markdown"><code class="language-markdown">language: "fr"  # your two-letter language codepipeline:  <span class="token list punctuation">-</span> name: WhitespaceTokenizer  <span class="token list punctuation">-</span> name: RegexFeaturizer  <span class="token list punctuation">-</span> name: LexicalSyntacticFeaturizer  <span class="token list punctuation">-</span> name: CountVectorsFeaturizer  <span class="token list punctuation">-</span> name: CountVectorsFeaturizer<span class="token code keyword">    analyzer: "char_wb"</span><span class="token code keyword">    min_ngram: 1</span><span class="token code keyword">    max_ngram: 4</span>  <span class="token list punctuation">-</span> name: DIETClassifier<span class="token code keyword">    epochs: 100</span>  <span class="token list punctuation">-</span> name: EntitySynonymMapper  <span class="token list punctuation">-</span> name: ResponseSelector<span class="token code keyword">    epochs: 100</span></code></pre><h4 id="更长回答"><a href="#更长回答" class="headerlink" title="更长回答"></a>更长回答</h4><p>如果您的培训数据是英语，我们建议使用以下管道：</p><blockquote><pre><code>language: "en"pipeline:  - name: ConveRTTokenizer  - name: ConveRTFeaturizer  - name: RegexFeaturizer  - name: LexicalSyntacticFeaturizer  - name: CountVectorsFeaturizer  - name: CountVectorsFeaturizer    analyzer: "char_wb"    min_ngram: 1    max_ngram: 4  - name: DIETClassifier    epochs: 100  - name: EntitySynonymMapper  - name: ResponseSelector    epochs: 100</code></pre></blockquote><p>管道包含<a href="#">ConveRTFeaturizer</a>，它提供用户话语的预训练词嵌入。预先训练的词嵌入非常有用，因为它们已经编码了某种语言知识。例如，如果您在训练数据中有一个类似“我想购买苹果”的句子，并且要求Rasa预测“获取梨”的意图，那么您的模型已经知道“苹果”和“豌豆”是非常相似。如果您没有足够的培训数据，这将特别有用。<a href="#">ConveRTFeaturizer</a>的优点是它不会单独处理用户消息的每个单词，而是为整个句子创建上下文向量表示。但是，<code>ConveRT</code>仅提供英语。</p><p>如果您的训练数据不是英语，但您仍想使用预先训练的词嵌入，则建议使用以下管道：</p><pre class=" language-markdown"><code class="language-markdown">language: "fr"  # your two-letter language codepipeline:  <span class="token list punctuation">-</span> name: SpacyNLP  <span class="token list punctuation">-</span> name: SpacyTokenizer  <span class="token list punctuation">-</span> name: SpacyFeaturizer  <span class="token list punctuation">-</span> name: RegexFeaturizer  <span class="token list punctuation">-</span> name: LexicalSyntacticFeaturizer  <span class="token list punctuation">-</span> name: CountVectorsFeaturizer  <span class="token list punctuation">-</span> name: CountVectorsFeaturizer<span class="token code keyword">    analyzer: "char_wb"</span><span class="token code keyword">    min_ngram: 1</span><span class="token code keyword">    max_ngram: 4</span>  <span class="token list punctuation">-</span> name: DIETClassifier<span class="token code keyword">    epochs: 100</span>  <span class="token list punctuation">-</span> name: EntitySynonymMapper  <span class="token list punctuation">-</span> name: ResponseSelector<span class="token code keyword">    epochs: 100</span></code></pre><p>它使用<a href="#">SpacyFeaturizer</a>而不是<a href="#">ConveRTFeaturizer</a>。 <a href="#">SpacyFeaturizer</a>以多种不同的语言提供来自GloVe或fastText的预训练词嵌入（请参阅<a href="#">预训练词向量</a>）。</p><p>如果您在管道中未使用任何预训练的单词嵌入，则无需绑定到特定语言，并且可以将模型训练为更特定于领域。如果您的语言没有单词嵌入，或者您具有非常特定于域的术语，我们建议使用以下管道：</p><pre class=" language-markdown"><code class="language-markdown">language: "fr"  # your two-letter language codepipeline:  <span class="token list punctuation">-</span> name: WhitespaceTokenizer  <span class="token list punctuation">-</span> name: RegexFeaturizer  <span class="token list punctuation">-</span> name: LexicalSyntacticFeaturizer  <span class="token list punctuation">-</span> name: CountVectorsFeaturizer  <span class="token list punctuation">-</span> name: CountVectorsFeaturizer<span class="token code keyword">    analyzer: "char_wb"</span><span class="token code keyword">    min_ngram: 1</span><span class="token code keyword">    max_ngram: 4</span>  <span class="token list punctuation">-</span> name: DIETClassifier<span class="token code keyword">    epochs: 100</span>  <span class="token list punctuation">-</span> name: EntitySynonymMapper  <span class="token list punctuation">-</span> name: ResponseSelector<span class="token code keyword">    epochs: 100</span></code></pre><blockquote><p>注意</p><p>我们鼓励每个人通过列出要使用的组件的名称来定义自己的管道。您可以在<a href="#">Components中</a>找到每个组件的详细信息。如果要在管道中使用自定义组件，请参阅“<a href="#">自定义NLU组件”</a>。</p></blockquote><h3 id="选择合适的组件"><a href="#选择合适的组件" class="headerlink" title="选择合适的组件"></a>选择合适的组件</h3><p>有用于实体提取，意图分类，响应选择，预处理等的组件。您可以在“<a href="#">组件”</a>页面上了解有关任何特定组件的更多信息。如果要添加自己的组件（例如进行拼写检查或进行情感分析），请签出“<a href="#">自定义NLU组件”</a>。</p><p>管道通常包含三个主要部分：</p><ul><li><a href="#">Tokenization</a></li><li><a href="#">Featurization</a></li><li><a href="#">实体识别/意图分类/响应选择器</a></li></ul><h4 id="Tokenization"><a href="#Tokenization" class="headerlink" title="Tokenization"></a>Tokenization</h4><p>对于英语输入的标记化，我们建议使用<a href="#">ConveRTTokenizer</a>。您可以使用<a href="https://legacy-docs-v1.rasa.com/nlu/components/#whitespacetokenizer">WhitespaceTokenizer</a>处理其他用空格标记的（用空格分隔的单词）语言。如果您的语言不是用空格标记的，则应使用其他标记器。我们支持许多不同的<a href="https://legacy-docs-v1.rasa.com/nlu/components/#tokenizers">令牌生成器</a>，或者您可以创建自己的<a href="https://legacy-docs-v1.rasa.com/api/custom-nlu-components/#custom-nlu-components">自定义令牌生成器</a>。</p><blockquote><p>注意</p><p>管道下游的某些组件可能需要特定的标记器。您可以在<a href="%EF%BC%83">Components中</a>的各个组件上找到这些要求。如果管道内缺少必需的组件，则会引发错误。</p></blockquote><h4 id="Featurization"><a href="#Featurization" class="headerlink" title="Featurization"></a>Featurization</h4><p>您需要确定是否使用提供预训练单词嵌入的组件。我们建议在训练数据量较少的情况下，从预先训练的词嵌入入手。一旦拥有大量数据并确保最相关的词将出现在数据中，因此将有词嵌入，监督嵌入，这些嵌入可直接从训练数据中学习词义，从而使您的模型对您的领域更加具体。如果找不到适合您语言的预先训练模型，则应使用监督性嵌入。</p><h5 id="预先训练的曲面嵌入"><a href="#预先训练的曲面嵌入" class="headerlink" title="预先训练的曲面嵌入"></a>预先训练的曲面嵌入</h5><p>在管道中使用经过预先训练的单词嵌入的优势在于，如果您有一个训练示例，例如：“我想买苹果”，并且要求Rasa预测“买梨”的意图，那么您的模型已经知道“苹果”和“豌豆”这两个词非常相似。如果您没有足够的培训数据，这将特别有用。我们支持提供预训练词嵌入的一些组件：</p><ol><li><a href="#">MitieFeaturizer</a></li><li><a href="#">SpacyFeaturizer</a></li><li><a href="https://legacy-docs-v1.rasa.com/nlu/components/#convertfeaturizer">ConveRTFeaturizer</a></li><li><a href="https://legacy-docs-v1.rasa.com/nlu/components/#languagemodelfeaturizer">LanguageModelFeaturizer</a></li></ol><p>如果您的训练数据是英语，我们建议使用<a href="#">ConveRTFeaturizer</a>。<a href="#">ConveRTFeaturizer</a>的优点是它不会单独处理用户消息的每个单词，而是为整个句子创建上下文向量表示。例如，如果您有一个培训示例，例如：“我可以预订汽车吗？”，并且Rasa被要求预测“我需要从我家乘车”的意图，因为两个示例的上下文矢量表示已经非常相似，两者的分类意图很有可能是相同的。如果您没有足够的训练数据，这也很有用。</p><p><a href="#">ConveRTFeaturizer</a>的替代<a href="#">方法</a>是<a href="#">LanguageModelFeaturizer</a>，它使用诸如BERT，GPT-2等的预训练语言模型来提取完整句子的相似上下文向量表示形式。有关支持的语言模型的完整列表，请参见 <a href="#">HFTransformersNLP</a>。</p><p>如果您的训练数据不是英语，您还可以使用另一种语言模型变体，该变体已经以特定于您训练数据的语言进行了预训练。例如，BERT模型有中文（<code>bert-base-chinese</code>）和日语（<code>bert-base-japanese</code>）变体。可以在<a href="https://huggingface.co/transformers/pretrained_models.html">Transformers库</a>的<a href="https://huggingface.co/transformers/pretrained_models.html">官方文档中</a>找到这些语言模型的不同变体的完整列表 。</p><p><a href="#">SpacyFeaturizer</a>还提供了许多不同语言的词嵌入（请参阅<a href="#">预先训练的词向量</a>），因此您可以根据训练数据的语言将其用作另一种选择。</p><h5 id="监督曲面嵌入"><a href="#监督曲面嵌入" class="headerlink" title="监督曲面嵌入"></a>监督曲面嵌入</h5><p>如果您在管道中未使用任何预训练的单词嵌入，则无需绑定到特定语言，并且可以将模型训练为更特定于领域。例如，在通用英语中，“平衡”一词与“对称性”紧密相关，但与“现金”一词有很大不同。在银行领域中，“余额”和“现金”密切相关，您希望模型能够抓住这一点。如果您不想使用预训练的单词嵌入，则仅应使用<a href="#">稀疏功能化器</a>类别中的<a href="#">功能化器</a>，例如 <a href="#">CountVectorsFeaturizer</a>，<a href="#">RegexFeaturizer</a>或<a href="#">LexicalSyntacticFeaturizer</a>。</p><h4 id="实体识别-意图分类-响应选择器"><a href="#实体识别-意图分类-响应选择器" class="headerlink" title="实体识别/意图分类/响应选择器"></a>实体识别/意图分类/响应选择器</h4><p>根据您的数据，您可能只希望执行意图分类，实体识别或响应选择。或者，您可能希望合并多个这些任务。我们为每个任务支持多个组件。所有这些都在<a href="#">Components</a>中列出。我们建议使用<a href="#">DIETClassifier</a>进行意图分类和实体识别，并使用<a href="#">ResponseSelector</a>进行响应选择。</p><h3 id="多意图分类"><a href="#多意图分类" class="headerlink" title="多意图分类"></a>多意图分类</h3><p>您可以使用Rasa开源组件将意图拆分为多个标签。例如，您可以预测多个<code>thank+goodbye</code>Intent （）或模型分层Intent结构（<code>feedback+positive</code>与<code>feedback+negative</code>相比更相似<code>chitchat</code>）。为此，您需要在管道中使用<a href="#">DIETClassifier</a>。您还需要在使用的任何标记程序中定义这些标志：</p><blockquote><ul><li><code>intent_tokenization_flag</code>：将其设置为<code>True</code>，以便标记意图标签。</li><li><code>intent_split_symbol</code>：将其设置为分隔意图标签的定界符字符串。在这种情况下<code>+</code>，默认设置<code>_</code>。</li></ul></blockquote><p>阅读 有关如何在Rasa中使用多个意图的<a href="#">教程</a>。</p><p>这是一个示例配置：</p><pre class=" language-markdown"><code class="language-markdown">language: "en"pipeline:<span class="token list punctuation">-</span> name: "WhitespaceTokenizer"  intent<span class="token italic"><span class="token punctuation">_</span>tokenization<span class="token punctuation">_</span></span>flag: True  intent<span class="token italic"><span class="token punctuation">_</span>split<span class="token punctuation">_</span></span>symbol: "_"<span class="token list punctuation">-</span> name: "CountVectorsFeaturizer"<span class="token list punctuation">-</span> name: "DIETClassifier"</code></pre><h3 id="比较管道"><a href="#比较管道" class="headerlink" title="比较管道"></a>比较管道</h3><p>Rasa为您提供了直接比较数据上多个管道性能的工具。有关更多信息，请参见<a href="#">比较NLU管道</a>。</p><blockquote><p>注意</p><p>意图分类与实体提取无关。因此，有时NLU会获得正确的意图，但实体会出错，反之亦然。您需要为意图和实体提供足够的数据</p></blockquote><h3 id="处理类不平衡"><a href="#处理类不平衡" class="headerlink" title="处理类不平衡"></a>处理类不平衡</h3><p>如果类别失衡很大，例如，如果您有很多针对某些意图的训练数据而很少有针对其他意图的训练数据，则分类算法通常不会表现良好。要缓解此问题，可以使用<code>balanced</code>批处理策略。该算法确保在每个批次中或至少在尽可能多的后续批次中代表所有类别，仍然模仿某些类别比其他类别更频繁的事实。默认情况下使用平衡批处理。为了将其关闭并使用经典的批处理策略，请在您的配置文件中添加该策略 。<code>batch_strategy: sequence</code></p><pre class=" language-markdown"><code class="language-markdown">language: "en"pipeline:<span class="token title important"><span class="token punctuation">#</span> - ... other components</span><span class="token list punctuation">-</span> name: "DIETClassifier"  batch_strategy: sequence</code></pre><h3 id="组件生命周期"><a href="#组件生命周期" class="headerlink" title="组件生命周期"></a>组件生命周期</h3><p>每个组件处理输入和/或创建输出。组件的顺序取决于它们在中列出的顺序<code>config.yml</code>；组件的输出可由管道中紧随其后的任何其他组件使用。某些组件仅生成管道中其他组件使用的信息。其他组件产生的<code>output</code>属性将在处理完成后返回。</p><p>例如，对于句子，输出为：<code>"I am looking for Chinese food"</code></p><pre class=" language-json"><code class="language-json">&amp;#<span class="token number">123</span><span class="token punctuation">;</span>    <span class="token property">"text"</span><span class="token operator">:</span> <span class="token string">"I am looking for Chinese food"</span><span class="token punctuation">,</span>    <span class="token property">"entities"</span><span class="token operator">:</span> <span class="token punctuation">[</span>        &amp;#<span class="token number">123</span><span class="token punctuation">;</span>            <span class="token property">"start"</span><span class="token operator">:</span> <span class="token number">8</span><span class="token punctuation">,</span>            <span class="token property">"end"</span><span class="token operator">:</span> <span class="token number">15</span><span class="token punctuation">,</span>            <span class="token property">"value"</span><span class="token operator">:</span> <span class="token string">"chinese"</span><span class="token punctuation">,</span>            <span class="token property">"entity"</span><span class="token operator">:</span> <span class="token string">"cuisine"</span><span class="token punctuation">,</span>            <span class="token property">"extractor"</span><span class="token operator">:</span> <span class="token string">"DIETClassifier"</span><span class="token punctuation">,</span>            <span class="token property">"confidence"</span><span class="token operator">:</span> <span class="token number">0.864</span>        &amp;#<span class="token number">125</span><span class="token punctuation">;</span>    <span class="token punctuation">]</span><span class="token punctuation">,</span>    <span class="token property">"intent"</span><span class="token operator">:</span> &amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"confidence"</span><span class="token operator">:</span> <span class="token number">0.6485910906220309</span><span class="token punctuation">,</span> <span class="token property">"name"</span><span class="token operator">:</span> <span class="token string">"restaurant_search"</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">,</span>    <span class="token property">"intent_ranking"</span><span class="token operator">:</span> <span class="token punctuation">[</span>        &amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"confidence"</span><span class="token operator">:</span> <span class="token number">0.6485910906220309</span><span class="token punctuation">,</span> <span class="token property">"name"</span><span class="token operator">:</span> <span class="token string">"restaurant_search"</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">,</span>        &amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"confidence"</span><span class="token operator">:</span> <span class="token number">0.1416153159565678</span><span class="token punctuation">,</span> <span class="token property">"name"</span><span class="token operator">:</span> <span class="token string">"affirm"</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span>    <span class="token punctuation">]</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span></code></pre><p>这是在以下管道中不同组件的结果的组合中创建的：</p><pre class=" language-markdown"><code class="language-markdown">pipeline:  <span class="token list punctuation">-</span> name: WhitespaceTokenizer  <span class="token list punctuation">-</span> name: RegexFeaturizer  <span class="token list punctuation">-</span> name: LexicalSyntacticFeaturizer  <span class="token list punctuation">-</span> name: CountVectorsFeaturizer  <span class="token list punctuation">-</span> name: CountVectorsFeaturizer<span class="token code keyword">    analyzer: "char_wb"</span><span class="token code keyword">    min_ngram: 1</span><span class="token code keyword">    max_ngram: 4</span>  <span class="token list punctuation">-</span> name: DIETClassifier  <span class="token list punctuation">-</span> name: EntitySynonymMapper  <span class="token list punctuation">-</span> name: ResponseSelector</code></pre><p>例如，<code>entities</code>此处的属性是由<code>DIETClassifier</code>组件创建的。</p><p>每个组件都可以实现<code>Component</code>基类中的几种方法。在管道中，将按特定顺序调用这些不同的方法。假设我们将以下管道添加到我们的<code>config.yml</code>：</p><pre class=" language-markdown"><code class="language-markdown">pipeline:  <span class="token list punctuation">-</span> name: "Component A"  <span class="token list punctuation">-</span> name: "Component B"  <span class="token list punctuation">-</span> name: "Last Component"</code></pre><p>下图显示了在训练该管道期间的呼叫顺序：</p><img src="/2020/10/04/rasa-wen-dang-zhi-nan-11/9.png" style="zoom:50%;"><p>在使用该<code>create</code>函数创建第一个组件之前，会创建一个所谓<code>context</code>的（仅是python dict）。此上下文用于在组件之间传递信息。例如，一个组件可以计算训练数据的特征向量，将其存储在上下文中，而另一个组件可以从上下文中检索这些特征向量并进行意图分类。</p><p>最初，上下文使用所有配置值填充。图像中的箭头显示了呼叫顺序，并可视化了所传递上下文的路径。在对所有组件进行了训练和持久化之后，最终的上下文字典用于持久化模型的元数据。</p><h3 id="管道模板（已弃用）"><a href="#管道模板（已弃用）" class="headerlink" title="管道模板（已弃用）"></a>管道模板（已弃用）</h3><p>模板只是完整组件列表的快捷方式。例如，此管道模板：</p><blockquote><pre class=" language-markdown"><code class="language-markdown">language: "en"pipeline: "pretrained<span class="token italic"><span class="token punctuation">_</span>embeddings<span class="token punctuation">_</span></span>spacy"</code></pre></blockquote><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> nlp </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Rasa </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Rasa文档指南(10)</title>
      <link href="/2020/10/04/rasa-wen-dang-zhi-nan-10/"/>
      <url>/2020/10/04/rasa-wen-dang-zhi-nan-10/</url>
      
        <content type="html"><![CDATA[<h1 id="云存储"><a href="#云存储" class="headerlink" title="云存储"></a>云存储</h1><p>Rasa支持使用<a href="https://aws.amazon.com/s3/">S3</a>， <a href="https://cloud.google.com/storage/">GCS</a>和<a href="https://azure.microsoft.com/services/storage/">Azure存储</a>来保存模型。</p><ul><li><p>Amazon S3存储</p><blockquote><p><code>boto3</code>您可以通过安装的模块来支持S3 。<code>pip install boto3</code></p><p>将Rasa服务器的<code>remote-storage</code>选项设置为 <code>aws</code>。获取您的S3凭据并设置以下环境变量：</p><ul><li><code>AWS_SECRET_ACCESS_KEY</code></li><li><code>AWS_ACCESS_KEY_ID</code></li><li><code>AWS_DEFAULT_REGION</code></li><li><code>BUCKET_NAME</code></li><li><code>AWS_ENDPOINT_URL</code></li></ul><p>如果没有名称为的存储桶<code>BUCKET_NAME</code>，Rasa将创建它。</p></blockquote></li><li><p>谷歌云存储</p><blockquote><p>使用该<code>google-cloud-storage</code>软件包支持GCS ，您可以使用该软件包进行安装。<code>pip install google-cloud-storage</code></p><p>将Rasa服务器的<code>remote-storage</code>选项设置为<code>gcs</code>。</p><p>在Google App Engine和计算引擎上运行时，已经设置了身份验证凭据。要在本地或其他地方运行，请签出其 <a href="https://github.com/GoogleCloudPlatform/python-docs-samples/tree/master/storage/cloud-client#authentication">客户端存储库</a> 以获取有关设置身份验证的详细信息。它涉及从Google Cloud Console创建服务帐户密钥文件，并将<code>GOOGLE_APPLICATION_CREDENTIALS</code>环境变量设置为该密钥文件的路径。</p></blockquote></li><li><p>Azure存储</p><blockquote><p>使用该<code>azure-storage-blob</code>软件包支持Azure ，可以使用进行安装。<code>pip install azure-storage-blob</code></p><p>将Rasa服务器的<code>remote-storage</code>选项设置为<code>azure</code>。</p><p>必须设置以下环境变量：</p><ul><li><code>AZURE_CONTAINER</code></li><li><code>AZURE_ACCOUNT_NAME</code></li><li><code>AZURE_ACCOUNT_KEY</code></li></ul><p>如果没有名称为的容器<code>AZURE_CONTAINER</code>，Rasa将创建它。</p></blockquote></li></ul><p>将模型压缩后再保存到云中。压缩文件的命名约定为<code>{MODEL_NAME}.tar.gz</code>，它存储在存储服务的根文件夹中。当前，您无法手动指定云存储上的路径。</p><p>如果存储受过训练的模型，Rasa将gzip新模型并将其上传到容器中。如果要从云存储中检索/加载模型，Rasa将在本地下载压缩的模型并将内容提取到临时目录中。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> nlp </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Rasa </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Rasa文档指南(9)</title>
      <link href="/2020/10/04/rasa-wen-dang-zhi-nan-9/"/>
      <url>/2020/10/04/rasa-wen-dang-zhi-nan-9/</url>
      
        <content type="html"><![CDATA[<h1 id="部署Rasa助手"><a href="#部署Rasa助手" class="headerlink" title="部署Rasa助手"></a>部署Rasa助手</h1><p>本页说明何时以及如何部署使用Rasa构建的助手。它将使您的助手可供用户使用，并为您准备好生产环境。</p><ul><li><a href="#">何时部署助手</a></li><li><a href="#">推荐的部署方法</a><ul><li><a href="#">服务器快速安装</a></li><li><a href="h#">舵图</a></li></ul></li><li><a href="#">替代部署方法</a><ul><li><a href="#">Docker撰写</a></li><li><a href="#">Rasa仅限开源部署</a></li></ul></li><li><a href="#">部署动作服务器</a><ul><li><a href="#">构建动作服务器映像</a></li><li><a href="#">使用自定义动作服务器映像</a></li></ul></li></ul><h2 id="何时部署您的助手"><a href="#何时部署您的助手" class="headerlink" title="何时部署您的助手"></a>何时部署您的助手</h2><p>部署助手并使之可供测试用户使用的最佳时间是可以处理最重要的快乐路径，或者是我们所谓的<a href="https://rasa.com/docs/rasa/glossary">最低可行助手</a>。</p><p>通过以下推荐的部署方法，可以通过<a href="https://rasa.com/docs/rasa-x/user-guide/share-assistant/#share-your-bot">Rasa X中</a>的<a href="https://rasa.com/docs/rasa-x/user-guide/share-assistant/#share-your-bot">共享助手功能</a>轻松地与测试用户<a href="https://rasa.com/docs/rasa-x/user-guide/share-assistant/#share-your-bot">共享助手</a>。然后，当您准备通过一个或多个<a href="https://legacy-docs-v1.rasa.com/user-guide/messaging-and-voice-channels/#messaging-and-voice-channels">消息和语音通道</a>使您的助手可用时，您可以轻松地将它们添加到现有的部署设置中。</p><h2 id="推荐部署方法"><a href="#推荐部署方法" class="headerlink" title="推荐部署方法"></a>推荐部署方法</h2><p>建议的部署助手的方法是使用我们支持的“服务器快速安装”或“头盔图表”选项。两者都部署Rasa X和您的助手。它们是部署助手，允许您使用Rasa X来查看对话并将其转换为培训数据的最简单方法，并且已准备就绪。有关部署方法的更多详细信息，请参见《<a href="https://rasa.com/docs/rasa-x/installation-and-setup/installation-guide/">Rasa X安装指南》</a>。</p><h3 id="服务器快速安装"><a href="#服务器快速安装" class="headerlink" title="服务器快速安装"></a>服务器快速安装</h3><p>服务器快速安装脚本是部署Rasa X和您的助手的最简单方法。它使用合理的默认值在您的机器上安装了Kubernetes集群，使您可以通过一条命令启动并运行。</p><blockquote><ul><li><p>默认值：确保满足<a href="https://rasa.com/docs/rasa-x/installation-and-setup/install/quick-install-script/#hardware-os-requirements">操作系统要求</a>，然后运行：</p><pre><code>curl -s get-rasa-x.rasa.com | 须藤bash    复制    复制！</code></pre></li><li><p>自定义：请参阅<a href="https://rasa.com/docs/rasa-x/installation-and-setup/customize/#server-quick-install">自定义脚本</a> 和<a href="https://rasa.com/docs/rasa-x/installation-and-setup/install/quick-install-script">服务器快速安装</a>文档。</p></li></ul></blockquote><h3 id="舵图"><a href="#舵图" class="headerlink" title="舵图"></a>舵图</h3><p>对于将吸引大量用户流量的助手，通过我们的Helm图表设置Kubernetes或Openshift部署是最佳选择。这提供了可扩展的体系结构，该体系结构也易于部署。但是，如果有特殊要求，您也可以自定义Helm图表。</p><blockquote><ul><li>默认值：阅读<a href="https://rasa.com/docs/rasa-x/installation-and-setup/install/helm-chart/">Helm Chart安装</a>文档。</li><li>自定义：阅读以上内容以及“<a href="https://rasa.com/docs/rasa-x/installation-and-setup/customize/#helm-chart">高级配置”</a> 文档，然后根据需要自定义<a href="https://github.com/RasaHQ/rasa-x-helm">开源Helm图表</a>。</li></ul></blockquote><h2 id="替代部署方法"><a href="#替代部署方法" class="headerlink" title="替代部署方法"></a>替代部署方法</h2><h3 id="Docker"><a href="#Docker" class="headerlink" title="Docker"></a>Docker</h3><p>您也可以在没有集群环境的情况下在Docker Compose设置中运行RasaX。我们有一个这样做的安装脚本，以及任何自定义设置的手动说明。</p><blockquote><ul><li>默认值：阅读<a href="https://rasa.com/docs/rasa-x/installation-and-setup/install/docker-compose/#docker-compose-install-script">Docker Compose安装脚本</a>文档或观看有关部署Rasa X的<a href="https://www.youtube.com/watch?v=IUYdwy8HPVc">Masterclass视频</a>。</li><li>自定义：阅读<a href="https://rasa.com/docs/rasa-x/installation-and-setup/install/docker-compose/#docker-compose-manual-install">Docker Compose手动安装</a>文档以获取完整的自定义选项。</li></ul></blockquote><h3 id="Rasa仅限开源部署"><a href="#Rasa仅限开源部署" class="headerlink" title="Rasa仅限开源部署"></a>Rasa仅限开源部署</h3><p>也可以使用Docker Compose在不使用Rasa X的情况下部署Rasa助手。为此，您可以在本地或在Docker中构建Rasa Assistant。然后，您可以在Docker Compose中部署模型。</p><ul><li><a href="https://legacy-docs-v1.rasa.com/user-guide/rasa-tutorial/">在本地构建Rasa助手</a></li><li><a href="https://legacy-docs-v1.rasa.com/user-guide/docker/building-in-docker/">在Docker中构建Rasa助手</a></li><li><a href="https://legacy-docs-v1.rasa.com/user-guide/docker/deploying-in-docker-compose/">在Docker Compose中部署Rasa开源助手</a></li></ul><h2 id="部署动作服务器"><a href="#部署动作服务器" class="headerlink" title="部署动作服务器"></a>部署动作服务器</h2><h3 id="建设行动服务器映像"><a href="#建设行动服务器映像" class="headerlink" title="建设行动服务器映像"></a>建设行动服务器映像</h3><p>如果您构建包含操作代码的映像并将其存储在容器注册表中，则可以将其作为部署的一部分运行，而不必在服务器之间移动代码。此外，您可以添加系统或Python库的任何其他依赖关系，这些依赖关系是动作代码的一部分，但不包含在基本<code>rasa/rasa-sdk</code>映像中。</p><p>创建图像：</p><blockquote><ol><li><p>将操作代码移到<code>actions</code>项目目录中的文件夹中。确保还添加一个空<code>actions/__init__.py</code>文件：</p><blockquote><pre><code>mkdir actionsmv actions.py actions/actions.pytouch actions/__init__.py  # the init file indicates actions.py is a python module</code></pre></blockquote><p>该<code>rasa/rasa-sdk</code>图像会自动寻找在行动<code>actions/actions.py</code>。</p></li><li><p>如果您的操作有任何其他依赖性，请在文件中创建一个列表 <code>actions/requirements-actions.txt</code>。</p></li><li><p><code>Dockerfile</code>在项目目录中创建一个名为的文件，您将在其中扩展官方SDK映像，在代码上进行复制并添加任何自定义依赖项（如果需要）。例如：</p><blockquote><pre><code>＃扩展官方Rasa SDK映像FROM rasa/rasa-sdk:1.10.2＃使用子目录作为工作目录WORKDIR /app＃如有必要，复制任何其他自定义要求（取消注释下一行# COPY actions/requirements-actions.txt ./＃更改回root用户以安装依赖项USER root＃如有必要，安装操作代码的其他要求（取消注释下一行）# RUN pip install -r requirements-actions.txt＃将动作文件夹复制到工作目录COPY ./actions /app/actions＃按照最佳做法，请勿以root用户身份运行代码USER 1001</code></pre></blockquote></li></ol></blockquote><p>然后，您可以通过以下命令生成映像：</p><blockquote><pre><code>docker build . -t &lt;account_username&gt;/&lt;repository_name&gt;:&lt;custom_image_tag&gt;</code></pre></blockquote><p>该<code>&lt;custom_image_tag&gt;</code>应参考这一形象将如何与其他人不同。例如，您可以标记标签的版本或日期，以及为生产和开发服务器创建具有不同代码的不同标签。每当您更新代码并想重新部署它时，都应该创建一个新标记。</p><h3 id="使用自定义动作服务器映像"><a href="#使用自定义动作服务器映像" class="headerlink" title="使用自定义动作服务器映像"></a>使用自定义动作服务器映像</h3><p>如果要构建此映像以使其可从其他服务器（例如Rasa X或Rasa Enterprise部署）使用，则应将映像推送到云存储库。</p><p>本文档假定您将映像推送到<a href="https://hub.docker.com/">DockerHub</a>。DockerHub将让您免费托管多个公共存储库和一个私有存储库。确保首先<a href="https://hub.docker.com/signup/">创建一个帐户</a> 并<a href="https://hub.docker.com/signup/">创建一个存储库</a>来存储您的图像。您还可以将映像推送到其他Docker注册表，例如<a href="https://cloud.google.com/container-registry">Google Container Registry</a>， <a href="https://aws.amazon.com/ecr/">Amazon Elastic Container Registry</a>或 <a href="https://azure.microsoft.com/en-us/services/container-registry/">Azure Container Registry</a>。</p><p>您可以通过以下方式将映像推送到DockerHub：</p><blockquote><pre><code>docker login --username &lt;account_username&gt; --password &lt;account_password&gt;docker push &lt;account_username&gt;/&lt;repository_name&gt;:&lt;custom_image_tag&gt;</code></pre></blockquote><p>要将图像进行身份验证并将其推送到其他容器注册表，请参阅所选容器注册表的文档。</p><p>您如何引用自定义操作映像取决于您的部署。选择与您的部署相关的文档：</p><blockquote><ul><li><a href="https://rasa.com/docs/rasa-x/installation-and-setup/customize/#quick-install-script-customizing">服务器快速安装</a></li><li><a href="https://rasa.com/docs/rasa-x/installation-and-setup/customize/#adding-a-custom-action-server">舵图</a></li><li><a href="https://rasa.com/docs/rasa-x/installation-and-setup/customize/#connecting-a-custom-action-server">Docker撰写</a></li><li><a href="https://legacy-docs-v1.rasa.com/user-guide/docker/deploying-in-docker-compose/#running-multiple-services">Rasa仅开源</a></li></ul></blockquote><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> nlp </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Rasa </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Rasa文档指南(8)</title>
      <link href="/2020/10/04/rasa-wen-dang-zhi-nan-8/"/>
      <url>/2020/10/04/rasa-wen-dang-zhi-nan-8/</url>
      
        <content type="html"><![CDATA[<h1 id="配置HTTP-API"><a href="#配置HTTP-API" class="headerlink" title="配置HTTP API"></a>配置HTTP API</h1><ul><li><a href="#">使用Rasa的HTTP API</a><ul><li><a href="#">从服务器获取模型</a></li><li><a href="#">从远程存储中获取模型</a></li></ul></li><li><a href="#">配置SSL / HTTPS</a></li><li><a href="#">安全注意事项</a></li><li><a href="#">端点配置</a><ul><li><a href="#">连接追踪商店</a></li><li><a href="#">连接事件代理</a></li></ul></li></ul><h2 id="使用Rasa的HTTP"><a href="#使用Rasa的HTTP" class="headerlink" title="使用Rasa的HTTP"></a>使用Rasa的HTTP</h2><p>注意</p><p>以下说明与配置模型如何在Docker容器中运行或在本地测试HTTP API有关。如果要向用户部署助手，请参阅<a href="#">部署Rasa助手</a>。</p><p>您可以运行一个简单的HTTP服务器，该服务器使用训练有素的Rasa模型处理以下请求：</p><pre><code>rasa run -m models --enable-api --log-file out.log</code></pre><p>该API公开的所有端点都记录在<a href="#">HTTP API中</a>。</p><p>不同的参数是：</p><ul><li><code>-m</code>：包含Rasa模型的文件夹的路径，</li><li><code>--enable-api</code>：启用此附加API，并</li><li><code>--log-file</code>：日志文件的路径。</li></ul><p>Rasa可以通过三种不同的方式加载模型：</p><ol><li>从服务器<a href="#">获取模型</a>（请参阅从服务器<a href="#">获取模型</a>），或者</li><li>从远程存储中获取模型（请参阅<a href="#">Cloud Storage</a>）。</li><li>通过<code>-m</code>本地存储系统加载指定的模型，</li></ol><p>Rasa尝试以上述顺序加载模型，即，如果未配置任何模型服务器和远程存储，它只会尝试从本地存储系统加载模型。</p><blockquote><p>警告</p><p>确保通过限制对服务器的访问（例如使用防火墙）或启用身份验证方法来确保服务器的安全：<a href="#">安全注意事项</a>。</p></blockquote><blockquote><p>注意</p><p>如果您正在使用自定义动作，请确保您的动作服务器正在运行（请参阅<a href="#">启动动作服务器</a>）。如果您的操作在其他计算机上运行，或者您没有使用Rasa SDK，请确保更新<code>endpoints.yml</code>文件。</p></blockquote><blockquote><p>注意</p><p>如果使用仅NLU模型启动服务器，则无法调用所有可用端点。请注意，某些端点将返回409状态代码，因为需要训练有素的Core模型来处理请求。</p></blockquote><blockquote><p>注意</p><p>默认情况下，HTTP服务器作为单个进程运行。您可以使用<code>SANIC_WORKERS</code>环境变量更改工作进程的数量。建议您将工作程序数设置为可用的CPU内核数（ 有关更多详细信息，请查看 <a href="#">Sanic文档</a>）。这只能与结合使用 <code>RedisLockStore</code>（请参阅<a href="#">锁存储</a>）</p></blockquote><h3 id="从服务器获取模型"><a href="#从服务器获取模型" class="headerlink" title="从服务器获取模型"></a>从服务器获取模型</h3><p>您可以将HTTP服务器配置为从另一个URL获取模型：</p><pre><code>rasa run --enable-api --log-file out.log --endpoints my_endpoints.yml</code></pre><p>在端点配置（<code>my_endpoints.yml</code>）中指定模型服务器，在该配置中您可以指定服务器URL Rasa定期查询压缩的Rasa模型：</p><pre><code>models:  url: http://my-server.com/models/default@latest  wait_time_between_pulls: 10   # [optional](default: 100)</code></pre><blockquote><p>注意</p><p>如果只想从服务器提取一次模型，请设置 <code>wait_time_between_pulls</code>为<code>None</code>。</p></blockquote><blockquote><p>注意</p><p>您的模型服务器必须提供压缩的Rasa模型，并且 标头之一。仅当此模型哈希值已更改时，Rasa才会下载新模型。<code>{"ETag": &lt;model_hash_string&gt;}</code></p></blockquote><p>Rasa使用<code>If-None-Match</code> 包含当前模型哈希的标头将请求发送到模型服务器。如果您的模型服务器可以为模型提供与您发送的哈希不同的哈希，则它应以zip文件的形式发送，并带有<code>ETag</code>包含新哈希的标头。如果不是，则Rasa期望带有<code>204</code>或<code>304</code>状态代码的空响应。</p><p>Rasa可能向模型服务器发出的示例请求如下所示：</p><pre class=" language-shell"><code class="language-shell"> curl --header "If-None-Match: d41d8cd98f00b204e9800998ecf8427e" http://my-server.com/models/default@latest</code></pre><h3 id="从远程存储获取模型"><a href="#从远程存储获取模型" class="headerlink" title="从远程存储获取模型"></a>从远程存储获取模型</h3><p>您还可以配置Rasa服务器以从远程存储中获取模型：</p><pre><code>rasa run -m 20190506-100418.tar.gz --enable-api --log-file out.log --remote-storage aws</code></pre><p>该模型将被下载并存储在本地存储系统上的临时目录中。有关更多信息，请参阅<a href="#">Cloud Storage</a>。</p><h2 id="配置SSL"><a href="#配置SSL" class="headerlink" title="配置SSL /"></a>配置SSL /</h2><p>默认情况下，Rasa服务器使用HTTP进行通信。为了确保与SSL的通信安全，您需要提供有效的证书和相应的私钥文件。</p><p>您可以在命令中指定这些文件：<code>rasa run</code></p><pre class=" language-shell"><code class="language-shell">rasa run --ssl-certificate myssl.crt --ssl-keyfile myssl.key</code></pre><p>如果在创建过程中使用密码对密钥文件进行了加密，则需要将此密码添加到命令中：</p><pre class=" language-shell"><code class="language-shell">rasa run --ssl-certificate myssl.crt --ssl-keyfile myssl.key --ssl-password mypassword</code></pre><h2 id="安全注意事项"><a href="#安全注意事项" class="headerlink" title="安全注意事项"></a>安全注意事项</h2><p>我们建议不要将Rasa Server暴露给外界，而应通过专用连接（例如docker容器之间）从后端连接到Rasa Server。</p><p>但是，内置了两种身份验证方法：</p><p><strong>基于令牌的身份验证：</strong></p><p>启动服务器时使用传递令牌：<code>--auth-token thisismysecret</code></p><pre class=" language-shell"><code class="language-shell">rasa run \    -m models \    --enable-api \    --log-file out.log \    --auth-token thisismysecret</code></pre><p>在我们的示例中<code>thisismysecret</code>，您的请求应将令牌作为参数传递：</p><pre class=" language-shell"><code class="language-shell"> curl -XGET localhost:5005/conversations/default/tracker?token=thisismysecret</code></pre><p><strong>基于JWT的身份验证：</strong></p><p>使用启用基于JWT的身份验证。发送到服务器的请求需要在使用此机密和算法签名的标头中包含有效的JWT令牌。<code>--jwt-secret thisismysecret Authorization HS256</code></p><p>用户必须具有<code>username</code>和<code>role</code>属性。如果<code>role</code>为<code>admin</code>，则所有端点均可访问。如果<code>role</code>为<code>user</code>，<code>sender_id</code>则只有<code>sender_id</code>与用户的匹配时，才可以访问带有参数的端点<code>username</code>。</p><pre class=" language-shell"><code class="language-shell">rasa run \    -m models \    --enable-api \    --log-file out.log \    --jwt-secret thisismysecret</code></pre><p>您的请求应设置正确的JWT标头：</p><pre class=" language-shell"><code class="language-shell">"Authorization": "Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ"                 "zdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIi"                 "wiaWF0IjoxNTE2MjM5MDIyfQ.qdrr2_a7Sd80gmCWjnDomO"                 "Gl8eZFVfKXA6jhncgRn-I"</code></pre><h2 id="端点配置"><a href="#端点配置" class="headerlink" title="端点配置"></a>端点配置</h2><p>要将Rasa连接到其他终结点，可以在YAML文件中指定终结点配置。然后使用flag运行Rasa 。<code>--endpoints &lt;path to endpoint configuration.yml&gt;</code></p><p>例如：</p><pre class=" language-shell"><code class="language-shell">rasa run \    --m <Rasa model> \    --endpoints <path to endpoint configuration>.yml</code></pre><blockquote><p>注意</p><p>您可以通过在中指定配置文件来使用环境变量。然后将这些占位符替换为环境变量的值。<code>${name of environment variable}</code></p></blockquote><h3 id="连接Tracker商店"><a href="#连接Tracker商店" class="headerlink" title="连接Tracker商店"></a>连接Tracker商店</h3><p>要在端点配置中配置跟踪器存储，请参阅<a href="#">跟踪器存储</a>。</p><h3 id="连接事件代理"><a href="#连接事件代理" class="headerlink" title="连接事件代理"></a>连接事件代理</h3><p>要在端点配置中配置事件代理，请参阅<a href="#">事件</a>代理。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> nlp </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Rasa </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Rasa文档指南(7)</title>
      <link href="/2020/10/04/rasa-wen-dang-zhi-nan-7/"/>
      <url>/2020/10/04/rasa-wen-dang-zhi-nan-7/</url>
      
        <content type="html"><![CDATA[<h1 id="验证数据"><a href="#验证数据" class="headerlink" title="验证数据"></a>验证数据</h1><h2 id="测试域和数据文件中的错误"><a href="#测试域和数据文件中的错误" class="headerlink" title="测试域和数据文件中的错误"></a>测试域和数据文件中的错误</h2><p>要验证您的域文件，NLU数据或故事数据中是否有任何错误，请运行验证脚本。您可以使用以下命令运行它：</p><pre><code>rasa data validate</code></pre><p>上面的脚本对文件运行所有验证，但故事结构验证除外，除非您提供<code>--max-history</code>参数，否则将省略该验证。这是脚本的选项列表：</p><pre class=" language-shell"><code class="language-shell">usage: rasa data validate [-h] [-v] [-vv] [--quiet]                          [--max-history MAX_HISTORY] [--fail-on-warnings]                          [-d DOMAIN] [--data DATA]                          &#123;stories&#125; ...positional arguments:  &#123;stories&#125;    stories             Checks for inconsistencies in the story files.optional arguments:  -h, --help            show this help message and exit  --max-history MAX_HISTORY                        Number of turns taken into account for story structure                        validation. (default: None)  --fail-on-warnings    Fail validation on warnings and errors. If omitted                        only errors will result in a non zero exit code.                        (default: False)  -d DOMAIN, --domain DOMAIN                        Domain specification (yml file). (default: domain.yml)  --data DATA           Path to the file or directory containing Rasa data.                        (default: data)Python Logging Options:  -v, --verbose         Be verbose. Sets logging level to INFO. (default:                        None)  -vv, --debug          Print lots of debugging statements. Sets logging level                        to DEBUG. (default: None)  --quiet               Be quiet! Sets logging level to WARNING. (default:                        None)</code></pre><p>默认情况下，验证器仅搜索数据中的错误（例如，同一示例被列为两个意图的示例），但不报告其他次要问题（例如未使用的意图，未作为动作列出的言语）。要同时报告以后，请使用该<code>-debug</code>标志。</p><p>您还可以通过导入Validator类通过Python API运行这些验证，该类具有以下方法：</p><p><strong>from_files（）：</strong>从字符串路径创建实例到必要的文件。</p><p><strong>verify_intents（）：</strong>检查域文件中列出的意图是否与NLU数据一致。</p><p><strong>verify_example_repetition_in_intents（）：</strong>检查NLU数据的不同意图之间是否没有重复的数据。</p><p><strong>verify_intents_in_stories（）：</strong>验证故事中的意图，以检查它们是否有效。</p><p><strong>verify_utterances（）：</strong>检查域文件中“响应”部分中列出的响应与您定义的发声操作之间的一致性。</p><p><strong>verify_utterances_in_stories（）：</strong>验证故事中的言语，以检查其是否有效。</p><p><strong>verify_all（）：</strong>运行上面的所有验证。</p><p><strong>verify_domain_validity（）：</strong>检查域是否有效。</p><p>要使用这些功能，必须创建一个Validator对象并初始化记录器。请参阅以下代码：</p><pre class=" language-shell"><code class="language-shell">import loggingfrom rasa import utilsfrom rasa.core.validator import Validatorlogger = logging.getLogger(__name__)utils.configure_colored_logging('DEBUG')validator = Validator.from_files(domain_file='domain.yml',                                 nlu_data='data/nlu_data.md',                                 stories='data/stories.md')validator.verify_all()</code></pre><h2 id="测试故事文件以解决冲突"><a href="#测试故事文件以解决冲突" class="headerlink" title="测试故事文件以解决冲突"></a>测试故事文件以解决冲突</h2><p>除了上述默认测试之外，您还可以对故事进行更深入的结构测试。特别是，您可以测试您的故事是否不一致，即是否从相同的对话历史中遵循了不同的漫游器操作。如果不是这种情况，则Rasa无法学习正确的行为。</p><p>以以下两个故事为例：</p><pre class=" language-text"><code class="language-text">## Story 1* greet  - utter_greet* inform_happy  - utter_happy  - utter_goodbye## Story 2* greet  - utter_greet* inform_happy  - utter_goodbye</code></pre><p>这两个故事是不一致的，因为莎不知道是否应该预测<code>utter_happy</code>或<code>utter_goodbye</code> 之后<code>inform_happy</code>，因为没有什么能区分对话国在<code>inform_happy</code>这两个故事和后续行动是在故事1和2的故事不同。</p><p>可以使用我们的故事结构验证工具自动识别此冲突。为此，请在命令行中使用，如下所示：<code>rasa data validate</code></p><pre class=" language-text"><code class="language-text">rasa data validate stories --max-history 3> 2019-12-09 09:32:13 INFO     rasa.core.validator  - Story structure validation...> 2019-12-09 09:32:13 INFO     rasa.core.validator  - Assuming max_history = 3>   Processed Story Blocks: 100% 2/2 [00:00<00:00, 3237.59it/s, # trackers=1]> 2019-12-09 09:32:13 WARNING  rasa.core.validator  - CONFLICT after intent 'inform_happy':>   utter_goodbye predicted in 'Story 2'>   utter_happy predicted in 'Story 1'</code></pre><p>在这里，我们将<code>max-history</code>值指定为3。这意味着，在进行动作预测时会考虑3个事件（用户消息/机器人操作），但是对于此示例，特定的设置并不重要，因为无论您花费多长时间的历史记录考虑到，冲突始终存在。</p><blockquote><p>警告</p><p>该脚本假定您所有的<strong>故事名称都是唯一的</strong>。如果您的故事采用Markdown格式，则可以使用诸如这样的命令查找重复的名称 。<code>rasa data validate stories``grep -h "##" data/*.md | uniq -c | grep "^[^1]"</code></p></blockquote><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> nlp </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Rasa </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Rasa文档指南(6)</title>
      <link href="/2020/10/03/rasa-wen-dang-zhi-nan-6/"/>
      <url>/2020/10/03/rasa-wen-dang-zhi-nan-6/</url>
      
        <content type="html"><![CDATA[<h1 id="设置CI-CD"><a href="#设置CI-CD" class="headerlink" title="设置CI / CD"></a>设置CI / CD</h1><p>即使开发上下文助手与开发传统软件有所不同，您仍应遵循软件开发最佳实践。设置持续集成（CI）和持续部署（CD）管道可确保对您的漫游器进行增量更新可以改善它，而不会损害它。</p><ul><li><a href="#">总览</a></li><li><a href="#">持续集成（CI）</a><ul><li><a href="#">验证数据和故事</a></li><li><a href="#">训练模型</a></li><li><a href="#">测试助手</a></li><li><a href="#">比较NLU性能</a></li><li><a href="#">测试动作代码</a></li></ul></li><li><a href="#">持续部署（CD）</a><ul><li><a href="#">部署您的Rasa模型</a></li><li><a href="#">部署动作服务器</a></li></ul></li><li><a href="#">CI / CD管道示例</a></li></ul><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>持续集成（CI）是一种经常合并代码更改并在提交更改时自动对其进行测试的做法。连续部署（CD）意味着将集成的更改自动部署到登台或生产环境。它们一起使您可以对助手进行更频繁的改进，并有效地测试和部署这些更改。</p><p>本指南将涵盖<strong>哪些</strong>应该在CI / CD管道，具体到一个项目拉莎。<strong>如何</strong>实现该管道取决于您。那里有许多CI / CD工具，例如<a href="https://github.com/features/actions">GitHub Actions</a>， <a href="https://docs.gitlab.com/ee/ci/">GitLab CI / CD</a>，<a href="https://jenkins.io/doc/">Jenkins</a>和 <a href="https://circleci.com/docs/2.0/">CircleCI</a>。我们建议选择与您使用的任何Git存储库集成的工具。</p><h2 id="持续集成（CI）"><a href="#持续集成（CI）" class="headerlink" title="持续集成（CI）"></a>持续集成（CI）</h2><p>改善助手的最佳方法是频繁进行<a href="https://rasa.com/docs/rasa-x/user-guide/fix-problems">增量更新</a>。无论变化有多小，您都希望确保它不会带来新的问题或对助手的性能产生负面影响。</p><p>通常最好对合并/提取请求或提交运行CI检查。大多数测试足够快，可以在每次更改时运行。但是，仅当某些文件已更改或存在某些其他指示符时，您才可以选择运行更多的资源密集型测试。例如，如果您的代码托管在Github上，则只有在拉取请求具有特定标签的情况下，您才能进行测试运行（例如“需要NLU测试”）。</p><ul><li><a href="#">验证数据和stories</a></li><li><a href="#">训练模型</a></li><li><a href="#">测试助手</a></li><li><a href="#">比较NLU性能</a></li><li><a href="#">测试动作代码</a></li></ul><h3 id="验证数据和案例"><a href="#验证数据和案例" class="headerlink" title="验证数据和案例"></a>验证数据和案例</h3><p><a href="#">数据验证</a>可验证域文件，NLU数据或故事数据中是否没有错误或重大不一致。</p><pre><code>rasa data validate --fail-on-warnings --max-history &lt;max_history&gt;</code></pre><p>如果数据验证导致错误，则训练模型也将失败。通过包含该<code>--fail-on-warnings</code>标志，验证也将在不会出现问题的警告上失败，这些问题不会阻止训练模型，但可能指示混乱的数据，例如域中列出的未在任何故事中使用的操作。</p><p>数据验证包括<a href="#">故事结构验证</a>。故事验证检查您是否有任何故事，其中相同的对话历史记录中会出现不同的机器人操作。故事之间的冲突将阻止模型学习正确的对话模式。将<code>--max-history</code>参数设置<code>max_history</code>为中的备注策略的值<code>config.yml</code>。如果尚未设置，请使用默认值<code>5</code>。</p><h3 id="火车模型"><a href="#火车模型" class="headerlink" title="火车模型"></a>火车模型</h3><pre><code>rasa train</code></pre><p>训练模型可验证您的NLU管道和策略配置有效且可训练，并且提供了用于测试对话的模型。如果它通过了CI测试，那么 作为持续部署过程的一部分，您还<a href="#">可以将经过训练的模型上传</a>到服务器。</p><h3 id="测试助手"><a href="#测试助手" class="headerlink" title="测试助手"></a>测试助手</h3><p>在<a href="#">测试对话中</a>测试训练有素的模型是对助手在某些情况下的行为有信心的最佳方法。这些以修改后的故事格式编写的故事，使您可以提供整个对话，并测试在给出此用户输入的情况下您的模型将以预期的方式运行。当您开始从用户对话中介绍更复杂的故事时，这一点尤其重要。</p><pre><code>rasa test --stories tests/conversation_tests.md --fail-on-prediction-errors</code></pre><p>该<code>--fail-on-prediction-errors</code>标志确保如果任何测试对话失败，则测试将失败。</p><p>端到端测试仅与您所包括的测试用例一样透彻和准确，因此，在对助手进行改进时，应继续增加测试对话集。遵循的一个很好的经验法则是，您应该以测试对话为目标，以代表真实对话的真实分布。Rasa X使得<a href="#">基于真实对话添加测试对话</a>变得容易。</p><p>注：终端到终端的测试并<strong>没有</strong>执行你的操作代码。您将需要 在单独的步骤中<a href="#">测试操作代码</a>。</p><h3 id="比较NLU性能"><a href="#比较NLU性能" class="headerlink" title="比较NLU性能"></a>比较NLU性能</h3><p>如果您对NLU训练数据进行了重大更改（例如，将一个意图分为两个意图或添加了很多训练示例），则应运行 <a href="#">完整的NLU评估</a>。您将需要比较没有更改的NLU模型的性能与更改。</p><p>您可以通过在交叉验证模式下运行NLU测试来做到这一点：</p><pre><code>rasa test nlu --cross-validation</code></pre><p>您还可以在训练集上训练模型并在测试集上进行测试。如果您使用训练测试集方法，则最好使用此CI步骤的一部分对<a href="#">数据</a>进行<a href="#">改组和拆分</a>，而不是使用静态NLU测试集，因为后者很容易过时。<code>rasa data split</code></p><p>因为此测试不会导致通过/失败退出代码，所以最好使结果可见，以便您可以解释它们。例如，<a href="#">此工作流程</a> 包括使用结果表注释PR，该结果表显示哪些意图与其他意图混淆。</p><p>由于NLU比较可能是相当耗费资源的测试，因此您可以选择仅在满足某些条件时才运行此测试。条件可能包括存在手动标签（例如，“需要进行NLU测试”），对NLU数据的更改或对NLU管道的更改。</p><h3 id="测试操作代码"><a href="#测试操作代码" class="headerlink" title="测试操作代码"></a>测试操作代码</h3><p>用于测试您的动作代码的方法将取决于其实现方式。例如，如果您连接到外部API，建议编写单元测试以确保这些API对常见输入的响应达到预期。无论您测试动作代码如何，都应将这些测试包括在CI管道中，以便每次进行更改时都可以运行。</p><h2 id="持续部署（CD）"><a href="#持续部署（CD）" class="headerlink" title="持续部署（CD）"></a>持续部署（CD）</h2><p>为了经常向用户提供改进，您将需要尽可能多的自动化部署过程。</p><p>一旦CI检查成功，CD步骤通常在推送或合并到某个分支上运行。</p><ul><li><a href="#">部署您的Rasa模型</a></li><li><a href="#">部署动作服务器</a></li></ul><h3 id="部署您的Rasa模型"><a href="#部署您的Rasa模型" class="headerlink" title="部署您的Rasa模型"></a>部署您的Rasa模型</h3><p>如果您在CI管道中运行<a href="#">端到端测试</a>，那么您将已经拥有训练有素的模型。如果CI结果令人满意，您可以设置CD管道以将训练好的模型上传到Rasa服务器。例如，要将模型上传到Rasa X：</p><pre><code>curl -k -F "model=@models/my_model.tar.gz" "https://example.rasa.com/api/projects/default/models?api_token=&amp;#123;your_api_token&amp;#125;"</code></pre><p>如果您使用的是Rasa X，还可以<a href="#">将上载的模型标记</a> 为<code>active</code>（如果使用多个<a href="#">部署环境</a>，则可以<a href="#">标记</a>为哪个部署）：</p><pre><code>curl -X PUT "https://example.rasa.com/api/projects/default/models/my_model/tags/active"</code></pre><p>但是，如果你的更新包括更改到这两个模型和你的操作代码，而这些变化以任何方式依赖于对方，你应该<strong>不会</strong> 自动标记模型<code>production</code>。首先，您将需要构建和部署更新的动作服务器，以便新模型不会（例如）调用更新前动作服务器中不存在的动作。</p><h3 id="部署操作服务器"><a href="#部署操作服务器" class="headerlink" title="部署操作服务器"></a>部署操作服务器</h3><p>您可以自动 <a href="#">构建动作服务器的新映像并将其上传</a>到映像存储库，以更新动作代码。如上所述，如果动作服务器与当前生产模型不兼容，则在将新的图像标签自动部署到生产时应格外小心。</p><h2 id="CI-CD管道示例"><a href="#CI-CD管道示例" class="headerlink" title="CI / CD管道示例"></a>CI / CD管道示例</h2><p>作为示例，请参见<a href="https://github.com/RasaHQ/rasa-demo/blob/master/.github/workflows/build_and_deploy.yml">Sara</a>的CI / CD管道 ，您可以在Rasa Docs中与之交谈的Rasa助手以及 <a href="https://github.com/RasaHQ/carbon-assistant/blob/master/.github/workflows/model_ci.yml">Carbon Bot</a>。两者都使用<a href="https://github.com/features/actions">Github Actions</a>作为CI / CD工具。</p><p>这些示例只是众多可能性中的两个。如果您喜欢设置CI / CD，请在<a href="https://forum.rasa.com/">论坛</a>上与Rasa社区共享。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> nlp </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Rasa </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Rasa文档指南(5)</title>
      <link href="/2020/10/03/rasa-wen-dang-zhi-nan-5/"/>
      <url>/2020/10/03/rasa-wen-dang-zhi-nan-5/</url>
      
        <content type="html"><![CDATA[<h1 id="测试您的助手"><a href="#测试您的助手" class="headerlink" title="测试您的助手"></a>测试您的助手</h1><ul><li><a href="#">端到端测试</a></li><li><a href="#">评估NLU模型</a><ul><li><a href="#">比较NLU管道</a></li><li><a href="#">意图分类</a></li><li><a href="#">响应选择</a></li><li><a href="#">实体提取</a></li><li><a href="#">实体计分</a></li></ul></li><li><a href="#">评估核心模型</a></li><li><a href="#">比较核心配置</a></li></ul><blockquote><p>注意</p><p>如果您想调整NLU模型的超参数，请查看本<a href="#">教程</a>。</p></blockquote><h2 id="端到端测试"><a href="#端到端测试" class="headerlink" title="端到端测试"></a>端到端测试</h2><p>Rasa Open Source使您可以通过运行测试对话并确保NLU和Core做出正确的预测来端对端地测试对话。</p><p>为此，您需要一些端到端格式的故事，其中包括NLU输出和原始文本。这里有些例子：</p><pre><code>基本## A basic end-to-end test* greet: hello   - utter_ask_howcanhelp* inform: show me [chinese](cuisine) restaurants   - utter_ask_location* inform: in [Paris](location)   - utter_ask_price   自定义动作   ## End-to-End tests where a custom action appends events* greet: hi    - my_custom_action    &lt;!-- The following events are emitted by `my_custom_action` --&gt;    - slot&amp;#123;"my_slot": "value added by custom action"&amp;#125;    - utter_ask_age* thankyou: thanks    - utter_noworriesForms Happy Path    ## Testing a conversation with a form* greet: hi    - utter_greet* request_restaurant: im looking for a restaurant    - restaurant_form    - form&amp;#123;"name": "restaurant_form"&amp;#125;* inform: [afghan](cuisine) food    - form: restaurant_form    - form&amp;#123;"name": null&amp;#125;    - utter_slots_values* thankyou: thanks    - utter_noworries    Forms Unhappy Path    ## Testing a conversation with a form and unexpected user input* greet: hi    - utter_greet* request_restaurant: im looking for a restaurant    - restaurant_form    - form&amp;#123;"name": "restaurant_form"&amp;#125;&lt;!-- The user sends a message which should not be handled by the form. --&gt;* chitchat: can you share your boss with me?    - utter_chitchat    - restaurant_form    - form&amp;#123;"name": null&amp;#125;    - utter_slots_values* thankyou: thanks    - utter_noworries</code></pre><p>默认情况下，Rasa Open Source将会话测试保存到<code>tests/conversation_tests.md</code>。您可以通过运行以下命令来对您的助手进行测试：</p><pre><code>rasa test</code></pre><blockquote><p>注意</p><p><a href="#">自定义操作</a>不会作为端到端测试的一部分执行。如果您的自定义操作将任何事件附加到跟踪器，则必须在端到端测试中反映出来（例如，通过将<code>slot</code>事件添加到端到端故事中）。</p></blockquote><p>如果您有任何疑问或问题，请<a href="#">在我们论坛</a>的专用<a href="#">测试部分</a>与我们分享 ！</p><blockquote><p>注意</p><p>请在确保您的模型文件<code>models</code>的组合<code>core</code> 和<code>nlu</code>型号。如果不包含NLU模型，则Core将使用default <code>RegexInterpreter</code>。</p></blockquote><h2 id="评估一个NLU模型"><a href="#评估一个NLU模型" class="headerlink" title="评估一个NLU模型"></a>评估一个NLU模型</h2><p>机器学习的标准技术是将一些数据分开作为<em>测试集</em>。您可以使用以下方法<a href="#">将NLU训练数据</a> 分为训练集和测试集：</p><pre><code>rasa data split nlu</code></pre><p>如果完成了此操作，则可以使用以下命令查看NLU模型对测试用例的预测程度：</p><pre><code>rasa test nlu -u train_test_split/test_data.md --model models/nlu-20180323-145833.tar.gz</code></pre><p>如果您不想创建单独的测试集，则仍可以使用交叉验证来估计模型的概括程度。为此，添加标志<code>--cross-validation</code>：</p><pre><code>rasa test nlu -u data/nlu.md --config config.yml --cross-validation</code></pre><p>脚本选项的完整列表为：</p><pre class=" language-shell"><code class="language-shell">usage: rasa test nlu [-h] [-v] [-vv] [--quiet] [-m MODEL] [-u NLU] [--out OUT]                     [--successes] [--no-errors] [--histogram HISTOGRAM]                     [--confmat CONFMAT] [-c CONFIG [CONFIG ...]]                     [--cross-validation] [-f FOLDS] [-r RUNS]                     [-p PERCENTAGES [PERCENTAGES ...]] [--no-plot]optional arguments:  -h, --help            show this help message and exit  -m MODEL, --model MODEL                        Path to a trained Rasa model. If a directory is                        specified, it will use the latest model in this                        directory. (default: models)  -u NLU, --nlu NLU     File or folder containing your NLU data. (default:                        data)  --out OUT             Output path for any files created during the                        evaluation. (default: results)  --successes           If set successful predictions (intent and entities)                        will be written to a file. (default: False)  --no-errors           If set incorrect predictions (intent and entities)                        will NOT be written to a file. (default: False)  --histogram HISTOGRAM                        Output path for the confidence histogram. (default:                        hist.png)  --confmat CONFMAT     Output path for the confusion matrix plot. (default:                        confmat.png)  -c CONFIG [CONFIG ...], --config CONFIG [CONFIG ...]                        Model configuration file. If a single file is passed                        and cross validation mode is chosen, cross-validation                        is performed, if multiple configs or a folder of                        configs are passed, models will be trained and                        compared directly. (default: None)  --no-plot             Don't render evaluation plots (default: False)Python Logging Options:  -v, --verbose         Be verbose. Sets logging level to INFO. (default:                        None)  -vv, --debug          Print lots of debugging statements. Sets logging level                        to DEBUG. (default: None)  --quiet               Be quiet! Sets logging level to WARNING. (default:                        None)Cross Validation:  --cross-validation    Switch on cross validation mode. Any provided model                        will be ignored. (default: False)  -f FOLDS, --folds FOLDS                        Number of cross validation folds (cross validation                        only). (default: 5)Comparison Mode:  -r RUNS, --runs RUNS  Number of comparison runs to make. (default: 3)  -p PERCENTAGES [PERCENTAGES ...], --percentages PERCENTAGES [PERCENTAGES ...]                        Percentages of training data to exclude during                        comparison. (default: [0, 25, 50, 75])</code></pre><h3 id="比较NLU管道"><a href="#比较NLU管道" class="headerlink" title="比较NLU管道"></a>比较NLU管道</h3><p>通过将多个管道配置（或包含它们的文件夹）传递到CLI，Rasa将在管道之间进行比较检查。</p><pre class=" language-yaml"><code class="language-yaml"> rasa test nlu <span class="token punctuation">-</span><span class="token punctuation">-</span>config pretrained_embeddings_spacy.yml supervised_embeddings.yml  <span class="token punctuation">-</span><span class="token punctuation">-</span>nlu data/nlu.md <span class="token punctuation">-</span><span class="token punctuation">-</span>runs 3 <span class="token punctuation">-</span><span class="token punctuation">-</span>percentages 0 25 50 70 90</code></pre><p>上面示例中的命令将根据您的数据创建训练/测试拆分，然后使用训练集中排除的意图数据的0％，25％，50％，70％和90％多次训练每个管道。然后在测试集上评估模型，并记录每个排除百分比的f1得分。此过程运行了3次（即总共有3个测试集），然后使用f1得分的平均值和标准偏差绘制了图表。</p><p>f1得分图以及所有训练/测试集，训练有素的模型，分类和错误报告将保存到名为的文件夹中<code>nlu_comparison_results</code>。</p><h3 id="意向分类"><a href="#意向分类" class="headerlink" title="意向分类"></a>意向分类</h3><p>评估脚本将为您的模型生成报告，混淆矩阵和置信度直方图。</p><p>该报告记录每个意图和实体的精度，召回率和f1度量，并提供总体平均值。您可以使用<code>--report</code>参数将这些报告另存为JSON文件。</p><p>混淆矩阵向您显示哪些意图被误认为其他意图；任何被错误预测的样本都会被记录并保存到一个文件中<code>errors.json</code>，以便于调试。</p><p>脚本生成的直方图使您可以可视化所有预测的置信度分布，正确和不正确的预测量分别由蓝色和红色条显示。提高训练数据的质量会将蓝色直方图条移到该图的右侧，将红色直方图条移至该图的左侧。</p><p>警告</p><p>如果您的任何实体注释不正确，则评估可能会失败。一个常见的问题是实体无法在令牌内部停止或启动。例如，如果您有一个<code>name</code>类似的实体的示例，则仅在您的令牌生成器拆分为多个令牌时才有效。在这种情况下，空白标记器将不起作用。<code>[Brian](name)'s house``Brian's</code></p><h3 id="响应选择"><a href="#响应选择" class="headerlink" title="响应选择"></a>响应选择</h3><p>评估脚本将为管道中的所有响应选择器模型生成组合报告。</p><p>该报告记录每个响应的精度，召回率和f1度量，并提供总体平均值。您可以使用<code>--report</code>参数将这些报告另存为JSON文件。</p><h3 id="实体提取"><a href="#实体提取" class="headerlink" title="实体提取"></a>实体提取</h3><p>，这<code>CRFEntityExtractor</code>是您使用自己的数据训练的唯一实体提取器，因此也是唯一要评估的实体提取器。如果您使用spaCy或小鸭经过预训练的实体提取器，则Rasa NLU将不会在评估中包括这些提取器。</p><p>Rasa NLU将报告受<code>CRFEntityExtractor</code>训练识别的每种实体类型的召回率，精度和f1度量 。</p><h3 id="实体计分"><a href="#实体计分" class="headerlink" title="实体计分"></a>实体计分</h3><p>为了评估实体提取，我们应用了一种基于标签的简单方法。我们不考虑BILOU标签，而仅考虑每个令牌基础上的实体类型标签。对于像“ Alexanderplatz附近”这样的位置实体，我们希望使用标签而不是基于BILOU的标签。对于评估，我们的方法更为宽容，因为它奖励部分提取，并且不会惩罚实体的分裂。例如，给定前述实体“ Alexanderplatz附近”和提取“ Alexanderplatz”的系统，我们的方法将奖励提取“ Alexanderplatz”并惩罚丢失的单词“ near”。但是，基于BILOU的方法会将其标记为完全失败，因为它希望将Alexanderplatz标记为实体（）中的最后一个标记，而不是单个标记实体（<code>LOC LOC``B-LOC L-LOC``L-LOC``U-LOC</code>）。还要注意，将“ near”和“ Alexanderplatz”分开提取将在我们的方法中获得满分，而在基于BILOU的方法中获得零分。</p><p>这是短语“near Alexanderplatz tonight”的两种评分机制之间的比较：</p><table><thead><tr><th align="left">extracted</th><th align="left">Simple tags (score)</th><th align="left">BILOU tags (score)</th></tr></thead><tbody><tr><td align="left"><a href="loc">near Alexanderplatz</a> <a href="time">tonight</a></td><td align="left">loc loc time (3)</td><td align="left">B-loc L-loc U-time (3)</td></tr><tr><td align="left"><a href="loc">near</a> <a href="loc">Alexanderplatz</a> <a href="time">tonight</a></td><td align="left">loc loc time (3)</td><td align="left">U-loc U-loc U-time (1)</td></tr><tr><td align="left">near <a href="loc">Alexanderplatz</a> <a href="time">tonight</a></td><td align="left">O loc time (2)</td><td align="left">O U-loc U-time (1)</td></tr><tr><td align="left"><a href="loc">near</a> Alexanderplatz <a href="time">tonight</a></td><td align="left">loc O time (2)</td><td align="left">U-loc O U-time (1)</td></tr><tr><td align="left"><a href="loc">near Alexanderplatz tonight</a></td><td align="left">loc loc loc (2)</td><td align="left">B-loc I-loc L-loc (1)</td></tr></tbody></table><h3 id="评估核心模型"><a href="#评估核心模型" class="headerlink" title="评估核心模型"></a>评估核心模型</h3><p>您可以使用评估脚本在一组测试案例中评估训练有素的模型：</p><pre><code>rasa test core --stories test_stories.md --out results</code></pre><p>这会将失败的故事打印到<code>results/failed_stories.md</code>。如果至少有一项行动被错误地预测，我们会将任何故事视为失败。</p><p>此外，这会将混乱矩阵保存到名为的文件中 <code>results/story_confmat.pdf</code>。对于您域中的每个动作，混淆矩阵显示了正确预测该动作的频率以及相反地预测了一个不正确动作的频率。</p><p>脚本选项的完整列表为：</p><pre class=" language-shell"><code class="language-shell">usage: rasa test core [-h] [-v] [-vv] [--quiet] [-m MODEL [MODEL ...]]                      [-s STORIES] [--max-stories MAX_STORIES] [--out OUT]                      [--e2e] [--endpoints ENDPOINTS]                      [--fail-on-prediction-errors] [--url URL]                      [--evaluate-model-directory] [--no-plot]optional arguments:  -h, --help            show this help message and exit  -m MODEL [MODEL ...], --model MODEL [MODEL ...]                        Path to a pre-trained model. If it is a 'tar.gz' file                        that model file will be used. If it is a directory,                        the latest model in that directory will be used                        (exception: '--evaluate-model-directory' flag is set).                        If multiple 'tar.gz' files are provided, all those                        models will be compared. (default: [None])  -s STORIES, --stories STORIES                        File or folder containing your test stories. (default:                        data)  --max-stories MAX_STORIES                        Maximum number of stories to test on. (default: None)  --out OUT             Output path for any files created during the                        evaluation. (default: results)  --e2e, --end-to-end   Run an end-to-end evaluation for combined action and                        intent prediction. Requires a story file in end-to-end                        format. (default: False)  --endpoints ENDPOINTS                        Configuration file for the connectors as a yml file.                        (default: None)  --fail-on-prediction-errors                        If a prediction error is encountered, an exception is                        thrown. This can be used to validate stories during                        tests, e.g. on travis. (default: False)  --url URL             If supplied, downloads a story file from a URL and                        trains on it. Fetches the data by sending a GET                        request to the supplied URL. (default: None)  --evaluate-model-directory                        Should be set to evaluate models trained via 'rasa                        train core --config <config-1> <config-2>'. All models                        in the provided directory are evaluated and compared                        against each other. (default: False)  --no-plot             Don't render evaluation plots (default: False)Python Logging Options:  -v, --verbose         Be verbose. Sets logging level to INFO. (default:                        None)  -vv, --debug          Print lots of debugging statements. Sets logging level                        to DEBUG. (default: None)  --quiet               Be quiet! Sets logging level to WARNING. (default:                        None)</code></pre><h3 id="比较核心的配置"><a href="#比较核心的配置" class="headerlink" title="比较核心的配置"></a>比较核心的配置</h3><p>要为您的核心模型选择配置，或为特定策略选择超参数，您想衡量Rasa Core将其推广 到以前从未见过的对话的程度。特别是在项目开始时，您没有太多真正的对话可用来训练您的机器人，因此您不只是想扔掉一些东西来用作测试集。</p><p>Rasa Core具有一些脚本来帮助您选择和调整策略配置。对它感到满意后，就可以在整个数据集上训练最终配置。为此，您首先必须针对不同的配置训练模型。创建两个（或更多）配置文件，包括要比较的策略，然后使用<code>compare</code>训练脚本的模式训练模型：</p><pre><code> rasa train core -c config_1.yml config_2.yml \  -d domain.yml -s stories_folder --out comparison_models --runs 3 \  --percentages 0 5 25 50 70 95</code></pre><p>对于提供的每种策略配置，Rasa Core将接受多次培训，培训数据中不包括0、5、25、50、70和95％的培训案例。进行多次运行以确保结果一致。</p><p>该脚本完成后，您可以在<code>compare</code> 模式下使用评估脚本来评估您刚刚训练的模型：</p><pre><code>rasa test core -m comparison_models --stories stories_folder--out comparison_results --evaluate-model-directory</code></pre><p>这将评估所提供故事中的每个模型（可以是训练集，也可以是测试集），并绘制一些图表以向您显示哪种策略效果最佳。通过评估整个故事，您可以衡量Rasa Core对预言故事的预测程度。</p><p>要比较单个策略，请创建每个仅包含一个策略的配置文件。如果您不确定要比较的策略，我们建议您试用 <code>EmbeddingPolicy</code>和<code>KerasPolicy</code>，看看哪种策略更适合您。</p><blockquote><p>注意</p><p>这个训练过程可能会花费很长时间，因此我们建议让它在不中断的后台运行。</p></blockquote><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> nlp </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Rasa </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Rasa文档指南(4)</title>
      <link href="/2020/10/03/rasa-wen-dang-zhi-nan-4/"/>
      <url>/2020/10/03/rasa-wen-dang-zhi-nan-4/</url>
      
        <content type="html"><![CDATA[<h1 id="消息和语音通道"><a href="#消息和语音通道" class="headerlink" title="消息和语音通道"></a>消息和语音通道</h1><p>要使您的助手在消息传递平台上可用，您需要在<code>credentials.yml</code>文件中提供凭据。运行时会创建一个示例文件，因此最容易编辑该文件并在其中添加凭据。这是一个使用Facebook凭证的示例：<code>rasa init</code></p><pre><code>facebook:  verify: "rasa-bot"  secret: "3e34709d01ea89032asdebfe5a74518"  page-access-token: "EAAbHPa7H9rEBAAuFk4Q3gPKbDedQnx4djJJ1JmQ7CAqO4iJKrQcNT0wtD"</code></pre><h2 id="使用Ngrok在本地计算机上测试通道"><a href="#使用Ngrok在本地计算机上测试通道" class="headerlink" title="使用Ngrok在本地计算机上测试通道"></a>使用Ngrok在本地计算机上测试通道</h2><p>您可以使用<a href="https://ngrok.com/">ngrok</a>建立与本地计算机的连接，该连接可以在Internet上公开获得。在服务器上运行Rasa时不需要此功能，因为您可以设置一个域名来指向该服务器的IP地址，也可以使用IP地址本身。</p><p>安装ngrok之后，运行：</p><pre class=" language-shell"><code class="language-shell">ngrok http 5005; rasa run</code></pre><p>您的Webhook地址如下所示：</p><ul><li><code>https://yyyyyy.ngrok.io/webhooks/&lt;CHANNEL&gt;/webhook</code>, e.g.</li><li><code>https://yyyyyy.ngrok.io/webhooks/facebook/webhook</code></li></ul><blockquote><p>警告</p><p>使用ngrok的免费层，您会遇到每分钟可以建立多少个连接的限制。撰写本文时，它设置为40个连接/分钟。</p></blockquote><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> nlp </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Rasa </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Rasa文档指南(3)</title>
      <link href="/2020/10/03/rasa-wen-dang-zhi-nan-3/"/>
      <url>/2020/10/03/rasa-wen-dang-zhi-nan-3/</url>
      
        <content type="html"><![CDATA[<h1 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h1><h2 id="消息处理"><a href="#消息处理" class="headerlink" title="消息处理"></a>消息处理</h2><p>此图显示了使用Rasa构建的助手如何响应消息的基本步骤：</p><p><img src="/2020/10/03/rasa-wen-dang-zhi-nan-3/8.png"></p><p>这些步骤是：</p><ol><li>接收到该消息<code>Interpreter</code>并将其传递给，该消息会将其转换为字典，包括原始文本，意图和找到的任何实体。这部分由NLU处理。</li><li>该<code>Tracker</code>是跟踪通话状态的对象。它接收到有新消息进入的信息。</li><li>该策略接收跟踪器的当前状态。</li><li>该策略选择下一步要采取的措施。</li><li>所选动作由跟踪器记录。</li><li>响应发送给用户。</li></ol><blockquote><p>注意</p><p>消息可以是人类键入的文本，也可以是结构化的输入，例如按下按钮。</p></blockquote><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> nlp </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Rasa </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Rasa文档指南(2)</title>
      <link href="/2020/10/03/rasa-wen-dang-zhi-nan-2/"/>
      <url>/2020/10/03/rasa-wen-dang-zhi-nan-2/</url>
      
        <content type="html"><![CDATA[<h1 id="Rasa文档指南"><a href="#Rasa文档指南" class="headerlink" title="Rasa文档指南"></a>Rasa文档指南</h1><h2 id="命令行接口"><a href="#命令行接口" class="headerlink" title="命令行接口"></a>命令行接口</h2><p><a href="#">备忘单</a></p><p><a href="#">创建一个新项目</a></p><p><a href="#">训练模型</a></p><p><a href="#">互动学习</a></p><p><a href="#">与您的助手交谈</a></p><p><a href="#">启动服务器</a></p><p><a href="#">启动动作服务器</a></p><p><a href="#">可视化您的故事</a></p><p><a href="#">评估测试数据模型</a></p><p><a href="#">创建火车测试拆分</a></p><p><a href="#">在Markdown和JSON之间转换数据</a></p><p><a href="#">将对话导出到事件代理</a></p><p><a href="#">启动Rasa X</a></p><h2 id="备忘单"><a href="#备忘单" class="headerlink" title="备忘单"></a>备忘单</h2><p>命令行界面（CLI）为您提供了易于记忆的常见任务命令。</p><table><thead><tr><th align="left">命令</th><th align="left">影响</th></tr></thead><tbody><tr><td align="left"><code>rasa init</code></td><td align="left">使用示例训练数据，操作和配置文件创建一个新项目。</td></tr><tr><td align="left"><code>rasa train</code></td><td align="left">使用您的NLU数据和故事来训练模型，并将训练后的模型保存在中<code>./models</code>。</td></tr><tr><td align="left"><code>rasa interactive</code></td><td align="left">开始一个交互式学习会话，以通过聊天创建新的训练数据。</td></tr><tr><td align="left"><code>rasa shell</code></td><td align="left">加载您训练有素的模型，并允许您在命令行上与助手交谈。</td></tr><tr><td align="left"><code>rasa run</code></td><td align="left">使用训练有素的模型启动Rasa服务器。有关详细信息，请参见<a href="#">配置HTTP API</a>文档。</td></tr><tr><td align="left"><code>rasa run actions</code></td><td align="left">使用Rasa SDK启动动作服务器。</td></tr><tr><td align="left"><code>rasa visualize</code></td><td align="left">可视化故事。</td></tr><tr><td align="left"><code>rasa test</code></td><td align="left">使用您的测试NLU数据和故事测试经过训练的Rasa模型。</td></tr><tr><td align="left"><code>rasa data split nlu</code></td><td align="left">根据指定的百分比对NLU数据进行拆分。</td></tr><tr><td align="left"><code>rasa data convert nlu</code></td><td align="left">在不同格式之间转换NLU训练数据。</td></tr><tr><td align="left"><code>rasa export</code></td><td align="left">将对话从跟踪商店存储到事件代理。</td></tr><tr><td align="left"><code>rasa x</code></td><td align="left">在本地启动RasaX。</td></tr><tr><td align="left"><code>rasa -h</code></td><td align="left">显示所有可用命令。</td></tr></tbody></table><h2 id="创建一个新的项目"><a href="#创建一个新的项目" class="headerlink" title="创建一个新的项目"></a>创建一个新的项目</h2><p>单个命令将为您提供一些示例培训数据的完整项目。</p><pre class=" language-shell"><code class="language-shell">rasa init</code></pre><p>这将创建以下文件：</p><pre class=" language-shell"><code class="language-shell">.├── __init__.py├── actions.py├── config.yml├── credentials.yml├── data│   ├── nlu.md│   └── stories.md├── domain.yml├── endpoints.yml└── models    └── <timestamp>.tar.gz</code></pre><p>该命令将询问您是否要使用此数据训练初始模型。如果回答“否”，则目录将为空。<code>rasa init  models</code></p><p>通过此项目设置，常用命令非常容易记住。要训练模型，请键入，以在命令行上与模型对话，以测试模型类型。<code>rasa train  rasa shell rasa test</code></p><h2 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h2><p>主要命令是：</p><pre><code>rasa train</code></pre><p>此命令训练一个结合了Rasa NLU和Rasa Core模型的Rasa模型。如果您只想训练NLU或Core模型，则可以运行或。但是，如果训练数据和配置没有更改，Rasa将自动跳过训练核心或NLU。<code>rasa train nlu``rasa train core</code></p><p><code>rasa train</code>会将训练后的模型存储在定义的目录中<code>--out</code>。默认情况下，模型名称为<code>&lt;timestamp&gt;.tar.gz</code>。如果要用不同的名称命名模型，可以使用来指定名称<code>--fixed-model-name</code>。</p><p>以下参数可用于配置培训过程：</p><pre class=" language-shell"><code class="language-shell">usage: rasa train [-h] [-v] [-vv] [--quiet] [--data DATA [DATA ...]]                  [-c CONFIG] [-d DOMAIN] [--out OUT]                  [--augmentation AUGMENTATION] [--debug-plots]                  [--fixed-model-name FIXED_MODEL_NAME] [--persist-nlu-data]                  [--force]                  &#123;core,nlu&#125; ...positional arguments:  &#123;core,nlu&#125;    core                Trains a Rasa Core model using your stories.    nlu                 Trains a Rasa NLU model using your NLU data.optional arguments:  -h, --help            show this help message and exit  --data DATA [DATA ...]                        Paths to the Core and NLU data files. (default:                        ['data'])  -c CONFIG, --config CONFIG                        The policy and NLU pipeline configuration of your bot.                        (default: config.yml)  -d DOMAIN, --domain DOMAIN                        Domain specification (yml file). (default: domain.yml)  --out OUT             Directory where your models should be stored.                        (default: models)  --augmentation AUGMENTATION                        How much data augmentation to use during training.                        (default: 50)  --debug-plots         If enabled, will create plots showing checkpoints and                        their connections between story blocks in a file                        called `story_blocks_connections.html`. (default:                        False)  --fixed-model-name FIXED_MODEL_NAME                        If set, the name of the model file/directory will be                        set to the given name. (default: None)  --persist-nlu-data    Persist the nlu training data in the saved model.                        (default: False)  --force               Force a model training even if the data has not                        changed. (default: False)Python Logging Options:  -v, --verbose         Be verbose. Sets logging level to INFO. (default:  None)  -vv, --debug          Print lots of debugging statements. Sets logging level                        to DEBUG. (default: None)  --quiet               Be quiet! Sets logging level to WARNING. (default:                        None)</code></pre><blockquote><p>注意</p><p>使用训练模型时，确保存在Core和NLU的训练数据。如果仅提供一种模型类型的训练数据，该命令将自动退回到 或取决于提供的训练文件。<code>rasa train  rasa train nlu  rasa train core</code></p></blockquote><h2 id="交互学习"><a href="#交互学习" class="headerlink" title="交互学习"></a>交互学习</h2><p>要与您的助手开始交互式学习会话，请运行</p><pre class=" language-shell"><code class="language-shell">rasa interactive</code></pre><p>如果您使用<code>--model</code>参数提供训练有素的模型，则交互式学习过程将从提供的模型开始。如果未指定任何模型，则在没有其他目录传递给该 标志的情况下，将使用位于其中的数据来训练新的Rasa模型。在训练了初始模型之后，交互式学习会话开始。如果训练数据和配置未更改，则将跳过训练。<code>rasa interactive``data/``--data</code></p><p>可以设置的参数的完整列表为：<code>rasa interactive</code></p><pre class=" language-shell"><code class="language-shell">usage: rasa interactive [-h] [-v] [-vv] [--quiet] [--e2e] [-m MODEL]                        [--data DATA [DATA ...]] [--skip-visualization]                        [--conversation-id CONVERSATION_ID]                        [--endpoints ENDPOINTS] [-c CONFIG] [-d DOMAIN]                        [--out OUT] [--augmentation AUGMENTATION]                        [--debug-plots] [--force] [--persist-nlu-data]                        &#123;core&#125; ... [model-as-positional-argument]positional arguments:  &#123;core&#125;    core                Starts an interactive learning session model to create                        new training data for a Rasa Core model by chatting.                        Uses the 'RegexInterpreter', i.e. `/<intent>` input                        format.  model-as-positional-argument                        Path to a trained Rasa model. If a directory is                        specified, it will use the latest model in this                        directory. (default: None)optional arguments:  -h, --help            show this help message and exit  --e2e                 Save story files in e2e format. In this format user                        messages will be included in the stories. (default:                        False)  -m MODEL, --model MODEL                        Path to a trained Rasa model. If a directory is                        specified, it will use the latest model in this                        directory. (default: None)  --data DATA [DATA ...]                        Paths to the Core and NLU data files. (default:                        ['data'])  --skip-visualization  Disable plotting the visualization during interactive                        learning. (default: False)  --conversation-id CONVERSATION_ID                        Specify the id of the conversation the messages are                        in. Defaults to a UUID that will be randomly                        generated. (default: 39156e0e50ca4ac2993dd943a540a3da)  --endpoints ENDPOINTS                        Configuration file for the model server and the                        connectors as a yml file. (default: None)Python Logging Options:  -v, --verbose         Be verbose. Sets logging level to INFO. (default:                        None)  -vv, --debug          Print lots of debugging statements. Sets logging level                        to DEBUG. (default: None)  --quiet               Be quiet! Sets logging level to WARNING. (default:                        None)Train Arguments:  -c CONFIG, --config CONFIG                        The policy and NLU pipeline configuration of your bot.                        (default: config.yml)  -d DOMAIN, --domain DOMAIN                        Domain specification (yml file). (default: domain.yml)  --out OUT             Directory where your models should be stored.                        (default: models)  --augmentation AUGMENTATION                        How much data augmentation to use during training.                        (default: 50)  --debug-plots         If enabled, will create plots showing checkpoints and                        their connections between story blocks in a file                        called `story_blocks_connections.html`. (default:                        False)  --force               Force a model training even if the data has not                        changed. (default: False)  --persist-nlu-data    Persist the nlu training data in the saved model.                        (default: False)</code></pre><h2 id="与您的助手交谈"><a href="#与您的助手交谈" class="headerlink" title="与您的助手交谈"></a>与您的助手交谈</h2><p>要在命令行上与您的助手开始聊天会话，请运行：</p><pre><code>rasa shell</code></pre><p>可与bot互动的模型可以通过指定<code>--model</code>。如果您使用仅NLU模型启动外壳程序，则可以获取在命令行上键入的任何文本的意图和实体。如果您的模型包括训练有素的Core模型，则可以与您的机器人聊天，并查看该机器人下一步的预测。如果您已经训练了一个组合的Rasa模型，但是仍然想查看模型从文本中提取的意图和实体，则可以使用命令。<code>rasa shell``rasa shell nlu</code></p><p>要提高调试的日志记录级别，请运行：</p><pre><code>rasa shell --debug</code></pre><p>选项的完整列表是<code>rasa shell</code></p><pre class=" language-shell"><code class="language-shell">usage: rasa shell [-h] [-v] [-vv] [--quiet]                  [--conversation-id CONVERSATION_ID] [-m MODEL]                  [--log-file LOG_FILE] [--endpoints ENDPOINTS] [-p PORT]                  [-t AUTH_TOKEN] [--cors [CORS [CORS ...]]] [--enable-api]                  [--response-timeout RESPONSE_TIMEOUT]                  [--remote-storage REMOTE_STORAGE]                  [--ssl-certificate SSL_CERTIFICATE]                  [--ssl-keyfile SSL_KEYFILE] [--ssl-ca-file SSL_CA_FILE]                  [--ssl-password SSL_PASSWORD] [--credentials CREDENTIALS]                  [--connector CONNECTOR] [--jwt-secret JWT_SECRET]                  [--jwt-method JWT_METHOD]                  &#123;nlu&#125; ... [model-as-positional-argument]positional arguments:  &#123;nlu&#125;    nlu                 Interprets messages on the command line using your NLU                        model.  model-as-positional-argument                        Path to a trained Rasa model. If a directory is                        specified, it will use the latest model in this                        directory. (default: None)optional arguments:  -h, --help            show this help message and exit  --conversation-id CONVERSATION_ID                        Set the conversation ID. (default:                        71b04d860ac8469c9d97c6b68226019a)  -m MODEL, --model MODEL                        Path to a trained Rasa model. If a directory is                        specified, it will use the latest model in this                        directory. (default: models)  --log-file LOG_FILE   Store logs in specified file. (default: None)  --endpoints ENDPOINTS                        Configuration file for the model server and the                        connectors as a yml file. (default: None)Python Logging Options:  -v, --verbose         Be verbose. Sets logging level to INFO. (default:                        None)  -vv, --debug          Print lots of debugging statements. Sets logging level                        to DEBUG. (default: None)  --quiet               Be quiet! Sets logging level to WARNING. (default:                        None)Server Settings:  -p PORT, --port PORT  Port to run the server at. (default: 5005)  -t AUTH_TOKEN, --auth-token AUTH_TOKEN                        Enable token based authentication. Requests need to                        provide the token to be accepted. (default: None)  --cors [CORS [CORS ...]]                        Enable CORS for the passed origin. Use * to whitelist                        all origins. (default: None)  --enable-api          Start the web server API in addition to the input                        channel. (default: False)  --response-timeout RESPONSE_TIMEOUT                        Maximum time a response can take to process (sec).                        (default: 3600)  --remote-storage REMOTE_STORAGE                        Set the remote location where your Rasa model is                        stored, e.g. on AWS. (default: None)  --ssl-certificate SSL_CERTIFICATE                        Set the SSL Certificate to create a TLS secured                        server. (default: None)  --ssl-keyfile SSL_KEYFILE                        Set the SSL Keyfile to create a TLS secured server.                        (default: None)  --ssl-ca-file SSL_CA_FILE                        If your SSL certificate needs to be verified, you can                        specify the CA file using this parameter. (default:                        None)  --ssl-password SSL_PASSWORD                        If your ssl-keyfile is protected by a password, you                        can specify it using this paramer. (default: None)Channels:  --credentials CREDENTIALS                        Authentication credentials for the connector as a yml                        file. (default: None)  --connector CONNECTOR                        Service to connect to. (default: None)JWT Authentication:  --jwt-secret JWT_SECRET                        Public key for asymmetric JWT methods or shared                        secretfor symmetric methods. Please also make sure to                        use --jwt-method to select the method of the                        signature, otherwise this argument will be ignored.                        (default: None)  --jwt-method JWT_METHOD                        Method used for the signature of the JWT                        authentication payload. (default: HS256)</code></pre><h2 id="启动服务器"><a href="#启动服务器" class="headerlink" title="启动服务器"></a>启动服务器</h2><p>要启动运行您的Rasa模型的服务器，请运行：</p><pre><code>rasa run</code></pre><p>以下参数可用于配置Rasa服务器：</p><pre class=" language-shell"><code class="language-shell">usage: rasa run [-h] [-v] [-vv] [--quiet] [-m MODEL] [--log-file LOG_FILE]                [--endpoints ENDPOINTS] [-p PORT] [-t AUTH_TOKEN]                [--cors [CORS [CORS ...]]] [--enable-api]                [--response-timeout RESPONSE_TIMEOUT]                [--remote-storage REMOTE_STORAGE]                [--ssl-certificate SSL_CERTIFICATE]                [--ssl-keyfile SSL_KEYFILE] [--ssl-ca-file SSL_CA_FILE]                [--ssl-password SSL_PASSWORD] [--credentials CREDENTIALS]                [--connector CONNECTOR] [--jwt-secret JWT_SECRET]                [--jwt-method JWT_METHOD]                &#123;actions&#125; ... [model-as-positional-argument]positional arguments:  &#123;actions&#125;    actions             Runs the action server.  model-as-positional-argument                        Path to a trained Rasa model. If a directory is                        specified, it will use the latest model in this                        directory. (default: None)optional arguments:  -h, --help            show this help message and exit  -m MODEL, --model MODEL                        Path to a trained Rasa model. If a directory is                        specified, it will use the latest model in this                        directory. (default: models)  --log-file LOG_FILE   Store logs in specified file. (default: None)  --endpoints ENDPOINTS                        Configuration file for the model server and the                        connectors as a yml file. (default: None)Python Logging Options:  -v, --verbose         Be verbose. Sets logging level to INFO. (default:                        None)  -vv, --debug          Print lots of debugging statements. Sets logging level                        to DEBUG. (default: None)  --quiet               Be quiet! Sets logging level to WARNING. (default:                        None)Server Settings:  -p PORT, --port PORT  Port to run the server at. (default: 5005)  -t AUTH_TOKEN, --auth-token AUTH_TOKEN                        Enable token based authentication. Requests need to                        provide the token to be accepted. (default: None)  --cors [CORS [CORS ...]]                        Enable CORS for the passed origin. Use * to whitelist                        all origins. (default: None)  --enable-api          Start the web server API in addition to the input                        channel. (default: False)  --response-timeout RESPONSE_TIMEOUT                        Maximum time a response can take to process (sec).                        (default: 3600)  --remote-storage REMOTE_STORAGE                        Set the remote location where your Rasa model is                        stored, e.g. on AWS. (default: None)  --ssl-certificate SSL_CERTIFICATE                        Set the SSL Certificate to create a TLS secured                        server. (default: None)  --ssl-keyfile SSL_KEYFILE                        Set the SSL Keyfile to create a TLS secured server.                        (default: None)  --ssl-ca-file SSL_CA_FILE                        If your SSL certificate needs to be verified, you can                        specify the CA file using this parameter. (default:                        None)  --ssl-password SSL_PASSWORD                        If your ssl-keyfile is protected by a password, you                        can specify it using this paramer. (default: None)Channels:  --credentials CREDENTIALS                        Authentication credentials for the connector as a yml                        file. (default: None)  --connector CONNECTOR                        Service to connect to. (default: None)JWT Authentication:  --jwt-secret JWT_SECRET                        Public key for asymmetric JWT methods or shared                        secretfor symmetric methods. Please also make sure to                        use --jwt-method to select the method of the                        signature, otherwise this argument will be ignored.                        (default: None)  --jwt-method JWT_METHOD                        Method used for the signature of the JWT                        authentication payload. (default: HS256)</code></pre><p>有关其他参数的更多信息，请参阅“<a href="#">配置HTTP API”</a>。有关所有端点的详细文档，请参见Rasa <a href="#">HTTP API</a>文档。</p><h2 id="启动动作服务器"><a href="#启动动作服务器" class="headerlink" title="启动动作服务器"></a>启动动作服务器</h2><p>要运行动作服务器，请运行</p><pre><code>rasa run actions</code></pre><p>以下参数可用于调整服务器设置：</p><pre class=" language-shell"><code class="language-shell">usage: rasa run actions [-h] [-v] [-vv] [--quiet] [-p PORT]                        [--cors [CORS [CORS ...]]] [--actions ACTIONS]                        [--ssl-keyfile SSL_KEYFILE]                        [--ssl-certificate SSL_CERTIFICATE]                        [--ssl-password SSL_PASSWORD] [--auto-reload]optional arguments:  -h, --help            show this help message and exit  -p PORT, --port PORT  port to run the server at (default: 5055)  --cors [CORS [CORS ...]]                        enable CORS for the passed origin. Use * to whitelist                        all origins (default: None)  --actions ACTIONS     name of action package to be loaded (default: None)  --ssl-keyfile SSL_KEYFILE                        Set the SSL certificate to create a TLS secured                        server. (default: None)  --ssl-certificate SSL_CERTIFICATE                        Set the SSL certificate to create a TLS secured                        server. (default: None)  --ssl-password SSL_PASSWORD                        If your ssl-keyfile is protected by a password, you                        can specify it using this paramer. (default: None)  --auto-reload         Enable auto-reloading of modules containing Action                        subclasses. (default: False)Python Logging Options:  -v, --verbose         Be verbose. Sets logging level to INFO. (default:                        None)  -vv, --debug          Print lots of debugging statements. Sets logging level                        to DEBUG. (default: None)  --quiet               Be quiet! Sets logging level to WARNING. (default:                        None)</code></pre><h2 id="可视化你的故事"><a href="#可视化你的故事" class="headerlink" title="可视化你的故事"></a>可视化你的故事</h2><p>要打开带有显示您的故事的图形的浏览器选项卡，请执行以下操作：</p><pre><code>rasa visualize</code></pre><p>通常，目录<code>data</code>中的培训案例是可视化的。如果您的故事位于其他地方，则可以使用来指定它们的位置<code>--stories</code>。</p><p>其他参数是：</p><pre class=" language-shell"><code class="language-shell">usage: rasa visualize [-h] [-v] [-vv] [--quiet] [-d DOMAIN] [-s STORIES]                      [-c CONFIG] [--out OUT] [--max-history MAX_HISTORY]                      [-u NLU]optional arguments:  -h, --help            show this help message and exit  -d DOMAIN, --domain DOMAIN                        Domain specification (yml file). (default: domain.yml)  -s STORIES, --stories STORIES                        File or folder containing your training stories.                        (default: data)  -c CONFIG, --config CONFIG                        The policy and NLU pipeline configuration of your bot.                        (default: config.yml)  --out OUT             Filename of the output path, e.g. 'graph.html'.                        (default: graph.html)  --max-history MAX_HISTORY                        Max history to consider when merging paths in the                        output graph. (default: 2)  -u NLU, --nlu NLU     File or folder containing your NLU data, used to                        insert example messages into the graph. (default:                        None)Python Logging Options:  -v, --verbose         Be verbose. Sets logging level to INFO. (default:                        None)  -vv, --debug          Print lots of debugging statements. Sets logging level                        to DEBUG. (default: None)  --quiet               Be quiet! Sets logging level to WARNING. (default:                        None)</code></pre><h2 id="在测试数据上评估模型"><a href="#在测试数据上评估模型" class="headerlink" title="在测试数据上评估模型"></a>在测试数据上评估模型</h2><p>要根据测试数据评估模型，请运行：</p><pre><code>rasa test</code></pre><p>指定要测试的模型<code>--model</code>。在“<a href="#">评估NLU模型</a>和<a href="#">评估核心模型”中</a>查看更多详细信息。</p><p>以下参数可用于：<code>rasa test</code></p><pre class=" language-shell"><code class="language-shell">usage: rasa test [-h] [-v] [-vv] [--quiet] [-m MODEL] [-s STORIES]                 [--max-stories MAX_STORIES] [--endpoints ENDPOINTS]                 [--fail-on-prediction-errors] [--url URL]                 [--evaluate-model-directory] [-u NLU] [--out OUT]                 [--successes] [--no-errors] [--histogram HISTOGRAM]                 [--confmat CONFMAT] [-c CONFIG [CONFIG ...]]                 [--cross-validation] [-f FOLDS] [-r RUNS]                 [-p PERCENTAGES [PERCENTAGES ...]] [--no-plot]                 &#123;core,nlu&#125; ...positional arguments:  &#123;core,nlu&#125;    core                Tests Rasa Core models using your test stories.    nlu                 Tests Rasa NLU models using your test NLU data.optional arguments:  -h, --help            show this help message and exit  -m MODEL, --model MODEL                        Path to a trained Rasa model. If a directory is                        specified, it will use the latest model in this                        directory. (default: models)Python Logging Options:  -v, --verbose         Be verbose. Sets logging level to INFO. (default:                        None)  -vv, --debug          Print lots of debugging statements. Sets logging level                        to DEBUG. (default: None)  --quiet               Be quiet! Sets logging level to WARNING. (default:                        None)Core Test Arguments:  -s STORIES, --stories STORIES                        File or folder containing your test stories. (default:                        tests)  --max-stories MAX_STORIES                        Maximum number of stories to test on. (default: None)  --endpoints ENDPOINTS                        Configuration file for the connectors as a yml file.                        (default: None)  --fail-on-prediction-errors                        If a prediction error is encountered, an exception is                        thrown. This can be used to validate stories during                        tests, e.g. on travis. (default: False)  --url URL             If supplied, downloads a story file from a URL and                        trains on it. Fetches the data by sending a GET                        request to the supplied URL. (default: None)  --evaluate-model-directory                        Should be set to evaluate models trained via 'rasa                        train core --config <config-1> <config-2>'. All models                        in the provided directory are evaluated and compared                        against each other. (default: False)NLU Test Arguments:  -u NLU, --nlu NLU     File or folder containing your NLU data. (default:                        data)  --out OUT             Output path for any files created during the                        evaluation. (default: results)  --successes           If set successful predictions (intent and entities)                        will be written to a file. (default: False)  --no-errors           If set incorrect predictions (intent and entities)                        will NOT be written to a file. (default: False)  --histogram HISTOGRAM                        Output path for the confidence histogram. (default:                        hist.png)  --confmat CONFMAT     Output path for the confusion matrix plot. (default:                        confmat.png)  -c CONFIG [CONFIG ...], --config CONFIG [CONFIG ...]                        Model configuration file. If a single file is passed                        and cross validation mode is chosen, cross-validation                        is performed, if multiple configs or a folder of                        configs are passed, models will be trained and                        compared directly. (default: None)  --no-plot             Don't render evaluation plots (default: False)</code></pre><h2 id="创建训练测试拆分"><a href="#创建训练测试拆分" class="headerlink" title="创建训练测试拆分"></a>创建训练测试拆分</h2><p>要创建NLU数据的拆分，请运行：</p><pre><code>rasa data split nlu</code></pre><p>您可以使用以下参数指定训练数据，分数和输出目录：</p><pre class=" language-shell"><code class="language-shell">usage: rasa data split nlu [-h] [-v] [-vv] [--quiet] [-u NLU]                           [--training-fraction TRAINING_FRACTION]                           [--random-seed RANDOM_SEED] [--out OUT]optional arguments:  -h, --help            show this help message and exit  -u NLU, --nlu NLU     File or folder containing your NLU data. (default:                        data)  --training-fraction TRAINING_FRACTION                        Percentage of the data which should be in the training                        data. (default: 0.8)  --random-seed RANDOM_SEED                        Seed to generate the same train/test split. (default:                        None)  --out OUT             Directory where the split files should be stored.                        (default: train_test_split)Python Logging Options:  -v, --verbose         Be verbose. Sets logging level to INFO. (default:                        None)  -vv, --debug          Print lots of debugging statements. Sets logging level                        to DEBUG. (default: None)  --quiet               Be quiet! Sets logging level to WARNING. (default:                        None)</code></pre><p>此命令将尝试在训练和测试中保持意图的比例相同。如果您有NLG数据用于检索操作，则将其保存到单独的文件中：</p><pre class=" language-shell"><code class="language-shell">ls train_test_split      nlg_test_data.md     test_data.json      nlg_training_data.md training_data.json</code></pre><h2 id="在Markdown和JSON之间转换数据"><a href="#在Markdown和JSON之间转换数据" class="headerlink" title="在Markdown和JSON之间转换数据"></a>在Markdown和JSON之间转换数据</h2><p>要将NLU数据从LUIS数据格式，WIT数据格式，Dialogflow数据格式，JSON或Markdown转换为JSON或Markdown，请运行：</p><pre><code>rasa data convert nlu</code></pre><p>您可以使用以下参数指定输入文件，输出文件和输出格式：</p><pre class=" language-shell"><code class="language-shell">usage: rasa data convert nlu [-h] [-v] [-vv] [--quiet] --data DATA --out OUT                             [-l LANGUAGE] -f &#123;json,md&#125;optional arguments:  -h, --help            show this help message and exit  --data DATA           Path to the file or directory containing Rasa NLU                        data. (default: None)  --out OUT             File where to save training data in Rasa format.                        (default: None)  -l LANGUAGE, --language LANGUAGE                        Language of data. (default: en)  -f &#123;json,md&#125;, --format &#123;json,md&#125;                        Output format the training data should be converted                        into. (default: None)Python Logging Options:  -v, --verbose         Be verbose. Sets logging level to INFO. (default:                        None)  -vv, --debug          Print lots of debugging statements. Sets logging level                        to DEBUG. (default: None)  --quiet               Be quiet! Sets logging level to WARNING. (default:                        None)</code></pre><h2 id="将对话导出到事件代理"><a href="#将对话导出到事件代理" class="headerlink" title="将对话导出到事件代理"></a>将对话导出到事件代理</h2><p>要使用事件代理从跟踪商店中导出事件，请运行：</p><pre><code>rasa export</code></pre><p>您可以指定环境文件的位置，应发布的事件的最小和最大时间戳以及应发布的对话ID。</p><pre class=" language-shell"><code class="language-shell">usage: rasa export [-h] [-v] [-vv] [--quiet] [--endpoints ENDPOINTS]                   [--minimum-timestamp MINIMUM_TIMESTAMP]                   [--maximum-timestamp MAXIMUM_TIMESTAMP]                   [--conversation-ids CONVERSATION_IDS]optional arguments:  -h, --help            show this help message and exit  --endpoints ENDPOINTS                        Endpoint configuration file specifying the tracker                        store and event broker. (default: endpoints.yml)  --minimum-timestamp MINIMUM_TIMESTAMP                        Minimum timestamp of events to be exported. The                        constraint is applied in a 'greater than or equal'                        comparison. (default: None)  --maximum-timestamp MAXIMUM_TIMESTAMP                        Maximum timestamp of events to be exported. The                        constraint is applied in a 'less than' comparison.                        (default: None)  --conversation-ids CONVERSATION_IDS                        Comma-separated list of conversation IDs to migrate.                        If unset, all available conversation IDs will be                        exported. (default: None)Python Logging Options:  -v, --verbose         Be verbose. Sets logging level to INFO. (default:                        None)  -vv, --debug          Print lots of debugging statements. Sets logging level                        to DEBUG. (default: None)  --quiet               Be quiet! Sets logging level to WARNING. (default:                        None)</code></pre><h2 id="启动Rasa"><a href="#启动Rasa" class="headerlink" title="启动Rasa"></a>启动Rasa</h2><p>Rasa X是一个工具集，可帮助您利用对话来改善您的助手。您可以<a href="#">在此处</a>找到有关它的更多信息。</p><p>您可以通过执行以下命令在本地启动Rasa X</p><pre><code>rasa x</code></pre><p>为了能够启动Rasa X，您需要安装Rasa X本地模式，并且需要处于Rasa项目中。</p><blockquote><p>注意</p><p>默认情况下，Rasa X在端口5002上运行。使用该参数<code>--rasa-x-port</code>可以将其更改为任何其他端口。</p></blockquote><p>以下参数可用于：<code>rasa x</code></p><pre class=" language-shell"><code class="language-shell">usage: rasa x [-h] [-v] [-vv] [--quiet] [-m MODEL] [--data DATA] [-c CONFIG]              [--no-prompt] [--production] [--rasa-x-port RASA_X_PORT]              [--config-endpoint CONFIG_ENDPOINT] [--log-file LOG_FILE]              [--endpoints ENDPOINTS] [-p PORT] [-t AUTH_TOKEN]              [--cors [CORS [CORS ...]]] [--enable-api]              [--response-timeout RESPONSE_TIMEOUT]              [--remote-storage REMOTE_STORAGE]              [--ssl-certificate SSL_CERTIFICATE] [--ssl-keyfile SSL_KEYFILE]              [--ssl-ca-file SSL_CA_FILE] [--ssl-password SSL_PASSWORD]              [--credentials CREDENTIALS] [--connector CONNECTOR]              [--jwt-secret JWT_SECRET] [--jwt-method JWT_METHOD]optional arguments:  -h, --help            show this help message and exit  -m MODEL, --model MODEL                        Path to a trained Rasa model. If a directory is                        specified, it will use the latest model in this                        directory. (default: models)  --data DATA           Path to the file or directory containing stories and                        Rasa NLU data. (default: data)  -c CONFIG, --config CONFIG                        The policy and NLU pipeline configuration of your bot.                        (default: config.yml)  --no-prompt           Automatic yes or default options to prompts and                        oppressed warnings. (default: False)  --production          Run Rasa X in a production environment. (default:                        False)  --rasa-x-port RASA_X_PORT                        Port to run the Rasa X server at. (default: 5002)  --config-endpoint CONFIG_ENDPOINT                        Rasa X endpoint URL from which to pull the runtime                        config. This URL typically contains the Rasa X token                        for authentication. Example:                        https://example.com/api/config?token=my_rasa_x_token                        (default: None)  --log-file LOG_FILE   Store logs in specified file. (default: None)  --endpoints ENDPOINTS                        Configuration file for the model server and the                        connectors as a yml file. (default: None)Python Logging Options:  -v, --verbose         Be verbose. Sets logging level to INFO. (default:                        None)  -vv, --debug          Print lots of debugging statements. Sets logging level                        to DEBUG. (default: None)  --quiet               Be quiet! Sets logging level to WARNING. (default:                        None)Server Settings:  -p PORT, --port PORT  Port to run the server at. (default: 5005)  -t AUTH_TOKEN, --auth-token AUTH_TOKEN                        Enable token based authentication. Requests need to                        provide the token to be accepted. (default: None)  --cors [CORS [CORS ...]]                        Enable CORS for the passed origin. Use * to whitelist                        all origins. (default: None)  --enable-api          Start the web server API in addition to the input                        channel. (default: False)  --response-timeout RESPONSE_TIMEOUT                        Maximum time a response can take to process (sec).                        (default: 3600)  --remote-storage REMOTE_STORAGE                        Set the remote location where your Rasa model is                        stored, e.g. on AWS. (default: None)  --ssl-certificate SSL_CERTIFICATE                        Set the SSL Certificate to create a TLS secured                        server. (default: None)  --ssl-keyfile SSL_KEYFILE                        Set the SSL Keyfile to create a TLS secured server.                        (default: None)  --ssl-ca-file SSL_CA_FILE                        If your SSL certificate needs to be verified, you can                        specify the CA file using this parameter. (default:                        None)  --ssl-password SSL_PASSWORD                        If your ssl-keyfile is protected by a password, you                        can specify it using this paramer. (default: None)Channels:  --credentials CREDENTIALS                        Authentication credentials for the connector as a yml                        file. (default: None)  --connector CONNECTOR                        Service to connect to. (default: None)JWT Authentication:  --jwt-secret JWT_SECRET                        Public key for asymmetric JWT methods or shared                        secretfor symmetric methods. Please also make sure to                        use --jwt-method to select the method of the                        signature, otherwise this argument will be ignored.                        (default: None)  --jwt-method JWT_METHOD                        Method used for the signature of the JWT                        authentication payload. (default: HS256)</code></pre><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> nlp </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Rasa </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Rasa文档指南(1)</title>
      <link href="/2020/10/02/rasa-zhi-nan/"/>
      <url>/2020/10/02/rasa-zhi-nan/</url>
      
        <content type="html"><![CDATA[<h1 id="Rasa文档指南"><a href="#Rasa文档指南" class="headerlink" title="Rasa文档指南"></a>Rasa文档指南</h1><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><ul><li>快速安装</li></ul><p>您可以使用pip安装Rasa Open Source（需要Python 3.6或3.7）。</p><pre class=" language-shell"><code class="language-shell">pip3 install rasa</code></pre><ul><li>从源代码构建</li></ul><pre class=" language-shell"><code class="language-shell">curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | pythongit clone https://github.com/RasaHQ/rasa.gitcd rasapoetry install</code></pre><ul><li><p>spaCy的依赖关系</p><p>有关spaCy的更多信息，请查看<a href="https://spacy.io/usage/models">spaCy文档</a>。</p></li></ul><pre class=" language-shell"><code class="language-shell">pip install rasa[spacy]python -m spacy download en_core_web_mdpython -m spacy link en_core_web_md en</code></pre><ul><li><p>MITIE的依赖项</p><p>然后下载 <a href="https://github.com/mit-nlp/MITIE/releases/download/v0.4/MITIE-models-v0.2.tar.bz2">MITIE模型</a>。您需要的文件是<code>total_word_feature_extractor.dat</code>。将此保存在任何地方。如果要使用MITIE，则需要告诉它在哪里可以找到此文件（在本示例中，该<code>data</code>文件已保存在项目目录的 文件夹中）。</p></li></ul><pre class=" language-shell"><code class="language-shell">pip install git+https://github.com/mit-nlp/MITIE.gitpip install rasa[mitie]</code></pre><h2 id="教程：Rasa基础"><a href="#教程：Rasa基础" class="headerlink" title="教程：Rasa基础"></a>教程：Rasa基础</h2><h3 id="基本步骤"><a href="#基本步骤" class="headerlink" title="基本步骤"></a>基本步骤</h3><ol><li><p>创建一个新项目</p></li><li><p>查看您的NLU培训数据</p></li><li><p>定义模型配置</p></li><li><p>写你的第一篇故事</p></li><li><p>定义域</p></li><li><p>训练模型</p></li><li><p>测试您的助手</p></li><li><p>与您的助手交谈</p></li></ol><p>在本教程中，您将构建一个简单友好的助手，该助手将询问您的工作状况，并向您发送有趣的图片以在您难过时为您加油。</p><p><img src="/2020/10/02/rasa-zhi-nan/1.png"></p><h3 id="1-创建一个新项目"><a href="#1-创建一个新项目" class="headerlink" title="1.创建一个新项目"></a>1.创建一个新项目</h3><p>第一步是创建一个新的Rasa项目。为此，请运行：</p><pre class=" language-shell"><code class="language-shell">rasa init --no-prompt</code></pre><p>该命令创建Rasa项目所需的所有文件，并在一些示例数据上训练一个简单的bot。如果您不做任何标记，系统将询问您一些有关如何设置项目的问题。<code>rasa init --no-prompt</code></p><p>这将创建以下文件：</p><table><thead><tr><th><code>__init__.py</code></th><th>一个空文件，可帮助python查找您的操作</th></tr></thead><tbody><tr><td><code>actions.py</code></td><td>您的自定义操作的代码</td></tr><tr><td><code>config.yml</code> ‘*’</td><td>您的NLU和Core模型的配置</td></tr><tr><td><code>credentials.yml</code></td><td>连接其他服务的详细信息</td></tr><tr><td><code>data/nlu.md</code> ‘*’</td><td>您的NLU训练数据</td></tr><tr><td><code>data/stories.md</code> ‘*’</td><td>你的故事</td></tr><tr><td><code>domain.yml</code> ‘*’</td><td>您助手的域名</td></tr><tr><td><code>endpoints.yml</code></td><td>连接到fb Messenger等频道的详细信息</td></tr><tr><td><code>models/&lt;timestamp&gt;.tar.gz</code></td><td>您的初始模型</td></tr></tbody></table><p>最重要的文件标有“ *”。您将在本教程中了解所有这些内容。</p><h3 id="2-查看您的NLU培训数据"><a href="#2-查看您的NLU培训数据" class="headerlink" title="2.查看您的NLU培训数据"></a>2.查看您的NLU培训数据</h3><p>Rasa助手的第一部分是NLU模型。NLU代表自然语言理解，这意味着将用户消息转换为结构化数据。要使用Rasa做到这一点，您需要提供训练示例，这些示例说明Rasa应该如何理解用户消息，然后通过显示这些示例来训练模型。</p><p>运行下面的代码单元以查看该命令创建的NLU训练数据：<code>rasa init</code></p><pre class=" language-text"><code class="language-text">## intent:greet- hey- hello- hi- good morning- good evening- hey there## intent:goodbye- bye- goodbye- see you around- see you later## intent:affirm- yes- indeed- of course- that sounds good- correct## intent:deny- no- never- I don't think so- don't like that- no way- not really## intent:mood_great- perfect- very good- great- amazing- wonderful- I am feeling very good- I am great- I'm good## intent:mood_unhappy- sad- very sad- unhappy- bad- very bad- awful- terrible- not very good- extremely sad- so sad## intent:bot_challenge- are you a bot?- are you a human?- am I talking to a bot?- am I talking to a human?</code></pre><p>以开头的行<code>##</code>定义your的名称<code>intents</code>，它们是具有相同含义的消息组。当您的用户向您的助手发送看不见的新消息时，Rasa的工作将是预测正确的意图。您可以在<a href="#">训练数据格式</a>中找到该数据格式的所有详细信息。</p><h3 id="3-定义你的模型配置"><a href="#3-定义你的模型配置" class="headerlink" title="3.定义你的模型配置"></a>3.定义你的模型配置</h3><p>配置文件定义了模型将使用的NLU和Core组件。在此示例中，您的NLU模型将使用 <code>supervised_embeddings</code>管道。您可以<a href="#">在此处</a>了解不同的NLU管道 。</p><p>让我们看一下您的模型配置文件。</p><pre class=" language-text"><code class="language-text"># Configuration for Rasa NLU.# https://rasa.com/docs/rasa/nlu/components/language: enpipeline:  - name: WhitespaceTokenizer  - name: RegexFeaturizer  - name: LexicalSyntacticFeaturizer  - name: CountVectorsFeaturizer  - name: CountVectorsFeaturizer    analyzer: "char_wb"    min_ngram: 1    max_ngram: 4  - name: DIETClassifier    epochs: 10  - name: EntitySynonymMapper  - name: ResponseSelector    epochs: 10# Configuration for Rasa Core.# https://rasa.com/docs/rasa/core/policies/policies:  - name: MemoizationPolicy  - name: TEDPolicy    max_history: 5    epochs: 10  - name: MappingPolicy</code></pre><p>在<code>language</code>和<code>pipeline</code>主键指定的NLU模型应该如何构建。该<code>policies</code>键定义<a href="#">策略</a>的核心车型将采用。</p><h3 id="4-编写第一个故事"><a href="#4-编写第一个故事" class="headerlink" title="4.编写第一个故事"></a>4.编写第一个故事</h3><p>在此阶段，您将教您的助手如何回复您的消息。这称为对话管理，由您的Core模型处理。</p><p>核心模型以训练“故事”的形式从真实的对话数据中学习。故事是用户和助手之间的真实对话。具有意图和实体的行反映了用户的输入，而动作名称则显示了助手应该做出的响应。</p><p>下面是一个简单对话的示例。用户打个招呼，助手打个招呼。这就是一个故事：</p><pre><code>## story1* greet   - utter_greet</code></pre><p>您可以在“<a href="#">故事”中</a>查看完整的详细信息。</p><p>开头的行是<code>-</code>助手执行的操作。在本教程中，我们所有的动作都是发送回用户的消息，例如<code>utter_greet</code>，但是一般而言，动作可以执行任何操作，包括调用API和与外界互动。</p><p>运行以下命令以查看文件中的示例故事<code>data/stories.md</code>：</p><pre class=" language-text"><code class="language-text">## happy path* greet  - utter_greet* mood_great  - utter_happy## sad path 1* greet  - utter_greet* mood_unhappy  - utter_cheer_up  - utter_did_that_help* affirm  - utter_happy## sad path 2* greet  - utter_greet* mood_unhappy  - utter_cheer_up  - utter_did_that_help* deny  - utter_goodbye## say goodbye* goodbye  - utter_goodbye## bot challenge* bot_challenge  - utter_iamabot</code></pre><h3 id="5-定义一个域"><a href="#5-定义一个域" class="headerlink" title="5.定义一个域"></a>5.定义一个域</h3><p>我们需要做的下一件事是定义一个<a href="#">Domain</a>。该域定义了您的助手所生活的世界：应该期望得到什么用户输入，应该能够预测什么操作，如何响应以及要存储什么信息。我们助手的域保存在一个名为的文件中<code>domain.yml</code>：</p><pre class=" language-text"><code class="language-text">intents:  - greet  - goodbye  - affirm  - deny  - mood_great  - mood_unhappy  - bot_challengeresponses:  utter_greet:  - text: "Hey! How are you?"  utter_cheer_up:  - text: "Here is something to cheer you up:"    image: "https://i.imgur.com/nGF1K8f.jpg"  utter_did_that_help:  - text: "Did that help you?"  utter_happy:  - text: "Great, carry on!"  utter_goodbye:  - text: "Bye"  utter_iamabot:  - text: "I am a bot, powered by Rasa."session_config:  session_expiration_time: 60  carry_over_slots_to_new_session: true</code></pre><p>那么不同的部分是什么意思呢？</p><table><thead><tr><th><code>intents</code></th><th>您希望用户说的话</th></tr></thead><tbody><tr><td><code>actions</code></td><td>您的助手可以说的事情</td></tr><tr><td><code>responses</code></td><td>您的助手可以说的话的回应字符串</td></tr></tbody></table><p><strong>这如何搭配在一起？</strong> Rasa Core的工作是选择正确的动作，以便在对话的每个步骤中执行。在这种情况下，我们的操作只会向用户发送一条消息。这些简单的发声动作是以<code>actions</code>开头的域<code>utter_</code>。助手将根据此<code>responses</code>部分的响应以一条消息进行响应。请参阅<a href="#">自定义操作</a> ，以构建除发送消息以外还可以做的事情。</p><h3 id="6-训练模型"><a href="#6-训练模型" class="headerlink" title="6.训练模型"></a>6.训练模型</h3><p>每当我们添加新的NLU或Core数据，或更新域或配置时，我们都需要根据示例故事和NLU数据重新训练神经网络。为此，请运行以下命令。该命令将调用Rasa Core和NLU训练函数，并将训练后的模型存储到<code>models/</code>目录中。如果不同模型零件的数据或配置有所更改，该命令将仅自动重新训练它们。</p><pre class=" language-shell"><code class="language-shell">rasa train</code></pre><p>该命令将同时查找NLU和Core数据，并训练组合模型。<code>rasa train</code></p><h3 id="7-测试您的助手"><a href="#7-测试您的助手" class="headerlink" title="7.测试您的助手"></a>7.测试您的助手</h3><p>训练模型后，您始终希望检查助手的行为是否仍然符合您的期望。在Rasa开放源代码中，您可以使用<code>tests/</code>目录中定义的端到端测试来运行测试对话，以确保NLU和Core都能做出正确的预测。</p><pre class=" language-shell"><code class="language-shell">rasa test</code></pre><p>请参阅<a href="#">测试您的助手</a>以了解有关在改进模型时如何评估模型的更多信息。</p><h3 id="8-与您的助手交谈"><a href="#8-与您的助手交谈" class="headerlink" title="8.与您的助手交谈"></a>8.与您的助手交谈</h3><p>恭喜你！🚀您刚刚构建了一个完全由机器学习驱动的助手。</p><p>下一步就是尝试一下！如果您在本地计算机上遵循本教程，请运行以下命令开始与助手交谈：</p><pre class=" language-shell"><code class="language-shell">rasa shell</code></pre><h2 id="教程：构建助理"><a href="#教程：构建助理" class="headerlink" title="教程：构建助理"></a>教程：构建助理</h2><p>在遵循了<a href="#">Rasa教程</a>中设置助手的基础之后，我们现在将逐步构建一个基本的FAQ聊天机器人，然后构建一个可以处理上下文对话的机器人。</p><ul><li><a href="#">构建一个简单的FAQ助手</a><ul><li><a href="#">记忆政策</a></li><li><a href="#">响应选择器</a></li></ul></li><li><a href="#">构建上下文助理</a><ul><li><a href="#">处理业务逻辑</a></li><li><a href="#">处理意外的用户输入</a><ul><li><a href="#">普通感叹词</a></li><li><a href="#">情境问题</a></li></ul></li><li><a href="#">优雅失败</a><ul><li><a href="#">后备政策</a></li><li><a href="#">超出范围意图</a></li></ul></li><li><a href="#">更复杂的上下文对话</a><ul><li><a href="#">增强记忆政策</a></li><li><a href="#">使用ML进行概括</a></li></ul></li></ul></li></ul><h3 id="构建一个简单的FAQ助手"><a href="#构建一个简单的FAQ助手" class="headerlink" title="构建一个简单的FAQ助手"></a>构建一个简单的FAQ助手</h3><p>FAQ助手是最简单的助手，也是入门的好地方。这些助手允许用户提出一个简单的问题并得到答复。我们将使用专门为此类助手设计的Rasa功能来构建基本的FAQ助手。</p><p>在本节中，我们将介绍以下主题：</p><ul><li>通过MemoizationPolicy<a href="#">响应简单的意图</a></li><li>使用ResponseSelector<a href="#">处理常见问题</a></li></ul><p>为了准备本教程，我们将创建一个新目录并启动一个新的Rasa项目。</p><pre class=" language-shell"><code class="language-shell">mkdir rasa-assistantrasa init</code></pre><p>让我们从bot中删除默认内容，以便<code>nlu.md</code>，<code>stories.md</code> 和<code>domain.yml</code>文件为空。</p><h4 id="Memoization-Policy"><a href="#Memoization-Policy" class="headerlink" title="Memoization Policy"></a>Memoization Policy</h4><p>MemoizationPolicy最多可以记住训练故事中的示例<code>max_history</code> 。“turns”次数包括用户发送的消息以及助手执行的操作。出于简单，无上下文的FAQ机器人的目的，我们只需要注意用户发送的最后一条消息，因此我们将其设置为<code>1</code>。</p><p>您可以按照以下步骤编辑<code>config.yml</code>文件来完成此操作（现在可以删除<code>TEDPolicy</code>）：</p><pre class=" language-shell"><code class="language-shell">policies:- name: MemoizationPolicy  max_history: 1- name: MappingPolicy</code></pre><blockquote><p>注意</p><p>MappingPolicy之所以存在，是因为它处理了<code>/restart</code>意图的逻辑，这使您可以清除对话历史记录并重新开始。</p></blockquote><p>现在，我们已经定义了策略，我们可以为文件添加一些故事<code>goodbye</code>，<code>thank</code>并向文件添加<code>greet</code> 意图<code>stories.md</code>：</p><pre class=" language-text"><code class="language-text">## greet* greet  - utter_greet## thank* thank  - utter_noworries## goodbye* bye  - utter_bye</code></pre><p>我们还需要<code>domain.yml</code>在以下各节中向文件添加意图，操作和响应：</p><pre class=" language-shell"><code class="language-shell">intents:  - greet  - bye  - thankresponses:  utter_noworries:    - text: No worries!  utter_greet:    - text: Hi  utter_bye:    - text: Bye!</code></pre><p>最后，我们将从Sara复制一些NLU数据到我们的<code>nlu.md</code>文件中（更多信息可以在<a href="#">这里</a>找到）：</p><pre class=" language-text"><code class="language-text">## intent:greet- Hi- Hey- Hi bot- Hey bot- Hello- Good morning- hi again- hi folks## intent:bye- goodbye- goodnight- good bye- good night- see ya- toodle-oo- bye bye- gotta go- farewell## intent:thank- Thanks- Thank you- Thank you so much- Thanks bot- Thanks for that- cheers</code></pre><p>您现在可以通过运行以下命令来训练第一个模型并测试机器人：</p><pre class=" language-shell"><code class="language-shell">rasa trainrasa shell</code></pre><p>该机器人现在应该能够以任何顺序答复我们一致定义的意图。</p><p>例如：</p><p><img src="/2020/10/02/rasa-zhi-nan/2.png"></p><p>交互式地测试bot很好，但是我们还应该添加端到端测试用例，这些用例以后可以包含在<a href="#">CI / CD系统中</a>。端到端的<a href="#">测试对话</a> 包括NLU数据，因此可以测试Rasa的两个组件。该文件 <code>tests/conversation_tests.md</code>包含示例测试对话。删除所有测试对话，并以迄今为止到目前为止您的助手的一些测试对话来代替它们：</p><pre><code>## greet + goodbye* greet: Hi!  - utter_greet* bye: Bye  - utter_bye## greet + thanks* greet: Hello there  - utter_greet* thank: thanks a bunch  - utter_noworries## greet + thanks + goodbye* greet: Hey  - utter_greet* thank: thank you  - utter_noworries* bye: bye bye  - utter_bye</code></pre><p>要针对测试文件测试模型，请运行以下命令：</p><pre><code>rasa test --stories tests/conversation_tests.md</code></pre><p>测试命令将产生一个名为的目录<code>results</code>。它应该包含一个名为的文件<code>failed_stories.md</code>，其中将打印任何失败的测试用例。它还将指定是错误的NLU还是Core预测。作为CI / CD管道的一部分，测试选项<code>--fail-on-prediction-errors</code>可用于引发异常，从而停止管道。</p><h4 id="响应选择器"><a href="#响应选择器" class="headerlink" title="响应选择器"></a>响应选择器</h4><p>该<a href="#">ResponseSelector</a> NLU组件的设计，使其更容易处理对话元素，如<a href="#">小对话</a>和常见问题的信息以简单的方式。通过使用ResponseSelector，您只需要一个故事即可处理所有FAQ，而无需在每次想要增加机器人的范围时添加新故事。</p><p>人们常常问Rasa周边Rasa产品不同的问题，所以让我们开始有三个意图：<code>ask_channels</code>，<code>ask_languages</code>，和<code>ask_rasax</code>。我们将从<a href="#">Sara训练数据中</a>复制一些NLU<a href="#">数据</a> 到我们的<code>nlu.md</code>。这些意图必须带有<code>faq/</code>前缀，这一点很重要，因此ResponseSelector会将它们识别为常见问题意图：</p><pre class=" language-text"><code class="language-text">## intent: faq/ask_channels- What channels of communication does rasa support?- what channels do you support?- what chat channels does rasa uses- channels supported by Rasa- which messaging channels does rasa support?## intent: faq/ask_languages- what language does rasa support?- which language do you support?- which languages supports rasa- can I use rasa also for another laguage?- languages supported## intent: faq/ask_rasax- I want information about rasa x- i want to learn more about Rasa X- what is rasa x?- Can you tell me about rasa x?- Tell me about rasa x- tell me what is rasa x</code></pre><p>接下来，我们需要<code>responses.md</code>在<code>data/</code>目录中的新文件中定义与这些FAQ相关的响应：</p><pre class=" language-reStructuredText"><code class="language-reStructuredText">* faq/ask_channels  - We have a comprehensive list of [supported connectors](https://rasa.com/docs/core/connectors/), but if    you don't see the one you're looking for, you can always create a custom connector by following    [this guide](https://rasa.com/docs/rasa/user-guide/connectors/custom-connectors/).## ask languages* faq/ask_languages  - You can use Rasa to build assistants in any language you want!## ask rasa x* faq/ask_rasax - Rasa X is a tool to learn from real conversations and improve your assistant. Read more [here](https://rasa.com/docs/rasa-x/)</code></pre><p>ResponseSelector应该已经在我们的NLU管道的末尾<code>config.yml</code>：</p><pre class=" language-shell"><code class="language-shell">language: enpipeline:  - name: WhitespaceTokenizer  - name: RegexFeaturizer  - name: LexicalSyntacticFeaturizer  - name: CountVectorsFeaturizer  - name: CountVectorsFeaturizer    analyzer: "char_wb"    min_ngram: 1    max_ngram: 4  - name: DIETClassifier    epochs: 100  - name: EntitySynonymMapper  - name: ResponseSelector    epochs: 100</code></pre><p>现在我们已经定义了NLU端，我们需要让Core意识到这些变化。打开<code>domain.yml</code>文件并添加<code>faq</code>意图：</p><pre><code>intents:  - greet  - bye  - thank  - faq</code></pre><p>我们还需要添加一个<a href="#">检索动作</a>，该<a href="#">动作</a>负责将ResponseSelector预测的响应发送回用户到动作列表。这些操作始终必须以<code>respond_</code>前缀开头：</p><pre><code>actions:  - respond_faq</code></pre><p>接下来，我们将编写一个story，以便Core知道要预测的动作：</p><pre><code>## Some question from FAQ* faq    - respond_faq</code></pre><p>如我们前面所述，此预测由MemoizationPolicy处理。</p><p>完成所有更改后，训练新模型并测试修改后的FAQ：</p><pre><code>rasa trainrasa shell</code></pre><p>在这个阶段，有必要<code>test_stories.md</code>再次向文件添加一些测试用例：</p><pre><code>## ask channels* faq: What messaging channels does Rasa support?  - respond_faq## ask languages* faq: Which languages can I build assistants in?  - respond_faq## ask rasa x* faq: What’s Rasa X?  - respond_faq</code></pre><p>您可以在此<a href="#">博客文章</a>和“ <a href="#">检索操作”</a>页面中阅读更多内容。</p><p>使用我们在本教程中描述的功能，您可以轻松构建无上下文助手。当您准备使用上下文增强助手时，请查看<a href="#">构建上下文助手</a>。</p><blockquote><p>注意</p><p>这是我们为构建基本的FAQ助手而修改的文件的最小清单：</p><ul><li><p>data/nlu.md：为faq/意图添加NLU训练数据</p></li><li><p>data/responses.md：添加与faq/意图相关的响应</p></li><li><p>config.yml：ReponseSelector在您的NLU管道中添加</p></li><li><p>domain.yml：添加检索动作respond_faq和意图faq</p></li><li><p>data/stories.md：为常见问题解答添加简单的故事</p></li><li><p>test_stories.md：为您的常见问题解答添加E2E测试案例</p></li></ul></blockquote><h4 id="构建一个上下文助手"><a href="#构建一个上下文助手" class="headerlink" title="构建一个上下文助手"></a>构建一个上下文助手</h4><p>无论您是刚刚创建FAQ机器人还是从头开始，下一步都是扩展您的bot来处理上下文对话。</p><p>在本教程中，我们将涵盖各种主题：</p><ul><li><a href="#">处理业务逻辑</a></li><li><a href="#">处理意外的用户输入</a></li><li><a href="#">优雅失败</a></li><li><a href="#">更复杂的上下文对话</a></li></ul><p>在开始本部分之前，请确保您已从“<a href="#">构建简单的FAQ助手”</a>部分获取了所有数据。您将需要对配置文件进行一些调整，因为我们现在需要注意上下文：</p><pre><code>policies:- name: MemoizationPolicy- name: MappingPolicy</code></pre><p>我们删除了配置。默认值为，这意味着Core进行预测时会注意过去5圈（请参阅<a href="#">最大历史记录的</a>说明）。<code>max_history: 15</code></p><h4 id="处理业务逻辑"><a href="#处理业务逻辑" class="headerlink" title="处理业务逻辑"></a>处理业务逻辑</h4><p>许多对话助手的用户目标都是在能够为用户做某事之前先从用户那里收集一堆信息。这称为插槽填充。例如，在银行业中，您可能有一个转移资金的用户目标，在这里您需要收集有关从哪个帐户转账，向谁转账以及转账金额的信息。可以而且应该以基于规则的方式处理这种行为，因为很清楚应该如何收集此信息。</p><p>对于这种类型的用例，我们可以使用Forms和FormPolicy。该<a href="#">FormPolicy</a> 的工作原理是，直到所有的信息从用户收集预测形式下一个动作。</p><p>作为示例，我们将构建Sara的SalesForm。用户想要联系我们的销售团队，为此，我们需要收集以下信息：</p><ul><li>他们的工作</li><li>他们的机器人用例</li><li>他们的名字</li><li>他们的电子邮件</li><li>他们的预算</li><li>他们的公司</li></ul><p>我们将从<code>SalesForm</code>在名为的文件中将其定义为新类开始<code>actions.py</code>。我们需要定义的第一个方法是名称，就像在常规Action中一样，它返回将在我们的story中使用的名称：</p><pre class=" language-python"><code class="language-python"><span class="token keyword">from</span> rasa_sdk<span class="token punctuation">.</span>forms <span class="token keyword">import</span> FormAction<span class="token keyword">class</span> <span class="token class-name">SalesForm</span><span class="token punctuation">(</span>FormAction<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""Collects sales information and adds it to the spreadsheet"""</span>    <span class="token keyword">def</span> <span class="token function">name</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token string">"sales_form"</span></code></pre><p>接下来，我们必须定义<code>required_slots</code>一种方法，该方法指定要询问的信息，即要填充的插槽。</p><pre class=" language-python"><code class="language-python">@staticmethod<span class="token keyword">def</span> <span class="token function">required_slots</span><span class="token punctuation">(</span>tracker<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> <span class="token punctuation">[</span>        <span class="token string">"job_function"</span><span class="token punctuation">,</span>        <span class="token string">"use_case"</span><span class="token punctuation">,</span>        <span class="token string">"budget"</span><span class="token punctuation">,</span>        <span class="token string">"person_name"</span><span class="token punctuation">,</span>        <span class="token string">"company"</span><span class="token punctuation">,</span>        <span class="token string">"business_email"</span><span class="token punctuation">,</span>        <span class="token punctuation">]</span></code></pre><p>注意：您可以将所需的插槽功能自定义为非静态。例如，如果<code>job_function</code>是开发人员，则可以添加<code>required_slot</code>与Rasa有关的用户体验级别</p><p>完成此操作后，您需要指定机器人应如何请求此信息。这是通过<code>utter_ask_{slotname}</code>在<code>domain.yml</code>文件中指定响应来完成的。对于上述内容，我们需要指定以下内容</p><pre class=" language-reStructuredText"><code class="language-reStructuredText">utter_ask_business_email:  - text: What's your business email?utter_ask_company:  - text: What company do you work for?utter_ask_budget:  - text: "What's your annual budget for conversational AI? 💸"utter_ask_job_function:  - text: "What's your job? 🕴"utter_ask_person_name:  - text: What's your name?utter_ask_use_case:  - text: What's your use case?</code></pre><p>我们还需要在<code>domain.yml</code>文件中定义所有这些插槽：</p><pre class=" language-reStructuredText"><code class="language-reStructuredText">slots:  company:    type: unfeaturized  job_function:    type: unfeaturized  person_name:    type: unfeaturized  budget:    type: unfeaturized  business_email:    type: unfeaturized  use_case:    type: unfeaturized</code></pre><p>回到我们的表单定义，我们还需要定义<code>submit</code>方法，一旦表单完成，它将对用户提供的信息做一些事情：</p><pre><code>def submit(        self,        dispatcher: CollectingDispatcher,        tracker: Tracker,        domain: Dict[Text, Any],    ) -&gt; List[Dict]:    dispatcher.utter_message("Thanks for getting in touch, we’ll contact you soon")    return []</code></pre><p>在这种情况下，我们仅告诉用户我们将与他们联系，但是通常您会将这些信息发送到API或数据库。有关 如何在电子表格中存储此信息的示例，请参见<a href="https://github.com/RasaHQ/rasa-demo/blob/master/actions/actions.py#L71">rasa-demo</a>。</p><p>我们需要将刚刚创建的表单添加到<code>domain.yml</code>文件中的新部分：</p><pre><code>forms:  - sales_form</code></pre><p>我们还需要创建一个激活表单的意图，以及一个提供表单要求用户的所有信息的意图。对于表单激活意图，我们可以创建一个名为的意图<code>contact_sales</code>。将以下训练数据添加到您的nlu文件中：</p><pre><code>## intent:contact_sales- I wanna talk to your sales people.- I want to talk to your sales people- I want to speak with sales- Sales- Please schedule a sales call- Please connect me to someone from sales- I want to get in touch with your sales guys- I would like to talk to someone from your sales team- sales please</code></pre><p>您可以在<a href="https://github.com/RasaHQ/rasa-demo/blob/master/data/nlu/nlu.md#intentcontact_sales">此处</a>查看完整的意图）</p><p>我们还将创建一个名为的意图<code>inform</code>，该意图涵盖用户提供给机器人的任何信息。<em>之所以将所有这些都放在一个意图下，是因为在提供信息之后并没有真正的意图，只有实体很重要。</em>将以下数据添加到您的NLU文件中：</p><pre><code>## intent:inform- [100k](budget)- [100k](budget)- [240k/year](budget)- [150,000 USD](budget)- I work for [Rasa](company)- The name of the company is [ACME](company)- company: [Rasa Technologies](company)- it's a small company from the US, the name is [Hooli](company)- it's a tech company, [Rasa](company)- [ACME](company)- [Rasa Technologies](company)- [maxmeier@firma.de](business_email)- [bot-fan@bots.com](business_email)- [maxmeier@firma.de](business_email)- [bot-fan@bots.com](business_email)- [my email is email@rasa.com](business_email)- [engineer](job_function)- [brand manager](job_function)- [marketing](job_function)- [sales manager](job_function)- [growth manager](job_function)- [CTO](job_function)- [CEO](job_function)- [COO](job_function)- [John Doe](person_name)- [Jane Doe](person_name)- [Max Mustermann](person_name)- [Max Meier](person_name)- We plan to build a [sales bot](use_case) to increase our sales by 500%.- we plan to build a [sales bot](use_case) to increase our revenue by 100%.- a [insurance tool](use_case) that consults potential customers on the best life insurance to choose.- we're building a [conversational assistant](use_case) for our employees to book meeting rooms.</code></pre><blockquote><p>注意</p><p>像<code>business_email</code>和<code>budget</code>这样的实体通常由经过预训练的实体提取器（例如<a href="#">DucklingHTTPExtractor</a>或<a href="#">SpacyEntityExtractor</a>）处理，但是对于本教程，我们希望避免任何其他设置。</p></blockquote><p>意图和实体也需要添加到您的<code>domain.yml</code>文件中：</p><pre><code>intents:  - greet  - bye  - thank  - faq  - contact_sales  - informentities:  - company  - job_function  - person_name  - budget  - business_email  - use_case</code></pre><p>表单的故事非常简单，因为所有广告位收集表单都发生在表单内部，因此不需要包含在故事中。您只需要编写一个故事，说明何时应激活表单。对于销售表单，将此故事添加到您的<code>stories.md</code>文件中：</p><pre><code>## sales form* contact_sales    - sales_form                   &lt;!--Run the sales_form action--&gt;    - form&amp;#123;"name": "sales_form"&amp;#125;   &lt;!--Activate the form--&gt;    - form&amp;#123;"name": null&amp;#125;           &lt;!--Deactivate the form--&gt;</code></pre><p>最后，让我们将FormPolicy添加到我们的配置文件中：</p><pre><code>policies:  - name: MemoizationPolicy  - name: KerasPolicy  - name: MappingPolicy  - name: FormPolicy</code></pre><p>至此，您已经有了一个有效的表格，因此让我们尝试一下。确保取消注释 <code>action_endpoint</code>您的，<code>endpoints.yml</code>以使Rasa知道将运行我们表单的动作服务器：</p><pre><code>action_endpoint: url: "http://localhost:5055/webhook"</code></pre><p>然后在新的终端窗口中启动动作服务器：</p><pre><code>rasa run actions</code></pre><p>然后，您可以重新训练并与您的机器人对话：</p><pre><code>rasa trainrasa shell</code></pre><p>这种简单的形式可以立即使用，但是您可能希望添加更多的功能来处理不同的情况。其中一个示例是验证插槽，以确保用户正确提供了信息（<a href="#">在此处</a>了解更多信息 ）。</p><p>另一个示例是，您可能想填充名称相同的实体以外的东西的插槽。例如，对于我们表单中的“用例”情况，我们希望用户键入完整的句子，而不是您可能必须提取为实体的东西。在这种情况下，我们可以利用该<code>slot_mappings</code>方法，您可以在其中描述应从中提取实体的内容。在这里，我们可以使用该<code>from_text</code>方法提取用户的整个消息：</p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">slot_mappings</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> Dict<span class="token punctuation">[</span>Text<span class="token punctuation">,</span> Union<span class="token punctuation">[</span>Dict<span class="token punctuation">,</span> List<span class="token punctuation">[</span>Dict<span class="token punctuation">[</span>Text<span class="token punctuation">,</span> Any<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""A dictionary to map required slots to    - an extracted entity    - intent: value pairs    - a whole message    or a list of them, where a first match will be picked"""</span>    <span class="token keyword">return</span> <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;"use_case": self.from_text(intent="inform")&amp;#125;</span></code></pre><p>现在，当请求用例插槽时，我们的机器人将提取完整的用户消息，并且我们不需要使用<code>use_case</code>之前定义的实体。</p><p>可以定制表单中的所有方法，以处理业务逻辑中的不同分支。<a href="https://rasa.com/docs/rasa/core/forms/#">在此处</a>阅读有关此内容的更多信息。但是，您应确保不要处理表单内所有不愉快的路径。应该通过编写常规故事来处理这些问题，以便您的模型可以学习这种行为。</p><blockquote><p>注意</p><p>这是我们为使用表单操作处理业务逻辑而修改的文件的最小清单：</p><ul><li><p><code>actions.py</code>：定义形式作用，包括<code>required_slots</code>，<code>slot_mappings</code>和<code>submit</code>方法</p></li><li><p><code>data/nlu.md</code>：</p><blockquote><ul><li>添加示例以激活表单</li><li>添加示例<code>inform</code>以填写表单</li></ul></blockquote></li><li><p><code>domain.yml</code>：</p><blockquote><ul><li>添加表格所需的所有插槽</li><li>添加<code>utter_ask_{slot}</code>所有必需插槽的响应</li><li>将表单动作添加到该<code>forms</code>部分</li><li>从您的NLU培训数据中添加所有意图和实体</li></ul></blockquote></li><li><p><code>data/stories.md</code>：为表单添加一个故事</p></li><li><p><code>config.yml</code>：</p><blockquote><ul><li>将新增<code>FormPolicy</code>至您的政策</li><li>将实体提取器添加到管道</li></ul></blockquote></li><li><p><code>endpoints.yml</code>：定义 <code>action_endpoint</code></p></li></ul></blockquote><h4 id="处理意想不到的用户输入"><a href="#处理意想不到的用户输入" class="headerlink" title="处理意想不到的用户输入"></a>处理意想不到的用户输入</h4><p>所有预期的用户输入都应通过我们上面定义的形式处理，即，如果用户提供了机器人所要求的信息。但是，在实际情况下，用户的行为通常会有所不同。在本节中，我们将介绍各种形式的“感叹词”以及如何在Rasa中处理它们。</p><p>处理这些类型的用户输入的决定应始终来自查看真实的对话。您应该首先构建助手的一部分，与真实用户（无论是最终用户还是同事）进行测试，然后添加缺失的内容。您不应该尝试实施您认为可能发生的所有可能的极端情况，因为最终用户可能永远不会以这种方式行事。 <a href="#">Rasa X</a> 是可以帮助您查看对话并做出此类决定的工具。</p><h4 id="通用感叹词"><a href="#通用感叹词" class="headerlink" title="通用感叹词"></a>通用感叹词</h4><p>如果您有通用感叹词，无论上下文如何，都应始终具有相同的单个响应，则可以使用<a href="#">映射策略</a>来处理这些感叹词。它将始终为意图预测相同的动作，并且与遗忘机制结合使用时，您也无需编写任何故事。</p><p>例如，假设您看到用户与您的助手进行如下对话，他们在对话中间写了问候-可能是因为他们离开了几分钟：</p><img src="/2020/10/02/rasa-zhi-nan/3.png" style="zoom:30%;"><p>问候意图是一个很好的例子，我们将始终给出相同的响应，但是我们不希望这种意图影响对话历史。为此，响应必须是返回<code>UserUtteranceReverted()</code>事件以从对话历史记录中删除交互的动作。</p><p>首先，打开<code>domain.yml</code>文件并修改问候意图，然后<code>actions</code>在文件中添加一个新块，然后，<code>action_greet</code>如下所示添加：</p><pre class=" language-reStructuredText"><code class="language-reStructuredText">intents:  - greet: &#123;triggers: action_greet&#125;  - bye  - thank  - faq  - contact_sales  - informactions:  - action_greet</code></pre><p>如果有，请使用“打招呼”意图删除所有故事。</p><p>接下来，我们需要定义<code>action_greet</code>。将以下操作添加到<code>actions.py</code>文件中：</p><pre class=" language-python"><code class="language-python"><span class="token keyword">from</span> rasa_sdk <span class="token keyword">import</span> Action<span class="token keyword">from</span> rasa_sdk<span class="token punctuation">.</span>events <span class="token keyword">import</span> UserUtteranceReverted<span class="token keyword">class</span> <span class="token class-name">ActionGreetUser</span><span class="token punctuation">(</span>Action<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token triple-quoted-string string">"""Revertible mapped action for utter_greet"""</span><span class="token keyword">def</span> <span class="token function">name</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> <span class="token string">"action_greet"</span><span class="token keyword">def</span> <span class="token function">run</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dispatcher<span class="token punctuation">,</span> tracker<span class="token punctuation">,</span> domain<span class="token punctuation">)</span><span class="token punctuation">:</span>    dispatcher<span class="token punctuation">.</span>utter_template<span class="token punctuation">(</span><span class="token string">"utter_greet"</span><span class="token punctuation">,</span> tracker<span class="token punctuation">)</span>    <span class="token keyword">return</span> <span class="token punctuation">[</span>UserUtteranceReverted<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span></code></pre><p>要测试修改后的意图，我们需要重新启动动作服务器：</p><pre class=" language-shell"><code class="language-shell">rasa run actions</code></pre><p>然后，我们可以重新训练模型，并尝试添加以下内容：</p><pre class=" language-shell"><code class="language-shell">rasa trainrasa shell</code></pre><p>常见问题解答是另一种通用感叹词，应始终得到相同的响应。例如，用户可能会在填写表单时询问相关的FAQ：</p><img src="/2020/10/02/rasa-zhi-nan/4.png" style="zoom:33%;"><p>要处理通过检索操作定义的FAQ，您可以添加一个简单的story，该故事将由MemoizationPolicy处理：</p><pre><code>## just sales, continue* contact_sales    - sales_form    - form&amp;#123;"name": "sales_form"&amp;#125;* faq    - respond_faq    - sales_form    - form&amp;#123;"name": null&amp;#125;</code></pre><p>这将脱离表格并处理用户的FAQ问题，然后返回到原始任务。例如：</p><img src="/2020/10/02/rasa-zhi-nan/5.png" style="zoom:33%;"><p>如果发现很难以这种格式编写故事，则可以随时使用“<a href="#">交互式学习”</a> 来帮助您创建故事。</p><p>与往常一样，请确保将端到端测试用例添加到您的test_stories.md文件中。</p><h5 id="语境的问题"><a href="#语境的问题" class="headerlink" title="语境的问题"></a>语境的问题</h5><p>您还可以处理<a href="#">上下文问题</a>，例如用户询问问题“为什么需要知道这一点”。用户可以根据机器人已请求的特定插槽来询问此问题，并且每个插槽的响应应有所不同。例如：</p><img src="/2020/10/02/rasa-zhi-nan/6.png" style="zoom:33%;"><p>为了解决这个问题，我们需要使<code>requested_slot</code>特征化，并为其分配分类类型：</p><pre class=" language-markdown"><code class="language-markdown">slots:  requested_slot:<span class="token code keyword">    type: categorical</span><span class="token code keyword">    values:</span><span class="token code keyword">      - business_email</span><span class="token code keyword">      - company</span><span class="token code keyword">      - person_name</span><span class="token code keyword">      - use_case</span><span class="token code keyword">      - budget</span><span class="token code keyword">      - job_function</span></code></pre><p>这意味着Core在进行预测时会注意插槽的值（了解更多有关其他<a href="#">功能化插槽的信息</a>），而未功能化插槽仅用于存储信息。有关的故事应如下所示：</p><pre class=" language-markdown"><code class="language-markdown"><span class="token title important"><span class="token punctuation">##</span> explain email</span><span class="token list punctuation">*</span> contact_sales<span class="token code keyword">    - sales_form</span><span class="token code keyword">    - form&amp;#123;"name": "sales_form"&amp;#125;</span><span class="token code keyword">    - slot&amp;#123;"requested_slot": "business_email"&amp;#125;</span><span class="token list punctuation">*</span> explain<span class="token code keyword">    - utter_explain_why_email</span><span class="token code keyword">    - sales_form</span><span class="token code keyword">    - form&amp;#123;"name": null&amp;#125;</span><span class="token title important"><span class="token punctuation">##</span> explain budget</span><span class="token list punctuation">*</span> contact_sales<span class="token code keyword">    - sales_form</span><span class="token code keyword">    - form&amp;#123;"name": "sales_form"&amp;#125;</span><span class="token code keyword">    - slot&amp;#123;"requested_slot": "budget"&amp;#125;</span><span class="token list punctuation">*</span> explain<span class="token code keyword">    - utter_explain_why_budget</span><span class="token code keyword">    - sales_form</span><span class="token code keyword">    - form&amp;#123;"name": null&amp;#125;</span></code></pre><p>我们需要添加刚添加到<code>domain.yml</code>文件中的意图和话语：</p><pre class=" language-markdown"><code class="language-markdown">intents:<span class="token list punctuation">-</span> greet: <span class="token entity" title="&#123;">&amp;#123;</span>triggers: action<span class="token italic"><span class="token punctuation">_</span>greet<span class="token punctuation">_</span></span>user<span class="token entity" title="&#125;">&amp;#125;</span><span class="token list punctuation">-</span> bye<span class="token list punctuation">-</span> thank<span class="token list punctuation">-</span> faq<span class="token list punctuation">-</span> explainresponses:  utter<span class="token italic"><span class="token punctuation">_</span>explain<span class="token punctuation">_</span></span>why_budget:  <span class="token list punctuation">-</span> text: We need to know your budget to recommend a subscription  utter<span class="token italic"><span class="token punctuation">_</span>explain<span class="token punctuation">_</span></span>why_email:  <span class="token list punctuation">-</span> text: We need your email so we can contact you</code></pre><p>最后，我们需要为解释目的添加一些NLU数据：</p><pre class=" language-markdown"><code class="language-markdown"><span class="token title important"><span class="token punctuation">##</span> intent:explain</span><span class="token list punctuation">-</span> why<span class="token list punctuation">-</span> why is that<span class="token list punctuation">-</span> why do you need it<span class="token list punctuation">-</span> why do you need to know that?<span class="token list punctuation">-</span> could you explain why you need it?</code></pre><p>然后，您可以重新训练您的机器人并再次进行测试:</p><pre><code>rasa trainrasa shell</code></pre><blockquote><p>注意</p><p>您需要为该广告位的每个<code>requested_slot</code>插槽值添加一个故事，以便该机器人可以处理“为什么要知道”的每种情况</p></blockquote><p>别忘了为您<code>test_stories.md</code>的测试添加一些端到端的故事。</p><blockquote><p>注意</p><p>这是我们为处理意外用户输入而修改的文件的最小清单</p><ul><li><p><code>actions.py</code>：定义 <code>action_greet</code></p></li><li><p><code>data/nlu.md</code>：添加<code>explain</code>意图的训练数据</p></li><li><p><code>domain.yml</code>：</p><blockquote><ul><li>地图意图<code>greet</code>以 <code>action_greet_user</code></li><li>使<code>requested_slot</code>一个分类时隙与所有需要的时隙作为值</li><li>添加<code>explain</code>意图</li><li>添加针对上下文问题打扰的回复</li></ul></blockquote></li><li><p><code>data/stories.md</code>：</p><blockquote><ul><li>如果有故事，请使用映射的意图删除故事</li><li>在填写表格时添加带有常见问题解答和上下文中断的故事</li></ul></blockquote></li></ul></blockquote><h4 id="正常失败"><a href="#正常失败" class="headerlink" title="正常失败"></a>正常失败</h4><p>即使您完美地设计了机器人，用户也不可避免地会向您的助手说出您意料之外的事情。在这些情况下，您的助手将失败，因此请确保其正常运行很重要。</p><h4 id="后备政策"><a href="#后备政策" class="headerlink" title="后备政策"></a>后备政策</h4><p>最常见的故障之一是NLU置信度低，使用TwoStageFallbackPolicy可以很好地处理它。您可以通过将以下内容添加到配置文件中来启用它，</p><pre><code>policies:  - name: TwoStageFallbackPolicy    nlu_threshold: 0.8</code></pre><p>并将<code>out_of_scope</code>意图添加到<code>domain.yml</code>文件中：</p><pre><code>intents:- out_of_scope</code></pre><p>当nlu置信度降至定义的阈值以下时，机器人将提示用户重新表达其消息。如果僵尸程序无法三遍获取其消息，则将采取最终行动，例如，僵尸程序可以移交给人类。</p><p>要尝试此操作，请重新训练您的模型，然后向您的机器人发送“向我订购披萨”之类的消息：</p><pre><code>rasa trainrasa shell</code></pre><p>您还可以通过多种方式自定义此策略。在我们的演示机器人Sara中，我们对其进行了自定义，以在一定的置信度范围内向用户建议意图，以使用户更轻松地向机器人提供所需的信息。</p><p>这是通过自定义动作来完成的，<code>ActionDefaultAskAffirmation</code>如<a href="#">Sara rasa-demo动作服务器中所示。</a> 我们定义了一些意图映射，以使用户对意图的理解更加直观。</p><img src="/2020/10/02/rasa-zhi-nan/7.png" style="zoom:33%;"><h4 id="输出范围意图"><a href="#输出范围意图" class="headerlink" title="输出范围意图"></a>输出范围意图</h4><p>优良作法是也要处理您知道用户可能会问的问题，但您尚未针对这些问题实现用户目标。</p><p>您可以定义一个<code>out_of_scope</code>意图来处理超出范围的通用请求，例如“我饿了”，并让漫游器响应默认消息，例如“抱歉，我无法处理该请求”：</p><pre><code>* out_of_scope  utter_out_of_scope</code></pre><p>我们还需要为<code>out_of_scope</code>意图添加NLU数据：</p><pre><code>## intent:out_of_scope- I want to order food- What is 2 + 2?- Who’s the US President?- I need a job</code></pre><p>最后，我们将对<code>domain.yml</code>文件添加响应：</p><pre><code>responses:  utter_out_of_scope:  - text: Sorry, I can’t handle that request.</code></pre><p>现在，我们可以重新训练并测试此添加项</p><pre><code>rasa trainrasa shell</code></pre><p>再往前走，如果您发现用户要求某些事情，将来又希望成为用户目标，则可以将它们作为单独的意图进行处理，以使用户知道您已经理解了他们的信息，但是还没有解决方案。例如，假设用户问“我想在Rasa申请工作”，然后我们可以回答“我了解您正在寻找工作，但我恐怕还无法处理该技能。”</p><pre><code>* ask_job  utter_job_not_handled</code></pre><blockquote><p>注意</p><p>这是我们为帮助助手正常退出而修改的文件的最小清单：</p><ul><li><p><code>data/nlu.md</code>：</p><blockquote><ul><li>添加<code>out_of_scope</code>意图和要单独处理的任何特定范围外意图的训练数据</li></ul></blockquote></li><li><p><code>data/stories.md</code>：</p><blockquote><ul><li>为任何超出范围的意图添加故事</li></ul></blockquote></li><li><p><code>domain.yml</code>：</p><blockquote><ul><li>添加<code>out_of_scope</code>意图和任何超出范围的意图</li><li><code>utter_out_of_scope</code>为任何超出范围的意图添加一个或多个响应</li></ul></blockquote></li><li><p><code>actions.py</code>：</p><blockquote><ul><li>自定义<code>ActionDefaultAskAffirmation</code>建议用户的意图，供用户选择</li></ul></blockquote></li><li><p><code>config.yml</code>：</p><blockquote><ul><li>将TwoStageFallbackPolicy添加到该<code>policies</code>部分</li></ul></blockquote></li></ul></blockquote><h4 id="更复杂的上下文对话"><a href="#更复杂的上下文对话" class="headerlink" title="更复杂的上下文对话"></a>更复杂的上下文对话</h4><p>并非您定义的每个用户目标都将属于业务逻辑类别。对于其他情况，您将需要使用故事和上下文来帮助用户实现其目标。</p><p>如果我们以Sara的“入门”技能为例，我们想根据他们是否曾经构建过AI助手并且正在从其他工具迁移而来，向他们提供不同的信息。这可以通过故事很简单地完成和<a href="#">最大历史</a>的概念。</p><pre class=" language-markdown"><code class="language-markdown"><span class="token title important"><span class="token punctuation">##</span> new to rasa + built a bot before</span> <span class="token list punctuation">*</span> how<span class="token italic"><span class="token punctuation">_</span>to<span class="token punctuation">_</span></span>get_started   <span class="token list punctuation">-</span> utter_getstarted   <span class="token list punctuation">-</span> utter<span class="token italic"><span class="token punctuation">_</span>first<span class="token punctuation">_</span></span>bot<span class="token italic"><span class="token punctuation">_</span>with<span class="token punctuation">_</span></span>rasa <span class="token list punctuation">*</span> affirm   <span class="token list punctuation">-</span> action<span class="token italic"><span class="token punctuation">_</span>set<span class="token punctuation">_</span></span>onboarding   <span class="token list punctuation">-</span> slot<span class="token entity" title="&#123;">&amp;#123;</span>"onboarding": true<span class="token entity" title="&#125;">&amp;#125;</span>   <span class="token list punctuation">-</span> utter<span class="token italic"><span class="token punctuation">_</span>built<span class="token punctuation">_</span></span>bot_before <span class="token list punctuation">*</span> affirm   <span class="token list punctuation">-</span> utter<span class="token italic"><span class="token punctuation">_</span>ask<span class="token punctuation">_</span></span>migration <span class="token list punctuation">*</span> deny   <span class="token list punctuation">-</span> utter<span class="token italic"><span class="token punctuation">_</span>explain<span class="token punctuation">_</span></span>rasa_components   <span class="token list punctuation">-</span> utter<span class="token italic"><span class="token punctuation">_</span>rasa<span class="token punctuation">_</span></span>components_details   <span class="token list punctuation">-</span> utter<span class="token italic"><span class="token punctuation">_</span>ask<span class="token punctuation">_</span></span>explain_nlucorex <span class="token list punctuation">*</span> affirm   <span class="token list punctuation">-</span> utter<span class="token italic"><span class="token punctuation">_</span>explain<span class="token punctuation">_</span></span>nlu   <span class="token list punctuation">-</span> utter<span class="token italic"><span class="token punctuation">_</span>explain<span class="token punctuation">_</span></span>core   <span class="token list punctuation">-</span> utter<span class="token italic"><span class="token punctuation">_</span>explain<span class="token punctuation">_</span></span>x   <span class="token list punctuation">-</span> utter<span class="token italic"><span class="token punctuation">_</span>direct<span class="token punctuation">_</span></span>to_step2 <span class="token title important"><span class="token punctuation">##</span> not new to rasa + core</span> <span class="token list punctuation">*</span> how<span class="token italic"><span class="token punctuation">_</span>to<span class="token punctuation">_</span></span>get_started   <span class="token list punctuation">-</span> utter_getstarted   <span class="token list punctuation">-</span> utter<span class="token italic"><span class="token punctuation">_</span>first<span class="token punctuation">_</span></span>bot<span class="token italic"><span class="token punctuation">_</span>with<span class="token punctuation">_</span></span>rasa <span class="token list punctuation">*</span> deny   <span class="token list punctuation">-</span> action<span class="token italic"><span class="token punctuation">_</span>set<span class="token punctuation">_</span></span>onboarding   <span class="token list punctuation">-</span> slot<span class="token entity" title="&#123;">&amp;#123;</span>"onboarding": false<span class="token entity" title="&#125;">&amp;#125;</span>   <span class="token list punctuation">-</span> utter<span class="token italic"><span class="token punctuation">_</span>ask<span class="token punctuation">_</span></span>which_product <span class="token list punctuation">*</span> how<span class="token italic"><span class="token punctuation">_</span>to<span class="token punctuation">_</span></span>get_started<span class="token entity" title="&#123;">&amp;#123;</span>"product": "core"<span class="token entity" title="&#125;">&amp;#125;</span>   <span class="token list punctuation">-</span> utter<span class="token italic"><span class="token punctuation">_</span>explain<span class="token punctuation">_</span></span>core   <span class="token list punctuation">-</span> utter<span class="token italic"><span class="token punctuation">_</span>anything<span class="token punctuation">_</span></span>else</code></pre><p>上面的示例主要利用意图来指导流程，但是您也可以使用实体和插槽来指导流程。例如，如果用户在一开始就向您提供了Rasa的新信息，则您可能希望通过将此信息存储在插槽中来跳过此问题。</p><pre class=" language-markdown"><code class="language-markdown"><span class="token list punctuation">*</span> how<span class="token italic"><span class="token punctuation">_</span>to<span class="token punctuation">_</span></span>get<span class="token italic"><span class="token punctuation">_</span>started&amp;#123;"user<span class="token punctuation">_</span></span>type": "new"<span class="token entity" title="&#125;">&amp;#125;</span>  <span class="token list punctuation">-</span> slot<span class="token entity" title="&#123;">&amp;#123;</span>"user_type":"new"<span class="token entity" title="&#125;">&amp;#125;</span>  <span class="token list punctuation">-</span> action<span class="token italic"><span class="token punctuation">_</span>set<span class="token punctuation">_</span></span>onboarding  <span class="token list punctuation">-</span> slot<span class="token entity" title="&#123;">&amp;#123;</span>"onboarding": true<span class="token entity" title="&#125;">&amp;#125;</span>  <span class="token list punctuation">-</span> utter<span class="token italic"><span class="token punctuation">_</span>getstarted<span class="token punctuation">_</span></span>new  <span class="token list punctuation">-</span> utter<span class="token italic"><span class="token punctuation">_</span>built<span class="token punctuation">_</span></span>bot_before</code></pre><p>为此，请记住必须在<code>domain.yml</code> 文件中设置该插槽的功能。这次我们可以使用<code>text</code>插槽类型，因为我们只在乎是否<a href="#">设置</a>了 <a href="#">插槽</a>。</p><h5 id="用ML概括"><a href="#用ML概括" class="headerlink" title="用ML概括"></a>用ML概括</h5><p>除了上文所述的基于规则的策略外，Core还提供了一些可以使用的ML策略。这些作为策略配置中的附加层进入，只有在用户遵循您未曾预料到的路径时才跳入。<strong>重要的是要了解使用这些策略并不意味着放开对助手的控制权。</strong>如果基于规则的策略能够做出预测，则该预测将始终具有更高的优先级（<a href="#">在此处</a>了解更多<a href="#">信息</a>）并预测下一个操作。基于ML的策略为您的助手提供了不失败的机会，而如果不使用它们，您的助手肯定会失败，就像在基于状态机的对话系统中一样。</p><p>这些类型的意外用户行为是我们的<a href="#">TEDPolicy</a>非常<a href="#">擅长</a>处理的事情。它可以学习在用户试图完成的主要用户目标中的一些插词之后使用户回到正轨。例如，在下面的对话中（摘自<a href="#">Rasa X</a>上的对话）：</p><pre class=" language-markdown"><code class="language-markdown"><span class="token title important"><span class="token punctuation">##</span> Story from conversation with a2baab6c83054bfaa8d598459c659d2a on November 28th 2019</span><span class="token list punctuation">*</span> greet  <span class="token list punctuation">-</span> action<span class="token italic"><span class="token punctuation">_</span>greet<span class="token punctuation">_</span></span>user  <span class="token list punctuation">-</span> slot<span class="token entity" title="&#123;">&amp;#123;</span>"shown_privacy":true<span class="token entity" title="&#125;">&amp;#125;</span><span class="token list punctuation">*</span> ask_whoisit  <span class="token list punctuation">-</span> action_chitchat<span class="token list punctuation">*</span> ask_whatspossible  <span class="token list punctuation">-</span> action_chitchat<span class="token list punctuation">*</span> telljoke  <span class="token list punctuation">-</span> action_chitchat<span class="token list punctuation">*</span> how<span class="token italic"><span class="token punctuation">_</span>to<span class="token punctuation">_</span></span>get_started<span class="token entity" title="&#123;">&amp;#123;</span>"product":"x"<span class="token entity" title="&#125;">&amp;#125;</span>  <span class="token list punctuation">-</span> slot<span class="token entity" title="&#123;">&amp;#123;</span>"product":"x"<span class="token entity" title="&#125;">&amp;#125;</span>  <span class="token list punctuation">-</span> utter<span class="token italic"><span class="token punctuation">_</span>explain<span class="token punctuation">_</span></span>x  <span class="token list punctuation">-</span> utter<span class="token italic"><span class="token punctuation">_</span>also<span class="token punctuation">_</span></span>explain_nlucore<span class="token list punctuation">*</span> affirm  <span class="token list punctuation">-</span> utter<span class="token italic"><span class="token punctuation">_</span>explain<span class="token punctuation">_</span></span>nlu  <span class="token list punctuation">-</span> utter<span class="token italic"><span class="token punctuation">_</span>explain<span class="token punctuation">_</span></span>core  <span class="token list punctuation">-</span> utter<span class="token italic"><span class="token punctuation">_</span>direct<span class="token punctuation">_</span></span>to_step2</code></pre><p>在这里，我们可以看到用户首先完成了一些闲聊任务，然后最终询问他们如何开始使用RasaX。TEDPolicy正确地预测了应该向用户解释Rasa X，然后也将其沿用下来路径，而不先问所有符合条件的问题。</p><p>由于在这种情况下机器学习策略的推广效果很好，因此有必要将此故事添加到您的训练数据中，以不断改进您的机器人并帮助机器学习进一步推广。<a href="#">Rasa X</a>是一款可以帮助您改善bot并使它与上下文相关的工具。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> nlp </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Rasa </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PaddleOCR Docker化部署服务</title>
      <link href="/2020/10/02/paddleocr-docker-hua-bu-shu-fu-wu/"/>
      <url>/2020/10/02/paddleocr-docker-hua-bu-shu-fu-wu/</url>
      
        <content type="html"><![CDATA[<h1 id="PaddleOCR-Docker化部署服务"><a href="#PaddleOCR-Docker化部署服务" class="headerlink" title="PaddleOCR Docker化部署服务"></a>PaddleOCR Docker化部署服务</h1><h2 id="准备Dockerfile"><a href="#准备Dockerfile" class="headerlink" title="准备Dockerfile"></a>准备Dockerfile</h2><pre class=" language-shell"><code class="language-shell"># Version: 1.0.0FROM hub.baidubce.com/paddlepaddle/paddle:latest-gpu-cuda9.0-cudnn7-dev# PaddleOCR base on Python3.7#RUN pip3.7 install --upgrade pip -i https://pypi.tuna.tsinghua.edu.cn/simpleRUN pip3.7 install --upgrade pip -i https://mirror.baidu.com/pypi/simple#RUN python3.7 -m pip install paddlepaddle==1.7.2 -i https://pypi.tuna.tsinghua.edu.cn/simpleRUN python3.7 -m pip install paddlepaddle==1.7.2 -i https://mirror.baidu.com/pypi/simple#RUN pip3.7 install paddlehub --upgrade -i https://pypi.tuna.tsinghua.edu.cn/simpleRUN pip3.7 install paddlehub --upgrade -i https://mirror.baidu.com/pypi/simple#RUN git clone https://gitee.com/PaddlePaddle/PaddleOCRRUN cd / && git clone -b develop --depth 1 https://github.com/PaddlePaddle/PaddleOCR.gitWORKDIR /PaddleOCR#RUN pip3.7 install -r requirments.txt -i https://pypi.tuna.tsinghua.edu.cn/simpleRUN pip3.7 install -r requirments.txt -i https://mirror.baidu.com/pypi/simpleRUN mkdir -p /PaddleOCR/inference/ch_rec_mv3_crnnRUN mkdir -p /PaddleOCR/inference/ch_det_mv3_db# Download orc detect model(light version). if you want to change normal version, you can change ch_det_mv3_db_infer to ch_det_r50_vd_db_infer, also remember$#ADD https://paddleocr.bj.bcebos.com/ch_models/ch_det_mv3_db_infer.tar /PaddleOCR/inference#ADD https://paddleocr.bj.bcebos.com/ch_models/ch_rec_r34_vd_crnn_enhance_infer.tar  /PaddleOCR/inferenceADD https://paddleocr.bj.bcebos.com/ch_models/ch_rec_r34_vd_crnn_infer.tar  /PaddleOCR/inference#RUN tar xf /PaddleOCR/inference/ch_rec_r34_vd_crnn_enhance_infer.tar --strip-components 1  -C /PaddleOCR/inference/ch_rec_mv3_crnnRUN tar xf /PaddleOCR/inference/ch_rec_r34_vd_crnn_infer.tar --strip-components 1  -C /PaddleOCR/inference/ch_rec_mv3_crnn# Download orc recognition model(light version). If you want to change normal version, you can change ch_rec_mv3_crnn_infer to ch_rec_r34_vd_crnn_enhance_inf$#ADD https://paddleocr.bj.bcebos.com/ch_models/ch_rec_mv3_crnn_infer.tar /PaddleOCR/inferenceADD https://paddleocr.bj.bcebos.com/ch_models/ch_det_r50_vd_db_infer.tar  /PaddleOCR/inferenceRUN tar xf /PaddleOCR/inference/ch_det_r50_vd_db_infer.tar --strip-components 1  -C /PaddleOCR/inference/ch_det_mv3_dbEXPOSE 8866CMD ["/bin/bash","-c","export PYTHONPATH=. && hub install deploy/hubserving/ocr_system/ && hub serving start -m ocr_system"]</code></pre><h2 id="构建镜像"><a href="#构建镜像" class="headerlink" title="构建镜像"></a>构建镜像</h2><pre class=" language-shell"><code class="language-shell">docker build -t paddleocr:cpu . </code></pre><h2 id="创建容器"><a href="#创建容器" class="headerlink" title="创建容器"></a>创建容器</h2><pre class=" language-shell"><code class="language-shell">docker run -dp 8866:8866 --name paddle_ocr paddleocr:cpu</code></pre><h2 id="测试CURL"><a href="#测试CURL" class="headerlink" title="测试CURL"></a>测试CURL</h2><pre class=" language-shell"><code class="language-shell">curl -H "Content-Type:application/json" -X POST --data "&#123;\"images\": [\"填入图片Base64编码(需要删除'data:image/jpg;base64,'）\"]&#125;" http://localhost:8866/predict/ocr_system</code></pre><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://gitee.com/paddlepaddle/PaddleOCR/tree/develop/deploy/docker/hubserving">https://gitee.com/paddlepaddle/PaddleOCR/tree/develop/deploy/docker/hubserving</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 运维 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> OCR </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>算法基础</title>
      <link href="/2020/09/28/suan-fa-ji-chu/"/>
      <url>/2020/09/28/suan-fa-ji-chu/</url>
      
        <content type="html"><![CDATA[<h1 id="算法基础"><a href="#算法基础" class="headerlink" title="算法基础"></a>算法基础</h1><h2 id="快速排序"><a href="#快速排序" class="headerlink" title="快速排序"></a>快速排序</h2><p>给定你一个长度为n的整数数列。</p><p>请你使用快速排序对这个数列按照从小到大进行排序。</p><p>并将排好序的数列按顺序输出。</p><ul><li>输入格式</li></ul><p>输入共两行，第一行包含整数 n。</p><p>第二行包含 n 个整数（所有整数均在1~109109范围内），表示整个数列。</p><ul><li>输出格式</li></ul><p>输出共一行，包含 n 个整数，表示排好序的数列。</p><ul><li>数据范围</li></ul><p>1≤<em>n</em>≤1000001≤n≤100000</p><ul><li>输入样例：</li></ul><pre><code>53 1 2 4 5</code></pre><ul><li>输出样例：</li></ul><pre><code>1 2 3 4 5</code></pre><h3 id="基本思路"><a href="#基本思路" class="headerlink" title="基本思路"></a>基本思路</h3><ol><li><p>确定中间分界点</p></li><li><p>所有小于分界点的在左侧，等于分界点的在中间，大于分界点的在右侧</p></li><li><p>递归排序</p></li></ol><img src="/2020/09/28/suan-fa-ji-chu/快速排序.png" style="zoom:80%;"><h3 id="模板"><a href="#模板" class="headerlink" title="模板"></a>模板</h3><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true">## python</span><span class="token keyword">from</span> typing <span class="token keyword">import</span> List<span class="token keyword">class</span> <span class="token class-name">Solution</span><span class="token punctuation">(</span>object<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">sort_list</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> nums<span class="token punctuation">:</span>List<span class="token punctuation">)</span><span class="token punctuation">:</span>        n <span class="token operator">=</span> len<span class="token punctuation">(</span>nums<span class="token punctuation">)</span>        <span class="token keyword">if</span> n <span class="token operator">&lt;=</span> <span class="token number">1</span><span class="token punctuation">:</span>            <span class="token keyword">return</span> nums        mid <span class="token operator">=</span> nums<span class="token punctuation">[</span>n<span class="token operator">//</span><span class="token number">2</span><span class="token punctuation">]</span>        right <span class="token operator">=</span> <span class="token punctuation">[</span>x <span class="token keyword">for</span> x <span class="token keyword">in</span> nums <span class="token keyword">if</span> x <span class="token operator">></span> mid<span class="token punctuation">]</span>        middle <span class="token operator">=</span> <span class="token punctuation">[</span>x <span class="token keyword">for</span> x <span class="token keyword">in</span> nums <span class="token keyword">if</span> x <span class="token operator">==</span> mid<span class="token punctuation">]</span>        left <span class="token operator">=</span> <span class="token punctuation">[</span>x <span class="token keyword">for</span> x <span class="token keyword">in</span> nums <span class="token keyword">if</span> x <span class="token operator">&lt;</span> mid<span class="token punctuation">]</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>sort_list<span class="token punctuation">(</span>left<span class="token punctuation">)</span> <span class="token operator">+</span> middle <span class="token operator">+</span> self<span class="token punctuation">.</span>sort_list<span class="token punctuation">(</span>right<span class="token punctuation">)</span></code></pre><h2 id="归并排序"><a href="#归并排序" class="headerlink" title="归并排序"></a>归并排序</h2><h3 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h3><ul><li><p>找分界点　mid = (l-r)//2</p></li><li><p>递归排序left right</p></li><li><p>归并　–&gt; 合而为一</p></li></ul><p><img src="/2020/09/28/suan-fa-ji-chu/%E5%BD%92%E5%B9%B6%E6%8E%92%E5%BA%8F.png"></p><p>模板</p><pre class=" language-python"><code class="language-python"><span class="token keyword">from</span> typing <span class="token keyword">import</span> List<span class="token keyword">class</span> <span class="token class-name">Solution</span><span class="token punctuation">(</span>object<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">merge_sort</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> list1<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> len<span class="token punctuation">(</span>list1<span class="token punctuation">)</span> <span class="token operator">&lt;=</span> <span class="token number">1</span><span class="token punctuation">:</span>            <span class="token keyword">return</span>        mid <span class="token operator">=</span> len<span class="token punctuation">(</span>list1<span class="token punctuation">)</span> <span class="token operator">//</span> <span class="token number">2</span>        L <span class="token operator">=</span> list1<span class="token punctuation">[</span><span class="token punctuation">:</span>mid<span class="token punctuation">]</span>        R <span class="token operator">=</span> list1<span class="token punctuation">[</span>mid<span class="token punctuation">:</span><span class="token punctuation">]</span>        self<span class="token punctuation">.</span>merge_sort<span class="token punctuation">(</span>L<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>merge_sort<span class="token punctuation">(</span>R<span class="token punctuation">)</span>        i <span class="token operator">=</span> j <span class="token operator">=</span> k <span class="token operator">=</span> <span class="token number">0</span>        <span class="token keyword">while</span> i <span class="token operator">&lt;</span> len<span class="token punctuation">(</span>L<span class="token punctuation">)</span> <span class="token operator">and</span> j <span class="token operator">&lt;</span> len<span class="token punctuation">(</span>R<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">if</span> L<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">&lt;=</span> R<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">:</span>                list1<span class="token punctuation">[</span>k<span class="token punctuation">]</span> <span class="token operator">=</span> L<span class="token punctuation">[</span>i<span class="token punctuation">]</span>                i <span class="token operator">+=</span> <span class="token number">1</span>            <span class="token keyword">else</span><span class="token punctuation">:</span>                list1<span class="token punctuation">[</span>k<span class="token punctuation">]</span> <span class="token operator">=</span> R<span class="token punctuation">[</span>j<span class="token punctuation">]</span>                j <span class="token operator">+=</span> <span class="token number">1</span>            k <span class="token operator">+=</span> <span class="token number">1</span>        <span class="token keyword">while</span> i <span class="token operator">&lt;</span> len<span class="token punctuation">(</span>L<span class="token punctuation">)</span><span class="token punctuation">:</span>            list1<span class="token punctuation">[</span>k<span class="token punctuation">]</span> <span class="token operator">=</span> L<span class="token punctuation">[</span>i<span class="token punctuation">]</span>            k <span class="token operator">+=</span> <span class="token number">1</span>            i <span class="token operator">+=</span> <span class="token number">1</span>        <span class="token keyword">while</span> j <span class="token operator">&lt;</span> len<span class="token punctuation">(</span>R<span class="token punctuation">)</span><span class="token punctuation">:</span>            list1<span class="token punctuation">[</span>k<span class="token punctuation">]</span> <span class="token operator">=</span> R<span class="token punctuation">[</span>j<span class="token punctuation">]</span>            k <span class="token operator">+=</span> <span class="token number">1</span>            j <span class="token operator">+=</span> <span class="token number">1</span>        <span class="token keyword">return</span> list1</code></pre><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Medical Entity Linking using Triplet Network</title>
      <link href="/2020/09/28/medical-entity-linking-using-triplet-network/"/>
      <url>/2020/09/28/medical-entity-linking-using-triplet-network/</url>
      
        <content type="html"><![CDATA[<p>国庆前完成</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Pointer Graph Networks</title>
      <link href="/2020/09/27/pointer-graph-networks/"/>
      <url>/2020/09/27/pointer-graph-networks/</url>
      
        <content type="html"><![CDATA[<p>国庆前完成</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>XgBoost原理</title>
      <link href="/2020/09/25/xgboost-yuan-li/"/>
      <url>/2020/09/25/xgboost-yuan-li/</url>
      
        <content type="html"><![CDATA[<h1 id="XgBoost原理"><a href="#XgBoost原理" class="headerlink" title="XgBoost原理"></a>XgBoost原理</h1><p>​         XGBoost全称是eXtreme Gradient Boosting,可译为极限梯度提升算法。它由陈天奇所设计,致力于让提升树突破自身的计算极限,以实现运算快速,性能优秀的工程目标。和传统的梯度提升算法相比,XGBoost进行了许多改进,它能够比其他使用梯度提升的集成算法更加快速,并且已经被认为是在分类和回归上都拥有超高性能的先进评估器。除了比赛之中,高科技行业和数据咨询等行业也已经开始逐步使用XGBoost,了解这个算法,已经成为学习机器学习中必要的一环。</p><p>​        除了比赛之中,高科技行业和数据咨询等行业也已经开始逐步使用XGBoost,了解这个算法,已经成为学习机器学习中必要的一环。 性能超强的算法往往有着复杂的原理,XGBoost也不能免俗,因此它背后的数学深奥复杂。除此之外,XGBoost与多年前就已经研发出来的算法,比如决策树,SVM等不同,它是一个集大成的机器学习算法,对大家掌握机器学习中各 种概念的程度有较高的要求。虽然要听懂今天这堂课,你不需要是一个机器学习专家,但你至少需要了解树模型是什么。如果你对机器学习比较好的了解,基础比较牢,那今天的课将会是使你融会贯通的一节课。理解XGBoost,一定能让你在机器学习上更上一层楼 。</p><h2 id="xgboost库与XGB的sklearn-API"><a href="#xgboost库与XGB的sklearn-API" class="headerlink" title="xgboost库与XGB的sklearn API"></a>xgboost库与XGB的sklearn API</h2><p>​       现在,我们有两种方式可以来使用我们的xgboost库。第一种方式,是直接使用xgboost库自己的建模流程。<br>​       xgboost documents:<a href="https://xgboost.readthedocs.io/en/latest/index.html">https://xgboost.readthedocs.io/en/latest/index.html</a></p><pre class=" language-shell"><code class="language-shell">#windowspip install xgboost #安装xgboost库pip install --upgrade xgboost #更新xgboost库#MACbrew install gcc@7pip3 install xgboostimport xgboost as xgb</code></pre><p>​        其中最核心的，是DMtarix这个读取数据的类，以及train()这个用于训练的类。与sklearn把所有的参数都写在类中的方式不同，xgboost库中必须先使用字典设定参数集，再使用train来将参数及输入，然后进行训练。会这样设计的原因，是因为XGB所涉及到的参数实在太多，全部写在xgb.train()中太长也容易出错。在这里，我为大家准备了params可能的取值以及xgboost.train的列表，给大家一个印象。</p><pre class=" language-json"><code class="language-json">params  &amp;#<span class="token number">123</span><span class="token punctuation">;</span>eta<span class="token punctuation">,</span> gamma<span class="token punctuation">,</span> max_depth<span class="token punctuation">,</span> min_child_weight<span class="token punctuation">,</span> max_delta_step<span class="token punctuation">,</span> subsample<span class="token punctuation">,</span> colsample_bytree<span class="token punctuation">,</span>colsample_bylevel<span class="token punctuation">,</span> colsample_bynode<span class="token punctuation">,</span> lambda<span class="token punctuation">,</span> alpha<span class="token punctuation">,</span> tree_method string<span class="token punctuation">,</span> sketch_eps<span class="token punctuation">,</span> scale_pos_weight<span class="token punctuation">,</span> updater<span class="token punctuation">,</span>refresh_leaf<span class="token punctuation">,</span> process_type<span class="token punctuation">,</span> grow_policy<span class="token punctuation">,</span> max_leaves<span class="token punctuation">,</span> max_bin<span class="token punctuation">,</span> predictor<span class="token punctuation">,</span> num_parallel_tree&amp;#<span class="token number">125</span><span class="token punctuation">;</span></code></pre><pre class=" language-python"><code class="language-python">xgboost<span class="token punctuation">.</span>train <span class="token punctuation">(</span>params<span class="token punctuation">,</span> dtrain<span class="token punctuation">,</span> num_boost_round<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> evals<span class="token operator">=</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> obj<span class="token operator">=</span>None<span class="token punctuation">,</span> feval<span class="token operator">=</span>None<span class="token punctuation">,</span> maximize<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>early_stopping_rounds<span class="token operator">=</span>None<span class="token punctuation">,</span> evals_result<span class="token operator">=</span>None<span class="token punctuation">,</span> verbose_eval<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> xgb_model<span class="token operator">=</span>None<span class="token punctuation">,</span> callbacks<span class="token operator">=</span>None<span class="token punctuation">,</span>learning_rates<span class="token operator">=</span>None<span class="token punctuation">)</span></code></pre><p>​       或者，我们也可以选择第二种方法，使用xgboost库中的sklearn的API。这是说，我们可以调用如下的类，并用我们sklearn当中惯例的实例化，ﬁt和predict的流程来运行XGB，并且也可以调用属性比如coef_等等。当然，这是我们回归的类，我们也有用于分类，用于排序的类。他们与回归的类非常相似，因此了解一个类即可。</p><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">xgboost</span><span class="token punctuation">.</span>XGBRegressor <span class="token punctuation">(</span>max_depth<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> learning_rate<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span> n_estimators<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span> silent<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>objective<span class="token operator">=</span><span class="token string">'reg:linear'</span><span class="token punctuation">,</span> booster<span class="token operator">=</span><span class="token string">'gbtree'</span><span class="token punctuation">,</span> n_jobs<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> nthread<span class="token operator">=</span>None<span class="token punctuation">,</span> gamma<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> min_child_weight<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> max_delta_step<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>subsample<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> colsample_bytree<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> colsample_bylevel<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> reg_alpha<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> reg_lambda<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> scale_pos_weight<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>base_score<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">,</span> random_state<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> seed<span class="token operator">=</span>None<span class="token punctuation">,</span> missing<span class="token operator">=</span>None<span class="token punctuation">,</span> importance_type<span class="token operator">=</span><span class="token string">'gain'</span><span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span></code></pre><p>​      看到这长长的参数条目，可能大家会感到头晕眼花——没错XGB就是这门复杂。但是眼尖的小伙伴可能已经发现了，调用xgboost.train和调用sklearnAPI中的类XGBRegressor，需要输入的参数是不同的，而且看起来相当的不同。但其实，<strong>这些参数只是写法不同，功能是相同的</strong>。比如说，我们的params字典中的第一个参数eta，其实就是我们XGBRegressor里面的参数learning_rate，他们的含义和实现的功能是一模一样的。只不过在sklearnAPI中，开发团队友好地帮助我们将参数的名称调节成了与sklearn中其他的算法类更相似的样子。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 树模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基于Conditional Layer Normalization的条件文本生成</title>
      <link href="/2020/09/22/ji-yu-conditional-layer-normalization-de-tiao-jian-wen-ben-sheng-cheng/"/>
      <url>/2020/09/22/ji-yu-conditional-layer-normalization-de-tiao-jian-wen-ben-sheng-cheng/</url>
      
        <content type="html"><![CDATA[<h1 id="基于Conditional-Layer-Normalization的条件文本生成"><a href="#基于Conditional-Layer-Normalization的条件文本生成" class="headerlink" title="基于Conditional Layer Normalization的条件文本生成"></a>基于Conditional Layer Normalization的条件文本生成</h1><p>​    利用Conditional Layer Normalization来将外部条件融入到预训练模型中的思路，其直接应用就是条件文本生成，但其实也不单单可以用于生成模型，也可以用于分类模型等场景（外部条件可能是其他模态的信息，来辅助分类）。</p><p>原文参考苏神的Blog <a href="https://spaces.ac.cn/archives/7124/comment-page-1">https://spaces.ac.cn/archives/7124/comment-page-1</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> nlp </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 归一化 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>中文医学文本命名实体识别</title>
      <link href="/2020/09/22/zhong-wen-yi-xue-wen-ben-ming-ming-shi-ti-shi-bie/"/>
      <url>/2020/09/22/zhong-wen-yi-xue-wen-ben-ming-ming-shi-ti-shi-bie/</url>
      
        <content type="html"><![CDATA[<h1 id="中文医学文本命名实体识别"><a href="#中文医学文本命名实体识别" class="headerlink" title="中文医学文本命名实体识别"></a>中文医学文本命名实体识别</h1><h2 id="任务简介"><a href="#任务简介" class="headerlink" title="任务简介"></a>任务简介</h2><p>​      实体识别作为信息抽取的一个重要子任务，近些年已经取得了阶段性成果。对于医学领域的自然语言文献，例如医学教材、医学百科、临床病例、医学期刊、入院记录、检验报告等，这些文本中蕴含大量医学专业知识和医学术语。将实体识别技术与医学专业领域结合，利用机器读取医学文本，可以显著提高临床科研的效率和质量，并且可服务于下游子任务。要想让机器“读懂”医学数据，核心在于让计算机在大量医学文本中准确的提取出关键信息，这就涉及到了命名实体识别、关系抽取等自然语言处理技术。医学领域中非结构化的文本，都是由中文自然语言句子或句子集合组成。实体抽取是从非结构化医学文本中找出医学实体，如疾病、症状的过程。</p><h2 id="任务详情"><a href="#任务详情" class="headerlink" title="任务详情"></a>任务详情</h2><p>本评测任务为面向中文医学文本的命名实体识别，即给定schema及句子sentence，对于给定的一组纯医学文本文档，任务的目标是识别并抽取出与医学临床相关的实体，并将他们归类到预先定义好的类别。将医学文本命名实体划分为九大类，包括：疾病，临床表现，药物，医疗设备，医疗程序，身体，医学检验项目，微生物类，科室。标注之前对文章进行自动分词处理，所有的医学实体均已正确切分。</p><h2 id="数据简介"><a href="#数据简介" class="headerlink" title="数据简介"></a>数据简介</h2><table><thead><tr><th>序号</th><th>命名实体类别</th><th>子类</th><th>标签</th><th>样例</th></tr></thead><tbody><tr><td>1</td><td>疾病</td><td>疾病或综合症</td><td>dis</td><td>尿潴留者易继发泌尿系感染。|||0 2 dis|||7 11 dis|||</td></tr><tr><td>2</td><td>临床表现</td><td>症状</td><td>sym</td><td>逐渐出现呼吸困难、阵发性喘憋，发作时呼吸快而浅，并伴有呼气性喘鸣，明显鼻扇及三凹征。|||4 7 sym|||9 13 sym|||18 31 sym|||35 35 bod|||33 36 sym|||38 40 sym|||</td></tr><tr><td>3</td><td>医疗程序</td><td>检查程序</td><td>pro</td><td>用免疫学方法检测某种病原体的特异抗原很有诊断价值，因其简单快速，常常用于早期诊断，诊断义常较抗体检测更为可靠。|||1 7 pro|||47 50 pro|||</td></tr><tr><td>4</td><td>医疗设备</td><td>检查设备</td><td>equ</td><td>（一）病原体的检出1.病原体的直接检出很多感染性疾病可以通过肉眼或显微镜直接检出病原体而确诊，这些病原体都有其形态学特征而易于认定。|||33 35 equ||| 寄生虫成虫可以在患儿的大便中看到，通过显微镜可以从大便查出各种寄生虫虫卵及阿米巴原虫等。|||0 4 mic|||31 33 mic|||</td></tr><tr><td>5</td><td>药物</td><td>药物</td><td>dru</td><td>已有2种EBV疫苗用于志愿者：表达EBVgp320的重组痘病毒疫苗和提纯病毒gp320膜糖蛋白的疫苗，有望开发应用于BV感染的预防。|||4 8 dru|||15 32 dru|||34 49 dru|||58 62 dis|||</td></tr><tr><td>6</td><td>医学检验项目</td><td>医学检验项目</td><td>ite</td><td>配方中的重要参数包括渗透压、肾溶质负荷、热能密度、黏稠度和组成成分等。|||10 12 ite|||14 18 ite|||20 23 ite|||25 27 ite|||29 32 ite|||</td></tr><tr><td>7</td><td>身体</td><td>身体物质</td><td>bod</td><td>脾破裂罕见，却为严重并发症，故检查脾脏时不宜重按。|||0 0 bod|||0 2 sym|||17 18 bod|||</td></tr><tr><td>8</td><td>科室</td><td>科室</td><td>dep</td><td>因此，应强调定期眼科随访。|||8 9 dep|||</td></tr><tr><td>9</td><td>微生物类</td><td>微生物类</td><td>mic</td><td>寄生虫成虫可以在患儿的大便中看到，通过显微镜可以从大便查出各种寄生虫虫卵及阿米巴原虫等。|||0 4 mic|||31 33 mic|||</td></tr></tbody></table><p>９种命名实体类别，</p><p>寄生虫成虫可以在患儿的大便中看到，通过显微镜可以从大便查出各种寄生虫虫卵及阿米巴原虫等。|||0 4 mic|||31 33 mic|||</p><p>文本 ||| start end category ||| start end category |||</p><h2 id="评估方法"><a href="#评估方法" class="headerlink" title="评估方法"></a>评估方法</h2><p>Precision，Recall和F1值作为评价指标。</p><h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><ol><li><p>原始文本大于1024个子，通过 [“，”,”；”]分割句子－－&gt;小于210个字</p><pre class=" language-python"><code class="language-python">   <span class="token triple-quoted-string string">"""过长句子分割"""</span>                punt_index <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>                <span class="token keyword">for</span> index<span class="token punctuation">,</span> val <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>text_list<span class="token punctuation">)</span><span class="token punctuation">:</span>                    <span class="token keyword">if</span> val <span class="token keyword">in</span> split_punt<span class="token punctuation">:</span>                        punt_index<span class="token punctuation">.</span>append<span class="token punctuation">(</span>index<span class="token punctuation">)</span>                <span class="token keyword">if</span> punt_index<span class="token punctuation">:</span>                    start <span class="token operator">=</span> <span class="token number">0</span>                    <span class="token keyword">for</span> idx <span class="token keyword">in</span> punt_index<span class="token punctuation">:</span>                        texts_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>text_list<span class="token punctuation">[</span>start<span class="token punctuation">:</span>idx<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>                        tags_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>tag_list<span class="token punctuation">[</span>start<span class="token punctuation">:</span>idx<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>                        start <span class="token operator">=</span> idx<span class="token operator">+</span><span class="token number">1</span>                    texts_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>text_list<span class="token punctuation">[</span>start<span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span>                    tags_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>tag_list<span class="token punctuation">[</span>start<span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span>                <span class="token keyword">else</span><span class="token punctuation">:</span>                    texts_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>text_list<span class="token punctuation">)</span>                    tags_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>tag_list<span class="token punctuation">)</span></code></pre></li><li><p>数据BIO标注</p><p><img src="/2020/09/22/zhong-wen-yi-xue-wen-ben-ming-ming-shi-ti-shi-bie/1.png"></p></li></ol><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>BERT+CRF</p><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">BertCrf</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> model_name_or_path<span class="token punctuation">:</span> str<span class="token punctuation">,</span> num_tags<span class="token punctuation">:</span> int <span class="token operator">=</span> <span class="token number">2</span><span class="token punctuation">,</span> batch_first<span class="token punctuation">:</span> bool <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> None<span class="token punctuation">:</span>        self<span class="token punctuation">.</span>batch_first <span class="token operator">=</span> batch_first        self<span class="token punctuation">.</span>model_name_or_path <span class="token operator">=</span> model_name_or_path        super<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>bert_config <span class="token operator">=</span> BertConfig<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_name_or_path<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>bert_config<span class="token punctuation">.</span>num_labels <span class="token operator">=</span> num_tags        self<span class="token punctuation">.</span>model_kwargs <span class="token operator">=</span> <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;'config': self.bert_config&amp;#125;</span>        self<span class="token punctuation">.</span>bertModel <span class="token operator">=</span> BertForTokenClassification<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_name_or_path<span class="token punctuation">,</span> <span class="token operator">**</span>self<span class="token punctuation">.</span>model_kwargs<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>crf_model <span class="token operator">=</span> CRF<span class="token punctuation">(</span>num_tags<span class="token operator">=</span>num_tags<span class="token punctuation">,</span> batch_first<span class="token operator">=</span>batch_first<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>lstm <span class="token operator">=</span> nn<span class="token punctuation">.</span>LSTM<span class="token punctuation">(</span>bidirectional<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> num_layers<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> input_size<span class="token operator">=</span>self<span class="token punctuation">.</span>bert_config<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> hidden_size<span class="token operator">=</span>self<span class="token punctuation">.</span>bert_config<span class="token punctuation">.</span>hidden_size<span class="token operator">//</span><span class="token number">2</span><span class="token punctuation">,</span> batch_first<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>bert_config<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> num_tags<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_ids<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span>                tags<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor <span class="token operator">=</span> None<span class="token punctuation">,</span>                attention_mask<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>ByteTensor<span class="token punctuation">]</span> <span class="token operator">=</span> None<span class="token punctuation">,</span>                token_type_ids<span class="token operator">=</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span>                decode<span class="token punctuation">:</span> bool <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">,</span>                reduction<span class="token punctuation">:</span> str <span class="token operator">=</span> <span class="token string">"mean"</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> List<span class="token punctuation">:</span>        emissions <span class="token operator">=</span> self<span class="token punctuation">.</span>bertModel<span class="token punctuation">(</span>input_ids<span class="token operator">=</span>input_ids<span class="token punctuation">,</span> attention_mask<span class="token operator">=</span>attention_mask<span class="token punctuation">,</span> token_type_ids<span class="token operator">=</span>token_type_ids<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        <span class="token comment" spellcheck="true"># emissions, _ = self.lstm(emissions)</span>        <span class="token comment" spellcheck="true"># emissions = self.fc(emissions)</span>        <span class="token comment" spellcheck="true"># 这里在seq_len的维度上去头，是去掉了[CLS]，去尾巴有两种情况</span>        <span class="token comment" spellcheck="true"># 1、是 &lt;pad> 2、[SEP]</span>        new_emissions <span class="token operator">=</span> emissions<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>  <span class="token comment" spellcheck="true">## [batch_size, seq_len-2, num_labels] 去掉cls 和 sep</span>        new_mask <span class="token operator">=</span> attention_mask<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">:</span><span class="token punctuation">]</span>  <span class="token comment" spellcheck="true">## [batch_size, seq_len-2] 去掉cls 和 sep</span>        <span class="token keyword">if</span> tags <span class="token keyword">is</span> None<span class="token punctuation">:</span>            loss <span class="token operator">=</span> None            <span class="token keyword">pass</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            new_tags <span class="token operator">=</span> tags<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>  <span class="token comment" spellcheck="true">#</span>            loss <span class="token operator">=</span> self<span class="token punctuation">.</span>crf_model<span class="token punctuation">(</span>emissions<span class="token operator">=</span>new_emissions<span class="token punctuation">,</span> tags<span class="token operator">=</span>new_tags<span class="token punctuation">,</span> mask<span class="token operator">=</span>new_mask<span class="token punctuation">.</span>byte<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> reduction<span class="token operator">=</span>reduction<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># [1]</span>        <span class="token keyword">if</span> decode<span class="token punctuation">:</span>            tag_list <span class="token operator">=</span> self<span class="token punctuation">.</span>crf_model<span class="token punctuation">.</span>decode<span class="token punctuation">(</span>emissions<span class="token operator">=</span>new_emissions<span class="token punctuation">,</span> mask<span class="token operator">=</span>new_mask<span class="token punctuation">.</span>byte<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>            <span class="token keyword">return</span> <span class="token punctuation">[</span>loss<span class="token punctuation">,</span> tag_list<span class="token punctuation">]</span>        <span class="token keyword">return</span> <span class="token punctuation">[</span>loss<span class="token punctuation">]</span></code></pre><h2 id="训练参数"><a href="#训练参数" class="headerlink" title="训练参数"></a>训练参数</h2><pre class=" language-shell"><code class="language-shell">--model_name_or_path/home/daiyizheng/.cache/torch/transformers/bert-pretrainmodel/roberta/chinese_roberta_wwm_ext_pytorch/--data_dir./input/middle_data/210--output_dir./output--do_predict</code></pre><h2 id="训练结果"><a href="#训练结果" class="headerlink" title="训练结果"></a>训练结果</h2><table><thead><tr><th></th><th>f1-score</th></tr></thead><tbody><tr><td>train</td><td>0.69</td></tr><tr><td>dev</td><td>0.63</td></tr><tr><td>test</td><td>0.61</td></tr></tbody></table><h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><pre class=" language-text"><code class="language-text">t       r           p成    B-DIS    O人    I-DIS    O亚    O    I-BOD群    O    I-BOD细    O    B-BOD胞    O    I-BOD</code></pre><p>对于这部分数据，受到主观标注影响</p><pre class=" language-text"><code class="language-text"> t          r           p肺    B-SYM    B-DIS内    I-SYM    I-DIS炎    I-SYM    I-DIS性    I-SYM    I-DIS病    I-SYM    I-DIS变    I-SYM    I-DIS</code></pre><p>模型理解错误，把肺部炎作为比重的大信息，因而分到疾病一类</p><pre class=" language-text"><code class="language-text">肺    B-SYM    B-BOD部    I-SYM    I-BOD病    I-SYM    O变    I-SYM    O</code></pre><p>分词边界，由于输入token子级别的，无法得到词向量信息，分词边界有问题</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> nlp </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 序列标注 </tag>
            
            <tag> 实体识别 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SVM原理</title>
      <link href="/2020/09/20/svm-yuan-li/"/>
      <url>/2020/09/20/svm-yuan-li/</url>
      
        <content type="html"><![CDATA[<p>#　支持向量机</p><p>支持向量机（support vector machines, SVM）是一种二分类模型，它的基本模型是定义在特征空间上的<strong>间隔最大的线性分类器</strong>，间隔最大使它有别于感知机；SVM还包括<strong>核技巧</strong>，这使它成为实质上的非线性分类器。SVM的的学习策略就是间隔最大化，可形式化为一个求解凸二次规划的问题，也等价于正则化的合页损失函数的最小化问题。SVM的的学习算法就是求解凸二次规划的最优化算法。</p><h2 id="SVM算法原理"><a href="#SVM算法原理" class="headerlink" title="SVM算法原理"></a>SVM算法原理</h2><p>SVM学习的基本想法是求解能够正确划分训练数据集并且几何间隔最大的分离超平面。如下图所示， $W \dot x + b = 0$ 即为分离超平面，对于线性可分的数据集来说，这样的超平面有无穷多个（即感知机），但是几何间隔最大的分离超平面却是唯一的。</p><p><img src="/2020/09/20/svm-yuan-li/1.jpg"></p><p>公式推导</p><p>参考李航&lt;&lt;统计学原理&gt;&gt;</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 总结 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>多轮对话框架Rasa代码解读-1</title>
      <link href="/2020/09/20/duo-lun-dui-hua-kuang-jia-rasa-dai-ma-jie-du-1/"/>
      <url>/2020/09/20/duo-lun-dui-hua-kuang-jia-rasa-dai-ma-jie-du-1/</url>
      
        <content type="html"><![CDATA[<h1 id="Rasa"><a href="#Rasa" class="headerlink" title="Rasa"></a>Rasa</h1><h2 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h2><p><img src="/2020/09/20/duo-lun-dui-hua-kuang-jia-rasa-dai-ma-jie-du-1/1.png"></p><p>两个核心的子模块：core和nlu</p><h2 id="Nlu模块"><a href="#Nlu模块" class="headerlink" title="Nlu模块"></a>Nlu模块</h2><p><img src="/2020/09/20/duo-lun-dui-hua-kuang-jia-rasa-dai-ma-jie-du-1/2.png"></p><ul><li>component：在我们做任何自然语言处理的任务时，不止是用单纯模型去做一些分类或者标注任务，在此之前，有相当一部分工作是对文本做一些预处理工作，包括但不限于：分词（尤其是中文文本），词性标注，特征提取（传统ML或者统计型方法），词库构建等等。在rasa中，这些不同的预处理工作以及后续的意图分类和实体识别都是通过单独的组件来完成，因此component在NLU中承担着完成NLU不同阶段任务的责任。component类型大致有以下几种：tokenizer,featurizer,extractor,classifier。当然还有emulators，这个主要用于进行对话仿真测试，我目前还没使用过，就不多描述这个组件了。</li><li>pipeline：有了组件之后，如何将组件按部就班，井然有序地拼装起来，并正常工作呢？因此就有了pipeline这个概念，其实在机器学习领域，pipeline这个概念已经存在很长时间了，它在很多框架中都有，比如大名鼎鼎的sklearn。使用pipeline的好处在于可以合理有序管理不同任务阶段的不同组件工具，当组件数量较多时，pipeline的好处就非常明显了。而在rasa中，pipeline的使用更为便捷，是通过yml配置文件实现。即开发者只需要定义好自己的组件，然后将组件配置在配置文件中就可以，即插即用。</li></ul><blockquote><p>tips1:这里注意一点，配置的组件名称name对应的是组件类的类名。而后面跟着的key-value键值对，对应的是组件类需要传入的初始的参数。参考</p></blockquote><ul><li>message：在rasa中，用户发送到chatbot的所有对话内容，都需要被封装在一个对象中，这个对象就是Message.而在整个rasa工作流中，存在两个不同的message封装对象，一个是UserMessage，另一个是Message。其中UserMessage是最上层的封装对象，即直接接收用户从某个平台接口传送过来的消息。而Message则是当用户消息流到NLU模块时，将用户消息进行封装。关于UserMessage的内容在后面代码详解时会涉及到，这里先解释一下Message对象。看一下它的类部分定义，其实很简单，就是将用户的对话文本，以及时间进行封装，由于这个Message是贯穿整个NLU工作流的统一数据对象，因此还承载着记忆各个组件临时生成的中间结果（比如分词和词性标注的结果）以及最终得到的意图和实体信息。其中data存放的是意图和实体信息，在后续组件处理时，还会再Message中增加一些变量存储中间结果，即set成员方法的职责。</li></ul><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Message</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>        self<span class="token punctuation">,</span> text<span class="token punctuation">:</span> Text<span class="token punctuation">,</span> data<span class="token operator">=</span>None<span class="token punctuation">,</span> output_properties<span class="token operator">=</span>None<span class="token punctuation">,</span> time<span class="token operator">=</span>None    <span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> None<span class="token punctuation">:</span>        self<span class="token punctuation">.</span>text <span class="token operator">=</span> text        self<span class="token punctuation">.</span>time <span class="token operator">=</span> time        self<span class="token punctuation">.</span>data <span class="token operator">=</span> data <span class="token keyword">if</span> data <span class="token keyword">else</span> <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;&amp;#125;</span>        <span class="token keyword">if</span> output_properties<span class="token punctuation">:</span>            self<span class="token punctuation">.</span>output_properties <span class="token operator">=</span> output_properties        <span class="token keyword">else</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>output_properties <span class="token operator">=</span> set<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">set</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> prop<span class="token punctuation">,</span> info<span class="token punctuation">,</span> add_to_output<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> None<span class="token punctuation">:</span>        self<span class="token punctuation">.</span>data<span class="token punctuation">[</span>prop<span class="token punctuation">]</span> <span class="token operator">=</span> info        <span class="token keyword">if</span> add_to_output<span class="token punctuation">:</span>            self<span class="token punctuation">.</span>output_properties<span class="token punctuation">.</span>add<span class="token punctuation">(</span>prop<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">get</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> prop<span class="token punctuation">,</span> default<span class="token operator">=</span>None<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> Any<span class="token punctuation">:</span>        <span class="token keyword">if</span> prop <span class="token operator">==</span> TEXT<span class="token punctuation">:</span>            <span class="token keyword">return</span> self<span class="token punctuation">.</span>text        <span class="token keyword">return</span> self<span class="token punctuation">.</span>data<span class="token punctuation">.</span>get<span class="token punctuation">(</span>prop<span class="token punctuation">,</span> default<span class="token punctuation">)</span></code></pre><p>对上面三个概念明确以后，下面列出不同组件的代码结构：</p><p>rasa中，已经预置了一些组件，方便用户直接使用。当然有些组件是需要先进行训练，得到模型后，才能使用，而有些则是使用正则表达式或者关键词等规则，直接就可以使用。</p><p>以CRFEntityExtractor为例，讲解一下Component的主要核心要素。</p><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">CRFEntityExtractor</span><span class="token punctuation">(</span>EntityExtractor<span class="token punctuation">)</span><span class="token punctuation">:</span>    @classmethod    <span class="token keyword">def</span> <span class="token function">required_components</span><span class="token punctuation">(</span>cls<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> List<span class="token punctuation">[</span>Type<span class="token punctuation">[</span>Component<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token punctuation">[</span>Tokenizer<span class="token punctuation">]</span> <span class="token keyword">class</span> <span class="token class-name">EntityExtractor</span><span class="token punctuation">(</span>Component<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">add_extractor_name</span><span class="token punctuation">(</span>        self<span class="token punctuation">,</span> entities<span class="token punctuation">:</span> List<span class="token punctuation">[</span>Dict<span class="token punctuation">[</span>Text<span class="token punctuation">,</span> Any<span class="token punctuation">]</span><span class="token punctuation">]</span>    <span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> List<span class="token punctuation">[</span>Dict<span class="token punctuation">[</span>Text<span class="token punctuation">,</span> Any<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">:</span>        <span class="token keyword">for</span> entity <span class="token keyword">in</span> entities<span class="token punctuation">:</span>            entity<span class="token punctuation">[</span>EXTRACTOR<span class="token punctuation">]</span> <span class="token operator">=</span> self<span class="token punctuation">.</span>name        <span class="token keyword">return</span> entities    <span class="token keyword">class</span> <span class="token class-name">Component</span><span class="token punctuation">(</span>metaclass<span class="token operator">=</span>ComponentMetaclass<span class="token punctuation">)</span><span class="token punctuation">:</span>         <span class="token keyword">pass</span></code></pre><ul><li>首先看到，该类继承了一个EntityExtractor，这是一个二级组件抽象类（我自己定义的说法），这个二层抽象类继承自Component这个一级抽象类。因为不同组件承担的任务不同，有些组件任务比较单一，可以直接继承Component比如tokenizer,classifier，而有些组件的任务比较复杂，则需要制定这一类型的二级接口，方便扩展，如featurizer,extractor。</li><li>对于CRFEntityExtractor来说，它提供了实体的抽取，同时为了进行实体抽取，需要先对文本进行分词，因此需要上游任务先完成tokenizer任务，提供tokens的中间成果。</li><li>train方法。既然是使用条件随机场来进行实体抽取，那么就需要进行模型训练。因此需要定义train方法，来训练模型。关注train方法的两个参数training_data和config。其中，config就是之前提到的配置pipeline的配置文件的读取对象。training_data是TrainingData类型的对象。你可以将其类比于pytorch中的data_loader功能，它的主要作用是对训练数据进行封装，拆分训练集验证集，做数据校验等工作。说到这里，提一下rasa支持的原始训练数据的存放格式，主要支持markdown，wit，luis等文件格式，当然也可以提供json格式的数据。rasa如何读取这些格式的训练数据则是在如下代码包里定义：</li></ul><p><img src="/2020/09/20/duo-lun-dui-hua-kuang-jia-rasa-dai-ma-jie-du-1/4.png"></p><ul><li>persist和load方法。当模型训练完成后，需要保存和加载模型，对生产环境上的实时业务流进行处理，因此需要定义persist和load方法加载模型。</li><li>process方法。这个可以说是组件里面最重要的一个方法。当前面一通操作之后，只得到了模型，如何调用这个模型并处理文本，就是process方法的工作了。最后在message中增加一个dict，名为entities，用来存放提取的实体信息，包括实体的类型，实体的在文本中的start和end的位置信息等。</li></ul><pre class=" language-python"><code class="language-python">    <span class="token keyword">def</span> <span class="token function">process</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> message<span class="token punctuation">:</span> Message<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">:</span> Any<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> None<span class="token punctuation">:</span>        entities <span class="token operator">=</span> self<span class="token punctuation">.</span>extract_entities<span class="token punctuation">(</span>message<span class="token punctuation">)</span>        entities <span class="token operator">=</span> self<span class="token punctuation">.</span>add_extractor_name<span class="token punctuation">(</span>entities<span class="token punctuation">)</span>        message<span class="token punctuation">.</span>set<span class="token punctuation">(</span>ENTITIES<span class="token punctuation">,</span> message<span class="token punctuation">.</span>get<span class="token punctuation">(</span>ENTITIES<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> entities<span class="token punctuation">,</span> add_to_output<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span></code></pre><blockquote><p>tips2：对于对话中，涉及到的所有intent和实体，均需要在配置文件中进行定义，方便各个组件在做相应的文本分析时进行lookup-table查找。这个配置文件叫domain.yml，一个简单的实例如下：</p></blockquote><p><img src="/2020/09/20/duo-lun-dui-hua-kuang-jia-rasa-dai-ma-jie-du-1/6.png"></p><pre class=" language-python"><code class="language-python">tips3：这里说一个实用技巧。在实际的对话场景中，用户的一个utterance（表达）通常会带有不止一个意图，有的人会将这种情况当做一个复合型单意图，将其添加到domain配置文件中。但是实际上大可以不必这么麻烦，此时相当于从一个意图多分类问题，转变为一个意图多标签分类问题，即每条数据可能不止一个标签，此时只需要将模型的最后一层softmax层，替换为n个sigmoid分类器就可以。在训练数据中，我则需要配置这种训练数据，将多个意图使用某个符号<span class="token string">"+"</span>或者<span class="token string">"_"</span>等进行字符串拼接。在classifier中进行处理。这样就无需在domain配置文件中配置诸如inform_affirm这样冗余的意图了。</code></pre><p>rasa中已经集成了许多有用的组件，可以看到针对中文文本，有jieba分词，另外还有专门对时间信息进行提取的组件ducklingHTTPExctractor,要使用这些组件都需要安装相应的依赖包。</p><h2 id="core"><a href="#core" class="headerlink" title="core"></a>core</h2><p><img src="/2020/09/20/duo-lun-dui-hua-kuang-jia-rasa-dai-ma-jie-du-1/3.png"></p><p>除去core根路径下的一些公共资源和入口方法外，可以看出core根据包名大概分成以下几个部分：</p><ul><li>actions,该包下面主要存放的是action具体的实现类。关于action的具体定义和描述在后面会有详细讲解，简单说就是chatbot执行的一些动作。</li><li>channels,该包下面主要存放的是rasa与前端平台进行对接的接口。因为rasa本身只提供对话系统的功能服务，具体还需要与用户在前端界面进行交互，这个包里定义了不同的接口和不同平台进行对接。例如，console.py，定义了最简单的直接在shell命令行中进行对话交互的接口。</li><li>events，这个是rasa中定义的chatbot能执行的最小粒度的动作。与action有一些关系，我们可以通过action调用不同的events来实现不同的操作。events的实例有“SlotSet”(槽位填充),”Restarted”（重启对话，将所有状态重置）等等。</li><li>nlg，rasa的response生成模块，即生产chatbot返回给用户的消息。目前，rasa支持通过模板生成话术，也支持通过machine learning的方式做NLG。nlg模块中定义了方法读取domain.yml中的预定义的话术模板，然后生成具体的消息。</li><li>policies，此模块是core最上层的对话管理控制模块。该包中，定义了不同类型的对话管理策略，rasa将依据这些策略，执行不同actions，完成多轮对话任务。这些策略包括人工规则策略如form_policy,memoization等，也包括通过机器学习、深度学习进行训练得到策略模型，如sklearn_policy,keras_policy等。</li><li>schemas，这里主要放置core的配置文件domian.yml，这个配置文件主要配置槽位定义，实体定义，话术模板，使用的actions的名称定义以及其他系统配置。开发者在开发自己的对话系统时，需要自定义这个配置文件来覆盖源码中预定义的配置。</li><li>training,这里主要存放的是如何将准备的数据转化为对话系统可训练的转化方法以及可视化方法。</li></ul><p>根路径上也有很多重要的类文件，这里就不全部拿出来说了，挑几个重点的说一下。</p><ul><li>agent，这是core专门设计的一个接口，可以将其视作bot主体，主要作用是封装和调用rasa中最重要的一些功能方法，包括上述提到的几个包里的功能模块。</li><li>featurizers: 这个文件主要是定义了一些方法将对话数据特征化，目的是为了将对话数据用于机器学习的训练。</li><li>interpreter: 这个方法是core与nlu的一个纽带，rasa管理模块通过定义interpreter类方法，调用nlu中的parser方法来对用户的发送到bot的消息文本进行实体抽取、意图识别等操作。</li><li>processor：定义了MessageProcess类，供agent调用，功能是有序得调用不同对话功能组件，例如调用interpreter解析用户文本、调用本轮对话的action完成一些操作、根据policy得到下一步的action、记录对话状态等。</li><li>trackers：这个也是rasa中比较重要的一个对象，它的作用是rasa对话系统中的状态记录器，每一轮对话中，对话的状态信息都会进行更新并保存在这个对象中。例如当前已填充的槽位、用户最后一次发送的文本、当前用户的意图等等。</li></ul><p><img src="/2020/09/20/duo-lun-dui-hua-kuang-jia-rasa-dai-ma-jie-du-1/7.png"></p><p>这次，我们以core为主视角，将这个图重新解析一遍，着重描述core在对话系统中的功能与作用。</p><img src="/2020/09/20/duo-lun-dui-hua-kuang-jia-rasa-dai-ma-jie-du-1/8.png" alt="8" style="zoom:75%;"><h3 id="InputChannel和OutputChannel"><a href="#InputChannel和OutputChannel" class="headerlink" title="InputChannel和OutputChannel"></a>InputChannel和OutputChannel</h3><p>OutputChannel封装了chatbot需要返回给用户的信息，需要注意，chatbot返回的消息不一定是纯文本，还可能是html，json，文件附件等等，因此需要OutputChannel这个统一接口进行封装处理，因此chatbot可以支持让用户进行点选功能（当然，前提是前端界面支持点选的适配）。</p><p>InputChannel主要负责将用户输入连同用户的身份信息封装成UserMessage对象，方便后面的Processor处理。对应的，如果在上一轮对话中，OutputChannel是点选或者其他非单纯文本输出，那么本轮对话中的InputChannel也需要接受用户点选或者其他非单纯文本的输入，封装成最终的UserMessage。</p><h3 id="UserMessage"><a href="#UserMessage" class="headerlink" title="UserMessage"></a>UserMessage</h3><p>关于UserMessage对象，上一篇文章中已经有过介绍，这里给出其具体的成员变量封装情况</p><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">UserMessage</span><span class="token punctuation">:</span>    DEFAULT_SENDER_ID <span class="token operator">=</span> <span class="token string">"default"</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>        self<span class="token punctuation">,</span>        text<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Text<span class="token punctuation">]</span> <span class="token operator">=</span> None<span class="token punctuation">,</span>        output_channel<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token string">"OutputChannel"</span><span class="token punctuation">]</span> <span class="token operator">=</span> None<span class="token punctuation">,</span>        sender_id<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Text<span class="token punctuation">]</span> <span class="token operator">=</span> None<span class="token punctuation">,</span>        parse_data<span class="token punctuation">:</span> Dict<span class="token punctuation">[</span>Text<span class="token punctuation">,</span> Any<span class="token punctuation">]</span> <span class="token operator">=</span> None<span class="token punctuation">,</span>        input_channel<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Text<span class="token punctuation">]</span> <span class="token operator">=</span> None<span class="token punctuation">,</span>        message_id<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Text<span class="token punctuation">]</span> <span class="token operator">=</span> None<span class="token punctuation">,</span>        metadata<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Dict<span class="token punctuation">]</span> <span class="token operator">=</span> None<span class="token punctuation">,</span>    <span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> None<span class="token punctuation">:</span></code></pre><p>可以看到作为贯穿整个core处理流程的用户消息对象，它的成员结构还是比较清晰的，包括了用户发送的文本，定义的OutputChannel类型，用户的id，parse_data(主要存放用户自己定义的实体键值对，开发调试用),inputChannel类型，以及message的id。</p><h3 id="policy"><a href="#policy" class="headerlink" title="policy"></a>policy</h3><p>对话管理策略是多轮对话系统的核心功能，相当于对话系统的大脑，它负责根据当前用户的反馈，告诉Processor当前轮对话中需要采取的后续action，以及如何更新对话状态信息等。rasa支持人工规则的策略，也支持机器学习、深度学习得到的数据驱动策略。</p><p>以Form_policy为例，这个策略是一种表单策略，对应的rasa预置了一种类型的action，叫form的action。这种action会将所有槽位作为表单的属性column，每一轮对话，都会去主动询问用户，引导用户将这些表单的属性填充，直到所有属性填充完成。而form_policy的核心就是检索当前是否配置了form类型的action，如果是，则将下一步的action置为form。有关action的描述将在后面详细给出。可以看出这是一个典型的人工规则策略。</p><p>在一次对话任务中，可以使用多个policy的组合来帮助bot完成既定的任务。比如策略A是一个使用深度学习训练得到的一个策略模型，但是一般使用data-driven得到的模型不会达到100%的准确率，总会有bad case的情况，此时如果只是用该策略，那么会话极有可能会陷入到bad case中，因此需要一个兜底的策略在策略A的bad case发生时，让对话能够平稳进行下去。rasa就预置了这样一个策略，叫fall_back，将fall_back与策略A进行组合，就能得到一个更加鲁棒的一个对话策略。</p><p>在实际项目生产中，如果在项目初期，领域数据比较少的情况下，通常会选择form policy或者其他规则型策略。当产品上线，在积累到一定的数据后，可以使用一些data-driven的模型来做策略。</p><h3 id="domain"><a href="#domain" class="headerlink" title="domain"></a>domain</h3><p>这个对象的数据来自于前述章节提到的配置文件domain.yml。该对象定义不同的方法，从配置文件domain.yml读取槽位模板，话术模板，定义的action名称,自定义的policy名称等信息，并封装到domain对象中。domain对象可以在action执行时为其提供槽位信息以及话术模板等字段。</p><p>设计domain的好处在哪儿呢？</p><p>个人认为主要是方便管理对话系统需要使用的模板信息。这里的模板信息包含定义的槽位，意图、实体、话术模板、自定义action、自定义policy。如果需要添加或者修改这些信息，只需要修改domain.yml里面的信息就可以了，不需要去修改任何代码，让配置和代码解耦。</p><p>列举一个本人在做项目时的一个domain使用实例。做对话系统的对话记录保存时，需要保存每一轮对话中chatbot发送给用户的信息。但是因为很多bot message都是读取的domain.yml中的话术模板再配置参数动态生成的。因此可以直接使用Domain的load方法读取yml中的键值对，生成一个字典。然后可以从字典中读取相应的话术模板得到bot message。</p><h3 id="CollectionDispatcher"><a href="#CollectionDispatcher" class="headerlink" title="CollectionDispatcher"></a>CollectionDispatcher</h3><p>这个对象的主要作用是设计了各种魔法函数，处理不同类型的bot输出，并将其输出到OutputChannel中。看一下它的成员方法就能知道它的作用：</p><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">CollectingDispatcher</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""Send messages back to user"""</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> None<span class="token punctuation">:</span>        self<span class="token punctuation">.</span>messages <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token keyword">def</span> <span class="token function">utter_message</span><span class="token punctuation">(</span>        self<span class="token punctuation">,</span>        text<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Text<span class="token punctuation">]</span> <span class="token operator">=</span> None<span class="token punctuation">,</span>        image<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Text<span class="token punctuation">]</span> <span class="token operator">=</span> None<span class="token punctuation">,</span>        json_message<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Dict<span class="token punctuation">[</span>Text<span class="token punctuation">,</span> Any<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> None<span class="token punctuation">,</span>        template<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Text<span class="token punctuation">]</span> <span class="token operator">=</span> None<span class="token punctuation">,</span>        attachment<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Text<span class="token punctuation">]</span> <span class="token operator">=</span> None<span class="token punctuation">,</span>        buttons<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>List<span class="token punctuation">[</span>Dict<span class="token punctuation">[</span>Text<span class="token punctuation">,</span> Any<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> None<span class="token punctuation">,</span>        elements<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>List<span class="token punctuation">[</span>Dict<span class="token punctuation">[</span>Text<span class="token punctuation">,</span> Any<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> None<span class="token punctuation">,</span>        <span class="token operator">**</span>kwargs<span class="token punctuation">:</span> Any<span class="token punctuation">,</span>    <span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> None<span class="token punctuation">:</span>     <span class="token comment" spellcheck="true"># noinspection PyUnusedLocal</span>    <span class="token keyword">def</span> <span class="token function">utter_template</span><span class="token punctuation">(</span>        self<span class="token punctuation">,</span> template<span class="token punctuation">:</span> Text<span class="token punctuation">,</span> tracker<span class="token punctuation">:</span> Tracker<span class="token punctuation">,</span> silent_fail<span class="token punctuation">:</span> bool <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">:</span> Any    <span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> None<span class="token punctuation">:</span>        <span class="token triple-quoted-string string">""""Send a message to the client based on a template."""</span>        warnings<span class="token punctuation">.</span>warn<span class="token punctuation">(</span>            <span class="token string">"Use of `utter_template` is deprecated. "</span>            <span class="token string">"Use `utter_message(template=&lt;template_name>)` instead."</span><span class="token punctuation">,</span>            FutureWarning<span class="token punctuation">,</span>        <span class="token punctuation">)</span>        self<span class="token punctuation">.</span>utter_message<span class="token punctuation">(</span>template<span class="token operator">=</span>template<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span></code></pre><p>根据方法名就能知道不同方法的作用，例如utter_message，接受text字符串参数，可以直接输出我们定义的文本。utter_template则是从domain中的话术模板读取相应的template，**kwargs表示如果话术模板中预留了参数槽位，则使用该参数进行填充，生成最终的bot message。</p><h2 id="Processor"><a href="#Processor" class="headerlink" title="Processor"></a>Processor</h2><p>这个对象是对话系统的核心处理模块。它通过execute_action完成bot处理对话的流程。</p><blockquote><p>这里需要注意一点，在processor执行action之前，agent将会调用processor的log_message方法，使用nlu_interpreter来对用户发送的文本做实体识别和意图识别，然后将信息保存在tracker中，这个逻辑比较简单，nlu模块也在前一篇文章中详细解析过了，因此这里就不详细展开了。</p></blockquote><pre class=" language-python"><code class="language-python"> <span class="token keyword">async</span> <span class="token keyword">def</span> <span class="token function">execute_action</span><span class="token punctuation">(</span>        self<span class="token punctuation">,</span>        sender_id<span class="token punctuation">:</span> Text<span class="token punctuation">,</span>        action_name<span class="token punctuation">:</span> Text<span class="token punctuation">,</span>        output_channel<span class="token punctuation">:</span> OutputChannel<span class="token punctuation">,</span>        nlg<span class="token punctuation">:</span> NaturalLanguageGenerator<span class="token punctuation">,</span>        policy<span class="token punctuation">:</span> Text<span class="token punctuation">,</span>        confidence<span class="token punctuation">:</span> float<span class="token punctuation">,</span>    <span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> Optional<span class="token punctuation">[</span>DialogueStateTracker<span class="token punctuation">]</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># we have a Tracker instance for each user</span>        <span class="token comment" spellcheck="true"># which maintains conversation state</span>        tracker <span class="token operator">=</span> <span class="token keyword">await</span> self<span class="token punctuation">.</span>get_tracker_with_session_start<span class="token punctuation">(</span>sender_id<span class="token punctuation">,</span> output_channel<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#---------->获取历史状态信息</span>        <span class="token keyword">if</span> tracker<span class="token punctuation">:</span>            action <span class="token operator">=</span> self<span class="token punctuation">.</span>_get_action<span class="token punctuation">(</span>action_name<span class="token punctuation">)</span>            <span class="token keyword">await</span> self<span class="token punctuation">.</span>_run_action<span class="token punctuation">(</span><span class="token comment" spellcheck="true">#------------------->执行action</span>                action<span class="token punctuation">,</span> tracker<span class="token punctuation">,</span> output_channel<span class="token punctuation">,</span> nlg<span class="token punctuation">,</span> policy<span class="token punctuation">,</span> confidence            <span class="token punctuation">)</span>            <span class="token comment" spellcheck="true"># save tracker state to continue conversation from this state</span>            self<span class="token punctuation">.</span>_save_tracker<span class="token punctuation">(</span>tracker<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#------------->更新对话状态</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            logger<span class="token punctuation">.</span>warning<span class="token punctuation">(</span>                f<span class="token string">"Failed to retrieve or create tracker for conversation ID "</span>                f<span class="token string">"'&amp;#123;sender_id&amp;#125;'."</span>            <span class="token punctuation">)</span>        <span class="token keyword">return</span> tracker</code></pre><p>可以看到，该模块涉及的核心对象有两个，action和tracker（当然还有其他对象如OutputChannel,policy等）。下面分别解析这两个对象。</p><h3 id="DialogueStateTracker"><a href="#DialogueStateTracker" class="headerlink" title="DialogueStateTracker"></a>DialogueStateTracker</h3><p>从名字上就可以看到这个对象的功能：在多轮对话过程中全程记录对话状态信息。这个对象在开发自己的对话系统时，作用可是非常大的。很多对话状态信息，都可以从它这里得到。当然， 我们并不能直接去读写其定义的成员变量信息，需要通过其成员方法来操作成员变量，例如current_sate()，其核心内容如下：</p><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">DialogueStateTracker</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>        self<span class="token punctuation">,</span>        sender_id<span class="token punctuation">:</span> Text<span class="token punctuation">,</span>        slots<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Iterable<span class="token punctuation">[</span>Slot<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        max_event_history<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>int<span class="token punctuation">]</span> <span class="token operator">=</span> None<span class="token punctuation">,</span>        sender_source<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Text<span class="token punctuation">]</span> <span class="token operator">=</span> None<span class="token punctuation">,</span>    <span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> None<span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># maximum number of events to store</span>        self<span class="token punctuation">.</span>_max_event_history <span class="token operator">=</span> max_event_history        <span class="token comment" spellcheck="true"># list of previously seen events</span>        self<span class="token punctuation">.</span>events <span class="token operator">=</span> self<span class="token punctuation">.</span>_create_events<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># id of the source of the messages</span>        self<span class="token punctuation">.</span>sender_id <span class="token operator">=</span> sender_id        <span class="token comment" spellcheck="true"># slots that can be filled in this domain</span>        <span class="token keyword">if</span> slots <span class="token keyword">is</span> <span class="token operator">not</span> None<span class="token punctuation">:</span>            self<span class="token punctuation">.</span>slots <span class="token operator">=</span> <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;slot.name: copy.copy(slot) for slot in slots&amp;#125;</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>slots <span class="token operator">=</span> AnySlotDict<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># file source of the messages</span>        self<span class="token punctuation">.</span>sender_source <span class="token operator">=</span> sender_source        <span class="token comment" spellcheck="true"># if tracker is paused, no actions should be taken</span>        self<span class="token punctuation">.</span>_paused <span class="token operator">=</span> <span class="token boolean">False</span>        <span class="token comment" spellcheck="true"># A deterministically scheduled action to be executed next</span>        self<span class="token punctuation">.</span>followup_action <span class="token operator">=</span> ACTION_LISTEN_NAME        self<span class="token punctuation">.</span>latest_action_name <span class="token operator">=</span> None        <span class="token comment" spellcheck="true"># Stores the most recent message sent by the user</span>        self<span class="token punctuation">.</span>latest_message <span class="token operator">=</span> None        self<span class="token punctuation">.</span>latest_bot_utterance <span class="token operator">=</span> None        self<span class="token punctuation">.</span>_reset<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>active_form <span class="token operator">=</span> <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;&amp;#125;</span>    <span class="token comment" spellcheck="true">###</span>    <span class="token comment" spellcheck="true"># Public tracker interface</span>    <span class="token comment" spellcheck="true">###</span>    <span class="token keyword">def</span> <span class="token function">current_state</span><span class="token punctuation">(</span>        self<span class="token punctuation">,</span> event_verbosity<span class="token punctuation">:</span> EventVerbosity <span class="token operator">=</span> EventVerbosity<span class="token punctuation">.</span>NONE    <span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> Dict<span class="token punctuation">[</span>Text<span class="token punctuation">,</span> Any<span class="token punctuation">]</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""Return the current tracker state as an object."""</span>        <span class="token keyword">if</span> event_verbosity <span class="token operator">==</span> EventVerbosity<span class="token punctuation">.</span>ALL<span class="token punctuation">:</span>            evts <span class="token operator">=</span> <span class="token punctuation">[</span>e<span class="token punctuation">.</span>as_dict<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> e <span class="token keyword">in</span> self<span class="token punctuation">.</span>events<span class="token punctuation">]</span>        <span class="token keyword">elif</span> event_verbosity <span class="token operator">==</span> EventVerbosity<span class="token punctuation">.</span>AFTER_RESTART<span class="token punctuation">:</span>            evts <span class="token operator">=</span> <span class="token punctuation">[</span>e<span class="token punctuation">.</span>as_dict<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> e <span class="token keyword">in</span> self<span class="token punctuation">.</span>events_after_latest_restart<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span>        <span class="token keyword">elif</span> event_verbosity <span class="token operator">==</span> EventVerbosity<span class="token punctuation">.</span>APPLIED<span class="token punctuation">:</span>            evts <span class="token operator">=</span> <span class="token punctuation">[</span>e<span class="token punctuation">.</span>as_dict<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> e <span class="token keyword">in</span> self<span class="token punctuation">.</span>applied_events<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            evts <span class="token operator">=</span> None        latest_event_time <span class="token operator">=</span> None        <span class="token keyword">if</span> len<span class="token punctuation">(</span>self<span class="token punctuation">.</span>events<span class="token punctuation">)</span> <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">:</span>            latest_event_time <span class="token operator">=</span> self<span class="token punctuation">.</span>events<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>timestamp        <span class="token keyword">return</span> <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;</span>            <span class="token string">"sender_id"</span><span class="token punctuation">:</span> self<span class="token punctuation">.</span>sender_id<span class="token punctuation">,</span>            <span class="token string">"slots"</span><span class="token punctuation">:</span> self<span class="token punctuation">.</span>current_slot_values<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            <span class="token string">"latest_message"</span><span class="token punctuation">:</span> self<span class="token punctuation">.</span>latest_message<span class="token punctuation">.</span>parse_data<span class="token punctuation">,</span>            <span class="token string">"latest_event_time"</span><span class="token punctuation">:</span> latest_event_time<span class="token punctuation">,</span>            <span class="token string">"followup_action"</span><span class="token punctuation">:</span> self<span class="token punctuation">.</span>followup_action<span class="token punctuation">,</span>            <span class="token string">"paused"</span><span class="token punctuation">:</span> self<span class="token punctuation">.</span>is_paused<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            <span class="token string">"events"</span><span class="token punctuation">:</span> evts<span class="token punctuation">,</span>            <span class="token string">"latest_input_channel"</span><span class="token punctuation">:</span> self<span class="token punctuation">.</span>get_latest_input_channel<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            <span class="token string">"active_form"</span><span class="token punctuation">:</span> self<span class="token punctuation">.</span>active_form<span class="token punctuation">,</span>            <span class="token string">"latest_action_name"</span><span class="token punctuation">:</span> self<span class="token punctuation">.</span>latest_action_name<span class="token punctuation">,</span>        <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#125;</span></code></pre><p>注意该方法的返回对象是一个字典，其包含了丰富的对话信息，例如用户的id、当前所有的槽位键值对（包括已填充和未被填充的）、用户最近一次发送的消息等等。</p><h3 id="Action"><a href="#Action" class="headerlink" title="Action"></a>Action</h3><p>下面将解析对话管理中最重要的一个概念——Action。前面已经说到，event对象是rasa中定义的chatbot能执行的最小粒度的动作。而Action则是比event更高层次的对象，会根据用户发送过来的消息，执行一些操作，这些操作可以是自定义的一些逻辑，也可以是系统预置的events。rasa中，action可以分为三大类：</p><p><strong>utterance actions</strong>：直接发送文本给用户，action文本模板是在domain.yml中进行定义。</p><p><strong>custom actions</strong>: 自定义action，由开发者自定义功能的action。个人认为这个是功能最强大的action，因为开发自由度很大，支持使用任何开发语言进行开发。最后只需要将其打包成一个restful服务接口暴露出来即可。因此这种action是可以和对话主系统分离部署的。下面给出自定义action server与bot agent和用户的交互流程图：</p><p><img src="/2020/09/20/duo-lun-dui-hua-kuang-jia-rasa-dai-ma-jie-du-1/9.png"></p><p>rasa action支持node.js, .NET, java等开发语言，当然也支持Python。但是对于Python来说，需要安装rasa-sdk工具包。这个工具包里预置了一些有用的action模板。例如form action。</p><p>当然，form action以及其他预置的action模板只能实现最简单的场景，如果要实现复杂的场景，需要根据不同场景，自定义action，可以选择继承这些模板，在上面进行功能的添加和完善。</p><p><strong>default action</strong>: rasa系统内置的粒度较小的action。与rasa_sdk中的action不同，这个是直接在core/actions下面的。相对于上面的form action来说，这里的action功能更单一，与events比较像，但是还是略有不同，下面举个实例ActionRestart</p><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">ActionRestart</span><span class="token punctuation">(</span>ActionUtterTemplate<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""Resets the tracker to its initial state.    Utters the restart template if available."""</span>    <span class="token keyword">def</span> <span class="token function">name</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> Text<span class="token punctuation">:</span>        <span class="token keyword">return</span> ACTION_RESTART_NAME    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> None<span class="token punctuation">:</span>        super<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token string">"utter_restart"</span><span class="token punctuation">,</span> silent_fail<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>    <span class="token keyword">async</span> <span class="token keyword">def</span> <span class="token function">run</span><span class="token punctuation">(</span>        self<span class="token punctuation">,</span>        output_channel<span class="token punctuation">:</span> <span class="token string">"OutputChannel"</span><span class="token punctuation">,</span>        nlg<span class="token punctuation">:</span> <span class="token string">"NaturalLanguageGenerator"</span><span class="token punctuation">,</span>        tracker<span class="token punctuation">:</span> <span class="token string">"DialogueStateTracker"</span><span class="token punctuation">,</span>        domain<span class="token punctuation">:</span> <span class="token string">"Domain"</span><span class="token punctuation">,</span>    <span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> List<span class="token punctuation">[</span>Event<span class="token punctuation">]</span><span class="token punctuation">:</span>        <span class="token keyword">from</span> rasa<span class="token punctuation">.</span>core<span class="token punctuation">.</span>events <span class="token keyword">import</span> Restarted        <span class="token comment" spellcheck="true"># only utter the template if it is available</span>        evts <span class="token operator">=</span> <span class="token keyword">await</span> super<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>run<span class="token punctuation">(</span>output_channel<span class="token punctuation">,</span> nlg<span class="token punctuation">,</span> tracker<span class="token punctuation">,</span> domain<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#*****</span>        <span class="token keyword">return</span> evts <span class="token operator">+</span> <span class="token punctuation">[</span>Restarted<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span></code></pre><p>可以看到它使用了一个Restarted()的event，这个event的功能是重启整个对话流程，重置对话状态。除此之外，该action需要先执行读取话术模板组装bot message，并将其发送给用户后，才去重启整个会话。</p><p>所有default action的列表如下，它们的命名都非常简单直接：</p><p><img src="/2020/09/20/duo-lun-dui-hua-kuang-jia-rasa-dai-ma-jie-du-1/10.png"></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> nlp </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Rasa </tag>
            
            <tag> QA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>聚类算法原理</title>
      <link href="/2020/09/19/ju-lei-suan-fa-yuan-li/"/>
      <url>/2020/09/19/ju-lei-suan-fa-yuan-li/</url>
      
        <content type="html"><![CDATA[<h1 id="聚类算法原理"><a href="#聚类算法原理" class="headerlink" title="聚类算法原理"></a>聚类算法原理</h1><h2 id="无监督学习与聚类算法"><a href="#无监督学习与聚类算法" class="headerlink" title="无监督学习与聚类算法"></a>无监督学习与聚类算法</h2><p>聚类算法又叫做“无监督分类”，其目的是将数据划分成有意义或有用的组（或簇）。这种划分可以基于我们的业务需求或建模需求来完成，也可以单纯地帮助我们探索数据的自然结构和分布。比如在商业中，如果我们手头有大量的当前和潜在客户的信息，我们可以使用聚类将客户划分为若干组，以便进一步分析和开展营销活动，最有名的客户价值判断模型RFM，就常常和聚类分析共同使用。再比如，聚类可以用于降维和矢量量化（vectorquantization），可以将高维特征压缩到一列当中，常常用于图像，声音，视频等非结构化数据，可以幅度压缩数据量。</p><p><img src="/2020/09/19/ju-lei-suan-fa-yuan-li/2.png"></p><h2 id="sklearn中的聚类算法"><a href="#sklearn中的聚类算法" class="headerlink" title="sklearn中的聚类算法"></a>sklearn中的聚类算法</h2><p>聚类算法在sklearn中有两种表现形式,一种是类(和我们目前为止学过的分类算法以及数据预处理方法们都一<br>样),需要实例化,训练并使用接口和属性来调用结果。另一种是函数(function),只需要输入特征矩阵和超参<br>数,即可返回聚类的结果和各种指标。</p><p><img src="/2020/09/19/ju-lei-suan-fa-yuan-li/1.png"></p><h3 id="KMeans是如何工作的"><a href="#KMeans是如何工作的" class="headerlink" title="KMeans是如何工作的"></a>KMeans是如何工作的</h3><p>作为聚类算法的典型代表,KMeans可以说是最简单的聚类算法没有之一,那它是怎么完成聚类的呢?</p><p>关键概念:簇与质心</p><blockquote><p>KMeans算法将一组N个样本的特征矩阵X划分为K个无交集的簇,直观上来看是簇是一组一组聚集在一起的数<br>据,在一个簇中的数据就认为是同一类。簇就是聚类的结果表现。<br>簇中所有数据的均值<br> 通常被称为这个簇的“质心”(centroids)。在一个二维平面中,一簇数据点的质心的<br>横坐标就是这一簇数据点的横坐标的均值,质心的纵坐标就是这一簇数据点的纵坐标的均值。同理可推广至高<br>维空间。</p></blockquote><p>在KMeans算法中,簇的个数K是一个超参数,需要我们人为输入来确定。KMeans的核心任务就是根据我们设定好<br>的K,找出K个最优的质心,并将离这些质心最近的数据分别分配到这些质心代表的簇中去。具体过程可以总结如<br>下:</p><p><img src="/2020/09/19/ju-lei-suan-fa-yuan-li/3.png"></p><p>输入数据</p><p>需要注意的一件重要事情是,该模块中实现的算法可以采用不同类型的矩阵作为输入。 所有方法都接受形状<br>[n_samples,n_features]的标准特征矩阵,这些可以从sklearn.feature_extraction模块中的类中获得。对于亲和<br>力传播,光谱聚类和DBSCAN,还可以输入形状[n_samples,n_samples]的相似性矩阵,我们可以使用<br>sklearn.metrics.pairwise模块中的函数来获取相似性矩阵。</p><h2 id="簇内误差平方和的定义和解惑"><a href="#簇内误差平方和的定义和解惑" class="headerlink" title="簇内误差平方和的定义和解惑"></a>簇内误差平方和的定义和解惑</h2><p>聚类算法聚出的类有什么含义呢?这些类有什么样的性质?我们认为,被分在同一个簇中的数据是有相似性的,而<br>不同簇中的数据是不同的,当聚类完毕之后,我们就要分别去研究每个簇中的样本都有什么样的性质,从而根据业<br>务需求制定不同的商业或者科技策略。这个听上去和我们在上周的评分卡案例中讲解的“分箱”概念有些类似,即我<br>们分箱的目的是希望,一个箱内的人有着相似的信用风险,而不同箱的人的信用风险差异巨大,以此来区别不同信<br>用度的人,因此我们追求“组内差异小,组间差异大”。聚类算法也是同样的目的,我们追求“簇内差异小,簇外差异<br>大”。而这个“差异“,由样本点到其所在簇的质心的距离来衡量。<br>对于一个簇来说,所有样本点到质心的距离之和越小,我们就认为这个簇中的样本越相似,簇内差异就越小。而距<br>离的衡量方法有多种,令 表示簇中的一个样本点, 表示该簇中的质心,n表示每个样本点中的特征数目,i表示组<br>成点 的每个特征,则该样本点到质心的距离可以由以下距离来度量:</p><p><img src="/2020/09/19/ju-lei-suan-fa-yuan-li/4.png"></p><p>如我们采用欧几里得距离,则一个簇中所有样本点到质心的距离的平方和为:</p><p><img src="/2020/09/19/ju-lei-suan-fa-yuan-li/5.png"></p><p>大家可以发现,我们的Inertia是基于欧几里得距离的计算公式得来的。实际上,我们也可以使用其他距离,每个距<br>离都有自己对应的Inertia。在过去的经验中,我们总结出不同距离所对应的质心选择方法和Inertia,在Kmeans<br>中,只要使用了正确的质心和距离组合,无论使用什么样的距离,都可以达到不错的聚类效果:</p><p><img src="/2020/09/19/ju-lei-suan-fa-yuan-li/6.png"></p><p>而这些组合,都可以由严格的数学证明来推导。在sklearn当中,我们无法选择使用的距离,只能使用欧式距离。<br>因此,我们也无需去担忧这些距离所搭配的质心选择是如何得来的了。</p><h2 id="sklearn-cluster-KMeans"><a href="#sklearn-cluster-KMeans" class="headerlink" title="sklearn.cluster.KMeans"></a>sklearn.cluster.KMeans</h2><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">sklearn</span><span class="token punctuation">.</span>cluster<span class="token punctuation">.</span>KMeans <span class="token punctuation">(</span>n_clusters<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span> init<span class="token operator">=</span>’k<span class="token operator">-</span>means<span class="token operator">+</span><span class="token operator">+</span>’<span class="token punctuation">,</span> n_init<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> max_iter<span class="token operator">=</span><span class="token number">300</span><span class="token punctuation">,</span> tol<span class="token operator">=</span><span class="token number">0.0001</span><span class="token punctuation">,</span>precompute_distances<span class="token operator">=</span>’auto’<span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> random_state<span class="token operator">=</span>None<span class="token punctuation">,</span> copy_x<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> n_jobs<span class="token operator">=</span>None<span class="token punctuation">,</span> algorithm<span class="token operator">=</span>’auto’<span class="token punctuation">)</span></code></pre><h3 id="重要参数"><a href="#重要参数" class="headerlink" title="重要参数"></a>重要参数</h3><p><strong>n_clusters</strong></p><p>​     n_clusters是KMeans中的k,表示着我们告诉模型我们要分几类。这是KMeans当中唯一一个必填的参数,默认为8类,但通常我们的聚类结果会是一个小于8的结果。通常,在开始聚类之前,我们并不知道n_clusters究竟是多少,</p><h3 id="聚类算法的模型评估指标"><a href="#聚类算法的模型评估指标" class="headerlink" title="聚类算法的模型评估指标"></a>聚类算法的模型评估指标</h3><p>​      不同于分类模型和回归,聚类算法的模型评估不是一件简单的事。在分类中,有直接结果(标签)的输出,并且分<br>类的结果有正误之分,所以我们使用预测的准确度,混淆矩阵,ROC曲线等等指标来进行评估,但无论如何评估,<br>都是在”模型找到正确答案“的能力。而回归中,由于要拟合数据,我们有SSE均方误差,有损失函数来衡量模型的<br>拟合程度。但这些衡量指标都不能够使用于聚类。</p><p>​     面试高危问题:如何衡量聚类算法的效果?<br>聚类模型的结果不是某种标签输出,并且聚类的结果是不确定的,其优劣由业务需求或者算法需求来决定,并<br>且没有永远的正确答案。那我们如何衡量聚类的效果呢?</p><h4 id="当真实标签已知的时候"><a href="#当真实标签已知的时候" class="headerlink" title="当真实标签已知的时候"></a>当真实标签已知的时候</h4><p>​       虽然我们在聚类中不输入真实标签,但这不代表我们拥有的数据中一定不具有真实标签,或者一定没有任何参考信息。当然,在现实中,拥有真实标签的情况非常少见(几乎是不可能的)。如果拥有真实标签,我们更倾向于使用<br>分类算法。但不排除我们依然可能使用聚类算法的可能性。如果我们有样本真实聚类情况的数据,我们可以对于聚<br>类算法的结果和真实结果来衡量聚类的效果。常用的有以下三种方法:<img src="/2020/09/19/ju-lei-suan-fa-yuan-li/7.png" alt="7"></p><h4 id="当真实标签未知的时候-轮廓系数"><a href="#当真实标签未知的时候-轮廓系数" class="headerlink" title="当真实标签未知的时候:轮廓系数"></a>当真实标签未知的时候:轮廓系数</h4><p>​      在99%的情况下,我们是对没有真实标签的数据进行探索,也就是对不知道真正答案的数据进行聚类。这样的聚<br>类,是完全依赖于评价簇内的稠密程度(簇内差异小)和簇间的离散程度(簇外差异大)来评估聚类的效果。其中<br>轮廓系数是最常用的聚类算法的评价指标。它是对每个样本来定义的,它能够同时衡量:<br>1)样本与其自身所在的簇中的其他样本的相似度a,等于样本与同一簇中所有其他点之间的平均距离<br>2)样本与其他簇中的样本的相似度b,等于样本与下一个最近的簇中的所有点之间的平均距离<br>根据聚类的要求”簇内差异小,簇外差异大“,我们希望b永远大于a,并且大得越多越好。</p><p>单个样本的轮廓系数计算为:</p><p><img src="/2020/09/19/ju-lei-suan-fa-yuan-li/8.png"></p><p>这个公式可以被解析为:</p><p><img src="/2020/09/19/ju-lei-suan-fa-yuan-li/9.png"></p><p>​      很容易理解轮廓系数范围是(-1,1),其中值越接近1表示样本与自己所在的簇中的样本很相似,并且与其他簇中的样<br>本不相似,当样本点与簇外的样本更相似的时候,轮廓系数就为负。当轮廓系数为0时,则代表两个簇中的样本相<br>似度一致,两个簇本应该是一个簇。可以总结为轮廓系数越接近于1越好,负数则表示聚类效果非常差。<br>如果一个簇中的大多数样本具有比较高的轮廓系数,则簇会有较高的总轮廓系数,则整个数据集的平均轮廓系数越<br>高,则聚类是合适的。如果许多样本点具有低轮廓系数甚至负值,则聚类是不合适的,聚类的超参数K可能设定得<br>太大或者太小。</p><p>​      在sklearn中,我们使用模块metrics中的类silhouette_score来计算轮廓系数,它返回的是一个数据集中,所有样<br>本的轮廓系数的均值。但我们还有同在metrics模块中的silhouette_sample,它的参数与轮廓系数一致,但返回的<br>是数据集中每个样本自己的轮廓系数。</p><h4 id="当真实标签未知的时候-Calinski-Harabaz-Index"><a href="#当真实标签未知的时候-Calinski-Harabaz-Index" class="headerlink" title="当真实标签未知的时候:Calinski-Harabaz Index"></a>当真实标签未知的时候:Calinski-Harabaz Index</h4><p>​       除了轮廓系数是最常用的,我们还有卡林斯基-哈拉巴斯指数(Calinski-Harabaz Index,简称CHI,也被称为方差<br>比标准),戴维斯-布尔丁指数(Davies-Bouldin)以及权变矩阵(Contingency Matrix)可以使用。</p><p><img src="/2020/09/19/ju-lei-suan-fa-yuan-li/12.png"></p><p>​         在这里我们重点来了解一下卡林斯基-哈拉巴斯指数。Calinski-Harabaz指数越高越好。对于有k个簇的聚类而言,Calinski-Harabaz指数s(k)写作如下公式:</p><p><img src="/2020/09/19/ju-lei-suan-fa-yuan-li/13.png"></p><p>​       其中N为数据集中的样本量,k为簇的个数(即类别的个数),是组间离散矩阵,即不同簇之间的协方差矩阵,$W _k$是簇内离散矩阵,即一个簇内数据的协方差矩阵,而tr表示矩阵的迹。在线性代数中,一个n×n矩阵A的主对角线(从左上方至右下方的对角线)上各个元素的总和被称为矩阵A的迹(或迹数),一般记作 。数据之间的离散程度越高,协方差矩阵的迹就会越大。组内离散程度低,协方差的迹就会越小, 也就越小,同时,组间离散程度大,协方差的的迹也会越大, 就越大,这正是我们希望的,因此Calinski-harabaz指数越高越好。</p><h2 id="重要参数init-amp-random-state-amp-n-init-初始质心怎么放好"><a href="#重要参数init-amp-random-state-amp-n-init-初始质心怎么放好" class="headerlink" title="重要参数init &amp; random_state &amp; n_init:初始质心怎么放好?"></a>重要参数init &amp; random_state &amp; n_init:初始质心怎么放好?</h2><p>​       在K-Means中有一个重要的环节,就是放置初始质心。如果有足够的时间,K-means一定会收敛,但Inertia可能收<br>敛到局部最小值。是否能够收敛到真正的最小值很大程度上取决于质心的初始化。init就是用来帮助我们决定初始<br>化方式的参数。</p><p>​         init:可输入”k-means++”,”random”或者一个n维数组。这是初始化质心的方法,默认”k-means++”。输入”k-means++”:一种为K均值聚类选择初始聚类中心的聪明的办法,以加速收敛。如果输入了n维数组,数组的形状应该是(n_clusters,n_features)并给出初始质心。</p><h2 id="重要参数max-iter-amp-tol-让迭代停下来"><a href="#重要参数max-iter-amp-tol-让迭代停下来" class="headerlink" title="重要参数max_iter &amp; tol:让迭代停下来"></a>重要参数max_iter &amp; tol:让迭代停下来</h2><p>在之前描述K-Means的基本流程时我们提到过,当质心不再移动,Kmeans算法就会停下来。但在完全收敛之前,我们也可以使用max_iter,最大迭代次数,或者tol,两次迭代间Inertia下降的量,这两个参数来让迭代提前停下来。有时候,当我们的n_clusters选择不符合数据的自然分布,或者我们为了业务需求,必须要填入与数据的自然分布不合的n_clusters,提前让迭代停下来反而能够提升模型的表现。</p><p><strong>max_iter</strong>:整数,默认300,单次运行的k-means算法的最大迭代次数<br><strong>tol</strong>:浮点数,默认1e-4,两次迭代间Inertia下降的量,如果两次迭代之间Inertia下降的值小于tol所设定的值,迭代就会停下</p><h2 id="重要属性与重要接口"><a href="#重要属性与重要接口" class="headerlink" title="重要属性与重要接口"></a>重要属性与重要接口</h2><p><img src="/2020/09/19/ju-lei-suan-fa-yuan-li/14.png"></p><h2 id="函数cluster-k-means"><a href="#函数cluster-k-means" class="headerlink" title="函数cluster.k_means"></a>函数cluster.k_means</h2><pre class=" language-python"><code class="language-python">sklearn<span class="token punctuation">.</span>cluster<span class="token punctuation">.</span>k_means <span class="token punctuation">(</span>X<span class="token punctuation">,</span> n_clusters<span class="token punctuation">,</span> sample_weight<span class="token operator">=</span>None<span class="token punctuation">,</span> init<span class="token operator">=</span>’k<span class="token operator">-</span>means<span class="token operator">+</span><span class="token operator">+</span>’<span class="token punctuation">,</span> precompute_distances<span class="token operator">=</span>’auto’<span class="token punctuation">,</span>n_init<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> max_iter<span class="token operator">=</span><span class="token number">300</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> tol<span class="token operator">=</span><span class="token number">0.0001</span><span class="token punctuation">,</span> random_state<span class="token operator">=</span>None<span class="token punctuation">,</span> copy_x<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> n_jobs<span class="token operator">=</span>None<span class="token punctuation">,</span>algorithm<span class="token operator">=</span>’auto’<span class="token punctuation">,</span> return_n_iter<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span></code></pre><p>​         函数k_means的用法其实和类非常相似,不过函数是输入一系列值,而直接返回结果。一次性地,函数k_means会依次返回质心,每个样本对应的簇的标签,inertia以及最佳迭代次数。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 树模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>多轮对话框架Rasa代码解读-2</title>
      <link href="/2020/09/19/duo-lun-dui-hua-kuang-jia-rasa-dai-ma-jie-du-xun-lian/"/>
      <url>/2020/09/19/duo-lun-dui-hua-kuang-jia-rasa-dai-ma-jie-du-xun-lian/</url>
      
        <content type="html"><![CDATA[<h1 id="Rasa源码解读"><a href="#Rasa源码解读" class="headerlink" title="Rasa源码解读"></a>Rasa源码解读</h1><h2 id="脚本总入口"><a href="#脚本总入口" class="headerlink" title="脚本总入口"></a>脚本总入口</h2><p><code>python __main__.py 　train --domain domain.yml --data data --config config.yml --out models</code></p><p>所有rasa指令的总入口函数</p><p><img src="/2020/09/19/duo-lun-dui-hua-kuang-jia-rasa-dai-ma-jie-du-xun-lian/1.png"></p><p>train 首先执行<code>cmdline_arguments.func(cmdline_arguments)</code>然后<code>create_argument_parser()</code>–&gt;<code>train.add_subparser</code>－－<code>train_parser.set_defaults(func=train)</code>，这样调用的就是<code>train</code>即执行了<code>.../rasa/train.py</code>文件中的train函数。</p><p>参数：</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 训练在`.../rasa/train.py`文件下，那真正入口是`train_async`函数</span>train_async<span class="token punctuation">(</span>    domain<span class="token operator">=</span>domain<span class="token punctuation">,</span>    config<span class="token operator">=</span>config<span class="token punctuation">,</span>    training_files<span class="token operator">=</span>training_files<span class="token punctuation">,</span>    output_path<span class="token operator">=</span>output<span class="token punctuation">,</span>    force_training<span class="token operator">=</span>force_training<span class="token punctuation">,</span>    fixed_model_name<span class="token operator">=</span>fixed_model_name<span class="token punctuation">,</span>    persist_nlu_training_data<span class="token operator">=</span>persist_nlu_training_data<span class="token punctuation">,</span>    additional_arguments<span class="token operator">=</span>additional_arguments<span class="token punctuation">,</span><span class="token punctuation">)</span></code></pre><p>在调用_do_training，入参和上述不同的地方在于domain不再是一个字符串，而是一个Domain对象，主要是把domain.yml文件中的内容变成字典from_dict。</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true">#_do_training 在文件 .../rasa/train.py  180行</span><span class="token keyword">if</span> fingerprint_comparison<span class="token punctuation">.</span>is_training_required<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">await</span> _do_training<span class="token punctuation">(</span>            file_importer<span class="token punctuation">,</span>　            output_path<span class="token operator">=</span>output_path<span class="token punctuation">,</span>            train_path<span class="token operator">=</span>train_path<span class="token punctuation">,</span>            fingerprint_comparison_result<span class="token operator">=</span>fingerprint_comparison<span class="token punctuation">,</span>            fixed_model_name<span class="token operator">=</span>fixed_model_name<span class="token punctuation">,</span>            persist_nlu_training_data<span class="token operator">=</span>persist_nlu_training_data<span class="token punctuation">,</span>            additional_arguments<span class="token operator">=</span>additional_arguments<span class="token punctuation">,</span>        <span class="token punctuation">)</span></code></pre><p>然后分别用<code>_train_core_with_validated_data</code>训练core,用<code>_train_nlu_with_validated_data</code>训练nlu.</p><p><img src="/2020/09/19/duo-lun-dui-hua-kuang-jia-rasa-dai-ma-jie-du-xun-lian/2.png"></p><h2 id="Core-对话管理模块的解读"><a href="#Core-对话管理模块的解读" class="headerlink" title="Core 对话管理模块的解读"></a>Core 对话管理模块的解读</h2><p><strong>整体的处理逻辑</strong></p><p>加载policies配置－－构造Agent对象－－加载训练数据－－进行训练－－模型持久化</p><p><strong>对话管理的入口</strong></p><p>　　<code>train.py</code>的<code>_do_training</code>函数，训练的入口是<code>_train_core_with_validated_data</code>，我们首先弄清楚传的入参：</p><p><code>domain</code>，是一个<code>Domain</code>对象，不在是一个字符串，和<code>domain.yml</code>文件对应。在<code>rasa/domain.py</code>文件中，</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># rasa/domain.py line 342   ----> intent_properties</span>    <span class="token keyword">def</span> <span class="token function">collect_intent_properties</span><span class="token punctuation">(</span>        cls<span class="token punctuation">,</span> intents<span class="token punctuation">:</span> List<span class="token punctuation">[</span>Union<span class="token punctuation">[</span>Text<span class="token punctuation">,</span> Dict<span class="token punctuation">[</span>Text<span class="token punctuation">,</span> Any<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> entities<span class="token punctuation">:</span> List<span class="token punctuation">[</span>Text<span class="token punctuation">]</span>    <span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> Dict<span class="token punctuation">[</span>Text<span class="token punctuation">,</span> Dict<span class="token punctuation">[</span>Text<span class="token punctuation">,</span> Union<span class="token punctuation">[</span>bool<span class="token punctuation">,</span> List<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># make a copy to not alter the input argument</span>        intents <span class="token operator">=</span> copy<span class="token punctuation">.</span>deepcopy<span class="token punctuation">(</span>intents<span class="token punctuation">)</span>        intent_properties <span class="token operator">=</span> <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;&amp;#125;</span>        duplicates <span class="token operator">=</span> set<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">for</span> intent <span class="token keyword">in</span> intents<span class="token punctuation">:</span>            <span class="token keyword">if</span> <span class="token operator">not</span> isinstance<span class="token punctuation">(</span>intent<span class="token punctuation">,</span> dict<span class="token punctuation">)</span><span class="token punctuation">:</span>                intent <span class="token operator">=</span> <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;intent: &amp;#123;USE_ENTITIES_KEY: True, IGNORE_ENTITIES_KEY: []&amp;#125;&amp;#125;</span>            name <span class="token operator">=</span> list<span class="token punctuation">(</span>intent<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>            <span class="token keyword">if</span> name <span class="token keyword">in</span> intent_properties<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                duplicates<span class="token punctuation">.</span>add<span class="token punctuation">(</span>name<span class="token punctuation">)</span>            intent <span class="token operator">=</span> cls<span class="token punctuation">.</span>_transform_intent_properties_for_internal_use<span class="token punctuation">(</span>intent<span class="token punctuation">,</span> entities<span class="token punctuation">)</span>            intent_properties<span class="token punctuation">.</span>update<span class="token punctuation">(</span>intent<span class="token punctuation">)</span></code></pre><p><img src="/2020/09/19/duo-lun-dui-hua-kuang-jia-rasa-dai-ma-jie-du-xun-lian/7.png"></p><p>config，是一个字符串’config.yml’</p><p>story_directory，字符串’/tmp/tmp110fpdym’，对应原始文件是data/core/stories.md,　在训练时拷贝到一个临时目录下</p><p>output　输出目录’models’</p><p>train_path　传入值为空，由系统生成一个临时目录’/tmp/tmp7odkccbr’</p><p>fixed_model_name　值为空</p><p>kwargs,是一个字典：</p><p>１．　加载policies配置</p><p>读取配置文件config.yml，构造PolicyEnsemble对象。这里给出运行时的样例数据，便于和配置文件做对比。</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># .../rasa/core/agent.py line 342</span>self<span class="token punctuation">.</span>policy_ensemble <span class="token operator">=</span> self<span class="token punctuation">.</span>_create_ensemble<span class="token punctuation">(</span>policies<span class="token punctuation">)</span><span class="token comment" spellcheck="true">##./policies/ensemble.py line 924</span><span class="token keyword">def</span> <span class="token function">_create_ensemble</span><span class="token punctuation">(</span>        policies<span class="token punctuation">:</span> Union<span class="token punctuation">[</span>List<span class="token punctuation">[</span>Policy<span class="token punctuation">]</span><span class="token punctuation">,</span> PolicyEnsemble<span class="token punctuation">,</span> None<span class="token punctuation">]</span>    <span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> Optional<span class="token punctuation">[</span>PolicyEnsemble<span class="token punctuation">]</span><span class="token punctuation">:</span>    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     <span class="token keyword">if</span> policies <span class="token keyword">is</span> None<span class="token punctuation">:</span>            <span class="token keyword">return</span> None        <span class="token keyword">if</span> isinstance<span class="token punctuation">(</span>policies<span class="token punctuation">,</span> list<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">return</span> SimplePolicyEnsemble<span class="token punctuation">(</span>policies<span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#&lt;-----------------构造PolicyEnsemble对象</span>                <span class="token keyword">elif</span> isinstance<span class="token punctuation">(</span>policies<span class="token punctuation">,</span> PolicyEnsemble<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment" spellcheck="true">#&lt;-----------------构造PolicyEnsemble对象</span>            <span class="token keyword">return</span> policies</code></pre><p><img src="/2020/09/19/duo-lun-dui-hua-kuang-jia-rasa-dai-ma-jie-du-xun-lian/8.png"></p><p>２．构造Agent对象</p><p>Agent对象在Rasa框架中提供了非常多的接口，包括训练，处理输入消息，加载对话模型，获取下一步的action，处理通道。</p><p>这里主要关注类的关联关系。</p><ul><li>domain指向Domain对象</li></ul><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true">## .../rasa/core/agent.py line 341</span>self<span class="token punctuation">.</span>domain <span class="token operator">=</span> self<span class="token punctuation">.</span>_create_domain<span class="token punctuation">(</span>domain<span class="token punctuation">)</span><span class="token comment" spellcheck="true">## .../rasa/core/agent.py line 887</span><span class="token keyword">def</span> <span class="token function">_create_domain</span><span class="token punctuation">(</span>domain<span class="token punctuation">:</span> Union<span class="token punctuation">[</span>Domain<span class="token punctuation">,</span> Text<span class="token punctuation">,</span> None<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> Domain<span class="token punctuation">:</span>    <span class="token keyword">if</span> isinstance<span class="token punctuation">(</span>domain<span class="token punctuation">,</span> str<span class="token punctuation">)</span><span class="token punctuation">:</span>        domain <span class="token operator">=</span> Domain<span class="token punctuation">.</span>load<span class="token punctuation">(</span>domain<span class="token punctuation">)</span><span class="token comment" spellcheck="true">##&lt;------domain指向Domain对象</span>        domain<span class="token punctuation">.</span>check_missing_templates<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> domain    <span class="token keyword">elif</span> isinstance<span class="token punctuation">(</span>domain<span class="token punctuation">,</span> Domain<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> domain<span class="token comment" spellcheck="true">##  &lt;-------domain指向Domain对象</span>    <span class="token keyword">elif</span> domain <span class="token keyword">is</span> None<span class="token punctuation">:</span>        <span class="token keyword">return</span> Domain<span class="token punctuation">.</span>empty<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><ul><li>interpreter指向NaturalLanguageInterpreter对象</li></ul><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true">## .../rasa/core/agent.py line 353</span>self<span class="token punctuation">.</span>interpreter <span class="token operator">=</span> NaturalLanguageInterpreter<span class="token punctuation">.</span>create<span class="token punctuation">(</span>interpreter<span class="token punctuation">)</span><span class="token comment" spellcheck="true">## ./rasa/core/interpreter.py line 32</span>    <span class="token keyword">def</span> <span class="token function">create</span><span class="token punctuation">(</span>        obj<span class="token punctuation">:</span> Union<span class="token punctuation">[</span><span class="token string">"NaturalLanguageInterpreter"</span><span class="token punctuation">,</span> EndpointConfig<span class="token punctuation">,</span> Text<span class="token punctuation">,</span> None<span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token comment" spellcheck="true"># this second parameter is deprecated!</span>        endpoint<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>EndpointConfig<span class="token punctuation">]</span> <span class="token operator">=</span> None<span class="token punctuation">,</span>    <span class="token punctuation">)</span>             obj <span class="token operator">=</span> endpoint <span class="token operator">or</span> obj        <span class="token keyword">if</span> isinstance<span class="token punctuation">(</span>obj<span class="token punctuation">,</span> NaturalLanguageInterpreter<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">return</span> obj        <span class="token keyword">elif</span> isinstance<span class="token punctuation">(</span>obj<span class="token punctuation">,</span> str<span class="token punctuation">)</span> <span class="token operator">and</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>exists<span class="token punctuation">(</span>obj<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">return</span> RasaNLUInterpreter<span class="token punctuation">(</span>model_directory<span class="token operator">=</span>obj<span class="token punctuation">)</span>        <span class="token keyword">elif</span> isinstance<span class="token punctuation">(</span>obj<span class="token punctuation">,</span> str<span class="token punctuation">)</span> <span class="token operator">and</span> <span class="token operator">not</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>exists<span class="token punctuation">(</span>obj<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token comment" spellcheck="true"># user passed in a string, but file does not exist</span>            logger<span class="token punctuation">.</span>warning<span class="token punctuation">(</span>                f<span class="token string">"No local NLU model '&amp;#123;obj&amp;#125;' found. Using RegexInterpreter instead."</span>            <span class="token punctuation">)</span>            <span class="token keyword">return</span> RegexInterpreter<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            <span class="token keyword">return</span> _create_from_endpoint_config<span class="token punctuation">(</span>obj<span class="token punctuation">)</span></code></pre><p><img src="/2020/09/19/duo-lun-dui-hua-kuang-jia-rasa-dai-ma-jie-du-xun-lian/12.png"></p><ul><li>nlg指向NaturalLanguageGenerator对象，本示例中实际是TemplatedNaturalLanguageGenerator对象。</li></ul><pre class=" language-python"><code class="language-python"> <span class="token comment" spellcheck="true">#    .../rasa/core/agent.py line 355</span>    self<span class="token punctuation">.</span>nlg <span class="token operator">=</span> NaturalLanguageGenerator<span class="token punctuation">.</span>create<span class="token punctuation">(</span>generator<span class="token punctuation">,</span> self<span class="token punctuation">.</span>domain<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">## ./rasa/core/nlg/generator.py line 905</span>    <span class="token keyword">def</span> <span class="token function">create</span><span class="token punctuation">(</span>        obj<span class="token punctuation">:</span> Union<span class="token punctuation">[</span><span class="token string">"NaturalLanguageGenerator"</span><span class="token punctuation">,</span> EndpointConfig<span class="token punctuation">,</span> None<span class="token punctuation">]</span><span class="token punctuation">,</span>        domain<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Domain<span class="token punctuation">]</span><span class="token punctuation">,</span>    <span class="token punctuation">)</span>        <span class="token keyword">if</span> isinstance<span class="token punctuation">(</span>obj<span class="token punctuation">,</span> NaturalLanguageGenerator<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">return</span> obj        <span class="token keyword">else</span><span class="token punctuation">:</span>            <span class="token keyword">return</span> _create_from_endpoint_config<span class="token punctuation">(</span>obj<span class="token punctuation">,</span> domain<span class="token punctuation">)</span>   <span class="token comment" spellcheck="true">### ./rasa/core/nlg/generator.py  </span><span class="token keyword">def</span> <span class="token function">_create_from_endpoint_config</span><span class="token punctuation">(</span>    endpoint_config<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>EndpointConfig<span class="token punctuation">]</span> <span class="token operator">=</span> None<span class="token punctuation">,</span> domain<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Domain<span class="token punctuation">]</span> <span class="token operator">=</span> None<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token string">"NaturalLanguageGenerator"</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""Given an endpoint configuration, create a proper NLG object."""</span>    domain <span class="token operator">=</span> domain <span class="token operator">or</span> Domain<span class="token punctuation">.</span>empty<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> endpoint_config <span class="token keyword">is</span> None<span class="token punctuation">:</span>        <span class="token keyword">from</span> rasa<span class="token punctuation">.</span>core<span class="token punctuation">.</span>nlg <span class="token keyword">import</span> <span class="token punctuation">(</span>  <span class="token comment" spellcheck="true"># pytype: disable=pyi-error</span>            TemplatedNaturalLanguageGenerator<span class="token punctuation">,</span>        <span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># this is the default type if no endpoint config is set</span>        nlg <span class="token operator">=</span> TemplatedNaturalLanguageGenerator<span class="token punctuation">(</span>domain<span class="token punctuation">.</span>templates<span class="token punctuation">)</span>    <span class="token keyword">elif</span> endpoint_config<span class="token punctuation">.</span>type <span class="token keyword">is</span> None <span class="token operator">or</span> endpoint_config<span class="token punctuation">.</span>type<span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token string">"callback"</span><span class="token punctuation">:</span>        <span class="token keyword">from</span> rasa<span class="token punctuation">.</span>core<span class="token punctuation">.</span>nlg <span class="token keyword">import</span> <span class="token punctuation">(</span>  <span class="token comment" spellcheck="true"># pytype: disable=pyi-error</span>            CallbackNaturalLanguageGenerator<span class="token punctuation">,</span>        <span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># this is the default type if no nlg type is set</span>        nlg <span class="token operator">=</span> CallbackNaturalLanguageGenerator<span class="token punctuation">(</span>endpoint_config<span class="token operator">=</span>endpoint_config<span class="token punctuation">)</span>    <span class="token keyword">elif</span> endpoint_config<span class="token punctuation">.</span>type<span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token string">"template"</span><span class="token punctuation">:</span>        <span class="token keyword">from</span> rasa<span class="token punctuation">.</span>core<span class="token punctuation">.</span>nlg <span class="token keyword">import</span> <span class="token punctuation">(</span>  <span class="token comment" spellcheck="true"># pytype: disable=pyi-error</span>            TemplatedNaturalLanguageGenerator<span class="token punctuation">,</span>        <span class="token punctuation">)</span>        nlg <span class="token operator">=</span> TemplatedNaturalLanguageGenerator<span class="token punctuation">(</span>domain<span class="token punctuation">.</span>templates<span class="token punctuation">)</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        nlg <span class="token operator">=</span> _load_from_module_string<span class="token punctuation">(</span>endpoint_config<span class="token punctuation">,</span> domain<span class="token punctuation">)</span>    logger<span class="token punctuation">.</span>debug<span class="token punctuation">(</span>f<span class="token string">"Instantiated NLG to '&amp;#123;nlg.__class__.__name__&amp;#125;'."</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> nlg</code></pre><p><img src="/2020/09/19/duo-lun-dui-hua-kuang-jia-rasa-dai-ma-jie-du-xun-lian/10.png"></p><ul><li>tracker_store指向InMemoryTrackerStore对象</li></ul><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true">#   .../rasa/core/agent.py line 355</span>self<span class="token punctuation">.</span>tracker_store <span class="token operator">=</span> self<span class="token punctuation">.</span>create_tracker_store<span class="token punctuation">(</span>tracker_store<span class="token punctuation">,</span> self<span class="token punctuation">.</span>domain<span class="token punctuation">)</span><span class="token comment" spellcheck="true">##..../rasa/core/agent.py line 912</span>tracker_store <span class="token operator">=</span> InMemoryTrackerStore<span class="token punctuation">(</span>domain<span class="token punctuation">)</span></code></pre><p><img src="/2020/09/19/duo-lun-dui-hua-kuang-jia-rasa-dai-ma-jie-du-xun-lian/11.png"></p><p>action_endpoint，本示例中为空。</p><pre class=" language-python"><code class="language-python">    <span class="token keyword">def</span> <span class="token function">create</span><span class="token punctuation">(</span>        obj<span class="token punctuation">:</span> Union<span class="token punctuation">[</span><span class="token string">"NaturalLanguageGenerator"</span><span class="token punctuation">,</span> EndpointConfig<span class="token punctuation">,</span> None<span class="token punctuation">]</span><span class="token punctuation">,</span>        domain<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Domain<span class="token punctuation">]</span><span class="token punctuation">,</span>    <span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token string">"NaturalLanguageGenerator"</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""Factory to create a generator."""</span>        <span class="token keyword">if</span> isinstance<span class="token punctuation">(</span>obj<span class="token punctuation">,</span> NaturalLanguageGenerator<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">return</span> obj        <span class="token keyword">else</span><span class="token punctuation">:</span>            <span class="token keyword">return</span> _create_from_endpoint_config<span class="token punctuation">(</span>obj<span class="token punctuation">,</span> domain<span class="token punctuation">)</span><span class="token comment" spellcheck="true">##&lt; ----</span></code></pre><p>３．加载训练数据</p><p>函数是agent.load_data，先分析入参：</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># ./core/train.py line 63 </span>training_data <span class="token operator">=</span> <span class="token keyword">await</span> agent<span class="token punctuation">.</span>load_data<span class="token punctuation">(</span>        training_resource<span class="token punctuation">,</span> exclusion_percentage<span class="token operator">=</span>exclusion_percentage<span class="token punctuation">,</span> <span class="token operator">**</span>data_load_args    <span class="token punctuation">)</span></code></pre><p>training_resource，对应原始文件是data/core/stories.md及其他数据,　在训练时拷贝到一个临时目录下</p><p><img src="/2020/09/19/duo-lun-dui-hua-kuang-jia-rasa-dai-ma-jie-du-xun-lian/13.png"></p><p>exclusion_percentage，本示例的值为none。具体含义通过变量的名称可以理解，就是指定训练时只使用训练数据集的一部分。</p><p>data_load_args，是一个字典，包括：augmentation_factor为50，debug_plots为false</p><p><img src="/2020/09/19/duo-lun-dui-hua-kuang-jia-rasa-dai-ma-jie-du-xun-lian/14.png"></p><p>出参是一个列表，每个元素是DialogueStateTracker对象。</p><p>这里先讲解下story的数据格式：</p><pre class=" language-python"><code class="language-python"> <span class="token operator">&lt;</span>!<span class="token operator">-</span><span class="token operator">-</span>这行是注释，<span class="token comment" spellcheck="true">##开头，后面的部分是story的名称，主要是调试用。每个story都是以##开头--></span><span class="token comment" spellcheck="true">## Generated Story No7</span><span class="token operator">*</span> greet    <span class="token operator">-</span> utter_greet<span class="token operator">&lt;</span>!<span class="token operator">-</span><span class="token operator">-</span> 以<span class="token operator">*</span>开头的部分，对应用户输入的部分，格式是意图<span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;实体：值，实体：值...&amp;#125; --></span><span class="token operator">&lt;</span>!<span class="token operator">-</span><span class="token operator">-</span> 其中<span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;实体：值&amp;#125;的部分可以不要--></span><span class="token operator">*</span> request_management<span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;"package": "套餐"&amp;#125;</span><span class="token operator">&lt;</span>!<span class="token operator">-</span><span class="token operator">-</span> 以<span class="token operator">-</span>开头的部分，表示系统的响应，也就是聊天机器人执行的Action<span class="token operator">-</span><span class="token operator">-</span><span class="token operator">></span><span class="token operator">&lt;</span>!<span class="token operator">-</span><span class="token operator">-</span> 以<span class="token operator">-</span> slot开头的部分，表示需要填充的槽<span class="token operator">-</span><span class="token operator">-</span><span class="token operator">></span>    <span class="token operator">-</span> slot<span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;"package": "套餐"&amp;#125;</span>    <span class="token operator">-</span> utter_ask_package<span class="token operator">*</span> inform<span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;"package": "套餐一"&amp;#125;</span>    <span class="token operator">-</span> slot<span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;"package": "套餐一"&amp;#125;</span>    <span class="token operator">-</span> utter_confirm<span class="token operator">*</span> confirm    <span class="token operator">-</span> utter_ack_management    <span class="token operator">-</span> utter_ask_morehelp<span class="token operator">*</span> deny    <span class="token operator">-</span> utter_goodbye<span class="token operator">*</span> thanks    <span class="token operator">-</span> utter_thanks</code></pre><p>读取文件，StoryFileReader对应一个story文件的类，成员变量story_steps是一个列表，每个元素是一个List[StoryStep]，这里存在list的嵌套。</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true">#..../rasa/importers/rasa.py  line50</span> story_steps <span class="token operator">=</span> <span class="token keyword">await</span> StoryFileReader<span class="token punctuation">.</span>read_from_files<span class="token punctuation">(</span>            self<span class="token punctuation">.</span>_story_files<span class="token punctuation">,</span>            <span class="token keyword">await</span> self<span class="token punctuation">.</span>get_domain<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            interpreter<span class="token punctuation">,</span>            template_variables<span class="token punctuation">,</span>            use_e2e<span class="token punctuation">,</span>            exclusion_percentage<span class="token punctuation">,</span>        <span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">## </span>        <span class="token keyword">async</span> <span class="token keyword">def</span> <span class="token function">read_from_files</span><span class="token punctuation">(</span>        files<span class="token punctuation">:</span> Iterable<span class="token punctuation">[</span>Text<span class="token punctuation">]</span><span class="token punctuation">,</span>        domain<span class="token punctuation">:</span> Domain<span class="token punctuation">,</span>        interpreter<span class="token punctuation">:</span> NaturalLanguageInterpreter <span class="token operator">=</span> RegexInterpreter<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        template_variables<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Dict<span class="token punctuation">]</span> <span class="token operator">=</span> None<span class="token punctuation">,</span>        use_e2e<span class="token punctuation">:</span> bool <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>        exclusion_percentage<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>int<span class="token punctuation">]</span> <span class="token operator">=</span> None<span class="token punctuation">,</span>    <span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> List<span class="token punctuation">[</span>StoryStep<span class="token punctuation">]</span><span class="token punctuation">:</span>        story_steps <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>     <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>        <span class="token keyword">for</span> f <span class="token keyword">in</span> files<span class="token punctuation">:</span>            steps <span class="token operator">=</span> <span class="token keyword">await</span> StoryFileReader<span class="token punctuation">.</span>read_from_file<span class="token punctuation">(</span>                f<span class="token punctuation">,</span> domain<span class="token punctuation">,</span> interpreter<span class="token punctuation">,</span> template_variables<span class="token punctuation">,</span> use_e2e            <span class="token punctuation">)</span>      <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>        <span class="token keyword">return</span> story_steps   <span class="token comment" spellcheck="true">## .../rasa/core/training/dsl.py</span> <span class="token keyword">async</span> <span class="token keyword">def</span> <span class="token function">read_from_file</span><span class="token punctuation">(</span>        filename<span class="token punctuation">:</span> Text<span class="token punctuation">,</span>        domain<span class="token punctuation">:</span> Domain<span class="token punctuation">,</span>        interpreter<span class="token punctuation">:</span> NaturalLanguageInterpreter <span class="token operator">=</span> RegexInterpreter<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        template_variables<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Dict<span class="token punctuation">]</span> <span class="token operator">=</span> None<span class="token punctuation">,</span>        use_e2e<span class="token punctuation">:</span> bool <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>    <span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> List<span class="token punctuation">[</span>StoryStep<span class="token punctuation">]</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""Given a md file reads the contained stories."""</span>        <span class="token keyword">try</span><span class="token punctuation">:</span>            <span class="token keyword">with</span> open<span class="token punctuation">(</span>filename<span class="token punctuation">,</span> <span class="token string">"r"</span><span class="token punctuation">,</span> encoding<span class="token operator">=</span>io_utils<span class="token punctuation">.</span>DEFAULT_ENCODING<span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>                lines <span class="token operator">=</span> f<span class="token punctuation">.</span>readlines<span class="token punctuation">(</span><span class="token punctuation">)</span>            reader <span class="token operator">=</span> StoryFileReader<span class="token punctuation">(</span>                interpreter<span class="token punctuation">,</span> domain<span class="token punctuation">,</span> template_variables<span class="token punctuation">,</span> use_e2e<span class="token punctuation">,</span> filename            <span class="token punctuation">)</span>            <span class="token keyword">return</span> <span class="token keyword">await</span> reader<span class="token punctuation">.</span>process_lines<span class="token punctuation">(</span>lines<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true">## .../rasa/core/training/dsl.py line 221</span>        story_steps <span class="token operator">=</span> <span class="token keyword">await</span> StoryFileReader<span class="token punctuation">.</span>read_from_files<span class="token punctuation">(</span>            self<span class="token punctuation">.</span>_story_files<span class="token punctuation">,</span>            <span class="token keyword">await</span> self<span class="token punctuation">.</span>get_domain<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            interpreter<span class="token punctuation">,</span>            template_variables<span class="token punctuation">,</span>            use_e2e<span class="token punctuation">,</span>            exclusion_percentage<span class="token punctuation">,</span>        <span class="token punctuation">)</span>        <span class="token keyword">return</span> StoryGraph<span class="token punctuation">(</span>story_steps<span class="token punctuation">)</span><span class="token comment" spellcheck="true">## .../rasa/core/training/structures.py</span><span class="token keyword">class</span> <span class="token class-name">Story</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>        self<span class="token punctuation">,</span> story_steps<span class="token punctuation">:</span> List<span class="token punctuation">[</span>StoryStep<span class="token punctuation">]</span> <span class="token operator">=</span> None<span class="token punctuation">,</span> story_name<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Text<span class="token punctuation">]</span> <span class="token operator">=</span> None    <span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> None<span class="token punctuation">:</span>        self<span class="token punctuation">.</span>story_steps <span class="token operator">=</span> story_steps <span class="token keyword">if</span> story_steps <span class="token keyword">else</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        self<span class="token punctuation">.</span>story_name <span class="token operator">=</span> story_name</code></pre><p>这里看下StoryStep的初始化，</p><pre class=" language-python"><code class="language-python">    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>        self<span class="token punctuation">,</span>        block_name<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Text<span class="token punctuation">]</span> <span class="token operator">=</span> None<span class="token punctuation">,</span>        start_checkpoints<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>List<span class="token punctuation">[</span>Checkpoint<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> None<span class="token punctuation">,</span>        end_checkpoints<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>List<span class="token punctuation">[</span>Checkpoint<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> None<span class="token punctuation">,</span>        events<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>List<span class="token punctuation">[</span>Event<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> None<span class="token punctuation">,</span>    <span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> None<span class="token punctuation">:</span></code></pre><p><img src="/2020/09/19/duo-lun-dui-hua-kuang-jia-rasa-dai-ma-jie-du-xun-lian/15.png"></p><p>处理的逻辑：</p><p><img src="/2020/09/19/duo-lun-dui-hua-kuang-jia-rasa-dai-ma-jie-du-xun-lian/16.png"></p><p>a．　以##开头， StoryFileReader的current_step_builder指向新创建的StoryStepBuilder，这个StoryStepBuilder可以对应文件中的一个story，其成员变量</p><pre class=" language-python"><code class="language-python">        self<span class="token punctuation">.</span>name <span class="token operator">=</span> name        self<span class="token punctuation">.</span>story_steps <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        self<span class="token punctuation">.</span>current_steps <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        self<span class="token punctuation">.</span>start_checkpoints <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        <span class="token comment" spellcheck="true">## #　.../rasa/core/training/dsl.py line line 401</span>        <span class="token keyword">def</span> <span class="token function">new_story_part</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> name<span class="token punctuation">:</span> Text<span class="token punctuation">,</span> source_name<span class="token punctuation">:</span> Text<span class="token punctuation">)</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>_add_current_stories_to_result<span class="token punctuation">(</span><span class="token punctuation">)</span>            self<span class="token punctuation">.</span>current_step_builder <span class="token operator">=</span> StoryStepBuilder<span class="token punctuation">(</span>name<span class="token punctuation">,</span> source_name<span class="token punctuation">)</span></code></pre><p>b．　以以*开头的部分，对应用户输入的部分，加入到current_steps中，这是一个列表，每个元素为List[UserUttered]，看下UserUttered的构造函数，其成员对应用户输入的意图，实体等。</p><pre class=" language-python"><code class="language-python">    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>        self<span class="token punctuation">,</span>        text<span class="token punctuation">,</span>        intent<span class="token operator">=</span>None<span class="token punctuation">,</span>        entities<span class="token operator">=</span>None<span class="token punctuation">,</span>        parse_data<span class="token operator">=</span>None<span class="token punctuation">,</span>        timestamp<span class="token operator">=</span>None<span class="token punctuation">,</span>        input_channel<span class="token operator">=</span>None<span class="token punctuation">,</span>        message_id<span class="token operator">=</span>None<span class="token punctuation">,</span>    <span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment" spellcheck="true">##.../rasa/core/training/dsl.py line 354</span>  <span class="token keyword">elif</span> line<span class="token punctuation">.</span>startswith<span class="token punctuation">(</span><span class="token string">"*"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>         <span class="token comment" spellcheck="true"># reached a user message</span>          user_messages <span class="token operator">=</span> <span class="token punctuation">[</span>el<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> el <span class="token keyword">in</span> line<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">" OR "</span><span class="token punctuation">)</span><span class="token punctuation">]</span>          <span class="token keyword">if</span> self<span class="token punctuation">.</span>use_e2e<span class="token punctuation">:</span>              <span class="token keyword">await</span> self<span class="token punctuation">.</span>add_e2e_messages<span class="token punctuation">(</span>user_messages<span class="token punctuation">,</span> line_num<span class="token punctuation">)</span>               <span class="token keyword">else</span><span class="token punctuation">:</span>                  <span class="token keyword">await</span> self<span class="token punctuation">.</span>add_user_messages<span class="token punctuation">(</span>user_messages<span class="token punctuation">,</span> line_num<span class="token punctuation">)</span></code></pre><p>c．　以-开头的部分，表示系统的响应，会添加到StoryStep中的events，这是一个列表，每个元素是List[Event]。Event类用来描述回话过程中发生的事件，用来通知<em>DialogueStateTracker来更新它的状态。</em></p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># ##.../rasa/core/training/dsl.py line 350</span><span class="token keyword">elif</span> line<span class="token punctuation">.</span>startswith<span class="token punctuation">(</span><span class="token string">"-"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># reached a slot, event, or executed action</span>    event_name<span class="token punctuation">,</span> parameters <span class="token operator">=</span> self<span class="token punctuation">.</span>_parse_event_line<span class="token punctuation">(</span>line<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    self<span class="token punctuation">.</span>add_event<span class="token punctuation">(</span>event_name<span class="token punctuation">,</span> parameters<span class="token punctuation">)</span></code></pre><p><em>d．在</em>extract_story_graph中，用story_steps来构造StoryGraph对象，然后TrainingDataGenerator来构造训练的内存数据格式。这块的代码比较复杂。这里先理解：返回的结果是List[“DialogueStateTracker”]。</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true">## ../rasa/core/training/__init__.py line 25</span> story_steps <span class="token operator">=</span> <span class="token keyword">await</span> StoryFileReader<span class="token punctuation">.</span>read_from_folder<span class="token punctuation">(</span>        resource_name<span class="token punctuation">,</span>        domain<span class="token punctuation">,</span>        interpreter<span class="token punctuation">,</span>        use_e2e<span class="token operator">=</span>use_e2e<span class="token punctuation">,</span>        exclusion_percentage<span class="token operator">=</span>exclusion_percentage<span class="token punctuation">,</span>    <span class="token punctuation">)</span>    <span class="token keyword">return</span> StoryGraph<span class="token punctuation">(</span>story_steps<span class="token punctuation">)</span></code></pre><ol start="4"><li>进行训练</li></ol><pre class=" language-python"><code class="language-python">  <span class="token comment" spellcheck="true">##训练起示入口　/rasa/core/policies/ensemble.py  line 124</span>    <span class="token keyword">def</span> <span class="token function">train</span><span class="token punctuation">(</span>        self<span class="token punctuation">,</span>        training_trackers<span class="token punctuation">:</span> List<span class="token punctuation">[</span>DialogueStateTracker<span class="token punctuation">]</span><span class="token punctuation">,</span>        domain<span class="token punctuation">:</span> Domain<span class="token punctuation">,</span>        <span class="token operator">**</span>kwargs<span class="token punctuation">:</span> Any<span class="token punctuation">,</span>    <span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> None<span class="token punctuation">:</span>        <span class="token keyword">if</span> training_trackers<span class="token punctuation">:</span>            <span class="token keyword">for</span> policy <span class="token keyword">in</span> self<span class="token punctuation">.</span>policies<span class="token punctuation">:</span>                policy<span class="token punctuation">.</span>train<span class="token punctuation">(</span>training_trackers<span class="token punctuation">,</span> domain<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#&lt;--------train</span>            training_events <span class="token operator">=</span> self<span class="token punctuation">.</span>_training_events_from_trackers<span class="token punctuation">(</span>training_trackers<span class="token punctuation">)</span>            self<span class="token punctuation">.</span>action_fingerprints <span class="token operator">=</span> self<span class="token punctuation">.</span>_create_action_fingerprints<span class="token punctuation">(</span>training_events<span class="token punctuation">)</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            logger<span class="token punctuation">.</span>info<span class="token punctuation">(</span><span class="token string">"Skipped training, because there are no training samples."</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>date_trained <span class="token operator">=</span> datetime<span class="token punctuation">.</span>now<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>strftime<span class="token punctuation">(</span><span class="token string">"%Y%m%d-%H%M%S"</span><span class="token punctuation">)</span></code></pre><h2 id="NLU训练模块的解读"><a href="#NLU训练模块的解读" class="headerlink" title="NLU训练模块的解读"></a>NLU训练模块的解读</h2><p>入口是rasa.nlu.train。处理逻辑：</p><p>加载配置文件－－－构造训练组件的对象－－－加载训练数据－－－每个组件依次对训练数据进行处理和训练－－－模型的持久化</p><p>1.加载配置</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true">## /rasa/train.py line 82</span>file_importer <span class="token operator">=</span> TrainingDataImporter<span class="token punctuation">.</span>load_from_config<span class="token punctuation">(</span>    config<span class="token punctuation">,</span> domain<span class="token punctuation">,</span> training_files<span class="token punctuation">)</span></code></pre><p>调用config.load，将配置文件config.yml的文件内容读出来，并保存到RasaNLUModelConfig对象中，看下这个对象的详细信息：</p><p><img src="/2020/09/19/duo-lun-dui-hua-kuang-jia-rasa-dai-ma-jie-du-xun-lian/3.png"></p><pre class=" language-python"><code class="language-python">   <span class="token comment" spellcheck="true"># 加载config.yml #../rasa/nlu/train.py line 69</span>    <span class="token keyword">if</span> <span class="token operator">not</span> isinstance<span class="token punctuation">(</span>nlu_config<span class="token punctuation">,</span> RasaNLUModelConfig<span class="token punctuation">)</span><span class="token punctuation">:</span>        nlu_config <span class="token operator">=</span> config<span class="token punctuation">.</span>load<span class="token punctuation">(</span>nlu_config<span class="token punctuation">)</span></code></pre><ol start="2"><li>构造训练组件的对象</li></ol><p>在Trainer类的构造函数中，主要是构造pipeline，也就是说根据配置文件config.yml中的pipeline中的每个类，依次调用类的构造函数。那么配置文件中的名称和类的名称的对应关系在registry.py中的component_classes变量中。</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># build pipeline  .../rasa/nlu/model.py line 144</span>        self<span class="token punctuation">.</span>pipeline <span class="token operator">=</span> self<span class="token punctuation">.</span>_build_pipeline<span class="token punctuation">(</span>cfg<span class="token punctuation">,</span> component_builder<span class="token punctuation">)</span></code></pre><pre class=" language-python"><code class="language-python">    <span class="token keyword">def</span> <span class="token function">_build_pipeline</span><span class="token punctuation">(</span>        self<span class="token punctuation">,</span> cfg<span class="token punctuation">:</span> RasaNLUModelConfig<span class="token punctuation">,</span> component_builder<span class="token punctuation">:</span> ComponentBuilder    <span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> List<span class="token punctuation">[</span>Component<span class="token punctuation">]</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""Transform the passed names of the pipeline components into classes."""</span>        pipeline <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        <span class="token comment" spellcheck="true"># Transform the passed names of the pipeline components into classes</span>        <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>cfg<span class="token punctuation">.</span>pipeline<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            component_cfg <span class="token operator">=</span> cfg<span class="token punctuation">.</span>for_component<span class="token punctuation">(</span>i<span class="token punctuation">)</span>            component <span class="token operator">=</span> component_builder<span class="token punctuation">.</span>create_component<span class="token punctuation">(</span>component_cfg<span class="token punctuation">,</span> cfg<span class="token punctuation">)</span>            pipeline<span class="token punctuation">.</span>append<span class="token punctuation">(</span>component<span class="token punctuation">)</span>        <span class="token keyword">if</span> <span class="token operator">not</span> self<span class="token punctuation">.</span>skip_validation<span class="token punctuation">:</span>            components<span class="token punctuation">.</span>validate_pipeline<span class="token punctuation">(</span>pipeline<span class="token punctuation">)</span>        <span class="token keyword">return</span> pipeline</code></pre><p><strong>配置文件中的名称和类的名称的对应关系在registry.py中的component_classes变量</strong></p><p><img src="/2020/09/19/duo-lun-dui-hua-kuang-jia-rasa-dai-ma-jie-du-xun-lian/4.png"></p><p>３．　加载训练数据</p><p>load_data会读取训练数据，都保存在TrainingData对象中，</p><p>training_examples，对应数据中common_examples</p><p>entity_synonyms，对应数据中entity_synonyms，目前样例为空。</p><p>regex_features，对应数据中regex_features，目前样例为空</p><p>lookup_tables，目前样例为空</p><p>在看看数据结构的定义，training_examples是一个列表，每个元素是一个Message对象。</p><p><img src="/2020/09/19/duo-lun-dui-hua-kuang-jia-rasa-dai-ma-jie-du-xun-lian/6.png"></p><p>分别对应Message对象中的text ，data[“intent”]，data[“entities”] ，data是一个字典。</p><p>４．　每个组件依次对训练数据进行处理和训练</p><p>trainer.train，依次调用pipeline每个组件的预处理函数和训练函数，这里需要理解过程中训练的结果保存到哪里？</p><pre class=" language-python"><code class="language-python"> <span class="token comment" spellcheck="true"># nlu/tain.py  line 90</span>interpreter <span class="token operator">=</span> trainer<span class="token punctuation">.</span>train<span class="token punctuation">(</span>training_data<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span></code></pre><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># ../nul/model.py line 188　不同方法训练部分</span><span class="token keyword">for</span> i<span class="token punctuation">,</span> component <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>self<span class="token punctuation">.</span>pipeline<span class="token punctuation">)</span><span class="token punctuation">:</span>            logger<span class="token punctuation">.</span>info<span class="token punctuation">(</span>f<span class="token string">"Starting to train component &amp;#123;component.name&amp;#125;"</span><span class="token punctuation">)</span>            component<span class="token punctuation">.</span>prepare_partial_processing<span class="token punctuation">(</span>self<span class="token punctuation">.</span>pipeline<span class="token punctuation">[</span><span class="token punctuation">:</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> context<span class="token punctuation">)</span>            updates <span class="token operator">=</span> component<span class="token punctuation">.</span>train<span class="token punctuation">(</span>working_data<span class="token punctuation">,</span> self<span class="token punctuation">.</span>config<span class="token punctuation">,</span> <span class="token operator">**</span>context<span class="token punctuation">)</span>            logger<span class="token punctuation">.</span>info<span class="token punctuation">(</span><span class="token string">"Finished training component."</span><span class="token punctuation">)</span>            <span class="token keyword">if</span> updates<span class="token punctuation">:</span>                context<span class="token punctuation">.</span>update<span class="token punctuation">(</span>updates<span class="token punctuation">)</span></code></pre><p>这里采用一个字典context，在训练开始前，每个组件都通过provide_context将需要提供的上下文信息更新到context中，大部分组件不需要提供预置信息。比如<em>MITIE ， spacy需要</em>提供框架的环境变量，比如词向量。</p><p>组件JiebaTokenizer的训练：将training_examples中的每个Message对象中的text 进行分词，并将分词结果保存到data[“tokens”]中。</p><p>组件CRFEntityExtractor的训练，输出：保存到data[“entities”]中。</p><p>组件JiebaPsegExtractor，没有定义训练过程，无需训练。</p><p>组件BertVectorsFeaturizer，</p><p>组件EmbeddingBertIntentClassifier，</p><ol start="5"><li>模型的持久化</li></ol><p>将模型的输出打包成一个<em>zip文件，</em>fingerprint的作用是，保存模型和训练数据的对应关系。当训练数据没有变化时，fingerprint也是一致的，就不用重新训练。</p><pre class=" language-python"><code class="language-python">   <span class="token comment" spellcheck="true">## 持久化　/rasa/nlu/model.py　line 242</span>    <span class="token keyword">def</span> <span class="token function">persist</span><span class="token punctuation">(</span>        self<span class="token punctuation">,</span>        path<span class="token punctuation">:</span> Text<span class="token punctuation">,</span>        persistor<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Persistor<span class="token punctuation">]</span> <span class="token operator">=</span> None<span class="token punctuation">,</span>        fixed_model_name<span class="token punctuation">:</span> Text <span class="token operator">=</span> None<span class="token punctuation">,</span>        persist_nlu_training_data<span class="token punctuation">:</span> bool <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>    <span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> Text<span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""Persist all components of the pipeline to the passed path.        Returns the directory of the persisted model."""</span>        <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>        <span class="token keyword">for</span> i<span class="token punctuation">,</span> component <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>self<span class="token punctuation">.</span>pipeline<span class="token punctuation">)</span><span class="token punctuation">:</span>            file_name <span class="token operator">=</span> self<span class="token punctuation">.</span>_file_name<span class="token punctuation">(</span>i<span class="token punctuation">,</span> component<span class="token punctuation">.</span>name<span class="token punctuation">)</span>            update <span class="token operator">=</span> component<span class="token punctuation">.</span>persist<span class="token punctuation">(</span>file_name<span class="token punctuation">,</span> dir_name<span class="token punctuation">)</span>            component_meta <span class="token operator">=</span> component<span class="token punctuation">.</span>component_config            <span class="token keyword">if</span> update<span class="token punctuation">:</span>                component_meta<span class="token punctuation">.</span>update<span class="token punctuation">(</span>update<span class="token punctuation">)</span>            component_meta<span class="token punctuation">[</span><span class="token string">"class"</span><span class="token punctuation">]</span> <span class="token operator">=</span> utils<span class="token punctuation">.</span>module_path_from_object<span class="token punctuation">(</span>component<span class="token punctuation">)</span>            metadata<span class="token punctuation">[</span><span class="token string">"pipeline"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span>component_meta<span class="token punctuation">)</span>        Metadata<span class="token punctuation">(</span>metadata<span class="token punctuation">,</span> dir_name<span class="token punctuation">)</span><span class="token punctuation">.</span>persist<span class="token punctuation">(</span>dir_name<span class="token punctuation">)</span>        <span class="token keyword">if</span> persistor <span class="token keyword">is</span> <span class="token operator">not</span> None<span class="token punctuation">:</span>            persistor<span class="token punctuation">.</span>persist<span class="token punctuation">(</span>dir_name<span class="token punctuation">,</span> model_name<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># &lt;-----------------</span>        logger<span class="token punctuation">.</span>info<span class="token punctuation">(</span>            <span class="token string">"Successfully saved model into '&amp;#123;&amp;#125;'"</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>abspath<span class="token punctuation">(</span>dir_name<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token punctuation">)</span>        <span class="token keyword">return</span> dir_name</code></pre><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> nlp </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Rasa </tag>
            
            <tag> QA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>随机森林原理</title>
      <link href="/2020/09/18/sui-ji-sen-lin-yuan-li/"/>
      <url>/2020/09/18/sui-ji-sen-lin-yuan-li/</url>
      
        <content type="html"><![CDATA[<h1 id="随机森林原理"><a href="#随机森林原理" class="headerlink" title="随机森林原理"></a>随机森林原理</h1><h2 id="集成算法概述"><a href="#集成算法概述" class="headerlink" title="集成算法概述"></a>集成算法概述</h2><p>集成学习（ensemble learning）是时下非常流行的机器学习算法，它本身不是一个单独的机器学习算法，而是通过在数据上构建多个模型，集成所有模型的建模结果。基本上所有的机器学习领域都可以看到集成学习的身影，在现实中集成学习也有相当大的作用，它可以用来做市场营销模拟的建模，统计客户来源，保留和流失，也可用来预测疾病的风险和病患者的易感性。在现在的各种算法竞赛中，随机森林，梯度提升树（GBDT），Xgboost等集成算法的身影也随处可见，可见其效果之好，应用之广。</p><p><strong>集成算法的目标</strong></p><p>集成算法会考虑多个评估器的建模结果，汇总之后得到一个综合的结果，以此来获取比单个模型更好的回归或分类</p><p>多个模型集成成为的模型叫做集成评估器（ensemble estimator），组成集成评估器的每个模型都叫做基评估器base estimator）。通常来说，有三类集成算法：装袋法（Bagging），提升法（Boosting）和stacking。</p><p><img src="/2020/09/18/sui-ji-sen-lin-yuan-li/1.png"></p><p>装袋法的核心思想是构建多个相互独立的评估器,然后对其预测进行平均或多数表决原则来决定集成评估器的结<br>果。装袋法的代表模型就是随机森林。</p><p>提升法中,基评估器是相关的,是按顺序一一构建的。其核心思想是结合弱评估器的力量一次次对难以评估的样本<br>进行预测,从而构成一个强评估器。提升法的代表模型有Adaboost和梯度提升树。</p><h2 id="sklearn中的集成算法"><a href="#sklearn中的集成算法" class="headerlink" title="sklearn中的集成算法"></a>sklearn中的集成算法</h2><table><thead><tr><th>类</th><th>类的功能</th></tr></thead><tbody><tr><td>ensemble.AdaBoostClassifier</td><td>AdaBoost分类</td></tr><tr><td>ensemble.AdaBoostRegressor</td><td>Adaboost回归</td></tr><tr><td>ensemble.BaggingClassifier</td><td>装袋分类器</td></tr><tr><td>ensemble.BaggingRegressor</td><td>装袋回归器</td></tr><tr><td>ensemble.ExtraTreesClassifier</td><td>Extra-trees分类(超树,极端随机树)</td></tr><tr><td>ensemble.ExtraTreesRegressor</td><td>Extra-trees回归</td></tr><tr><td>ensemble.GradientBoostingClassifier</td><td>梯度提升分类</td></tr><tr><td>ensemble.GradientBoostingRegressor</td><td>梯度提升回归</td></tr><tr><td>ensemble.IsolationForest</td><td>隔离森林</td></tr><tr><td>ensemble.RandomForestClassifier</td><td>随机森林分类</td></tr><tr><td>ensemble.RandomForestRegressor</td><td>随机森林回归</td></tr><tr><td>ensemble.RandomTreesEmbedding</td><td>完全随机树的集成</td></tr><tr><td>ensemble.VotingClassifier</td><td>用于不合适估算器的软投票/多数规则分类器</td></tr></tbody></table><h2 id="RandomForestClassifier"><a href="#RandomForestClassifier" class="headerlink" title="RandomForestClassifier"></a>RandomForestClassifier</h2><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">sklearn</span><span class="token punctuation">.</span>ensemble<span class="token punctuation">.</span>RandomForestClassifier <span class="token punctuation">(</span>n_estimators<span class="token operator">=</span>’<span class="token number">10</span>’<span class="token punctuation">,</span> criterion<span class="token operator">=</span>’gini’<span class="token punctuation">,</span> max_depth<span class="token operator">=</span>None<span class="token punctuation">,</span>min_samples_split<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> min_samples_leaf<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> min_weight_fraction_leaf<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span> max_features<span class="token operator">=</span>’auto’<span class="token punctuation">,</span>max_leaf_nodes<span class="token operator">=</span>None<span class="token punctuation">,</span> min_impurity_decrease<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span> min_impurity_split<span class="token operator">=</span>None<span class="token punctuation">,</span> bootstrap<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> oob_score<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>n_jobs<span class="token operator">=</span>None<span class="token punctuation">,</span> random_state<span class="token operator">=</span>None<span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> warm_start<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> class_weight<span class="token operator">=</span>None<span class="token punctuation">)</span></code></pre><p>随机森林是非常具有代表性的Bagging集成算法,它的所有基评估器都是决策树,分类树组成的森林就叫做随机森<br>林分类器,回归树所集成的森林就叫做随机森林回归器。这一节主要讲解RandomForestClassifier,随机森林分类<br>器。</p><h3 id="重要参数"><a href="#重要参数" class="headerlink" title="重要参数"></a>重要参数</h3><h4 id="控制基评估器的参数"><a href="#控制基评估器的参数" class="headerlink" title="控制基评估器的参数"></a>控制基评估器的参数</h4><p>criterion  不纯度的衡量指标,有基尼系数和信息熵两种选择</p><p>max_depth  树的最大深度,超过最大深度的树枝都会被剪掉 </p><p>min_samples_leaf 一个节点在分枝后的每个子节点都必须包含至少min_samples_leaf个训练样本,否则分枝就不会发生</p><p>min_samples_split 一个节点必须要包含至少min_samples_split个训练样本,这个节点才允许被分枝,否则分枝就不会发生</p><p>max_features  max_features限制分枝时考虑的特征个数,超过限制个数的特征都会被舍弃,i  默认值为总特征个数开平方取整</p><p>min_impurity_decrease  限制信息增益的大小,信息增益小于设定数值的分枝不会发生</p><h3 id="n-estimators"><a href="#n-estimators" class="headerlink" title="n_estimators"></a>n_estimators</h3><p>这是森林中树木的数量，即基评估器的数量。这个参数对随机森林模型的精确性影响是单调的，<strong>n_estimators</strong>越<strong>大，模型的效果往往越好</strong>。但是相应的，任何模型都有决策边界，n_estimators达到一定的程度之后，随机森林的精确性往往不在上升或开始波动，并且，n_estimators越大，需要的计算量和内存也越大，训练的时间也会越来越长。对于这个参数，我们是渴望在训练难度和模型效果之间取得平衡。</p><p>n_estimators的默认值在现有版本的sklearn中是10，但是在即将更新的0.22版本中，这个默认值会被修正为100。这个修正显示出了使用者的调参倾向：要更大的n_estimators。</p><h3 id="random-state"><a href="#random-state" class="headerlink" title="random_state"></a><strong>random_state</strong></h3><p>随机森林的本质是一种装袋集成算法（bagging），装袋集成算法是对基评估器的预测结果进行平均或用多数表决原则来决定集成评估器的结果。在刚才的红酒例子中，我们建立了25棵树，对任何一个样本而言，平均或多数表决原则下，当且仅当有13棵以上的树判断错误的时候，随机森林才会判断错误。</p><h3 id="bootstrap-amp-oob-score"><a href="#bootstrap-amp-oob-score" class="headerlink" title="bootstrap &amp; oob_score"></a><strong>bootstrap &amp; oob_score</strong></h3><p>​       要让基分类器尽量都不一样，一种很容易理解的方法是使用不同的训练集来进行训练，而袋装法正是通过有放回的随机抽样技术来形成不同的训练数据，bootstrap就是用来控制抽样技术的参数。在一个含有n个样本的原始训练集中，我们进行随机采样，每次采样一个样本，并在抽取下一个样本之前将该样本放回原始训练集，也就是说下次采样时这个样本依然可能被采集到，这样采集n次，最终得到一个和原始训练集一样大的，n个样本组成的自助集。由于是随机采样，这样每次的自助集和原始数据集不同，和其他的采样集也是不同的。这样我们就可以自由创造取之不尽用之不竭，并且互不相同的自助集，用这些自助集来训练我们的基分类器，我们的基分类器自然也就各不相同了。</p><p>​         <strong>bootstrap</strong>参数默认<strong>True</strong>，代表采用这种有放回的随机抽样技术。通常，这个参数不会被我们设置为False。</p><p>​         当n足够大时，这个概率收敛于1-(1/e)，约等于0.632。因此，会有约37%的训练数据被浪费掉，没有参与建模，这些数据被称为袋外数据(out of bag data，简写为oob)。除了我们最开始就划分好的测试集之外，这些数据也可以被用来作为集成算法的测试集。<strong>也就是说，在使用随机森林时，我们可以不划分测试集和训练集，只需要用袋外数据来测试我们的模型即可。</strong>当然，这也不是绝对的，当n和n_estimators都不够大的时候，很可能就没有数据掉落在袋外，自然也就无法使用oob数据来测试模型了。</p><h3 id="重要属性和接口"><a href="#重要属性和接口" class="headerlink" title="重要属性和接口"></a><strong>重要属性和接口</strong></h3><p>至此，我们已经讲完了所有随机森林中的重要参数，为大家复习了一下决策树的参数，并通过n_estimators，random_state，boostrap和oob_score这四个参数帮助大家了解了袋装法的基本流程和重要概念。同时，我们还介绍了**.estimators_** 和 <strong>.oob_score_</strong> 这两个重要属性。除了这两个属性之外，作为树模型的集成算法，随机森林自然也有**.feature_importances_**这个属性。</p><p>随机森林的接口与决策树完全一致，因此依然有四个常用接口：<strong>apply, ﬁt, predict</strong>和<strong>score</strong>。除此之外，还需要注意随机森林的predict_proba接口，这个接口返回每个测试样本对应的被分到每一类标签的概率，标签有几个分类就返回几个概率。如果是二分类问题，则predict_proba返回的数值大于0.5的，被分为1，小于0.5的，被分为0。传统的随机森林是利用袋装法中的规则，平均或少数服从多数来决定集成的结果，而sklearn中的随机森林是平均每个样本对应的predict_proba返回的概率，得到一个平均概率，从而决定测试样本的分类。</p><h2 id="RandomForestRegressor"><a href="#RandomForestRegressor" class="headerlink" title="RandomForestRegressor"></a><strong>RandomForestRegressor</strong></h2><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">sklearn</span><span class="token punctuation">.</span>ensemble<span class="token punctuation">.</span>RandomForestRegressor  <span class="token punctuation">(</span>n_estimators<span class="token operator">=</span>’warn’<span class="token punctuation">,</span> criterion<span class="token operator">=</span>’mse’<span class="token punctuation">,</span> max_depth<span class="token operator">=</span>None<span class="token punctuation">,</span>min_samples_split<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> min_samples_leaf<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> min_weight_fraction_leaf<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span> max_features<span class="token operator">=</span>’auto’<span class="token punctuation">,</span>max_leaf_nodes<span class="token operator">=</span>None<span class="token punctuation">,</span> min_impurity_decrease<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span> min_impurity_split<span class="token operator">=</span>None<span class="token punctuation">,</span> bootstrap<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> oob_score<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>n_jobs<span class="token operator">=</span>None<span class="token punctuation">,</span> random_state<span class="token operator">=</span>None<span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> warm_start<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span></code></pre><h2 id="重要参数-属性与接口"><a href="#重要参数-属性与接口" class="headerlink" title="重要参数,属性与接口"></a>重要参数,属性与接口</h2><p>基本与决策树一致：<a href="/2020/09/17/jue-ce-shu-yuan-li/" title="决策树原理">决策树原理</a></p><p>　　最重要的属性和接口,都与随机森林的分类器相一致,还是apply, fit, predict和score最为核心。值得一提的是,随机森林回归并没有predict_proba这个接口,因为对于回归来说,并不存在一个样本要被分到某个类别的概率问<br>题,因此没有predict_proba这个接口。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 树模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>word2vec实战：获取和预处理中文维基百科(Wikipedia)语料库，并训练成word2vec模型</title>
      <link href="/2020/09/18/word2vec-shi-zhan-huo-qu-he-yu-chu-li-zhong-wen-wei-ji-bai-ke-wikipedia-yu-liao-ku-bing-xun-lian-cheng-word2vec-mo-xing/"/>
      <url>/2020/09/18/word2vec-shi-zhan-huo-qu-he-yu-chu-li-zhong-wen-wei-ji-bai-ke-wikipedia-yu-liao-ku-bing-xun-lian-cheng-word2vec-mo-xing/</url>
      
        <content type="html"><![CDATA[<h1 id="word2vec实战：获取和预处理中文维基百科-Wikipedia-语料库，并训练成word2vec模型"><a href="#word2vec实战：获取和预处理中文维基百科-Wikipedia-语料库，并训练成word2vec模型" class="headerlink" title="word2vec实战：获取和预处理中文维基百科(Wikipedia)语料库，并训练成word2vec模型"></a>word2vec实战：获取和预处理中文维基百科(Wikipedia)语料库，并训练成word2vec模型</h1><p>转载：<a href="https://blog.csdn.net/qq_32166627/article/details/68942216">https://blog.csdn.net/qq_32166627/article/details/68942216</a></p><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>自然语言处理有很多方法，最近很流行的是谷歌开源项目word2vec，详见谷歌官网：<a href="https://code.google.com/archive/p/word2vec/">官网链接</a>。其主要理论由Tomas Mikolov大神团队的2篇论文组成：<a href="https://arxiv.org/pdf/1301.3781.pdf">Efficient Estimation of Word Representations in Vector Space</a>， <a href="https://arxiv.org/pdf/1310.4546.pdf">Distributed Representations of Words and Phrases and their Compositionality</a>。</p><p>传统的方法是将词汇作为离散的单一符号，这些符号编码毫无规则，无法提供词汇之间可能存在的关联关系，而词汇的向量表示将克服上述难题。</p><p>向量空间模型（VSM）将词汇表示在一个连续的向量空间中，语义近似的词被映射为相邻的数据点。VSM依赖于分布式假设思想，该思想的核心是：出现于相同的上下文情景中的词汇都有相似的语义。</p><p>基于VSM假设有2种研究方法：<br>1，基于计数的方法：计算某词汇极其临近词在一个大型语料库中共同出现的频率，然后将其映射到一个小而稠密的向量中。<br>2，预测方法：该方法试图直接从某词汇的临近词对其进行预测，此过程利用学习到的向量。</p><p>word2vec是一种可以进行高效率词嵌套学习的预测模型，该模型有2种具体的形式：<br>1，CBOW模型：根据上下文词汇“the cat sits on the”，来预测目标词“mat”。<br>2，skip-gram模型：通过目标词来预测源词汇。</p><p>本文不关注具体理论，而是详细介绍如何获取并预处理中文维基百科语料库，并训练成word2vec模型。</p><p>实验环境：Ubuntu14.04/Python2.7（Anaconda版）</p><h2 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h2><p>1，下载原始数据<br>数据下载地址：<a href="https://dumps.wikimedia.org/zhwiki/latest/zhwiki-latest-pages-articles.xml.bz2">https://dumps.wikimedia.org/zhwiki/latest/zhwiki-latest-pages-articles.xml.bz2</a></p><p>下载的文件是一个大小为1.3G的压缩包，解压后是个5.8G左右的xml文件，内容是网页标签形式的。我们需要抽取出其中的有效信息。</p><p>2，使用Wikipedia Extractor抽取正文<br>Wikipedia Extractor是意大利人用Python写的一个维基百科抽取器，使用非常方便。下载之后直接使用这条命令即可完成抽取，运行时间很快。执行以下命令。</p><pre class=" language-shell"><code class="language-shell">$ sudo apt-get install unzip python python-dev python-pip$ git clone https://github.com/attardi/wikiextractor.git wikiextractor$ cd wikiextractor$ sudo python setup.py install$ ./WikiExtractor.py -b 1024M -o extracted zhwiki-latest-pages-articles.xml.bz2</code></pre><p>参数-b 1024M表示以1024M为单位切分文件，默认是1M。由于最后生成的正文文本约1060M，把参数设置的大一些可以保证最后的抽取结果全部存在一个文件里。这里我们设为1024M，可以分成一个1G的大文件和一个36M的小文件，后续的步骤可以先在小文件上实验，再应用到大文件上。</p><p>这里，我们得到了2个文本文件：wiki_00, wiki_01。大小分别为：1024M, 36.7M。</p><p>3，繁体转简体<br>维基百科的中文数据是繁简混杂的，里面包含大陆简体、台湾繁体、港澳繁体等多种不同的数据。有时候在一篇文章的不同段落间也会使用不同的繁简字。</p><p>为了处理方便起见，我们直接使用了开源项目opencc。参照安装说明的方法，安装完成之后，使用下面的命令进行繁简转换，整个过程也很快：</p><pre class=" language-shell"><code class="language-shell">$ sudo apt-get install opencc$ opencc -i wiki_00 -o zh_wiki_00 -c zht2zhs.ini$ opencc -i wiki_01 -o zh_wiki_01 -c zht2zhs.ini</code></pre><p>命令中的wiki_00/wiki_01这个文件是此前使用Wikipedia Extractor得到的。到了这里，我们已经完成了大部分繁简转换工作。</p><p>这里，我们得到了2个简体文件：zh_wiki_00，zh_wiki_01。大小分别为：1024M，36.7M。同上步结果大小不变。</p><p>4，符号处理<br>由于Wikipedia Extractor抽取正文时，会将有特殊标记的外文直接剔除。我们最后再将「」『』这些符号替换成引号，顺便删除空括号，就大功告成了！代码如下：</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true">#!/usr/bin/python</span><span class="token comment" spellcheck="true"># -*- coding: utf-8 -*-</span><span class="token keyword">import</span> re<span class="token keyword">import</span> sys<span class="token keyword">import</span> codecs<span class="token keyword">def</span> <span class="token function">myfun</span><span class="token punctuation">(</span>input_file<span class="token punctuation">)</span><span class="token punctuation">:</span>    p1 <span class="token operator">=</span> re<span class="token punctuation">.</span>compile<span class="token punctuation">(</span>ur<span class="token string">'-\&amp;#123;.*?(zh-hans|zh-cn):([^;]*?)(;.*?)?\&amp;#125;-'</span><span class="token punctuation">)</span>    p2 <span class="token operator">=</span> re<span class="token punctuation">.</span>compile<span class="token punctuation">(</span>ur<span class="token string">'[（\(][，；。？！\s]*[）\)]'</span><span class="token punctuation">)</span>    p3 <span class="token operator">=</span> re<span class="token punctuation">.</span>compile<span class="token punctuation">(</span>ur<span class="token string">'[「『]'</span><span class="token punctuation">)</span>    p4 <span class="token operator">=</span> re<span class="token punctuation">.</span>compile<span class="token punctuation">(</span>ur<span class="token string">'[」』]'</span><span class="token punctuation">)</span>    outfile <span class="token operator">=</span> codecs<span class="token punctuation">.</span>open<span class="token punctuation">(</span><span class="token string">'std_'</span> <span class="token operator">+</span> input_file<span class="token punctuation">,</span> <span class="token string">'w'</span><span class="token punctuation">,</span> <span class="token string">'utf-8'</span><span class="token punctuation">)</span>    <span class="token keyword">with</span> codecs<span class="token punctuation">.</span>open<span class="token punctuation">(</span>input_file<span class="token punctuation">,</span> <span class="token string">'r'</span><span class="token punctuation">,</span> <span class="token string">'utf-8'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> myfile<span class="token punctuation">:</span>        <span class="token keyword">for</span> line <span class="token keyword">in</span> myfile<span class="token punctuation">:</span>            line <span class="token operator">=</span> p1<span class="token punctuation">.</span>sub<span class="token punctuation">(</span>ur<span class="token string">'\2'</span><span class="token punctuation">,</span> line<span class="token punctuation">)</span>            line <span class="token operator">=</span> p2<span class="token punctuation">.</span>sub<span class="token punctuation">(</span>ur<span class="token string">''</span><span class="token punctuation">,</span> line<span class="token punctuation">)</span>            line <span class="token operator">=</span> p3<span class="token punctuation">.</span>sub<span class="token punctuation">(</span>ur<span class="token string">'“'</span><span class="token punctuation">,</span> line<span class="token punctuation">)</span>            line <span class="token operator">=</span> p4<span class="token punctuation">.</span>sub<span class="token punctuation">(</span>ur<span class="token string">'”'</span><span class="token punctuation">,</span> line<span class="token punctuation">)</span>            outfile<span class="token punctuation">.</span>write<span class="token punctuation">(</span>line<span class="token punctuation">)</span>    outfile<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> len<span class="token punctuation">(</span>sys<span class="token punctuation">.</span>argv<span class="token punctuation">)</span> <span class="token operator">!=</span> <span class="token number">2</span><span class="token punctuation">:</span>        <span class="token keyword">print</span> <span class="token string">"Usage: python script.py inputfile"</span>        sys<span class="token punctuation">.</span>exit<span class="token punctuation">(</span><span class="token punctuation">)</span>    reload<span class="token punctuation">(</span>sys<span class="token punctuation">)</span>    sys<span class="token punctuation">.</span>setdefaultencoding<span class="token punctuation">(</span><span class="token string">'utf-8'</span><span class="token punctuation">)</span>    input_file <span class="token operator">=</span> sys<span class="token punctuation">.</span>argv<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>    myfun<span class="token punctuation">(</span>input_file<span class="token punctuation">)</span></code></pre><p>将上述代码保存到exec.py文件，并将该文件放到与数据文件相同的目录，执行命令：</p><pre class=" language-shell"><code class="language-shell">$ python exec.py zh_wiki_00$ python exec.py zh_wiki_01</code></pre><p>这里，我们又得到2个格式化文件：std_zh_wiki_00，std_zh_wiki_01。大小分别为：1021M，36.6M。大小比之前的文件要小，因为修改删除了文件中的符号。</p><p>5，中文分词<br>中文分词工具有很多种， 这里我们使用python版本的结巴分词：<a href="https://github.com/fxsjy/jieba">https://github.com/fxsjy/jieba</a></p><p>安装很简单：$ pip install jieba</p><p>安装好后执行命令进行分词：</p><pre class=" language-shell"><code class="language-shell">python -m jieba -d " " ./std_zh_wiki_00 > ./cut_std_zh_wiki_00python -m jieba -d " " ./std_zh_wiki_01 > ./cut_std_zh_wiki_01</code></pre><p>命令中的-d ” “选项，双引号中是一个空格，指的是以空格分割词汇。</p><p>这里，我们又得到2个分词文件：cut_std_zh_wiki_00，cut_std_zh_wiki_01。大小分别为：1.21G，44.6M。比之前的文件要大，因为文件中加入了很多空格符。</p><p>6，训练word2vec模型<br>训练模型我们使用python的gensim库提供的方法，<a href="https://radimrehurek.com/gensim/">gensim官网</a>。</p><p>安装非常简单：$ pip install gensim</p><p>接下来，我们用1.21G的大文件进行训练，训练代码也很简单：</p><pre class=" language-python"><code class="language-python"><span class="token keyword">from</span> gensim<span class="token punctuation">.</span>models <span class="token keyword">import</span> word2vec<span class="token keyword">import</span> logginglogging<span class="token punctuation">.</span>basicConfig<span class="token punctuation">(</span>format<span class="token operator">=</span><span class="token string">'%(asctime)s : %(levelname)s : %(message)s'</span><span class="token punctuation">,</span> level<span class="token operator">=</span>logging<span class="token punctuation">.</span>INFO<span class="token punctuation">)</span>sentences <span class="token operator">=</span> word2vec<span class="token punctuation">.</span>LineSentence<span class="token punctuation">(</span>u<span class="token string">'./cut_std_zh_wiki_00'</span><span class="token punctuation">)</span>model <span class="token operator">=</span> word2vec<span class="token punctuation">.</span>Word2Vec<span class="token punctuation">(</span>sentences<span class="token punctuation">,</span>size<span class="token operator">=</span><span class="token number">200</span><span class="token punctuation">,</span>window<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span>min_count<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span>workers<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">)</span>model<span class="token punctuation">.</span>save<span class="token punctuation">(</span><span class="token string">'./word2vecModel/WikiCHModel'</span><span class="token punctuation">)</span></code></pre><p>训练的过程进度会打印在控制台。训练结束后我们就得到了一个word2vec模型。</p><p>7，调用并测试word2vec模型<br>调用模型也很简单，同样使用gensim库。</p><pre class=" language-python"><code class="language-python"><span class="token keyword">from</span> gensim<span class="token punctuation">.</span>models <span class="token keyword">import</span> word2vecmodel <span class="token operator">=</span> word2vec<span class="token punctuation">.</span>Word2Vec<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'./word2vecModel/WikiCHModel'</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>model<span class="token punctuation">.</span>wv<span class="token punctuation">.</span>similarity<span class="token punctuation">(</span><span class="token string">'奥运会'</span><span class="token punctuation">,</span><span class="token string">'金牌'</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#两个词的相关性</span><span class="token keyword">print</span><span class="token punctuation">(</span>model<span class="token punctuation">.</span>wv<span class="token punctuation">.</span>most_similar<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">'伦敦'</span><span class="token punctuation">,</span><span class="token string">'中国'</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token string">'北京'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 北京is to中国 as 伦敦is to？</span></code></pre><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> nlp </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语言模型 </tag>
            
            <tag> 词向量 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>rasa使用指南</title>
      <link href="/2020/09/18/rasa-shi-yong-zhi-nan/"/>
      <url>/2020/09/18/rasa-shi-yong-zhi-nan/</url>
      
        <content type="html"><![CDATA[<p>转载：<a href="https://www.jianshu.com/u/fd743bd5ad2d">https://www.jianshu.com/u/fd743bd5ad2d</a></p><h1 id="Rasa"><a href="#Rasa" class="headerlink" title="Rasa"></a>Rasa</h1><p><code>Rasa</code>是一个开源机器学习框架，用于构建上下文AI助手和聊天机器人。<br> Rasa有两个主要模块：</p><ul><li><code>Rasa NLU</code> ：用于理解用户消息，包括意图识别和实体识别，它会把用户的输入转换为结构化的数据。</li><li><code>Rasa Core</code>：是一个对话管理平台，用于举行对话和决定下一步做什么。</li></ul><p><code>Rasa X</code>是一个工具，可帮助您构建、改进和部署由Rasa框架提供支持的AI Assistants。 Rasa X包括用户界面和REST API。</p><p><strong>Rasa官方文档</strong>： <a href="https://links.jianshu.com/go?to=https://rasa.com/docs/rasa/">Build contextual chatbots and AI assistants with Rasa</a></p><p><strong>github地址</strong>：<a href="https://links.jianshu.com/go?to=https://github.com/RasaHQ/rasa">RasaHQ/rasa</a></p><p><strong>pip安装</strong></p><pre class=" language-shell"><code class="language-shell">pip install rasa_nlupip install rasa_core[tensorflow]</code></pre><h2 id="响应消息的基本步骤："><a href="#响应消息的基本步骤：" class="headerlink" title="响应消息的基本步骤："></a>响应消息的基本步骤：</h2><p><img src="/2020/09/18/rasa-shi-yong-zhi-nan/1.png"></p><ul><li>首先，将用户输入的Message传递到Interpreter(Rasa NLU模块)，该模块负责识别Message中的”意图(intent)“和提取所有”实体”(entity)数据；</li><li>其次，Rasa Core会将Interpreter提取到的意图和识别传给Tracker对象，该对象的主要作用是跟踪会话状态(conversation state)；</li><li>第三，利用policy记录Tracker对象的当前状态，并选择执行相应的action，其中，这个action是被记录在Track对象中的；</li><li>最后，将执行action返回的结果输出即完成一次人机交互。</li></ul><h3 id="术语"><a href="#术语" class="headerlink" title="术语"></a>术语</h3><ul><li><code>intents</code>：意图</li><li><code>pipeline</code>：</li><li><code>story</code>：Core model 以训练“stories”的形式从真实的会话数据中学习。故事是用户和助手之间的真实对话.</li><li><code>domain</code>：定义了助手所处的universe：它应该获得的用户输入，应该能够预测的操作，如何响应以及要存储的信息</li></ul><h2 id="Rasa-NLU"><a href="#Rasa-NLU" class="headerlink" title="Rasa_NLU"></a>Rasa_NLU</h2><p>Rasa NLU曾经是一个独立的库，但它现在是Rasa框架的一部分。</p><p>Rasa_NLU是一个开源的、可本地部署并配套有语料标注工具<a href="https://links.jianshu.com/go?to=https://rasahq.github.io/rasa-nlu-trainer/">RASA NLU Trainer</a>。其本身可支持任何语言，中文因其特殊性需要加入特定的<code>tokenizer</code>作为整个流程的一部分。</p><p><code>Rasa NLU</code> 用于聊天机器人中的<strong>意图识别</strong>和<strong>实体提取</strong>。例如，下面句子：</p><pre><code>"I am looking for a Mexican restaurant in the center of town"</code></pre><p>返回结构化数据:</p><pre class=" language-json"><code class="language-json">&amp;#<span class="token number">123</span><span class="token punctuation">;</span>  <span class="token property">"intent"</span><span class="token operator">:</span> <span class="token string">"search_restaurant"</span><span class="token punctuation">,</span>  <span class="token property">"entities"</span><span class="token operator">:</span> &amp;#<span class="token number">123</span><span class="token punctuation">;</span>    <span class="token property">"cuisine"</span> <span class="token operator">:</span> <span class="token string">"Mexican"</span><span class="token punctuation">,</span>    <span class="token property">"location"</span> <span class="token operator">:</span> <span class="token string">"center"</span>  &amp;#<span class="token number">125</span><span class="token punctuation">;</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span></code></pre><p><a href="https://links.jianshu.com/go?to=https://github.com/crownpku/Rasa_NLU_Chi">Rasa_NLU_Chi</a> 作为 <code>Rasa_NLU 的一个 fork 版本</code>，加入了<code>jieba</code> 作为中文的 tokenizer，实现了中文支持。</p><p>该部分简单介绍基于 <code>Rasa_NLU_Chi</code> 构建一个本地部署的特定领域的中文 NLU 系统的过程。</p><p><strong>目标</strong></p><ul><li>输入： 中文测试文本</li><li>输出： 结构化的数据，识别出文本中对应的意图和实体</li></ul><h3 id="第一步：-Pipeline"><a href="#第一步：-Pipeline" class="headerlink" title="第一步： Pipeline"></a>第一步： Pipeline</h3><p>rasa nlu 支持不同的 Pipeline，其后端实现可支持<code>spaCy、MITIE、MITIE + sklearn 以及 tensorflow</code>，其中 spaCy 是官方推荐的，另外值得注意的是从 0.12 版本后，MITIE 就被列入 Deprecated 了。</p><p>本例使用的 <strong>pipeline</strong> 为 <code>MITIE+Jieba+sklearn</code>， rasa nlu 的<strong>配置文件</strong>为 <code>config_jieba_mitie_sklearn.yml</code>如下：</p><pre class=" language-c"><code class="language-c">language<span class="token punctuation">:</span> <span class="token string">"zh"</span>pipeline<span class="token punctuation">:</span><span class="token operator">-</span> name<span class="token punctuation">:</span> <span class="token string">"nlp_mitie"</span>  model<span class="token punctuation">:</span> <span class="token string">"data/total_word_feature_extractor_zh.dat"</span>  <span class="token comment" spellcheck="true">// 加载 mitie 模型</span><span class="token operator">-</span> name<span class="token punctuation">:</span> <span class="token string">"tokenizer_jieba"</span>   <span class="token comment" spellcheck="true">// 使用 jieba 进行分词</span><span class="token operator">-</span> name<span class="token punctuation">:</span> <span class="token string">"ner_mitie"</span>   <span class="token comment" spellcheck="true">// mitie 的命名实体识别</span><span class="token operator">-</span> name<span class="token punctuation">:</span> <span class="token string">"ner_synonyms"</span><span class="token operator">-</span> name<span class="token punctuation">:</span> <span class="token string">"intent_entity_featurizer_regex"</span><span class="token operator">-</span> name<span class="token punctuation">:</span> <span class="token string">"intent_featurizer_mitie"</span>  <span class="token comment" spellcheck="true">// 特征提取</span><span class="token operator">-</span> name<span class="token punctuation">:</span> <span class="token string">"intent_classifier_sklearn"</span> <span class="token comment" spellcheck="true">// sklearn 的意图分类模型</span></code></pre><h3 id="第二步：准备工作：训练MITIE模型文件"><a href="#第二步：准备工作：训练MITIE模型文件" class="headerlink" title="第二步：准备工作：训练MITIE模型文件"></a>第二步：准备工作：训练MITIE模型文件</h3><p>rasa NLU的实体识别和意图识别的任务，需要一个训练好的MITIE的模型。这个MITIE模型是非监督训练得到的，类似于word2vec中的word embedding。</p><p>要训练这个MITIE模型，我们需要一个规模比较大的中文语料。最好的方法是用对应自己需求的语料，比如做金融的chatbot就多去爬取些财经新闻，做医疗的chatbot就多获取些医疗相关文章。</p><p>使用<a href="https://github.com/crownpku/awesome-chinese-nlp">awesome-chinese-nlp</a>中列出的中文wikipedia dump和百度百科语料为例。其中关于wikipedia dump的处理可以参考<a href="http://blog.csdn.net/qq_32166627/article/details/68942216">这篇帖子</a>。</p><p>仅仅获取语料还不够，因为MITIE模型训练的输入是以词为单位的。所以要先进行分词，我们使用结巴分词。</p><p>安装结巴分词：</p><pre class=" language-shell"><code class="language-shell">$ pip install jieba</code></pre><p>将一个语料文件分词，以空格为分隔符：</p><pre class=" language-shell"><code class="language-shell">$ python -m jieba -d " " ./test > ./test_cut</code></pre><h4 id="MITIE模型训练"><a href="#MITIE模型训练" class="headerlink" title="MITIE模型训练"></a>MITIE模型训练</h4><p>我们把所有分好词的语料文件放在同一个文件路径下。接下来我们要训练MITIE模型。</p><p>首先将MITIE clone下来：</p><pre><code>$ git clone https://github.com/mit-nlp/MITIE.git</code></pre><p>我们要使用的只是MITIE其中wordrep这一个工具。我们先build它。</p><pre><code>$ cd MITIE/tools/wordrep$ mkdir build$ cd build$ cmake ..$ cmake --build . --config Release</code></pre><p>然后训练模型，得到total_word_feature_extractor.dat。注意这一步训练会耗费几十GB的内存，大概需要两到三天的时间。。。</p><pre><code>$ ./wordrep -e /path/to/your/folder_of_cutted_text_files</code></pre><p>中文wikipedia和百度百科语料生成了一个total_word_feature_extractor_chi.dat，分享如下。</p><pre><code>链接：https://pan.baidu.com/s/1kNENvlHLYWZIddmtWJ7Pdg 密码：p4vx</code></pre><p>对于一些比较general的NLU使用场景，这个应该是够用了。</p><h3 id="第三步：rasa-nlu-语料"><a href="#第三步：rasa-nlu-语料" class="headerlink" title="第三步：rasa_nlu 语料"></a>第三步：rasa_nlu 语料</h3><p>得到MITIE词向量模型之后，就可以使用标注好语料训练Rasa NLU模型。<br> Rasa提供了<strong>数据标注平台</strong>: <a href="https://links.jianshu.com/go?to=https://rasahq.github.io/rasa-nlu-trainer/">rasa-nlu-trainer</a></p><p>那标注好的数据是什么样的呢？</p><p>标注好的语料存储在<code>json</code>文件中，具体格式如下所示，包含<code>text</code>， <code>intent</code>，<code>entities</code>，实体中<code>start</code>和<code>end</code>是实体对应在<code>text</code>中的起止<code>index</code>。</p><p>以<code>data/examples/rasa/demo-rasa_zh.json</code>为例：</p><pre class=" language-json"><code class="language-json">&amp;#<span class="token number">123</span><span class="token punctuation">;</span>  <span class="token property">"rasa_nlu_data"</span><span class="token operator">:</span> &amp;#<span class="token number">123</span><span class="token punctuation">;</span>    <span class="token property">"common_examples"</span><span class="token operator">:</span> <span class="token punctuation">[</span>      &amp;#<span class="token number">123</span><span class="token punctuation">;</span>        <span class="token property">"text"</span><span class="token operator">:</span> <span class="token string">"你好"</span><span class="token punctuation">,</span>        <span class="token property">"intent"</span><span class="token operator">:</span> <span class="token string">"greet"</span><span class="token punctuation">,</span>        <span class="token property">"entities"</span><span class="token operator">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>      &amp;#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">,</span>      &amp;#<span class="token number">123</span><span class="token punctuation">;</span>        <span class="token property">"text"</span><span class="token operator">:</span> <span class="token string">"我想找地方吃饭"</span><span class="token punctuation">,</span>        <span class="token property">"intent"</span><span class="token operator">:</span> <span class="token string">"restaurant_search"</span><span class="token punctuation">,</span>        <span class="token property">"entities"</span><span class="token operator">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>      &amp;#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">,</span>      &amp;#<span class="token number">123</span><span class="token punctuation">;</span>        <span class="token property">"text"</span><span class="token operator">:</span> <span class="token string">"我想吃火锅啊"</span><span class="token punctuation">,</span>        <span class="token property">"intent"</span><span class="token operator">:</span> <span class="token string">"restaurant_search"</span><span class="token punctuation">,</span>        <span class="token property">"entities"</span><span class="token operator">:</span> <span class="token punctuation">[</span>          &amp;#<span class="token number">123</span><span class="token punctuation">;</span>            <span class="token property">"start"</span><span class="token operator">:</span> <span class="token number">2</span><span class="token punctuation">,</span>            <span class="token property">"end"</span><span class="token operator">:</span> <span class="token number">5</span><span class="token punctuation">,</span>            <span class="token property">"value"</span><span class="token operator">:</span> <span class="token string">"火锅"</span><span class="token punctuation">,</span>            <span class="token property">"entity"</span><span class="token operator">:</span> <span class="token string">"food"</span>          &amp;#<span class="token number">125</span><span class="token punctuation">;</span>        <span class="token punctuation">]</span>      &amp;#<span class="token number">125</span><span class="token punctuation">;</span>    <span class="token punctuation">]</span>  &amp;#<span class="token number">125</span><span class="token punctuation">;</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span> </code></pre><h3 id="第四步：训练模型"><a href="#第四步：训练模型" class="headerlink" title="第四步：训练模型"></a>第四步：训练模型</h3><p>到目前，已经获取了训练所需的标注好的语料，以及词向量模型MITIE文件。接下来就可以训练Rasa_NLU模型了。</p><p>插一句  安装：</p><ul><li><p>源码安装</p><pre class=" language-py"><code class="language-py">$ git clone https://github.com/crownpku/Rasa_NLU_Chi.git // clone 源码$ cd Rasa_NLU_Chi$ python setup.py install  // 安装依赖</code></pre></li></ul><p><strong>模型训练命令</strong></p><pre class=" language-kotlin"><code class="language-kotlin">python <span class="token operator">-</span>m rasa_nlu<span class="token punctuation">.</span>train <span class="token operator">-</span>c sample_configs<span class="token operator">/</span>config_jieba_mitie_sklearn<span class="token punctuation">.</span>yml <span class="token operator">--</span><span class="token keyword">data</span> <span class="token keyword">data</span><span class="token operator">/</span>examples<span class="token operator">/</span>rasa<span class="token operator">/</span>demo<span class="token operator">-</span>rasa_zh<span class="token punctuation">.</span>json <span class="token operator">--</span>path models <span class="token operator">--</span>project nlu</code></pre><p><strong>所需参数：</strong></p><ul><li>训练配置文件：<code>-c</code></li><li>训练语料：<code>--data</code></li><li>模型保存路径：<code>--path</code></li><li>项目名称：<code>--project</code></li></ul><p>模型训练完成后，会在<code>--path</code>指定的路径下保存训练好的模型文件，如果训练时指定了模型名称（即–project），模型就会存储在<code>models/project_name/model_**</code>目录中，如<code>models/chat_nlu_test/model_20190821-160150</code><br> 结构如下：</p><p><img src="/2020/09/18/rasa-shi-yong-zhi-nan/1.webp"></p><h3 id="第五步：测试验证"><a href="#第五步：测试验证" class="headerlink" title="第五步：测试验证"></a>第五步：测试验证</h3><ul><li>启动服务</li></ul><pre class=" language-swift"><code class="language-swift">python <span class="token operator">-</span>m rasa_nlu<span class="token punctuation">.</span>server <span class="token operator">-</span>c sample_configs<span class="token operator">/</span>config_jieba_mitie_sklearn<span class="token punctuation">.</span>yml <span class="token operator">--</span>path models</code></pre><ul><li>测试服务(打开一个新的终端，使用curl命令获取结果)</li></ul><pre class=" language-rust"><code class="language-rust">curl <span class="token operator">-</span>XPOST localhost<span class="token punctuation">:</span><span class="token number">5000</span><span class="token operator">/</span>parse <span class="token operator">-</span>d <span class="token string">'&amp;#123;"q":"明天天气预报", "project":"nlu", "model":"model_20190821-160150"&amp;#125;'</span></code></pre><p>结果如下：</p><p><img src="/2020/09/18/rasa-shi-yong-zhi-nan/2.webp"></p><h2 id="Rasa-Core"><a href="#Rasa-Core" class="headerlink" title="Rasa Core"></a>Rasa Core</h2><p>Rasa Core是用于构建AI助手的对话引擎，是开源Rasa框架的一部分。</p><p><strong>Rasa Core消息处理流程</strong><br> 由前面描述的对话管理模块了解到，它应该是负责协调聊天机器人的各个模块，起到维护人机对话的结构和状态的作用。对话管理模块涉及到的关键技术包括对话行为识别、对话状态识别、对话策略学习以及行为预测、对话奖励等。下面是Rasa Core消息处理流程：</p><p><img src="/2020/09/18/rasa-shi-yong-zhi-nan/1.png"></p><ul><li>首先，将用户输入的Message传递到<code>Interpreter</code>(NLU模块)，该模块负责识别Message中的”意图(intent)“和提取所有”实体”(entity)数据；</li><li>其次，Rasa Core会将Interpreter提取到的意图和识别传给<code>Tracker</code>对象，该对象的主要作用是跟踪会话状态(conversation state)；</li><li>第三，利用<code>policy</code>记录<code>Tracker</code>对象的当前状态，并选择执行相应的<code>action</code>，其中，这个action是被记录在Track对象中的；</li><li>最后，将执行action返回的结果输出即完成一次人机交互。</li></ul><p>Rasa Core包含两个内容： <code>stories</code>和<code>domain</code>。</p><ul><li><code>domain.yml</code>：包括对话系统所适用的领域，包含意图集合，实体集合和相应集合</li><li><code>story.md</code>：训练数据集合，原始对话在domain中的映射。</li></ul><h3 id="1-Stories"><a href="#1-Stories" class="headerlink" title="1. Stories"></a>1. Stories</h3><p>stories可以理解为对话的场景流程，我们需要告诉机器我们的多轮场景是怎样的。Story样本数据就是Rasa Core对话系统要训练的样本，它描述了人机对话过程中可能出现的故事情节，通过对Stories样本和domain的训练得到人机对话系统所需的对话模型。</p><p>Stories存储在md文件中。story的符号说明如下：</p><table><thead><tr><th align="center">符号</th><th align="center">说明</th></tr></thead><tbody><tr><td align="center">##</td><td align="center">sotry标题</td></tr><tr><td align="center">*</td><td align="center">意图和填充的slot</td></tr><tr><td align="center">-</td><td align="center">动作</td></tr></tbody></table><p><strong>Story格式大致包含三个部分：</strong></p><ul><li><strong>1. 用户输入</strong>（<code>User Messages</code>）</li></ul><p> 使用<code>*</code>开头的语句表示用户的输入消息，我们无需使用包含某个具体内容的输入，而是使用NLU管道输出的intent和entities来表示可能的输入。需要注意的是，如果用户的输入可能包含entities，建议将其包括在内，将有助于policies预测下一步action。这部分大致包含三种形式，示例如下：</p><p>（1）<code>* greet</code> 表示用户输入没有entity情况；<br>（2）<code>* inform{"people": "six"}</code> 表示用户输入包含entity情况，响应这类intent为普通action；<br>（3）<code>* request_weather</code> 表示用户输入Message对应的intent为form action情况；</p><ul><li><strong>2. 动作</strong>（<code>Actions</code>）</li></ul><p> 使用<code>-</code>开头的语句表示要执行动作(<code>Action</code>)，可分为<code>utterance actions</code>和<code>custom actions</code>，其中，前者在domain.yaml中定义以<code>utter_</code>为前缀，比如名为greet的意图，它的回复应为<code>utter_greet</code>；后者为自定义动作，具体逻辑由我们自己实现，虽然在定义action名称的时候没有限制，但是还是建议以<code>action_</code>为前缀，比如名为inform的意图fetch_profile的意图，它的response可为<code>action_fetch_profile</code>。</p><ul><li><strong>3. 事件</strong>（<code>Events</code>）</li></ul><p> Events也是使用<code>-</code>开头，主要包含槽值设置(<code>SlotSet</code>)和激活/注销表单(<code>Form</code>)，它是是Story的一部分，并且必须显示的写出来。Slot Events和Form Events的作用如下：</p><p>（1）Slot Events</p><p> <code>Slot Events</code>的作用当我们在自定义Action中设置了某个槽值，那么我们就需要在Story中Action执行之后显著的将这个SlotSet事件标注出来，格式为<code>- slot{"slot_name": "value"}</code>。比如，我们在action_fetch_profile中设置了Slot名为account_type的值，代码如下：</p><pre class=" language-python"><code class="language-python"><span class="token keyword">from</span> rasa_sdk<span class="token punctuation">.</span>actions <span class="token keyword">import</span> Action<span class="token keyword">from</span> rasa_sdk<span class="token punctuation">.</span>events <span class="token keyword">import</span> SlotSet<span class="token keyword">import</span> requests<span class="token keyword">class</span> <span class="token class-name">FetchProfileAction</span><span class="token punctuation">(</span>Action<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">name</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token string">"fetch_profile"</span>    <span class="token keyword">def</span> <span class="token function">run</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dispatcher<span class="token punctuation">,</span> tracker<span class="token punctuation">,</span> domain<span class="token punctuation">)</span><span class="token punctuation">:</span>        url <span class="token operator">=</span> <span class="token string">"http://myprofileurl.com"</span>        data <span class="token operator">=</span> requests<span class="token punctuation">.</span>get<span class="token punctuation">(</span>url<span class="token punctuation">)</span><span class="token punctuation">.</span>json        <span class="token keyword">return</span> <span class="token punctuation">[</span>SlotSet<span class="token punctuation">(</span><span class="token string">"account_type"</span><span class="token punctuation">,</span> data<span class="token punctuation">[</span><span class="token string">"account_type"</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">]</span></code></pre><p> 那么，就需要在Story中执行<code>action_fetch_profile</code>之后，添加<code>- slot{"account_type" : "premium"}</code>。虽然，这么做看起来有点多余，但是Rasa规定这么做必须的，目的是提高训练时准确度。</p><pre class=" language-markdown"><code class="language-markdown"><span class="token title important"><span class="token punctuation">##</span> fetch_profile</span><span class="token list punctuation">*</span> fetch_profile   <span class="token list punctuation">-</span> action<span class="token italic"><span class="token punctuation">_</span>fetch<span class="token punctuation">_</span></span>profile   <span class="token list punctuation">-</span> slot<span class="token entity" title="&#123;">&amp;#123;</span>"account_type" : "premium"<span class="token entity" title="&#125;">&amp;#125;</span>   <span class="token list punctuation">-</span> utter<span class="token italic"><span class="token punctuation">_</span>welcome<span class="token punctuation">_</span></span>premium</code></pre><p> 当然，如果您的自定义Action中将槽值重置为None，则对应的事件为<code>-slot{"slot_name": null}</code>。</p><p>（2）Form Events</p><p> 在Story中主要存在三种形式的表单事件(Form Events)，它们可表述为：</p><ul><li><code>Form Action</code>事件</li></ul><p> Form Action即表单动作事件，是自定义Action的一种，用于一个表单操作。示例如下：</p><pre class=" language-markdown"><code class="language-markdown"><span class="token list punctuation">-</span> restaurant_form</code></pre><ul><li><code>Form activation</code>事件</li></ul><p> form activation即激活表单事件，当form action事件执行后，会立马执行该事件。示例如下：</p><pre class=" language-markdown"><code class="language-markdown"><span class="token list punctuation">-</span> form<span class="token entity" title="&#123;">&amp;#123;</span>"name": "restaurant_form"<span class="token entity" title="&#125;">&amp;#125;</span></code></pre><ul><li><code>Form deactivation</code>事件</li></ul><p> form deactivation即注销表单事件，作用与form activation相反。示例如下：</p><pre class=" language-markdown"><code class="language-markdown"><span class="token list punctuation">-</span> form<span class="token entity" title="&#123;">&amp;#123;</span>"name": null<span class="token entity" title="&#125;">&amp;#125;</span></code></pre><p> 总之，我们在构建Story时，可以说是多种多样的，因为设计的故事情节是多种多样的，这就意味着上述三种内容的组合也是非常灵活的。另外，在设计Story时Rasa还提供了<a href="https://rasa.com/docs/rasa/core/stories/#id9">Checkpoints </a>和<a href="https://rasa.com/docs/rasa/core/stories/#id9">OR statements</a>两种功能，来提升构建Story的灵活度，但是需要注意的是，东西虽好，但是不要太贪了，过多的使用不仅增加了复杂度，同时也会拖慢训练的速度。其中，<a href="https://rasa.com/docs/rasa/core/stories/#id10">Checkpoints</a>用于模块化和简化训练数据，示例如下：</p><pre class=" language-markdown"><code class="language-markdown"><span class="token title important"><span class="token punctuation">##</span> first story</span><span class="token list punctuation">*</span> greet   <span class="token list punctuation">-</span> action<span class="token italic"><span class="token punctuation">_</span>ask<span class="token punctuation">_</span></span>user_question<span class="token blockquote punctuation">></span> check<span class="token italic"><span class="token punctuation">_</span>asked<span class="token punctuation">_</span></span>question<span class="token title important"><span class="token punctuation">##</span> user affirms question</span><span class="token blockquote punctuation">></span> check<span class="token italic"><span class="token punctuation">_</span>asked<span class="token punctuation">_</span></span>question<span class="token list punctuation">*</span> affirm  <span class="token list punctuation">-</span> action<span class="token italic"><span class="token punctuation">_</span>handle<span class="token punctuation">_</span></span>affirmation<span class="token blockquote punctuation">></span> check<span class="token italic"><span class="token punctuation">_</span>handled<span class="token punctuation">_</span></span>affirmation<span class="token title important"><span class="token punctuation">##</span> user denies question</span><span class="token blockquote punctuation">></span> check<span class="token italic"><span class="token punctuation">_</span>asked<span class="token punctuation">_</span></span>question<span class="token list punctuation">*</span> deny  <span class="token list punctuation">-</span> action<span class="token italic"><span class="token punctuation">_</span>handle<span class="token punctuation">_</span></span>denial<span class="token blockquote punctuation">></span> check<span class="token italic"><span class="token punctuation">_</span>handled<span class="token punctuation">_</span></span>denial<span class="token title important"><span class="token punctuation">##</span> user leaves</span><span class="token blockquote punctuation">></span> check<span class="token italic"><span class="token punctuation">_</span>handled<span class="token punctuation">_</span></span>denial<span class="token blockquote punctuation">></span> check<span class="token italic"><span class="token punctuation">_</span>handled<span class="token punctuation">_</span></span>affirmation<span class="token list punctuation">*</span> goodbye  <span class="token list punctuation">-</span> utter_goodbye</code></pre><p> 在上面的例子中，可以使用<code>&gt; check_asked_question</code>表示first story，这样在其他story中，如果有相同的first story部分，可以直接用<code>&gt; check_asked_question</code>代替。而<a href="https://rasa.com/docs/rasa/core/stories/#id11">OR Statements</a>主要用于实现某一个action可同时响应多个意图的情况，比如下面的例子：</p><pre class=" language-markdown"><code class="language-markdown"><span class="token title important"><span class="token punctuation">##</span> story</span><span class="token list punctuation">*</span> affirm OR thankyou  <span class="token list punctuation">-</span> action<span class="token italic"><span class="token punctuation">_</span>handle<span class="token punctuation">_</span></span>affirmation</code></pre><p>示例：</p><pre class=" language-yaml"><code class="language-yaml"><span class="token comment" spellcheck="true">## simple_story_with_multiple_turns</span>* affirm OR thank_you    <span class="token punctuation">-</span> utter_default* goodbye    <span class="token punctuation">-</span> utter_goodbye<span class="token punctuation">></span> check_goodbye<span class="token comment" spellcheck="true">## story_04649138</span>* greet <span class="token punctuation">-</span> utter_ask_howcanhelp* inform&amp;<span class="token comment" spellcheck="true">#123;"location": "london", "people": "two", "price": "moderate"&amp;#125;</span> <span class="token punctuation">-</span> utter_on_it <span class="token punctuation">-</span> utter_ask_cuisine* inform&amp;<span class="token comment" spellcheck="true">#123;"cuisine": "spanish"&amp;#125;</span> <span class="token punctuation">-</span> utter_ask_moreupdates* inform&amp;<span class="token comment" spellcheck="true">#123;"cuisine": "british"&amp;#125;</span> <span class="token punctuation">-</span> utter_ask_moreupdates* deny <span class="token punctuation">-</span> utter_ack_dosearch <span class="token punctuation">-</span> action_search_restaurants <span class="token punctuation">-</span> action_suggest* affirm <span class="token punctuation">-</span> utter_ack_makereservation* thankyou <span class="token punctuation">-</span> utter_goodbye</code></pre><p>如上所示，几个需要注意的点：</p><ul><li>**<code>&gt; check_\*</code>**： 用于模块化和简化训练数据，即story复用。</li><li><strong><code>OR</code></strong> ：用于处理同一个story中可能出现2个以上意图，这有利于简化story，但是相应的训练时间等于训练了两个以上的故事，不建议密集使用。</li></ul><h4 id="可视化stories"><a href="#可视化stories" class="headerlink" title="可视化stories"></a>可视化stories</h4><p>Rasa Core中提供了<code>rasa_core.visualize</code>模块可视化故事，有利于掌握设计故事流程。</p><p><strong>命令：</strong></p><pre class=" language-kotlin"><code class="language-kotlin">python <span class="token operator">-</span>m rasa_core<span class="token punctuation">.</span>visualize <span class="token operator">-</span>d domain<span class="token punctuation">.</span>yml <span class="token operator">-</span>s <span class="token keyword">data</span><span class="token operator">/</span>sotries<span class="token punctuation">.</span>md <span class="token operator">-</span>o graph<span class="token punctuation">.</span>html <span class="token operator">-</span>c config<span class="token punctuation">.</span>yml</code></pre><p>参数：</p><ul><li><code>-m</code>：指定运行模块</li><li><code>-d</code>：指定domain.yml文件路径</li><li><code>-s</code>：指定story路径</li><li><code>-o</code>：指定输出文件名</li><li><code>-c</code>：指定Policy配置文件。</li></ul><p>最终，在项目根目录下得到一个<code>graph.html</code>，可用浏览器打开。</p><p>具体实现，可参考源代码<code>rasa/core/visualize.py</code>。</p><p>最终，在项目根目录下得到一个<code>graph.html</code>，可用浏览器打开。</p><p>具体实现，可参考源代码<code>rasa/core/visualize.py</code>。</p><pre class=" language-python"><code class="language-python"><span class="token keyword">from</span> rasa_core<span class="token punctuation">.</span>agent <span class="token keyword">import</span> Agent<span class="token keyword">from</span> rasa_core<span class="token punctuation">.</span>policies<span class="token punctuation">.</span>keras_policy <span class="token keyword">import</span> KerasPolicy<span class="token keyword">from</span> rasa_core<span class="token punctuation">.</span>policies<span class="token punctuation">.</span>memoization <span class="token keyword">import</span> MemoizationPolicy<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>    agent <span class="token operator">=</span> Agent<span class="token punctuation">(</span><span class="token string">"domain.yml"</span><span class="token punctuation">,</span> policies<span class="token operator">=</span><span class="token punctuation">[</span>MemoizationPolicy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> KerasPolicy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    agent<span class="token punctuation">.</span>visualize<span class="token punctuation">(</span><span class="token string">"data/stories.md"</span><span class="token punctuation">,</span> output_file<span class="token operator">=</span><span class="token string">"graph.html"</span><span class="token punctuation">,</span> max_history<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span></code></pre><p><img src="/2020/09/18/rasa-shi-yong-zhi-nan/2.png"></p><h3 id="2-Domain"><a href="#2-Domain" class="headerlink" title="2. Domain"></a>2. Domain</h3><p>domain.yml定义了对话机器人应知道的所有信息，相当于大脑框架，指定了意图<code>intents</code>， 实体<code>entities</code>， 插槽<code>slots</code>以及动作<code>actions</code>。<br> 其种，<code>intents</code>和<code>entities</code>与Rasa NLU模型训练样本中标记的一致。<code>slot</code>与标记的<code>entities</code>一致，<code>actions</code>为对话机器人对应用户的请求作出的动作。<br> 此外，domain.yml中的<code>templates</code>部分针对<code>utter_</code>类型<code>action</code>定义了模板消息，便于对话机器人对相关动作自动回复。</p><table><thead><tr><th align="center">项</th><th align="center">说明</th></tr></thead><tbody><tr><td align="center"><code>intents</code></td><td align="center">意图</td></tr><tr><td align="center"><code>entities</code></td><td align="center">实体信息</td></tr><tr><td align="center"><code>slots</code></td><td align="center">词槽，对话中想要跟踪的信息</td></tr><tr><td align="center"><code>actions</code></td><td align="center">机器人作出的动作</td></tr><tr><td align="center"><code>templates</code></td><td align="center">回复的模板语句</td></tr></tbody></table><h4 id="意图intents"><a href="#意图intents" class="headerlink" title="意图intents"></a>意图intents</h4><p>使用<code>-</code>符号表示每个意图</p><pre class=" language-undefined"><code class="language-undefined">intents:  - greet  - goodbye  - search_weather</code></pre><h4 id="实体entities"><a href="#实体entities" class="headerlink" title="实体entities"></a>实体entities</h4><p>实体，即样本中标出的所有entity</p><pre class=" language-undefined"><code class="language-undefined">entities:  - city</code></pre><h4 id="槽slot"><a href="#槽slot" class="headerlink" title="槽slot"></a>槽slot</h4><p>插槽是机器人的记忆。它们充当键值存储器，其可用于存储用户提供的信息（例如，他们的家乡）以及关于外部世界收集的信息（例如，数据库查询的结果）。<br> 以天气查询为例，对话机器人必须知道地点和日期才可以查询，因此在domain.yml中需要在slots部分定义两个插槽，即<code>city</code>和<code>datatime</code>，而<code>matches</code>则用来存储最后查询的结果。示例如下：</p><pre class=" language-bash"><code class="language-bash">slots:  city:    type: text    initial_value:<span class="token string">"北京"</span>  datatime:    type:text    initial_value:<span class="token string">"明天"</span>  matches:    type:unfeaturized    initial_value:<span class="token string">"none"</span></code></pre><p><strong>Slot Types类型：</strong></p><ul><li><p><code>Text Slot</code> ：文本</p></li><li><p><code>Boolean Slot</code>：布尔值</p></li><li><p>Categorical Slot：接受枚举所列的值</p><pre class=" language-bash"><code class="language-bash">slots:   risk_level:      type: categorical      values:      - low      - medium      - high</code></pre></li><li><p>Float Slot：浮点型</p><pre class=" language-bash"><code class="language-bash">slots:   temperature:      type: float      min_value: -100.0      max_value:  100.0</code></pre><pre><code>Defaults: max_value=1.0, min_value=0.0</code></pre><p>设置max_value和min_value后，大于max_value和小于min_value的值被设为max_value和min_value。</p></li><li><p><code>List Slot</code>：列表型数据，且长度不影响对话</p></li><li><p><code>Unfeaturized Slot</code>：存储不影响会话流程的数据。</p></li></ul><p>如果值本身很重要，请使用<code>categorical</code>或<code>bool</code>槽。还有<code>float</code>和<code>list slots</code>。如果您只想存储一些数据，但不希望它影响会话流，请使用<code>unfeaturized</code>的插槽。<code>type</code>表示slot存储的数据类型，<code>initial_value</code>为slot初始值，该值可有可无(无意义)。</p><pre class=" language-bash"><code class="language-bash">slots:  name:    type: text    initial_value: <span class="token string">"human"</span>  matches:    type:unfeaturized</code></pre><h4 id="actions"><a href="#actions" class="headerlink" title="actions"></a>actions</h4><p>当Rasa NLU识别到用户输入信息的意图后，Rasa Core对话管理模块会对其作出回应，回应的操作就是action。<br> Rasa Core支持三种action：</p><ul><li><p>default actions</p><p>：默认的一组动作，无需定义，可以直接使用</p><ul><li><strong>action_listen</strong>：监听action</li><li><strong>action_restart</strong>：重置状态</li><li><strong>action_default_fallback</strong>：当Rasa Core得到的置信度低于设置的阈值时，默认执行该动作。</li></ul></li><li><p>utter actions</p><p>：以utter_为开头，只发送一条信息给用户作为反馈的动作。</p><p>定义很简单，只需在domain.yml文件中的<code>actions:</code>字段定义以<code>utter_</code>为开头即可。具体的回复内容将被定义在<code>templates</code>部分。如果没有<code>utter_</code>这个前缀，那么<code>action</code>就会被识别为<code>custom actions</code>。</p><pre class=" language-undefined"><code class="language-undefined">actions:  - utter_greet  - utter_cheer_up</code></pre></li><li><p>custom actions</p><p>：自定义动作，允许开发者执行任何操作并反馈给用户，是action多轮的关键点。需要在domain.yml</p><p>文件中的<code>actions</code>部分先定义，然后在指定的<code>webserver</code>中实现它。其中，webserver的<code>url</code></p><p>地址在<code>endpoint.yml</code>文件中指定。</p><p>官方提供了一个小的python sdk来方便用户编写自定义的action，首先需要安装一下对应的<code>rasa_core_sdk</code>后续再讲。。</p><pre class=" language-bash"><code class="language-bash">actions:  - action_search_weather</code></pre></li></ul><p><strong>templates</strong><br>此次定义了<code>utter actions</code>具体的回复内容，且每个<code>utter actions</code>下可以<strong>定义多条</strong>回复信息。当用户发起一个意图，比如”你好！”，就触发<code>utter_greet</code>操作，Rasa Core会从该action的模板中自动选择其中的一条信息作为结果反馈给用户。</p><pre class=" language-bash"><code class="language-bash">templates:  utter_greet:    - text: <span class="token string">"您好！请问我可以帮到您吗？"</span>    - text: <span class="token string">"您好！请说出您要查询的具体业务，比如跟我说'查询身份证号码'"</span>    - text: <span class="token string">"您好！"</span></code></pre><p><code>utter_default</code>是Rasa Core默认的<code>action_default_fallback</code>，当Rasa NLU识别该意图时，它的置信度低于设定的阈值时，就会默认执行utter_default中的模板。</p><p>除了回复简单的Text Message，Rasa Core还支持在Text Message后添加<strong>按钮和图片</strong>，以及<strong>访问插槽中的值</strong>（如果该插槽的值有被填充的话，否则返回None）。举个栗子</p><pre class=" language-bash"><code class="language-bash">  utter_introduce_self:    - text: <span class="token string">"您好！我是您的AI机器人呀~"</span>      image: <span class="token string">"https://i.imgur.com/sayhello.jpg"</span>  utter_introduce_selfcando:    - text: <span class="token string">"我能帮你查询天气信息"</span>      buttons:        - title: <span class="token string">"好的"</span>          payload: <span class="token string">"ok"</span>            - title: <span class="token string">"不了"</span>          payload: <span class="token string">"no"</span>  utter_ask_city:    - text: <span class="token string">"请问您要查询&amp;#123; datetime &amp;#125;哪里的天气？"</span>    utter_ask_datetime:    - text: <span class="token string">"请问您要查询&amp;#123; city &amp;#125;哪天的天气"</span></code></pre><p><strong>一个示例：</strong></p><pre class=" language-bash"><code class="language-bash">intents:  - greet  - goodbye  - affirm  - deny  - search_weatherslots:    city:        type: text    matches:        type: unfeaturizedentities:    - cityactions:    - utter_greet    - utter_cheer_up    - utter_did_that_help    - utter_happy    - utter_goodbyetemplates:  utter_greet:  - text: <span class="token string">"Hey! How are you?"</span>  utter_cheer_up:  - text: <span class="token string">"Here is something to cheer you up:"</span>    image: <span class="token string">"https://i.imgur.com/nGF1K8f.jpg"</span>  utter_did_that_help:  - text: <span class="token string">"Did that help you?"</span>  utter_happy:  - text: <span class="token string">"Great carry on!"</span>  utter_goodbye:  - text: <span class="token string">"Bye"</span>  utter_default:    - text: <span class="token string">"小x还在学习中，请换种说法吧~"</span>    - text: <span class="token string">"小x正在学习中，等我升级了您再试试吧~"</span>    - text: <span class="token string">"对不起，主人，您要查询的功能小x还没学会呢~"</span></code></pre><h3 id="3-训练对话模型"><a href="#3-训练对话模型" class="headerlink" title="3. 训练对话模型"></a>3. 训练对话模型</h3><p>准备好domain.yml和sotries.md数据之后，就可以进行模型的训练了。<br>模型的输入数据是历史对话记录，lable是下一个决策action。模型本质上是num_actions个类别的多分类。</p><p><strong>训练命令如下：</strong></p><pre class=" language-python"><code class="language-python">python <span class="token operator">-</span>m rasa_core<span class="token punctuation">.</span>train <span class="token operator">-</span>d domain<span class="token punctuation">.</span>yml <span class="token operator">-</span>s stories<span class="token punctuation">.</span>md <span class="token operator">-</span>o models<span class="token operator">/</span>chat1</code></pre><p><strong>参数解释：</strong></p><p><strong>参数解释：</strong></p><ul><li>**<code>-d或--domain</code>**：指domain.yml文件的路径</li><li>**<code>-s或--stories</code>**：指定stories.md文件路径。可以将故事请假保存在一个md文件中，也可以分类保存在多个md文件中（存放到一个目录下）</li><li>**<code>-o或--out</code>**：指对话模型的输出路径，保存训练好的模型文件</li><li>**<code>-c或--c</code>**：指定Policy规范文件</li></ul><p><strong>训练所需数据示例：</strong></p><pre class=" language-yaml"><code class="language-yaml"><span class="token key atrule">intent</span><span class="token punctuation">:</span>  <span class="token punctuation">-</span> greet  <span class="token punctuation">-</span> goodbye  <span class="token punctuation">-</span> search_weather<span class="token key atrule">entities</span><span class="token punctuation">:</span>  <span class="token punctuation">-</span> city<span class="token key atrule">actions</span><span class="token punctuation">:</span>  <span class="token punctuation">-</span> utter_greet  <span class="token punctuation">-</span> utter_goodbye  <span class="token punctuation">-</span> utter_ask_city<span class="token key atrule">templates</span><span class="token punctuation">:</span>  <span class="token key atrule">utter_greet</span><span class="token punctuation">:</span>  <span class="token punctuation">-</span> <span class="token key atrule">text</span><span class="token punctuation">:</span> <span class="token string">"你好啊"</span>  <span class="token punctuation">-</span> <span class="token key atrule">text</span><span class="token punctuation">:</span> <span class="token string">"又见面了"</span>  <span class="token key atrule">utter_goodbye</span><span class="token punctuation">:</span>  <span class="token punctuation">-</span> <span class="token key atrule">text</span><span class="token punctuation">:</span> <span class="token string">"再见"</span>  <span class="token punctuation">-</span> <span class="token key atrule">text</span><span class="token punctuation">:</span> <span class="token string">"下次再见啊"</span>  <span class="token key atrule">utter_ask_city</span><span class="token punctuation">:</span>  <span class="token punctuation">-</span> <span class="token key atrule">text</span><span class="token punctuation">:</span> <span class="token string">"请问您要查询哪里的天气？"</span></code></pre><p><strong>stories.md文件：</strong></p><pre class=" language-yaml"><code class="language-yaml"><span class="token comment" spellcheck="true">## search weather</span>* greet  <span class="token punctuation">-</span> utter_greet* search_weather&amp;<span class="token comment" spellcheck="true">#123; "datatime" : "明天"&amp;#125;</span>  <span class="token punctuation">-</span> utter_ask_city* goodbye  <span class="token punctuation">-</span> utter_goodbye</code></pre><p><strong>训练网络结构：</strong></p><p><img src="/2020/09/18/rasa-shi-yong-zhi-nan/3.png"></p><p><img src="/2020/09/18/rasa-shi-yong-zhi-nan/4.png"></p><h4 id="测试对话模型"><a href="#测试对话模型" class="headerlink" title="测试对话模型"></a>测试对话模型</h4><p>经过训练，我们已经得到了对话模型，那现在我们就来测试一下。</p><p>测试命令如下：</p><pre class=" language-py"><code class="language-py">python -m rasa_core.run -d models/chat1</code></pre><p>参数<code>-d</code>：指定模型路径</p><p>注：此时测试rasa_core对话模型，并没有加入core_nlu模型，因此还无法进行意图识别，只能够根据已知的意图（输入意图）返回特定的答案。因此，我们测试的时候，需要手动输入在domain.yml中定义好的意图，输入意图以<code>/</code>符号开头。<br>如输入greet意图：<code>/greet</code></p><p><img src="/2020/09/18/rasa-shi-yong-zhi-nan/5.png"></p><h2 id="测试聊天机器人"><a href="#测试聊天机器人" class="headerlink" title="测试聊天机器人"></a>测试聊天机器人</h2><p>通过前面的步骤，已经训练了Rasa_nlu的意图识别模型和Rasa_Core的对话模型。接下来就进行两者的整体的测试。</p><ul><li>输入：待测试文本</li><li>输出：机器人回应</li><li>中间过程：包含意图识别、实体识别和会话流程</li></ul><p><strong>测试命令</strong><br><code>python -m rasa_core.run -d models/chat1 -u models/nlu/model_20190820-105546</code></p><p><strong>参数解释;</strong></p><ul><li><code>-d</code>：modeldir 指定对话模型路径（即Rasa_core训练的模型路径）</li><li><code>-u</code>：Rasa NLU训练的模型路径</li><li><code>--port</code>：指定Rasa Core Web应用运行的端口号</li><li><code>--credentials</code>：指定通道（input channels）属性</li><li><code>--endpoints</code>：用于指定Rasa Core连接其他web server的url地址，比如nlu web</li><li><code>-o</code>：指定log日志文件输出路径</li><li><code>--debug</code>：打印调试信息。在显示的信息中，我们可以看到输入message后Rasa NLU模型是否识别出意图、实体及其置信度信息。槽位的填充以及policy预测的下一个action信息。</li></ul><p><img src="/2020/09/18/rasa-shi-yong-zhi-nan/6.png"></p><p>加入<code>--debug</code>参数后打印出的调试信息如下：</p><p><img src="/2020/09/18/rasa-shi-yong-zhi-nan/7.png"></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.jianshu.com/p/5d9aa2a444a3">rasa对话系统踩坑记</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> nlp </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 聊天系统 </tag>
            
            <tag> rasa </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>sklearn机器学习原理汇总</title>
      <link href="/2020/09/17/ji-qi-xue-xi-yuan-li-hui-zong/"/>
      <url>/2020/09/17/ji-qi-xue-xi-yuan-li-hui-zong/</url>
      
        <content type="html"><![CDATA[<h1 id="sklearn机器学习汇总"><a href="#sklearn机器学习汇总" class="headerlink" title="sklearn机器学习汇总"></a>sklearn机器学习汇总</h1><h2 id="sklearn-建模流程"><a href="#sklearn-建模流程" class="headerlink" title="sklearn 建模流程"></a>sklearn 建模流程</h2><p>数据集处理</p><ul><li><strong>数据集加载</strong> X=[],标签Y=[] , 比例X:Y=1:1</li><li><strong>数据集分类</strong> <code>train_test_split</code> -&gt;（X_train,X_test,y_train,y_test）</li></ul><p>特征提取</p><ul><li><strong>特征提取</strong>（直方图、轮廓描述子…特征数组）</li></ul><p>训练模型</p><ul><li><strong>特征分类</strong>（构造分类器（lr线性分类，svm支持向量机分类，beyes贝叶斯分类…））</li><li>用<strong>训练</strong>数据（X_train,y_train）拟合分类器<strong>模型</strong> 、<strong>保存模型</strong></li></ul><p>预测</p><ul><li>预测 <code>result = predit(X_test)</code></li><li>分析 <code>classification_report (result)</code></li></ul><h2 id="sklearn-API"><a href="#sklearn-API" class="headerlink" title="sklearn API"></a>sklearn API</h2><p>Key_Word</p><p>数据获取: sklearn, <strong>datasets</strong>, DataFrame, load_*</p><p>数据标准化**: preprocessing**, <strong>MinMaxScaler</strong>, scaler, <strong>fit</strong>, <strong>transform</strong>, data, target</p><p>划分测试集: model_selection, <strong>train_test_split</strong>, test_size</p><p>训练模型: fit ,predict, <strong>kernel=”linear”</strong>, <strong>probability=True</strong></p><p>模型评估: <strong>score,</strong> <strong>predict_proba</strong></p><p>使用metrics模块评估: classification_report</p><p>使用交叉验证方法评估: cross_val_score</p><p>模型的优化: <strong>GridSearchCV</strong>, C, kernel, gamma, param_grid, svc, cv</p><p>模型持久化: pikle, joblib, dump, load</p><h2 id="总体流程"><a href="#总体流程" class="headerlink" title="总体流程"></a>总体流程</h2><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>数据集处理中我们常用到：</p><ul><li>datasets ： 是sklearn库自带的数据集（iris鸢尾花数据等），前期学习非常友好</li><li>train_test_split 将数据分为测试集和训练集</li></ul><pre class=" language-python"><code class="language-python"><span class="token keyword">from</span> sklearn <span class="token keyword">import</span> datasets<span class="token comment" spellcheck="true">#引入数据集,sklearn包含众多数据集</span><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> train_test_split<span class="token comment" spellcheck="true">#将数据分为测试集和训练集</span>iris <span class="token operator">=</span> datasets<span class="token punctuation">.</span>load_iris<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#引入iris鸢尾花数据,iris数据包含4个特征变量</span>X <span class="token operator">=</span> iris<span class="token punctuation">.</span>data <span class="token comment" spellcheck="true"># 特征变量</span>y <span class="token operator">=</span> iris<span class="token punctuation">.</span>target <span class="token comment" spellcheck="true"># 目标值</span>X_train<span class="token punctuation">,</span>X_test<span class="token punctuation">,</span>y_train<span class="token punctuation">,</span>y_test <span class="token operator">=</span> train_test_split<span class="token punctuation">(</span>X<span class="token punctuation">,</span>y<span class="token punctuation">,</span>test_size<span class="token operator">=</span><span class="token number">0.3</span><span class="token punctuation">,</span> random_state<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#利用train_test_split进行将训练集和测试集进行分开，test_size占30%</span><span class="token keyword">print</span><span class="token punctuation">(</span>y_train<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#我们看到训练数据的特征值分为3类</span><span class="token number">12345678</span></code></pre><h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>Sklearn的模型导出本质上是利用Python的Pickle机制。对Python的函数进行序列化，也就是把训练好的Transformer函数序列化并存为文件。</p><p><strong>.pkl</strong></p><p>pkl文件是python里面保存文件的一种格式，如果直接打开会显示一堆序列化的东西。</p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> pickle<span class="token comment" spellcheck="true"># 重点是rb和r的区别，rb是打开2进制文件，文本文件用r</span>f <span class="token operator">=</span> open<span class="token punctuation">(</span><span class="token string">'test.pkl'</span><span class="token punctuation">,</span><span class="token string">'rb'</span><span class="token punctuation">)</span>data <span class="token operator">=</span> pickle<span class="token punctuation">.</span>load<span class="token punctuation">(</span>f<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>data<span class="token punctuation">)</span><span class="token number">123456</span></code></pre><blockquote><p>参考这篇文章：<a href="https://www.cnblogs.com/Allen-rg/p/9548539.html">sklearn 中模型保存的两种方法</a><br>既然joblib在使用上比较容易，读取速度也相对pickle快，那么我们后面就用joblib进行讨论。</p></blockquote><p><strong>.m</strong></p><p>保存 、读取、预测</p><pre class=" language-python"><code class="language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>externals <span class="token keyword">import</span> joblibjoblib<span class="token punctuation">.</span>dump<span class="token punctuation">(</span>lr<span class="token punctuation">,</span> <span class="token string">'lr.model'</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 保存   lr是一个LogisticRegression模型</span>lr <span class="token operator">=</span> joblib<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'lr.model'</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 加载</span>lr<span class="token punctuation">.</span>predit<span class="token punctuation">(</span>test_X<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 预测    此处test_X为特征集</span><span class="token number">1234567</span></code></pre><h4 id="一个完整的svm分类例子"><a href="#一个完整的svm分类例子" class="headerlink" title="一个完整的svm分类例子"></a>一个完整的svm分类例子</h4><ol><li>dataset：特征数组X，标签Y，比例X:Y=1:1</li><li>clf ： 构造分类器（lr线性分类，svm支持向量机分类，beyes贝叶斯分类…）</li><li>clf.fit()：用训练数据拟合分类器模型</li><li>joblib.dump：保存模型</li><li>joblib.load：加载模型</li><li>clf.predit()：预测</li></ol><pre class=" language-python"><code class="language-python">os<span class="token punctuation">.</span>chdir<span class="token punctuation">(</span><span class="token string">"workspace/model_save"</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 保存Model(注:model_save文件夹要预先建立，否则会报错)</span><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>externals <span class="token keyword">import</span> joblib<span class="token keyword">from</span> sklearn <span class="token keyword">import</span> svmX <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span>y <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span>clf <span class="token operator">=</span> svm<span class="token punctuation">.</span>SVC<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># clf是训练的分类器</span>clf<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">)</span>  clf<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>train_X<span class="token punctuation">,</span>train_y<span class="token punctuation">)</span>joblib<span class="token punctuation">.</span>dump<span class="token punctuation">(</span>clf<span class="token punctuation">,</span> <span class="token string">"train_model.m"</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 保存</span>clf <span class="token operator">=</span> joblib<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">"train_model.m"</span><span class="token punctuation">)</span>clf<span class="token punctuation">.</span>predit<span class="token punctuation">(</span>test_X<span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#此处test_X为特征集</span><span class="token number">123456789101112</span></code></pre><h3 id="封装"><a href="#封装" class="headerlink" title="封装"></a>封装</h3><pre class=" language-python"><code class="language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>externals <span class="token keyword">import</span> joblibClass <span class="token class-name">classificationTest</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true">#保存模型</span>    <span class="token keyword">def</span> <span class="token function">Save_Model</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> model<span class="token punctuation">,</span> filepath<span class="token punctuation">)</span><span class="token punctuation">:</span>        joblib<span class="token punctuation">.</span>dump<span class="token punctuation">(</span>model<span class="token punctuation">,</span> filename<span class="token operator">=</span>filepath<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">SVM_classifier</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x_train<span class="token punctuation">,</span>y_train<span class="token punctuation">)</span><span class="token punctuation">:</span>        clf <span class="token operator">=</span> svm<span class="token punctuation">.</span>SVC<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># clf是训练的分类器</span>        clf<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span>y_train<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>Save_Model<span class="token punctuation">(</span>clf<span class="token punctuation">,</span><span class="token string">"train_model.m"</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 保存模型</span>        <span class="token keyword">return</span> clf    <span class="token keyword">def</span> <span class="token function">Load_Model</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> filepath<span class="token punctuation">)</span><span class="token punctuation">:</span>        model <span class="token operator">=</span> joblib<span class="token punctuation">.</span>load<span class="token punctuation">(</span>filepath<span class="token punctuation">)</span>        <span class="token keyword">return</span> model    <span class="token keyword">def</span> <span class="token function">Predict_Model</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x_test<span class="token punctuation">,</span> filpath<span class="token punctuation">)</span><span class="token punctuation">:</span>        model <span class="token operator">=</span> self<span class="token punctuation">.</span>Load_Model<span class="token punctuation">(</span>filepath<span class="token punctuation">)</span>        result <span class="token operator">=</span> model<span class="token punctuation">.</span>predit<span class="token punctuation">(</span>x_test<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 预测</span>        <span class="token keyword">return</span> result<span class="token number">123456789101112131415161718192021</span></code></pre><h3 id="其他模型"><a href="#其他模型" class="headerlink" title="其他模型"></a>其他模型</h3><p>其他几种模型后缀的对比：</p><ul><li><p>.pb<br>MetaGraph的protocol buffer格式的文件，MetaGraph包括计算图，数据流，以及相关的变量和输入输出。</p><p>在tensorflow训练中直接保存为pb为格式，保存pb的好处就是使用场景是实现创建模型与使用模型的解耦，使得创建模型与使用模型的解耦，使得前向推导inference代码统一。另外的好处就是保存为pb的时候，模型的变量会变成固定的，导致模型的大小会大大减小。</p></li><li><p>.dat<br>.dat并不是一种标准文件。DATA的意思，即数据文件，这类文件并没有进行绝对化的定义。<br>在深度学习里指存放数据的文件。</p></li><li><p>.h5即（.hdf5）<br>HDF5是一种全新的分层数据格式产品，由数据格式规范和支持库实现组成。</p></li><li><p>.ckpt<br>这种模型文件是依赖 TensorFlow 的，只能在其框架下使用。</p><p>checkpoint文件：用于告知某些TF函数，这是最新的检查点文件。<br>.data文件：保存图中所有变量的值，没有结构。<br>.index文件：保存索引。<br>.meta文件：保存计算图的结构，但是不包含里面变量的值。</p></li></ul><h3 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h3><p>介绍一个sklearn中高效的分析函数：<code>classification_report ()</code></p><p>使用：</p><pre class=" language-python"><code class="language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> classification_reportclf <span class="token operator">=</span> BernoulNB<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>fit<span class="token punctuation">(</span>XX_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 伯努利贝叶斯分类器</span>os<span class="token punctuation">.</span>chdir<span class="token punctuation">(</span><span class="token string">"model/"</span><span class="token punctuation">)</span><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>externals <span class="token keyword">import</span> joblib<span class="token keyword">import</span> timeticks <span class="token operator">=</span> time<span class="token punctuation">.</span>strftime<span class="token punctuation">(</span><span class="token string">"%Y_%m_%d_%H_%M_%S"</span><span class="token punctuation">,</span> time<span class="token punctuation">.</span>localtime<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>joblib<span class="token punctuation">.</span>dump<span class="token punctuation">(</span>clf<span class="token punctuation">,</span> ticks<span class="token operator">+</span><span class="token string">"train_model.m"</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#joblib.dump(clf, ticks+"train_model.pkl")</span>clf <span class="token operator">=</span> joblib<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">"train_model.m"</span><span class="token punctuation">)</span>predictions_lables <span class="token operator">=</span> clf<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>XX_test<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 使用测试集预测结果</span><span class="token keyword">print</span><span class="token punctuation">(</span>u<span class="token string">"预测结果"</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>predictions_lables<span class="token punctuation">)</span><span class="token comment" spellcheck="true">## 生成文本行分类报告</span><span class="token keyword">print</span><span class="token punctuation">(</span>u<span class="token string">"算法报告"</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token punctuation">(</span>classification_report<span class="token punctuation">(</span>y_test<span class="token punctuation">,</span> predictions_labels<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#生成字典报告</span>report <span class="token operator">=</span> classifcation_report<span class="token punctuation">(</span>y_test<span class="token punctuation">,</span> predictions_labels<span class="token punctuation">,</span> output_dict<span class="token operator">=</span>true<span class="token punctuation">)</span></code></pre><h3 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h3><h2 id="sklearn中的常规模型"><a href="#sklearn中的常规模型" class="headerlink" title="sklearn中的常规模型"></a>sklearn中的常规模型</h2><h3 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h3><p>模型评估（待写）</p><a href="/2020/09/17/jue-ce-shu-yuan-li/" title="决策树原理">决策树原理</a><a href="/2020/09/18/sui-ji-sen-lin-yuan-li/" title="随机森林原理">随机森林原理</a><a href="/2020/09/19/ju-lei-suan-fa-yuan-li/" title="聚类算法原理">聚类算法原理</a><a href="/2020/09/20/svm-yuan-li/" title="SVM原理">SVM原理</a><h2 id="参考"><a href="#参考" class="headerlink" title="参考　"></a>参考　</h2><p><a href="https://www.cnblogs.com/draven123/p/11408086.html">https://www.cnblogs.com/draven123/p/11408086.html</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 总结 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>决策树原理</title>
      <link href="/2020/09/17/jue-ce-shu-yuan-li/"/>
      <url>/2020/09/17/jue-ce-shu-yuan-li/</url>
      
        <content type="html"><![CDATA[<h1 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h1><p> (Tree)是一种非参数的有监督学习方法,它能够从一系列有特征和标签的数据中总结出决策规则,并用树状图的结构来呈现这些规则,以解决分类和回归问题。</p><p>这些算法基本都执行”贪心策略“,即通过局部的最优来达到我们相信是最接近全局最优的结果。只考虑当前纯度差最大的情况作为分割点。</p><p>Hunt算法是许多决策树算法的基础,包括ID3、C4.5和CART等。</p><p><strong>决策树构建基本步骤</strong></p><p> 1） 开始，所有记录看作一个节点</p><p> 2）遍历每个变量的每一种分割方式，找到最好的分割点</p><p> 3）分割成两个节点N1和N2</p><ol start="4"><li>对N1和N2分别继续执行2-3步，直到每个节点足够“纯”为止</li></ol><p><img src="/2020/09/17/jue-ce-shu-yuan-li/1.png"></p><h2 id="模块sklearn-tree"><a href="#模块sklearn-tree" class="headerlink" title="模块sklearn.tree"></a>模块sklearn.tree</h2><p>sklearn中决策树的类都在”tree“这个模块之下。这个模块总共包含五个类:</p><table><thead><tr><th>tree.DecisionTreeClassifier</th><th>分类树</th></tr></thead><tbody><tr><td>tree.DecisionTreeRegressor</td><td>回归树</td></tr><tr><td>tree.export_graphviz</td><td>将生成的决策树导出为DOT格式,画图专用</td></tr><tr><td>tree.ExtraTreeClassifier</td><td>高随机版本的分类树</td></tr><tr><td>tree.ExtraTreeRegressor</td><td>高随机版本的回归树</td></tr></tbody></table><h2 id="DecisionTreeClassiﬁer"><a href="#DecisionTreeClassiﬁer" class="headerlink" title="DecisionTreeClassiﬁer"></a>DecisionTreeClassiﬁer</h2><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">sklearn</span><span class="token punctuation">.</span>tree<span class="token punctuation">.</span>DecisionTreeClassifier <span class="token punctuation">(</span>criterion<span class="token operator">=</span>’gini’<span class="token punctuation">,</span> splitter<span class="token operator">=</span>’best’<span class="token punctuation">,</span> max_depth<span class="token operator">=</span>None<span class="token punctuation">,</span>min_samples_split<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> min_samples_leaf<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> min_weight_fraction_leaf<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span> max_features<span class="token operator">=</span>None<span class="token punctuation">,</span>random_state<span class="token operator">=</span>None<span class="token punctuation">,</span> max_leaf_nodes<span class="token operator">=</span>None<span class="token punctuation">,</span> min_impurity_decrease<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span> min_impurity_split<span class="token operator">=</span>None<span class="token punctuation">,</span>class_weight<span class="token operator">=</span>None<span class="token punctuation">,</span> presort<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span></code></pre><h3 id="重要参数"><a href="#重要参数" class="headerlink" title="重要参数"></a>重要参数</h3><ul><li><strong>criterion</strong></li></ul><p>为了要将表格转化为一棵树，决策树需要找出最佳节点和最佳的分枝方法，对分类树来说，衡量这个“最佳”的指标</p><p>叫做“不纯度”。通常来说，不纯度越低，决策树对训练集的拟合越好。现在使用的决策树算法在分枝方法上的核心</p><p>大多是围绕在对某个不纯度相关指标的最优化上。</p><p>不纯度基于节点来计算，树中的每个节点都会有一个不纯度，并且子节点的不纯度一定是低于父节点的，也就是</p><p>说，在同一棵决策树上，叶子节点的不纯度一定是最低的。</p><p>Criterion这个参数正是用来决定不纯度的计算方法的。sklearn提供了两种选择：</p><p>1）输入”entropy“，使用<strong>信息熵</strong>（Entropy）</p><p>2）输入”gini“，使用<strong>基尼系数</strong>（Gini Impurity）</p><ul><li><strong>random_state &amp; splitter</strong></li></ul><p>random_state用来设置分枝中的随机模式的参数，默认None，在高维度时随机性会表现更明显，低维度的数据（比如鸢尾花数据集），随机性几乎不会显现。输入任意整数，会一直长出同一棵树，让模型稳定下来。</p><p>splitter也是用来控制决策树中的随机选项的，有两种输入值，输入”best”，决策树在分枝时虽然随机，但是还是会优先选择更重要的特征进行分枝（重要性可以通过属性feature_importances_查看），输入“random”，决策树在分枝时会更加随机，树会因为含有更多的不必要信息而更深更大，并因这些不必要信息而降低对训练集的拟合。这也是防止过拟合的一种方式。当你预测到你的模型会过拟合，用这两个参数来帮助你降低树建成之后过拟合的可能性。当然，树一旦建成，我们依然是使用剪枝参数来防止过拟合。</p><ul><li><strong>剪枝参数</strong></li></ul><p>在不加限制的情况下，一棵决策树会生长到衡量不纯度的指标最优，或者没有更多的特征可用为止。这样的决策树往往会过拟合，这就是说，<strong>它会在训练集上表现很好，在测试集上却表现糟糕。</strong>我们收集的样本数据不可能和整体的状况完全一致，因此当一棵决策树对训练数据有了过于优秀的解释性，它找出的规则必然包含了训练样本中的噪声，并使它对未知数据的拟合程度不足。</p><p>为了让决策树有更好的泛化性，我们要对决策树进行剪枝。<strong>剪枝策略对决策树的影响巨大，正确的剪枝策略是优化决策树算法的核心。</strong>sklearn为我们提供了不同的剪枝策略：</p><p>1)<strong>max_depth</strong>  : 限制树的最大深度，超过设定深度的树枝全部剪掉</p><p>2)<strong>min_samples_leaf &amp; min_samples_split</strong>:</p><p>min_samples_leaf限定，一个节点在分枝后的每个子节点都必须包含至少min_samples_leaf个训练样本，否则分枝就不会发生，或者，分枝会朝着满足每个子节点都包含min_samples_leaf个样本的方向去发生一般搭配。max_depth使用，在回归树中有神奇的效果，可以让模型变得更加平滑。这个参数的数量设置得太小会引起过拟合，设置得太大就会阻止模型学习数据。一般来说，建议从=5开始使用。如果叶节点中含有的样本量变化很大，建议输入浮点数作为样本量的百分比来使用。同时，这个参数可以保证每个叶子的最小尺寸，可以在回归问题中避免低方差，过拟合的叶子节点出现。对于类别不多的分类问题，=1通常就是最佳选择。min_samples_split限定，一个节点必须要包含至少min_samples_split个训练样本，这个节点才允许被分枝，否则分枝就不会发生。</p><ol start="3"><li><strong>max_features &amp; min_impurity_decrease</strong>:</li></ol><p>一般max_depth使用，用作树的”精修“</p><p>max_features限制分枝时考虑的特征个数，超过限制个数的特征都会被舍弃。和max_depth异曲同工，max_features是用来限制高维度数据的过拟合的剪枝参数，但其方法比较暴力，是直接限制可以使用的特征数量而强行使决策树停下的参数，在不知道决策树中的各个特征的重要性的情况下，强行设定这个参数可能会导致模型学习不足。如果希望通过降维的方式防止过拟合，建议使用PCA，ICA或者特征选择模块中的降维算法。</p><p>min_impurity_decrease限制信息增益的大小，信息增益小于设定数值的分枝不会发生。这是在0.19版本中更新的功能，在0.19版本之前时使用min_impurity_split。</p><p><strong>确认最优的剪枝参数</strong></p><p>那具体怎么来确定每个参数填写什么值呢？这时候，我们就要使用确定超参数的曲线来进行判断了，继续使用我们已经训练好的决策树模型clf。超参数的学习曲线，是一条以超参数的取值为横坐标，模型的度量指标为纵坐标的曲线，它是用来衡量不同超参数取值下模型的表现的线。在我们建好的决策树里，我们的模型度量指标就score。</p><pre class=" language-python"><code class="language-python">plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>range<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">11</span><span class="token punctuation">)</span><span class="token punctuation">,</span>test<span class="token punctuation">,</span>color<span class="token operator">=</span><span class="token string">"red"</span><span class="token punctuation">,</span>label<span class="token operator">=</span><span class="token string">"max_depth"</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><h3 id="目标权重参数"><a href="#目标权重参数" class="headerlink" title="目标权重参数"></a><strong>目标权重参数</strong></h3><ul><li><strong>class_weight &amp; min_weight_fraction_leaf</strong></li></ul><p>完成样本标签平衡的参数。样本不平衡是指在一组数据集中，标签的一类天生占有很大的比例。比如说，在银行要判断“一个办了信用卡的人是否会违约”，就是是vs否（1%：99%）的比例。这种分类状况下，即便模型什么也不做，全把结果预测成“否”，正确率也能有99%。因此我们要使用class_weight参数对样本标签进行一定的均衡，给少量的标签更多的权重，让模型更偏向少数类，向捕获少数类的方向建模。该参数默认None，此模式表示自动给与数据集中的所有标签相同的权重。</p><p> <strong>重要属性和接口</strong></p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true">#apply返回每个测试样本所在的叶子节点的索引</span>clf<span class="token punctuation">.</span>apply<span class="token punctuation">(</span>Xtest<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#predict返回每个测试样本的分类/回归结果</span>clf<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>Xtest<span class="token punctuation">)</span></code></pre><p><strong>总结</strong></p><p>八个参数：Criterion，两个随机性相关的参数（random_state，splitter），五个剪枝参数（max_depth,min_samples_split，min_samples_leaf，max_feature，min_impurity_decrease）</p><p>一个属性：feature_importances_</p><p>四个接口：ﬁt，score，apply，predict</p><h2 id="DecisionTreeRegressor"><a href="#DecisionTreeRegressor" class="headerlink" title="DecisionTreeRegressor"></a><strong>DecisionTreeRegressor</strong></h2><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">sklearn</span><span class="token punctuation">.</span>tree<span class="token punctuation">.</span>DecisionTreeRegressor  <span class="token punctuation">(</span>criterion<span class="token operator">=</span>’mse’<span class="token punctuation">,</span> splitter<span class="token operator">=</span>’best’<span class="token punctuation">,</span> max_depth<span class="token operator">=</span>None<span class="token punctuation">,</span>min_samples_split<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> min_samples_leaf<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> min_weight_fraction_leaf<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span> max_features<span class="token operator">=</span>None<span class="token punctuation">,</span>random_state<span class="token operator">=</span>None<span class="token punctuation">,</span> max_leaf_nodes<span class="token operator">=</span>None<span class="token punctuation">,</span> min_impurity_decrease<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span> min_impurity_split<span class="token operator">=</span>None<span class="token punctuation">,</span> presort<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span></code></pre><h3 id="重要参数，属性及接口"><a href="#重要参数，属性及接口" class="headerlink" title="重要参数，属性及接口"></a>重要参数，属性及接口</h3><p>回归树衡量分枝质量的指标，支持的标准有三种：</p><p>1）输入”mse”使用均方误差mean squared error(MSE)，父节点和叶子节点之间的均方误差的差额将被用来作为特征选择的标准，这种方法通过使用叶子节点的均值来最小化L2损失</p><p>2）输入“friedman_mse”使用费尔德曼均方误差，这种指标使用弗里德曼针对潜在分枝中的问题改进后的均方误差</p><p>3）输入”mae”使用绝对平均误差MAE（mean absolute error），这种指标使用叶节点的中值来最小化L1损失属性中最重要的依然是feature_importances_，接口依然是apply, ﬁt, predict, score最核心。</p><p>​       其中u是残差平方和（MSE * N），v是总平方和，N是样本数量，i是每一个数据样本，ﬁ是模型回归出的数值，yi是样本点i实际的数值标签。y帽是真实数值标签的平均数。R平方可以为正为负（如果模型的残差平方和远远大于模型的总平方和，模型非常糟糕，R平方就会为负），而均方误差永远为正。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 树模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transformers中使用TorchScript</title>
      <link href="/2020/09/17/transformers-zhong-shi-yong-torchscript/"/>
      <url>/2020/09/17/transformers-zhong-shi-yong-torchscript/</url>
      
        <content type="html"><![CDATA[<h1 id="Transformers中使用TorchScript"><a href="#Transformers中使用TorchScript" class="headerlink" title="Transformers中使用TorchScript"></a>Transformers中使用TorchScript</h1><p>转载：<a href="https://www.cnblogs.com/panchuangai/p/12567841.html">https://www.cnblogs.com/panchuangai/p/12567841.html</a></p><p>根据Pytorch的文档：“TorchScript是一种从PyTorch代码创建可序列化和可优化模型的方法”。Pytorch的两个模块<code>JIT和TRACE</code>允许开发人员导出他们的模型，这些模型可以在其他程序中重用，例如面向效率的C++程序。</p><p>我们提供了一个接口，该接口允许将<em>transformers</em>模型导出到TorchScript，以便他们可在与基于Pytorch的python程序不同的环境中重用。在这里，我们解释如何使用我们的模型，以便可以导出它们，以及将这些模型与TorchScript一起使用时要注意的事项。</p><p>导出模型需要两件事：</p><ul><li>虚拟化输入以执行模型正向传播。</li><li>需要使用<code>torchscript</code>标志实例化该模型。</li></ul><p>这些必要性意味着开发人员应注意几件事。这些在下面详细说明。</p><h3 id="含义"><a href="#含义" class="headerlink" title="含义"></a>含义</h3><h4 id="TorchScript标志和解绑权重"><a href="#TorchScript标志和解绑权重" class="headerlink" title="TorchScript标志和解绑权重"></a>TorchScript标志和解绑权重</h4><p>该标志是必需的，因为该存储库中的大多数语言模型都在它们的<code>Embedding</code>层及其<code>Decoding</code>层具有绑定权重关系。TorchScript不允许导出绑定权重的模型，因此，有必要事先解绑权重。</p><p>这意味着以<code>torchscript</code>标志实例化的模型使得<code>Embedding</code>层和<code>Decoding</code>层分开，这意味着不应该对他们进行同时训练，导致意外的结果。</p><p>对于没有语言模型头(Language Model head)的模型，情况并非如此，因为那些模型没有绑定权重。这些型号可以在没有<code>torchscript</code>标志的情况下安全地导出。</p><h4 id="虚拟-dummy-输入和标准长度"><a href="#虚拟-dummy-输入和标准长度" class="headerlink" title="虚拟(dummy)输入和标准长度"></a>虚拟(dummy)输入和标准长度</h4><p>虚拟输入用于进行模型前向传播。当输入值在各层中传播时，Pytorch跟踪在每个张量上执行的不同操作。然后使用这些记录的操作创建模型的“迹”。</p><p>迹是相对于输入的尺寸创建的。因此，它受到虚拟输入尺寸的限制，并且不适用于任何其他序列长度或批次大小。尝试使用其他尺寸时，会出现如下错误，如：</p><pre><code>The expanded size of the tensor (3) must match the existing size (7) at non-singleton dimension 2</code></pre><p>因此，建议使用至少与最大输入大小相同的虚拟输入大小来跟踪模型。在推理期间对于模型的输入，可以执行填充来填充缺少的值。作为模型<br>将以较大的输入大小来进行跟踪，但是，不同矩阵的尺寸也将很大，从而导致更多的计算。</p><p>建议注意每个输入上完成的操作总数，并密切关注各种序列长度对应性能的变化。</p><h3 id="在Python中使用TorchScript"><a href="#在Python中使用TorchScript" class="headerlink" title="在Python中使用TorchScript"></a>在Python中使用TorchScript</h3><p>以下是使用Python保存，加载模型以及如何使用”迹”进行推理的示例。</p><h4 id="保存模型"><a href="#保存模型" class="headerlink" title="保存模型"></a>保存模型</h4><p>该代码段显示了如何使用TorchScript导出<code>BertModel</code>。在这里实例化<code>BertModel</code>，根据<code>BertConfig</code>类，然后以文件名<code>traced_bert.pt</code>保存到磁盘</p><pre class=" language-python"><code class="language-python"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> BertModel<span class="token punctuation">,</span> BertTokenizer<span class="token punctuation">,</span> BertConfig<span class="token keyword">import</span> torchenc <span class="token operator">=</span> BertTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"bert-base-uncased"</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 标记输入文本</span>text <span class="token operator">=</span> <span class="token string">"[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]"</span>tokenized_text <span class="token operator">=</span> enc<span class="token punctuation">.</span>tokenize<span class="token punctuation">(</span>text<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 输入标记之一进行掩码</span>masked_index <span class="token operator">=</span> <span class="token number">8</span>tokenized_text<span class="token punctuation">[</span>masked_index<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">'[MASK]'</span>indexed_tokens <span class="token operator">=</span> enc<span class="token punctuation">.</span>convert_tokens_to_ids<span class="token punctuation">(</span>tokenized_text<span class="token punctuation">)</span>segments_ids <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token comment" spellcheck="true"># 创建虚拟输入</span>tokens_tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>indexed_tokens<span class="token punctuation">]</span><span class="token punctuation">)</span>segments_tensors <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>segments_ids<span class="token punctuation">]</span><span class="token punctuation">)</span>dummy_input <span class="token operator">=</span> <span class="token punctuation">[</span>tokens_tensor<span class="token punctuation">,</span> segments_tensors<span class="token punctuation">]</span><span class="token comment" spellcheck="true"># 使用torchscript标志初始化模型</span><span class="token comment" spellcheck="true"># 标志被设置为True，即使没有必要，因为该型号没有LM Head。</span>config <span class="token operator">=</span> BertConfig<span class="token punctuation">(</span>vocab_size_or_config_json_file<span class="token operator">=</span><span class="token number">32000</span><span class="token punctuation">,</span> hidden_size<span class="token operator">=</span><span class="token number">768</span><span class="token punctuation">,</span>    num_hidden_layers<span class="token operator">=</span><span class="token number">12</span><span class="token punctuation">,</span> num_attention_heads<span class="token operator">=</span><span class="token number">12</span><span class="token punctuation">,</span> intermediate_size<span class="token operator">=</span><span class="token number">3072</span><span class="token punctuation">,</span> torchscript<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 实例化模型</span>model <span class="token operator">=</span> BertModel<span class="token punctuation">(</span>config<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 模型设置为评估模式</span>model<span class="token punctuation">.</span>eval<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 如果您要使用from_pretrained实例化模型，则还可以设置TorchScript标志</span>model <span class="token operator">=</span> BertModel<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"bert-base-uncased"</span><span class="token punctuation">,</span> torchscript<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 创建迹</span>traced_model <span class="token operator">=</span> torch<span class="token punctuation">.</span>jit<span class="token punctuation">.</span>trace<span class="token punctuation">(</span>model<span class="token punctuation">,</span> <span class="token punctuation">[</span>tokens_tensor<span class="token punctuation">,</span> segments_tensors<span class="token punctuation">]</span><span class="token punctuation">)</span>torch<span class="token punctuation">.</span>jit<span class="token punctuation">.</span>save<span class="token punctuation">(</span>traced_model<span class="token punctuation">,</span> <span class="token string">"traced_bert.pt"</span><span class="token punctuation">)</span></code></pre><h4 id="载入模型"><a href="#载入模型" class="headerlink" title="载入模型"></a>载入模型</h4><p>该代码段显示了如何加载以前以名称<code>traced_bert.pt</code>保存到磁盘的<code>BertModel</code>。<br>我们重新使用之前初始化的<code>dummy_input</code>。</p><pre class=" language-python"><code class="language-python">loaded_model <span class="token operator">=</span> torch<span class="token punctuation">.</span>jit<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">"traced_model.pt"</span><span class="token punctuation">)</span>loaded_model<span class="token punctuation">.</span>eval<span class="token punctuation">(</span><span class="token punctuation">)</span>all_encoder_layers<span class="token punctuation">,</span> pooled_output <span class="token operator">=</span> loaded_model<span class="token punctuation">(</span>dummy_input<span class="token punctuation">)</span></code></pre><h4 id="使用跟踪模型进行推理"><a href="#使用跟踪模型进行推理" class="headerlink" title="使用跟踪模型进行推理"></a>使用跟踪模型进行推理</h4><p>使用跟踪模型进行推理就像使用其<code>__call__</code> 方法一样简单：</p><pre class=" language-python"><code class="language-python">traced_model<span class="token punctuation">(</span>tokens_tensor<span class="token punctuation">,</span> segments_tensors<span class="token punctuation">)</span></code></pre><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 运维 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> -部署 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TorchServe部署方法及注意事项</title>
      <link href="/2020/09/17/torchserve-bu-shu-zhu-yi-shi-xiang/"/>
      <url>/2020/09/17/torchserve-bu-shu-zhu-yi-shi-xiang/</url>
      
        <content type="html"><![CDATA[<h1 id="TorchServe部署方法及问题"><a href="#TorchServe部署方法及问题" class="headerlink" title="TorchServe部署方法及问题"></a>TorchServe部署方法及问题</h1><ul><li><p>Java 环境问题</p><p>一定要安装JDK11（官网要求最低8，但是我不能使用，打包一定要再部署的环境打包，避免java版本不同，部署报错）</p></li><li><p>在别的地方使用cuda训练的模型在其他只有cpu的环境可能在加载模型时报错</p><p> raise RuntimeError(‘Attempting to deserialize object on a CUDA ‘<br>RuntimeError: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device(‘cpu’) to map your storages to the CPU.</p><p>解决办法　现在原来地方加载，然后使用cpu保存</p></li><li><p>torchserve 模型的入口是</p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">handle</span><span class="token punctuation">(</span>data<span class="token punctuation">,</span> context<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">pass</span></code></pre></li><li><p>返回必须是数组，并且需要多加一列数组，返回的数据会少一维数组</p><p>不然报错信息：b’{\n  “code”: 503,\n  “type”: “InternalServerException”,\n  “message”: “Invalid model predict output”\n}\n</p></li><li><p>若使用了transformers，docker镜像没有安装transformers（目前所有可用的镜像<a href="https://hub.docker.com/r/pytorch/torchserve/tags%EF%BC%89">https://hub.docker.com/r/pytorch/torchserve/tags）</a></p><p>若要使用docker自己制作：</p><ol><li>Create TorchServe docker image  <a href="https://github.com/pytorch/serve/tree/master/docker">https://github.com/pytorch/serve/tree/master/docker</a></li><li>Create TorchServe docker image from source   <a href="https://github.com/pytorch/serve/tree/master/docker">https://github.com/pytorch/serve/tree/master/docker</a></li></ol></li></ul><p><strong>打包脚本</strong></p><pre class=" language-shell"><code class="language-shell">torch-model-archiver     --model-name chatbotBERT     --version 1.0 --serialized-file ./pytorch_model.bin     --extra-files "./config.json,./best_ner.bin,./config.json,./vocab.txt"     --handler "chatbot_medical_qa_ner_handler.py"</code></pre><p><strong>启动</strong></p><pre class=" language-shell"><code class="language-shell">torchserve --start --ncs --model-store model-store --models chatbotBERT.mar --ts-config ./model-store/config.properties</code></pre><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 运维 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 部署 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>curl命令</title>
      <link href="/2020/09/15/curl-ming-ling/"/>
      <url>/2020/09/15/curl-ming-ling/</url>
      
        <content type="html"><![CDATA[<h1 id="curl"><a href="#curl" class="headerlink" title="curl"></a>curl</h1><h2 id="命令：curl"><a href="#命令：curl" class="headerlink" title="命令：curl"></a>命令：curl</h2><p>​      原地址：<a href="https://www.cnblogs.com/duhuo/p/5695256.html">https://www.cnblogs.com/duhuo/p/5695256.html</a></p><p>​       在Linux中curl是一个利用URL规则在命令行下工作的文件传输工具，可以说是一款很强大的http命令行工具。它支持文件的上传和下载，是综合传输工具，但按传统，习惯称url为下载工具。</p><pre><code>语法：# curl [option] [url]</code></pre><p>常见参数：</p><pre class=" language-shell"><code class="language-shell">-A/--user-agent <string>              设置用户代理发送给服务器-b/--cookie <name=string/file>    cookie字符串或文件读取位置-c/--cookie-jar <file>                    操作结束后把cookie写入到这个文件中-C/--continue-at <offset>            断点续转-D/--dump-header <file>              把header信息写入到该文件中-e/--referer                                  来源网址-f/--fail                                          连接失败时不显示http错误-o/--output                                  把输出写到该文件中-O/--remote-name                      把输出写到该文件中，保留远程文件的文件名-r/--range <range>                      检索来自HTTP/1.1或FTP服务器字节范围-s/--silent                                    静音模式。不输出任何东西-T/--upload-file <file>                  上传文件-u/--user <user[:password]>      设置服务器的用户和密码-w/--write-out [format]                什么输出完成后-x/--proxy <host[:port]>              在给定的端口上使用HTTP代理-#/--progress-bar                        进度条显示当前的传送状态</code></pre><p>例子：<br>1、基本用法</p><pre class=" language-shell"><code class="language-shell"># curl http://www.linux.com</code></pre><p>执行后，<a href="http://www.linux.com/">www.linux.com</a> 的html就会显示在屏幕上了<br>Ps：由于安装linux的时候很多时候是没有安装桌面的，也意味着没有浏览器，因此这个方法也经常用于测试一台服务器是否可以到达一个网站</p><p>2、保存访问的网页<br>2.1:使用linux的重定向功能保存</p><pre class=" language-shell"><code class="language-shell"># curl http://www.linux.com >> linux.html</code></pre><p>2.2:可以使用curl的内置option:-o(小写)保存网页</p><pre class=" language-shell"><code class="language-shell">$ curl -o linux.html http://www.linux.com</code></pre><p>执行完成后会显示如下界面，显示100%则表示保存成功</p><pre class=" language-shell"><code class="language-shell">% Total    % Received % Xferd  Average Speed  Time    Time    Time  Current                                Dload  Upload  Total  Spent    Left  Speed100 79684    0 79684    0    0  3437k      0 --:--:-- --:--:-- --:--:-- 7781k</code></pre><p>2.3:可以使用curl的内置option:-O(大写)保存网页中的文件<br>要注意这里后面的url要具体到某个文件，不然抓不下来</p><pre class=" language-shell"><code class="language-shell"># curl -O http://www.linux.com/hello.sh</code></pre><p>3、测试网页返回值</p><pre class=" language-shell"><code class="language-shell"># curl -o /dev/null -s -w %&#123;http_code&#125; www.linux.com</code></pre><p>Ps:在脚本中，这是很常见的测试网站是否正常的用法</p><p>4、指定proxy服务器以及其端口<br>很多时候上网需要用到代理服务器(比如是使用代理服务器上网或者因为使用curl别人网站而被别人屏蔽IP地址的时候)，幸运的是curl通过使用内置option：-x来支持设置代理</p><pre class=" language-shell"><code class="language-shell"># curl -x 192.168.100.100:1080 http://www.linux.com</code></pre><p>5、cookie<br>有些网站是使用cookie来记录session信息。对于chrome这样的浏览器，可以轻易处理cookie信息，但在curl中只要增加相关参数也是可以很容易的处理cookie<br>5.1:保存http的response里面的cookie信息。内置option:-c（小写）</p><pre class=" language-shell"><code class="language-shell"># curl -c cookiec.txt  http://www.linux.com</code></pre><p>执行后cookie信息就被存到了cookiec.txt里面了</p><p>5.2:保存http的response里面的header信息。内置option: -D</p><pre class=" language-shell"><code class="language-shell"># curl -D cookied.txt http://www.linux.com</code></pre><p>执行后cookie信息就被存到了cookied.txt里面了</p><p>注意：-c(小写)产生的cookie和-D里面的cookie是不一样的。</p><p>5.3:使用cookie<br>很多网站都是通过监视你的cookie信息来判断你是否按规矩访问他们的网站的，因此我们需要使用保存的cookie信息。内置option: -b</p><pre class=" language-shell"><code class="language-shell"># curl -b cookiec.txt http://www.linux.com</code></pre><p>6、模仿浏览器<br>有些网站需要使用特定的浏览器去访问他们，有些还需要使用某些特定的版本。curl内置option:-A可以让我们指定浏览器去访问网站</p><pre class=" language-shell"><code class="language-shell"># curl -A "Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.0)" http://www.linux.com</code></pre><p>这样服务器端就会认为是使用IE8.0去访问的</p><p>7、伪造referer（盗链）<br>很多服务器会检查http访问的referer从而来控制访问。比如：你是先访问首页，然后再访问首页中的邮箱页面，这里访问邮箱的referer地址就是访问首页成功后的页面地址，如果服务器发现对邮箱页面访问的referer地址不是首页的地址，就断定那是个盗连了<br>curl中内置option：-e可以让我们设定referer</p><pre class=" language-shell"><code class="language-shell"># curl -e "www.linux.com" http://mail.linux.com</code></pre><p>这样就会让服务器其以为你是从<a href="http://www.linux.com点击某个链接过来的/">www.linux.com点击某个链接过来的</a></p><p>8、下载文件<br>8.1：利用curl下载文件。<br>#使用内置option：-o(小写)</p><pre class=" language-shell"><code class="language-shell"># curl -o dodo1.jpg http:www.linux.com/dodo1.JPG</code></pre><p>#使用内置option：-O（大写)</p><pre class=" language-shell"><code class="language-shell"># curl -O http://www.linux.com/dodo1.JPG</code></pre><p>这样就会以服务器上的名称保存文件到本地</p><p>8.2：循环下载<br>有时候下载图片可以能是前面的部分名称是一样的，就最后的尾椎名不一样</p><pre class=" language-shell"><code class="language-shell"># curl -O http://www.linux.com/dodo[1-5].JPG</code></pre><p>这样就会把dodo1，dodo2，dodo3，dodo4，dodo5全部保存下来</p><p>8.3：下载重命名</p><pre class=" language-shell"><code class="language-shell"># curl -O http://www.linux.com/&#123;hello,bb&#125;/dodo[1-5].JPG</code></pre><p>由于下载的hello与bb中的文件名都是dodo1，dodo2，dodo3，dodo4，dodo5。因此第二次下载的会把第一次下载的覆盖，这样就需要对文件进行重命名。</p><pre class=" language-shell"><code class="language-shell"># curl -o #1_#2.JPG http://www.linux.com/&#123;hello,bb&#125;/dodo[1-5].JPG</code></pre><p>这样在hello/dodo1.JPG的文件下载下来就会变成hello_dodo1.JPG,其他文件依此类推，从而有效的避免了文件被覆盖</p><p>8.4：分块下载<br>有时候下载的东西会比较大，这个时候我们可以分段下载。使用内置option：-r</p><pre class=" language-shell"><code class="language-shell"># curl -r 0-100 -o dodo1_part1.JPG http://www.linux.com/dodo1.JPG# curl -r 100-200 -o dodo1_part2.JPG http://www.linux.com/dodo1.JPG# curl -r 200- -o dodo1_part3.JPG http://www.linux.com/dodo1.JPG# cat dodo1_part* > dodo1.JPG</code></pre><p>这样就可以查看dodo1.JPG的内容了</p><p>8.5：通过ftp下载文件<br>curl可以通过ftp下载文件，curl提供两种从ftp中下载的语法</p><pre class=" language-shell"><code class="language-shell"># curl -O -u 用户名:密码 ftp://www.linux.com/dodo1.JPG# curl -O ftp://用户名:密码@www.linux.com/dodo1.JPG</code></pre><p>8.6：显示下载进度条</p><pre class=" language-shell"><code class="language-shell"># curl -# -O http://www.linux.com/dodo1.JPG</code></pre><p>8.7：不会显示下载进度信息</p><pre class=" language-shell"><code class="language-shell"># curl -s -O http://www.linux.com/dodo1.JPG</code></pre><p>9、断点续传<br>在windows中，我们可以使用迅雷这样的软件进行断点续传。curl可以通过内置option:-C同样可以达到相同的效果<br>如果在下载dodo1.JPG的过程中突然掉线了，可以使用以下的方式续传</p><pre class=" language-shell"><code class="language-shell"># curl -C -O http://www.linux.com/dodo1.JPG</code></pre><p>10、上传文件<br>curl不仅仅可以下载文件，还可以上传文件。通过内置option:-T来实现</p><pre class=" language-shell"><code class="language-shell"># curl -T dodo1.JPG -u 用户名:密码 ftp://www.linux.com/img/</code></pre><p>这样就向ftp服务器上传了文件dodo1.JPG</p><p>11、显示抓取错误</p><pre class=" language-shell"><code class="language-shell"># curl -f http://www.linux.com/error</code></pre><p>其他参数(此处翻译为转载)：</p><pre class=" language-shell"><code class="language-shell">-a/--append                        上传文件时，附加到目标文件--anyauth                            可以使用“任何”身份验证方法--basic                                使用HTTP基本验证-B/--use-ascii                      使用ASCII文本传输-d/--data <data>                  HTTP POST方式传送数据--data-ascii <data>            以ascii的方式post数据--data-binary <data>          以二进制的方式post数据--negotiate                          使用HTTP身份验证--digest                        使用数字身份验证--disable-eprt                  禁止使用EPRT或LPRT--disable-epsv                  禁止使用EPSV--egd-file <file>              为随机数据(SSL)设置EGD socket路径--tcp-nodelay                  使用TCP_NODELAY选项-E/--cert <cert[:passwd]>      客户端证书文件和密码 (SSL)--cert-type <type>              证书文件类型 (DER/PEM/ENG) (SSL)--key <key>                    私钥文件名 (SSL)--key-type <type>              私钥文件类型 (DER/PEM/ENG) (SSL)--pass  <pass>                  私钥密码 (SSL)--engine <eng>                  加密引擎使用 (SSL). "--engine list" for list--cacert <file>                CA证书 (SSL)--capath <directory>            CA目   (made using c_rehash) to verify peer against (SSL)--ciphers <list>                SSL密码--compressed                    要求返回是压缩的形势 (using deflate or gzip)--connect-timeout <seconds>    设置最大请求时间--create-dirs                  建立本地目录的目录层次结构--crlf                          上传是把LF转变成CRLF--ftp-create-dirs              如果远程目录不存在，创建远程目录--ftp-method [multicwd/nocwd/singlecwd]    控制CWD的使用--ftp-pasv                      使用 PASV/EPSV 代替端口--ftp-skip-pasv-ip              使用PASV的时候,忽略该IP地址--ftp-ssl                      尝试用 SSL/TLS 来进行ftp数据传输--ftp-ssl-reqd                  要求用 SSL/TLS 来进行ftp数据传输-F/--form <name=content>        模拟http表单提交数据-form-string <name=string>      模拟http表单提交数据-g/--globoff                    禁用网址序列和范围使用&#123;&#125;和[]-G/--get                        以get的方式来发送数据-h/--help                      帮助-H/--header <line>              自定义头信息传递给服务器--ignore-content-length        忽略的HTTP头信息的长度-i/--include                    输出时包括protocol头信息-I/--head                      只显示文档信息-j/--junk-session-cookies      读取文件时忽略session cookie--interface <interface>        使用指定网络接口/地址--krb4 <level>                  使用指定安全级别的krb4-k/--insecure                  允许不使用证书到SSL站点-K/--config                    指定的配置文件读取-l/--list-only                  列出ftp目录下的文件名称--limit-rate <rate>            设置传输速度--local-port<NUM>              强制使用本地端口号-m/--max-time <seconds>        设置最大传输时间--max-redirs <num>              设置最大读取的目录数--max-filesize <bytes>          设置最大下载的文件总量-M/--manual                    显示全手动-n/--netrc                      从netrc文件中读取用户名和密码--netrc-optional                使用 .netrc 或者 URL来覆盖-n--ntlm                          使用 HTTP NTLM 身份验证-N/--no-buffer                  禁用缓冲输出-p/--proxytunnel                使用HTTP代理--proxy-anyauth                选择任一代理身份验证方法--proxy-basic                  在代理上使用基本身份验证--proxy-digest                  在代理上使用数字身份验证--proxy-ntlm                    在代理上使用ntlm身份验证-P/--ftp-port <address>        使用端口地址，而不是使用PASV-Q/--quote <cmd>                文件传输前，发送命令到服务器--range-file                    读取（SSL）的随机文件-R/--remote-time                在本地生成文件时，保留远程文件时间--retry <num>                  传输出现问题时，重试的次数--retry-delay <seconds>        传输出现问题时，设置重试间隔时间--retry-max-time <seconds>      传输出现问题时，设置最大重试时间-S/--show-error                显示错误--socks4 <host[:port]>          用socks4代理给定主机和端口--socks5 <host[:port]>          用socks5代理给定主机和端口-t/--telnet-option <OPT=val>    Telnet选项设置--trace <file>                  对指定文件进行debug--trace-ascii <file>            Like --跟踪但没有hex输出--trace-time                    跟踪/详细输出时，添加时间戳--url <URL>                    Spet URL to work with-U/--proxy-user <user[:password]>  设置代理用户名和密码-V/--version                    显示版本信息-X/--request <command>          指定什么命令-y/--speed-time                放弃限速所要的时间。默认为30-Y/--speed-limit                停止传输速度的限制，速度时间'秒-z/--time-cond                  传送时间设置-0/--http1.0                    使用HTTP 1.0-1/--tlsv1                      使用TLSv1（SSL）-2/--sslv2                      使用SSLv2的（SSL）-3/--sslv3                      使用的SSLv3（SSL）--3p-quote                      like -Q for the source URL for 3rd party transfer--3p-url                        使用url，进行第三方传送--3p-user                      使用用户名和密码，进行第三方传送-4/--ipv4                      使用IP4-6/--ipv6                      使用IP6</code></pre><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> shell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 命令 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>中文医学文本实体关系抽取</title>
      <link href="/2020/09/14/zhong-wen-yi-xue-wen-ben-shi-ti-guan-xi-chou-qu/"/>
      <url>/2020/09/14/zhong-wen-yi-xue-wen-ben-shi-ti-guan-xi-chou-qu/</url>
      
        <content type="html"><![CDATA[<h1 id="中文医学文本实体关系抽取总结"><a href="#中文医学文本实体关系抽取总结" class="headerlink" title="中文医学文本实体关系抽取总结"></a>中文医学文本实体关系抽取总结</h1><h2 id="任务简介"><a href="#任务简介" class="headerlink" title="任务简介"></a>任务简介</h2><p>实体和关系抽取作为信息抽取的重要子任务，近些年众多学者利用多种技术在该领域开展深入研究。将这些技术应用于医学领域，抽取非结构化和半结构化的医学文本构建成医学知识图谱，可服务于下游子任务。非结构化的医学文本，如医学教材每一个自然段落，临床实践中每种疾病下的主题，电子病历数据中的主诉、现病史、鉴别诊断等，都是由中文自然语言句子或句子集合组成。实体关系抽取是从非结构化医学文本中找出医学实体，并确定实体对关系事实的过程。</p><h2 id="任务详情"><a href="#任务详情" class="headerlink" title="任务详情"></a>任务详情</h2><p>给定schema约束集合及句子sentence，其中schema定义了关系Predicate以及其对应的主体Subject和客体Object的类别，例如：</p><pre class=" language-json"><code class="language-json">（“subject_type”<span class="token operator">:</span>“疾病”，“predicate”<span class="token operator">:</span> “药物治疗”，“object_type”<span class="token operator">:</span>“药物”）（“subject_type”<span class="token operator">:</span>“疾病”，“predicate”<span class="token operator">:</span> “实验室检查”，“object_type”<span class="token operator">:</span>“检查”）</code></pre><p><strong>输入/输出:</strong><br>(1) 输入:schema约束集合及句子sentence<br>(2) 输出:句子sentence中包含的符合给定schema约束的三元组知识Triples</p><h2 id="评价标准"><a href="#评价标准" class="headerlink" title="评价标准"></a>评价标准</h2><p>采用Precision，Recall和F1值作为评价指标。</p><h2 id="数据简介"><a href="#数据简介" class="headerlink" title="数据简介"></a>数据简介</h2><h3 id="约束集"><a href="#约束集" class="headerlink" title="约束集"></a>约束集</h3><pre class=" language-json"><code class="language-json">&amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"subject_type"</span><span class="token operator">:</span> <span class="token string">"疾病"</span><span class="token punctuation">,</span> <span class="token property">"predicate"</span><span class="token operator">:</span> <span class="token string">"化疗"</span><span class="token punctuation">,</span> <span class="token property">"object_type"</span><span class="token operator">:</span> <span class="token string">"其他治疗"</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span>&amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"subject_type"</span><span class="token operator">:</span> <span class="token string">"疾病"</span><span class="token punctuation">,</span> <span class="token property">"predicate"</span><span class="token operator">:</span> <span class="token string">"放射治疗"</span><span class="token punctuation">,</span> <span class="token property">"object_type"</span><span class="token operator">:</span> <span class="token string">"其他治疗"</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span>&amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"subject_type"</span><span class="token operator">:</span> <span class="token string">"其他治疗"</span><span class="token punctuation">,</span> <span class="token property">"predicate"</span><span class="token operator">:</span> <span class="token string">"同义词"</span><span class="token punctuation">,</span> <span class="token property">"object_type"</span><span class="token operator">:</span> <span class="token string">"其他治疗"</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span>&amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"subject_type"</span><span class="token operator">:</span> <span class="token string">"疾病"</span><span class="token punctuation">,</span> <span class="token property">"predicate"</span><span class="token operator">:</span> <span class="token string">"手术治疗"</span><span class="token punctuation">,</span> <span class="token property">"object_type"</span><span class="token operator">:</span> <span class="token string">"手术治疗"</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span>&amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"subject_type"</span><span class="token operator">:</span> <span class="token string">"手术治疗"</span><span class="token punctuation">,</span> <span class="token property">"predicate"</span><span class="token operator">:</span> <span class="token string">"同义词"</span><span class="token punctuation">,</span> <span class="token property">"object_type"</span><span class="token operator">:</span> <span class="token string">"手术治疗"</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span>&amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"subject_type"</span><span class="token operator">:</span> <span class="token string">"疾病"</span><span class="token punctuation">,</span> <span class="token property">"predicate"</span><span class="token operator">:</span> <span class="token string">"实验室检查"</span><span class="token punctuation">,</span> <span class="token property">"object_type"</span><span class="token operator">:</span> <span class="token string">"检查"</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span>&amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"subject_type"</span><span class="token operator">:</span> <span class="token string">"疾病"</span><span class="token punctuation">,</span> <span class="token property">"predicate"</span><span class="token operator">:</span> <span class="token string">"影像学检查"</span><span class="token punctuation">,</span> <span class="token property">"object_type"</span><span class="token operator">:</span> <span class="token string">"检查"</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span>&amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"subject_type"</span><span class="token operator">:</span> <span class="token string">"疾病"</span><span class="token punctuation">,</span> <span class="token property">"predicate"</span><span class="token operator">:</span> <span class="token string">"辅助检查"</span><span class="token punctuation">,</span> <span class="token property">"object_type"</span><span class="token operator">:</span> <span class="token string">"检查"</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span>&amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"subject_type"</span><span class="token operator">:</span> <span class="token string">"疾病"</span><span class="token punctuation">,</span> <span class="token property">"predicate"</span><span class="token operator">:</span> <span class="token string">"组织学检查"</span><span class="token punctuation">,</span> <span class="token property">"object_type"</span><span class="token operator">:</span> <span class="token string">"检查"</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span>&amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"subject_type"</span><span class="token operator">:</span> <span class="token string">"检查"</span><span class="token punctuation">,</span> <span class="token property">"predicate"</span><span class="token operator">:</span> <span class="token string">"同义词"</span><span class="token punctuation">,</span> <span class="token property">"object_type"</span><span class="token operator">:</span> <span class="token string">"检查"</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span>&amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"subject_type"</span><span class="token operator">:</span> <span class="token string">"疾病"</span><span class="token punctuation">,</span> <span class="token property">"predicate"</span><span class="token operator">:</span> <span class="token string">"内窥镜检查"</span><span class="token punctuation">,</span> <span class="token property">"object_type"</span><span class="token operator">:</span> <span class="token string">"检查"</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span>&amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"subject_type"</span><span class="token operator">:</span> <span class="token string">"疾病"</span><span class="token punctuation">,</span> <span class="token property">"predicate"</span><span class="token operator">:</span> <span class="token string">"筛查"</span><span class="token punctuation">,</span> <span class="token property">"object_type"</span><span class="token operator">:</span> <span class="token string">"检查"</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span>&amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"subject_type"</span><span class="token operator">:</span> <span class="token string">"疾病"</span><span class="token punctuation">,</span> <span class="token property">"predicate"</span><span class="token operator">:</span> <span class="token string">"多发群体"</span><span class="token punctuation">,</span> <span class="token property">"object_type"</span><span class="token operator">:</span> <span class="token string">"流行病学"</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span>&amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"subject_type"</span><span class="token operator">:</span> <span class="token string">"疾病"</span><span class="token punctuation">,</span> <span class="token property">"predicate"</span><span class="token operator">:</span> <span class="token string">"发病率"</span><span class="token punctuation">,</span> <span class="token property">"object_type"</span><span class="token operator">:</span> <span class="token string">"流行病学"</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span>&amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"subject_type"</span><span class="token operator">:</span> <span class="token string">"疾病"</span><span class="token punctuation">,</span> <span class="token property">"predicate"</span><span class="token operator">:</span> <span class="token string">"发病年龄"</span><span class="token punctuation">,</span> <span class="token property">"object_type"</span><span class="token operator">:</span> <span class="token string">"流行病学"</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span>&amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"subject_type"</span><span class="token operator">:</span> <span class="token string">"疾病"</span><span class="token punctuation">,</span> <span class="token property">"predicate"</span><span class="token operator">:</span> <span class="token string">"多发地区"</span><span class="token punctuation">,</span> <span class="token property">"object_type"</span><span class="token operator">:</span> <span class="token string">"流行病学"</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span>&amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"subject_type"</span><span class="token operator">:</span> <span class="token string">"疾病"</span><span class="token punctuation">,</span> <span class="token property">"predicate"</span><span class="token operator">:</span> <span class="token string">"发病性别倾向"</span><span class="token punctuation">,</span> <span class="token property">"object_type"</span><span class="token operator">:</span> <span class="token string">"流行病学"</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span>&amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"subject_type"</span><span class="token operator">:</span> <span class="token string">"疾病"</span><span class="token punctuation">,</span> <span class="token property">"predicate"</span><span class="token operator">:</span> <span class="token string">"死亡率"</span><span class="token punctuation">,</span> <span class="token property">"object_type"</span><span class="token operator">:</span> <span class="token string">"流行病学"</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span>&amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"subject_type"</span><span class="token operator">:</span> <span class="token string">"疾病"</span><span class="token punctuation">,</span> <span class="token property">"predicate"</span><span class="token operator">:</span> <span class="token string">"多发季节"</span><span class="token punctuation">,</span> <span class="token property">"object_type"</span><span class="token operator">:</span> <span class="token string">"流行病学"</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span>&amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"subject_type"</span><span class="token operator">:</span> <span class="token string">"疾病"</span><span class="token punctuation">,</span> <span class="token property">"predicate"</span><span class="token operator">:</span> <span class="token string">"传播途径"</span><span class="token punctuation">,</span> <span class="token property">"object_type"</span><span class="token operator">:</span> <span class="token string">"流行病学"</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span>&amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"subject_type"</span><span class="token operator">:</span> <span class="token string">"流行病学"</span><span class="token punctuation">,</span> <span class="token property">"predicate"</span><span class="token operator">:</span> <span class="token string">"同义词"</span><span class="token punctuation">,</span> <span class="token property">"object_type"</span><span class="token operator">:</span> <span class="token string">"流行病学"</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span>&amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"subject_type"</span><span class="token operator">:</span> <span class="token string">"疾病"</span><span class="token punctuation">,</span> <span class="token property">"predicate"</span><span class="token operator">:</span> <span class="token string">"同义词"</span><span class="token punctuation">,</span> <span class="token property">"object_type"</span><span class="token operator">:</span> <span class="token string">"疾病"</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span>&amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"subject_type"</span><span class="token operator">:</span> <span class="token string">"疾病"</span><span class="token punctuation">,</span> <span class="token property">"predicate"</span><span class="token operator">:</span> <span class="token string">"并发症"</span><span class="token punctuation">,</span> <span class="token property">"object_type"</span><span class="token operator">:</span> <span class="token string">"疾病"</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span>&amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"subject_type"</span><span class="token operator">:</span> <span class="token string">"疾病"</span><span class="token punctuation">,</span> <span class="token property">"predicate"</span><span class="token operator">:</span> <span class="token string">"病理分型"</span><span class="token punctuation">,</span> <span class="token property">"object_type"</span><span class="token operator">:</span> <span class="token string">"疾病"</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span>&amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"subject_type"</span><span class="token operator">:</span> <span class="token string">"疾病"</span><span class="token punctuation">,</span> <span class="token property">"predicate"</span><span class="token operator">:</span> <span class="token string">"相关（导致）"</span><span class="token punctuation">,</span> <span class="token property">"object_type"</span><span class="token operator">:</span> <span class="token string">"疾病"</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span>&amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"subject_type"</span><span class="token operator">:</span> <span class="token string">"疾病"</span><span class="token punctuation">,</span> <span class="token property">"predicate"</span><span class="token operator">:</span> <span class="token string">"鉴别诊断"</span><span class="token punctuation">,</span> <span class="token property">"object_type"</span><span class="token operator">:</span> <span class="token string">"疾病"</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span>&amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"subject_type"</span><span class="token operator">:</span> <span class="token string">"疾病"</span><span class="token punctuation">,</span> <span class="token property">"predicate"</span><span class="token operator">:</span> <span class="token string">"相关（转化）"</span><span class="token punctuation">,</span> <span class="token property">"object_type"</span><span class="token operator">:</span> <span class="token string">"疾病"</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span>&amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"subject_type"</span><span class="token operator">:</span> <span class="token string">"疾病"</span><span class="token punctuation">,</span> <span class="token property">"predicate"</span><span class="token operator">:</span> <span class="token string">"相关（症状）"</span><span class="token punctuation">,</span> <span class="token property">"object_type"</span><span class="token operator">:</span> <span class="token string">"疾病"</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span>&amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"subject_type"</span><span class="token operator">:</span> <span class="token string">"疾病"</span><span class="token punctuation">,</span> <span class="token property">"predicate"</span><span class="token operator">:</span> <span class="token string">"临床表现"</span><span class="token punctuation">,</span> <span class="token property">"object_type"</span><span class="token operator">:</span> <span class="token string">"症状"</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span>&amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"subject_type"</span><span class="token operator">:</span> <span class="token string">"疾病"</span><span class="token punctuation">,</span> <span class="token property">"predicate"</span><span class="token operator">:</span> <span class="token string">"治疗后症状"</span><span class="token punctuation">,</span> <span class="token property">"object_type"</span><span class="token operator">:</span> <span class="token string">"症状"</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span>&amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"subject_type"</span><span class="token operator">:</span> <span class="token string">"疾病"</span><span class="token punctuation">,</span> <span class="token property">"predicate"</span><span class="token operator">:</span> <span class="token string">"侵及周围组织转移的症状"</span><span class="token punctuation">,</span> <span class="token property">"object_type"</span><span class="token operator">:</span> <span class="token string">"症状"</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span>&amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"subject_type"</span><span class="token operator">:</span> <span class="token string">"症状"</span><span class="token punctuation">,</span> <span class="token property">"predicate"</span><span class="token operator">:</span> <span class="token string">"同义词"</span><span class="token punctuation">,</span> <span class="token property">"object_type"</span><span class="token operator">:</span> <span class="token string">"症状"</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span>&amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"subject_type"</span><span class="token operator">:</span> <span class="token string">"疾病"</span><span class="token punctuation">,</span> <span class="token property">"predicate"</span><span class="token operator">:</span> <span class="token string">"病因"</span><span class="token punctuation">,</span> <span class="token property">"object_type"</span><span class="token operator">:</span> <span class="token string">"社会学"</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span>&amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"subject_type"</span><span class="token operator">:</span> <span class="token string">"疾病"</span><span class="token punctuation">,</span> <span class="token property">"predicate"</span><span class="token operator">:</span> <span class="token string">"高危因素"</span><span class="token punctuation">,</span> <span class="token property">"object_type"</span><span class="token operator">:</span> <span class="token string">"社会学"</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span>&amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"subject_type"</span><span class="token operator">:</span> <span class="token string">"疾病"</span><span class="token punctuation">,</span> <span class="token property">"predicate"</span><span class="token operator">:</span> <span class="token string">"风险评估因素"</span><span class="token punctuation">,</span> <span class="token property">"object_type"</span><span class="token operator">:</span> <span class="token string">"社会学"</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span>&amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"subject_type"</span><span class="token operator">:</span> <span class="token string">"疾病"</span><span class="token punctuation">,</span> <span class="token property">"predicate"</span><span class="token operator">:</span> <span class="token string">"病史"</span><span class="token punctuation">,</span> <span class="token property">"object_type"</span><span class="token operator">:</span> <span class="token string">"社会学"</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span>&amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"subject_type"</span><span class="token operator">:</span> <span class="token string">"疾病"</span><span class="token punctuation">,</span> <span class="token property">"predicate"</span><span class="token operator">:</span> <span class="token string">"遗传因素"</span><span class="token punctuation">,</span> <span class="token property">"object_type"</span><span class="token operator">:</span> <span class="token string">"社会学"</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span>&amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"subject_type"</span><span class="token operator">:</span> <span class="token string">"社会学"</span><span class="token punctuation">,</span> <span class="token property">"predicate"</span><span class="token operator">:</span> <span class="token string">"同义词"</span><span class="token punctuation">,</span> <span class="token property">"object_type"</span><span class="token operator">:</span> <span class="token string">"社会学"</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span>&amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"subject_type"</span><span class="token operator">:</span> <span class="token string">"疾病"</span><span class="token punctuation">,</span> <span class="token property">"predicate"</span><span class="token operator">:</span> <span class="token string">"发病机制"</span><span class="token punctuation">,</span> <span class="token property">"object_type"</span><span class="token operator">:</span> <span class="token string">"社会学"</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span>&amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"subject_type"</span><span class="token operator">:</span> <span class="token string">"疾病"</span><span class="token punctuation">,</span> <span class="token property">"predicate"</span><span class="token operator">:</span> <span class="token string">"病理生理"</span><span class="token punctuation">,</span> <span class="token property">"object_type"</span><span class="token operator">:</span> <span class="token string">"社会学"</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span>&amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"subject_type"</span><span class="token operator">:</span> <span class="token string">"疾病"</span><span class="token punctuation">,</span> <span class="token property">"predicate"</span><span class="token operator">:</span> <span class="token string">"药物治疗"</span><span class="token punctuation">,</span> <span class="token property">"object_type"</span><span class="token operator">:</span> <span class="token string">"药物"</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span>&amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"subject_type"</span><span class="token operator">:</span> <span class="token string">"药物"</span><span class="token punctuation">,</span> <span class="token property">"predicate"</span><span class="token operator">:</span> <span class="token string">"同义词"</span><span class="token punctuation">,</span> <span class="token property">"object_type"</span><span class="token operator">:</span> <span class="token string">"药物"</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span>&amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"subject_type"</span><span class="token operator">:</span> <span class="token string">"疾病"</span><span class="token punctuation">,</span> <span class="token property">"predicate"</span><span class="token operator">:</span> <span class="token string">"发病部位"</span><span class="token punctuation">,</span> <span class="token property">"object_type"</span><span class="token operator">:</span> <span class="token string">"部位"</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span>&amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"subject_type"</span><span class="token operator">:</span> <span class="token string">"疾病"</span><span class="token punctuation">,</span> <span class="token property">"predicate"</span><span class="token operator">:</span> <span class="token string">"转移部位"</span><span class="token punctuation">,</span> <span class="token property">"object_type"</span><span class="token operator">:</span> <span class="token string">"部位"</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span>&amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"subject_type"</span><span class="token operator">:</span> <span class="token string">"疾病"</span><span class="token punctuation">,</span> <span class="token property">"predicate"</span><span class="token operator">:</span> <span class="token string">"外侵部位"</span><span class="token punctuation">,</span> <span class="token property">"object_type"</span><span class="token operator">:</span> <span class="token string">"部位"</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span>&amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"subject_type"</span><span class="token operator">:</span> <span class="token string">"部位"</span><span class="token punctuation">,</span> <span class="token property">"predicate"</span><span class="token operator">:</span> <span class="token string">"同义词"</span><span class="token punctuation">,</span> <span class="token property">"object_type"</span><span class="token operator">:</span> <span class="token string">"部位"</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span>&amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"subject_type"</span><span class="token operator">:</span> <span class="token string">"疾病"</span><span class="token punctuation">,</span> <span class="token property">"predicate"</span><span class="token operator">:</span> <span class="token string">"预后状况"</span><span class="token punctuation">,</span> <span class="token property">"object_type"</span><span class="token operator">:</span> <span class="token string">"预后"</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span>&amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"subject_type"</span><span class="token operator">:</span> <span class="token string">"疾病"</span><span class="token punctuation">,</span> <span class="token property">"predicate"</span><span class="token operator">:</span> <span class="token string">"预后生存率"</span><span class="token punctuation">,</span> <span class="token property">"object_type"</span><span class="token operator">:</span> <span class="token string">"预后"</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span></code></pre><p>展示了数据集中包含的53个schema，包含10种同义词子关系，43种其他子关系。同一个predicate可能有多个subject_type和object_type，这在对predicate分类的时候要注意！</p><p><strong>数据集</strong></p><p>数据集中的语料来自医学教材和临床实践等医学文本。数据集划分方式：训练集（17,924）、测试集Test1（4,482）、测试集Test2（5,602）</p><pre class=" language-json"><code class="language-json">&amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"text"</span><span class="token operator">:</span> <span class="token string">"慢性胰腺炎@### 低剂量放射 自1964年起，有几项病例系列报道称外照射 (5-50Gy) 可以有效改善慢性胰腺炎患者的疼痛症状。慢性胰腺炎@从概念上讲，外照射可以起到抗炎和止痛作用，并且已经开始被用于非肿瘤性疼痛的治疗。"</span><span class="token punctuation">,</span> <span class="token property">"spo_list"</span><span class="token operator">:</span> <span class="token punctuation">[</span>&amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"Combined"</span><span class="token operator">:</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token property">"predicate"</span><span class="token operator">:</span> <span class="token string">"放射治疗"</span><span class="token punctuation">,</span> <span class="token property">"subject"</span><span class="token operator">:</span> <span class="token string">"慢性胰腺炎"</span><span class="token punctuation">,</span> <span class="token property">"subject_type"</span><span class="token operator">:</span> <span class="token string">"疾病"</span><span class="token punctuation">,</span> <span class="token property">"object"</span><span class="token operator">:</span> &amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"@value"</span><span class="token operator">:</span> <span class="token string">"外照射"</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">,</span> <span class="token property">"object_type"</span><span class="token operator">:</span> &amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"@value"</span><span class="token operator">:</span> <span class="token string">"其他治疗"</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">,</span> &amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"Combined"</span><span class="token operator">:</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token property">"predicate"</span><span class="token operator">:</span> <span class="token string">"放射治疗"</span><span class="token punctuation">,</span> <span class="token property">"subject"</span><span class="token operator">:</span> <span class="token string">"非肿瘤性疼痛"</span><span class="token punctuation">,</span> <span class="token property">"subject_type"</span><span class="token operator">:</span> <span class="token string">"疾病"</span><span class="token punctuation">,</span> <span class="token property">"object"</span><span class="token operator">:</span> &amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"@value"</span><span class="token operator">:</span> <span class="token string">"外照射"</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">,</span> <span class="token property">"object_type"</span><span class="token operator">:</span> &amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"@value"</span><span class="token operator">:</span> <span class="token string">"其他治疗"</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">]</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span></code></pre><p><code>text</code>是文本，<code>Combined</code>这边没有用到，<code>predicate</code>谓语，也叫关系 , <code>subject</code>：主语,<code>subject_type</code>主语类型, <code>object</code></p><p>宾语,<code>object_type</code>宾语类型</p><h2 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h2><p>数据清晰并缓存到文件中</p><pre class=" language-python"><code class="language-python"> <span class="token keyword">def</span> <span class="token function">_read</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> filename<span class="token punctuation">,</span> data_type<span class="token punctuation">)</span><span class="token punctuation">:</span>        examples <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        <span class="token keyword">with</span> open<span class="token punctuation">(</span>filename<span class="token punctuation">,</span> <span class="token string">'r'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> fr<span class="token punctuation">:</span>            p_id <span class="token operator">=</span> <span class="token number">0</span>            <span class="token keyword">for</span> line <span class="token keyword">in</span> tqdm<span class="token punctuation">(</span>fr<span class="token punctuation">.</span>readlines<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                p_id <span class="token operator">+=</span> <span class="token number">1</span>                src_data <span class="token operator">=</span> json<span class="token punctuation">.</span>loads<span class="token punctuation">(</span>line<span class="token punctuation">)</span>                text_raw <span class="token operator">=</span> src_data<span class="token punctuation">[</span><span class="token string">'text'</span><span class="token punctuation">]</span>                text_raw <span class="token operator">=</span> text_raw<span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">'®'</span><span class="token punctuation">,</span> <span class="token string">''</span><span class="token punctuation">)</span>                text_raw <span class="token operator">=</span> text_raw<span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">'◆'</span><span class="token punctuation">,</span> <span class="token string">''</span><span class="token punctuation">)</span>                text_raw <span class="token operator">=</span> text_raw<span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">,</span> <span class="token string">""</span><span class="token punctuation">)</span>                tokens<span class="token punctuation">,</span> tok_to_orig_start_index<span class="token punctuation">,</span> tok_to_orig_end_index <span class="token operator">=</span> covert_to_tokens<span class="token punctuation">(</span>text_raw<span class="token punctuation">,</span> self<span class="token punctuation">.</span>tokenizer<span class="token punctuation">,</span> self<span class="token punctuation">.</span>max_seq_length<span class="token punctuation">,</span> return_orig_index<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>                tokens <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"[CLS]"</span><span class="token punctuation">]</span> <span class="token operator">+</span> tokens <span class="token operator">+</span> <span class="token punctuation">[</span><span class="token string">"[SEP]"</span><span class="token punctuation">]</span>                sub_po_dict<span class="token punctuation">,</span> sub_ent_list<span class="token punctuation">,</span> spo_list <span class="token operator">=</span> dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> list<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> list<span class="token punctuation">(</span><span class="token punctuation">)</span>                <span class="token keyword">if</span> <span class="token string">'spo_list'</span> <span class="token operator">not</span> <span class="token keyword">in</span> src_data<span class="token punctuation">:</span>                    examples<span class="token punctuation">.</span>append<span class="token punctuation">(</span>                        Example<span class="token punctuation">(</span>                            p_id<span class="token operator">=</span>p_id<span class="token punctuation">,</span>                            raw_text<span class="token operator">=</span>src_data<span class="token punctuation">[</span><span class="token string">'text'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                            context<span class="token operator">=</span>text_raw<span class="token punctuation">,</span>                            tok_to_orig_start_index<span class="token operator">=</span>tok_to_orig_start_index<span class="token punctuation">,</span>                            tok_to_orig_end_index<span class="token operator">=</span>tok_to_orig_end_index<span class="token punctuation">,</span>                            bert_tokens<span class="token operator">=</span>tokens<span class="token punctuation">,</span>                            sub_entity_list<span class="token operator">=</span>None<span class="token punctuation">,</span>                            gold_answer<span class="token operator">=</span>None<span class="token punctuation">,</span>                            spoes<span class="token operator">=</span>None                        <span class="token punctuation">)</span><span class="token punctuation">)</span>                <span class="token keyword">else</span><span class="token punctuation">:</span>                    spoes <span class="token operator">=</span> <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;&amp;#125;</span>                    <span class="token keyword">for</span> spo <span class="token keyword">in</span> src_data<span class="token punctuation">[</span><span class="token string">'spo_list'</span><span class="token punctuation">]</span><span class="token punctuation">:</span>                        spo_dict <span class="token operator">=</span> dict<span class="token punctuation">(</span><span class="token punctuation">)</span>                        <span class="token keyword">for</span> spo_object <span class="token keyword">in</span> spo<span class="token punctuation">[</span><span class="token string">'object'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                            label <span class="token operator">=</span> spo<span class="token punctuation">[</span><span class="token string">'subject_type'</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token string">"|||"</span> <span class="token operator">+</span> spo<span class="token punctuation">[</span><span class="token string">'predicate'</span><span class="token punctuation">]</span>                            spo_dict<span class="token punctuation">[</span>self<span class="token punctuation">.</span>spo_conf<span class="token punctuation">[</span>label<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> spo<span class="token punctuation">[</span><span class="token string">'object'</span><span class="token punctuation">]</span><span class="token punctuation">[</span>spo_object<span class="token punctuation">]</span>                        <span class="token keyword">for</span> spo_object <span class="token keyword">in</span> spo<span class="token punctuation">[</span><span class="token string">'object'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                            <span class="token comment" spellcheck="true"># assign relation label</span>                            predicate_label <span class="token operator">=</span> self<span class="token punctuation">.</span>spo_conf<span class="token punctuation">[</span>spo<span class="token punctuation">[</span><span class="token string">'subject_type'</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token string">"|||"</span> <span class="token operator">+</span> spo<span class="token punctuation">[</span><span class="token string">'predicate'</span><span class="token punctuation">]</span><span class="token punctuation">]</span>                            subject_sub_tokens <span class="token operator">=</span> covert_to_tokens<span class="token punctuation">(</span>spo<span class="token punctuation">[</span><span class="token string">'subject'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>tokenizer<span class="token punctuation">,</span> self<span class="token punctuation">.</span>max_seq_length<span class="token punctuation">)</span>                            object_sub_tokens <span class="token operator">=</span> covert_to_tokens<span class="token punctuation">(</span>spo<span class="token punctuation">[</span><span class="token string">'object'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'@value'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>tokenizer<span class="token punctuation">,</span> self<span class="token punctuation">.</span>max_seq_length<span class="token punctuation">)</span>                            sub_ent_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>spo<span class="token punctuation">[</span><span class="token string">'subject'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>                            subject_start<span class="token punctuation">,</span> object_start <span class="token operator">=</span> search_spo_index<span class="token punctuation">(</span>tokens<span class="token punctuation">,</span> subject_sub_tokens<span class="token punctuation">,</span> object_sub_tokens<span class="token punctuation">)</span>                            <span class="token keyword">if</span> subject_start <span class="token operator">==</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span>                                subject_start <span class="token operator">=</span> search<span class="token punctuation">(</span>subject_sub_tokens<span class="token punctuation">,</span> tokens<span class="token punctuation">)</span>                            <span class="token keyword">if</span> object_start <span class="token operator">==</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span>                                object_start <span class="token operator">=</span> search<span class="token punctuation">(</span>object_sub_tokens<span class="token punctuation">,</span> tokens<span class="token punctuation">)</span>                            <span class="token keyword">if</span> subject_start <span class="token operator">!=</span> <span class="token operator">-</span><span class="token number">1</span> <span class="token operator">and</span> object_start <span class="token operator">!=</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span>                                s <span class="token operator">=</span> <span class="token punctuation">(</span>subject_start<span class="token punctuation">,</span> subject_start <span class="token operator">+</span> len<span class="token punctuation">(</span>subject_sub_tokens<span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span>                                o <span class="token operator">=</span> <span class="token punctuation">(</span>object_start<span class="token punctuation">,</span> object_start <span class="token operator">+</span> len<span class="token punctuation">(</span>object_sub_tokens<span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">,</span> predicate_label<span class="token punctuation">)</span>                                <span class="token keyword">if</span> s <span class="token operator">not</span> <span class="token keyword">in</span> spoes<span class="token punctuation">:</span>                                    spoes<span class="token punctuation">[</span>s<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>                                spoes<span class="token punctuation">[</span>s<span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span>o<span class="token punctuation">)</span>                    examples<span class="token punctuation">.</span>append<span class="token punctuation">(</span>                        Example<span class="token punctuation">(</span>                            p_id<span class="token operator">=</span>p_id<span class="token punctuation">,</span>                            raw_text<span class="token operator">=</span>src_data<span class="token punctuation">[</span><span class="token string">'text'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                            context<span class="token operator">=</span>text_raw<span class="token punctuation">,</span>                            tok_to_orig_start_index<span class="token operator">=</span>tok_to_orig_start_index<span class="token punctuation">,</span>                            tok_to_orig_end_index<span class="token operator">=</span>tok_to_orig_end_index<span class="token punctuation">,</span>                            bert_tokens<span class="token operator">=</span>tokens<span class="token punctuation">,</span>                            sub_entity_list<span class="token operator">=</span>sub_ent_list<span class="token punctuation">,</span>                            gold_answer<span class="token operator">=</span>src_data<span class="token punctuation">[</span><span class="token string">'spo_list'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                            spoes<span class="token operator">=</span>spoes                        <span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># print('total gold num is &amp;#123;&amp;#125;'.format(gold_num))</span>        logging<span class="token punctuation">.</span>info<span class="token punctuation">(</span><span class="token string">"&amp;#123;&amp;#125; total size is  &amp;#123;&amp;#125; "</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>data_type<span class="token punctuation">,</span> len<span class="token punctuation">(</span>examples<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> examples</code></pre><p>数据的输入：</p><pre class=" language-python"><code class="language-python">input_ids <span class="token operator">-</span><span class="token operator">-</span><span class="token operator">></span> 文本的token_idsegment_ids<span class="token operator">-</span><span class="token operator">-</span><span class="token operator">></span>token_type_idstoken_type_ids<span class="token operator">-</span><span class="token operator">-</span><span class="token operator">></span>就是 token 对应的句子id，值为<span class="token number">0</span>或<span class="token number">1</span>（<span class="token number">0</span>表示对应的token属于第一句，<span class="token number">1</span>表示属于第二句）。subject_ids<span class="token punctuation">,</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">></span>subject<span class="token punctuation">[</span><span class="token punctuation">(</span>start<span class="token punctuation">,</span>end<span class="token punctuation">)</span><span class="token punctuation">]</span>位置subject_labels<span class="token operator">-</span><span class="token operator">></span>主语启示指针［batch_size<span class="token punctuation">,</span> seq_len<span class="token punctuation">,</span> <span class="token number">2</span>］object_labels<span class="token operator">></span>宾语＋谓语启示指针［batch_size<span class="token punctuation">,</span> seq_len<span class="token punctuation">,</span> num_classify<span class="token punctuation">,</span> <span class="token number">2</span>］</code></pre><p><img src="/2020/09/14/zhong-wen-yi-xue-wen-ben-shi-ti-guan-xi-chou-qu/1.png"></p><h2 id="模型部分"><a href="#模型部分" class="headerlink" title="模型部分"></a>模型部分</h2><p>理论参考：<a href="/2020/09/13/joint-extraction-of-entities-and-relations-based-on-a-novel-decomposition-strategy/" title="Joint-Extraction-of-Entities-and-Relations-Based-on-a-Novel-Decomposition-Strategy">Joint-Extraction-of-Entities-and-Relations-Based-on-a-Novel-Decomposition-Strategy</a></p><pre><code>bert_encoder = self.bert(passage_ids, token_type_ids=segment_ids, attention_mask=mask)[0] #[batch_size, seq_len, hidden_site]        if not is_eval:            sub_start_encoder = batch_gather(bert_encoder, subject_ids[:, 0]) #[batch_size, hidden_site]            sub_end_encoder = batch_gather(bert_encoder, subject_ids[:, 1]) #[batch_size, hidden_site]            subject = torch.cat([sub_start_encoder, sub_end_encoder], 1) #[batch_size, 2*hidden_site]            #[batch_size,seq_len, hidden_site] 归一化            context_encoder = self.LayerNorm(bert_encoder, subject)             sub_preds = self.subject_dense(bert_encoder) #[batch_size,seq_len, 2]            ##[batch_size,seq_len, num_classify,2]            po_preds = self.po_dense(context_encoder).reshape(passage_ids.size(0), -1, self.classes_num, 2) </code></pre><p>模型很简单，fine-tune　bert后接# pointer net work</p><h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><pre class=" language-python"><code class="language-python"><span class="token operator">-</span><span class="token operator">-</span>input<span class="token operator">/</span>media<span class="token operator">/</span>daiyizheng<span class="token operator">/</span>SSD<span class="token operator">/</span>data<span class="token operator">/</span>CCKS<span class="token operator">/</span><span class="token number">2020</span><span class="token operator">/</span>ccks2020中文医学文本实体关系抽取<span class="token operator">/</span><span class="token operator">-</span><span class="token operator">-</span>outputoutput<span class="token operator">-</span><span class="token operator">-</span>bert_model<span class="token operator">/</span>media<span class="token operator">/</span>daiyizheng<span class="token operator">/</span>SSD<span class="token operator">/</span>bert<span class="token operator">-</span>pretrainmodel<span class="token operator">/</span>roberta<span class="token operator">/</span>chinese_roberta_wwm_large_ext_pytorch<span class="token operator">/</span><span class="token operator">-</span><span class="token operator">-</span>max_len<span class="token number">310</span><span class="token operator">-</span><span class="token operator">-</span>train_batch_size<span class="token number">3</span><span class="token operator">-</span><span class="token operator">-</span>learning_rate<span class="token number">2e</span><span class="token operator">-</span><span class="token number">5</span><span class="token operator">-</span><span class="token operator">-</span>epoch_num<span class="token number">100</span></code></pre><p>使用BERT,ROBERTA以及ROBERA_large,发现ROBERA_large训练效果最好(训练了２天，有点慢)</p><h2 id="评估"><a href="#评估" class="headerlink" title="评估"></a>评估</h2><pre class=" language-python"><code class="language-python">entity_em <span class="token operator">=</span> <span class="token number">0</span>        entity_pred_num <span class="token operator">=</span> <span class="token number">0</span>        entity_gold_num <span class="token operator">=</span> <span class="token number">0</span>        tp<span class="token punctuation">,</span> fp<span class="token punctuation">,</span> fn <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span>        <span class="token keyword">for</span> key <span class="token keyword">in</span> answer_dict<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            triple_gold <span class="token operator">=</span> eval_file<span class="token punctuation">[</span>key<span class="token punctuation">]</span><span class="token punctuation">.</span>gold_answer            entity_gold <span class="token operator">=</span> eval_file<span class="token punctuation">[</span>key<span class="token punctuation">]</span><span class="token punctuation">.</span>sub_entity_list            entity_pred <span class="token operator">=</span> answer_dict<span class="token punctuation">[</span>key<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>            triple_pred <span class="token operator">=</span> answer_dict<span class="token punctuation">[</span>key<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>            entity_em <span class="token operator">+=</span> len<span class="token punctuation">(</span>set<span class="token punctuation">(</span>entity_pred<span class="token punctuation">)</span> <span class="token operator">&amp;</span> set<span class="token punctuation">(</span>entity_gold<span class="token punctuation">)</span><span class="token punctuation">)</span>            entity_pred_num <span class="token operator">+=</span> len<span class="token punctuation">(</span>set<span class="token punctuation">(</span>entity_pred<span class="token punctuation">)</span><span class="token punctuation">)</span>            entity_gold_num <span class="token operator">+=</span> len<span class="token punctuation">(</span>set<span class="token punctuation">(</span>entity_gold<span class="token punctuation">)</span><span class="token punctuation">)</span>            tp_tmp<span class="token punctuation">,</span> fp_tmp<span class="token punctuation">,</span> fn_tmp <span class="token operator">=</span> calculate_metric<span class="token punctuation">(</span>                triple_gold<span class="token punctuation">,</span> triple_pred<span class="token punctuation">)</span>            tp <span class="token operator">+=</span> tp_tmp            fp <span class="token operator">+=</span> fp_tmp            fn <span class="token operator">+=</span> fn_tmp        p <span class="token operator">=</span> tp <span class="token operator">/</span> <span class="token punctuation">(</span>tp <span class="token operator">+</span> fp<span class="token punctuation">)</span> <span class="token keyword">if</span> tp <span class="token operator">+</span> fp <span class="token operator">!=</span> <span class="token number">0</span> <span class="token keyword">else</span> <span class="token number">0</span> <span class="token comment" spellcheck="true">#-->精确率</span>        r <span class="token operator">=</span> tp <span class="token operator">/</span> <span class="token punctuation">(</span>tp <span class="token operator">+</span> fn<span class="token punctuation">)</span> <span class="token keyword">if</span> tp <span class="token operator">+</span> fn <span class="token operator">!=</span> <span class="token number">0</span> <span class="token keyword">else</span> <span class="token number">0</span><span class="token comment" spellcheck="true">#召回率</span>        f <span class="token operator">=</span> <span class="token number">2</span> <span class="token operator">*</span> p <span class="token operator">*</span> r <span class="token operator">/</span> <span class="token punctuation">(</span>p <span class="token operator">+</span> r<span class="token punctuation">)</span> <span class="token keyword">if</span> p <span class="token operator">+</span> r <span class="token operator">!=</span> <span class="token number">0</span> <span class="token keyword">else</span> <span class="token number">0</span><span class="token comment" spellcheck="true"># f1-score</span></code></pre><p>最总结果：train f1:0.71  dev 0.59 test 0.56</p><h2 id="优化方向"><a href="#优化方向" class="headerlink" title="优化方向"></a>优化方向</h2><ol><li>fine-tune一些医学数据做预训练</li></ol><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> nlp </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 关系抽取 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TorchServe</title>
      <link href="/2020/09/14/torchserve/"/>
      <url>/2020/09/14/torchserve/</url>
      
        <content type="html"><![CDATA[<h1 id="TorchServe"><a href="#TorchServe" class="headerlink" title="TorchServe"></a>TorchServe</h1><h2 id="什么是TorchServe"><a href="#什么是TorchServe" class="headerlink" title="什么是TorchServe"></a>什么是TorchServe</h2><p>TorchServe是用于服务PyTorch模型的灵活易用的工具。它没有TFX的复杂性，因此，它没有提供那么多的功能。但是，这是完成工作的直接方法！</p><p><img src="/2020/09/14/torchserve/1.png"></p><p>​       TorchServe提供了一组必要的功能，例如服务器，模型存档器工具，API端点规范，日志记录，度量，批处理推断和模型快照等。它还提供了一系列高级功能，例如，对定制推理服务的支持，单元测试以及通过JMeter收集基准数据的简便方法。目前，它处于实验阶段，但在大多数情况下，它的工作原理就像是一种魅力。实际上，我们将在本文后面的部分中进行测试。</p><ul><li>支持<a href="https://huggingface.co/">HuggingFace</a>，这是一个深度学习库，其任务是为所有人提高NLP的使用程度，并提供文档和示例</li><li>支持<a href="https://github.com/NVIDIA/waveglow">Nvidia Waveglow</a>（一个基于流的语音合成生成网络，提供文档和示例）</li><li>与Model Zoo紧密集成，Model Zoo是一个深度学习注册表，其中包含根据流行的预训练模型创建的模型档案</li><li>支持<a href="https://aws.amazon.com/cloudformation/">AWS Cloud Forming</a>，可通过简单的配置文件（YAML或JSON）方便地在EC2实例上启动服务器</li><li>支持通过<a href="https://jiffyclub.github.io/snakeviz/">snakevize</a>分析器对TorchServe Python执行<a href="https://jiffyclub.github.io/snakeviz/">性能</a>分析，以详细报告执行时间</li><li>具有清晰说明的重构文档</li></ul><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><ul><li>JAVA 安装　JDK&gt;=8</li></ul><p>For Ubuntu:</p><pre><code>sudo apt-get install openjdk-11-jdk</code></pre><p>For CentOS:</p><pre><code>openjdk-11-jdksudo yum install java-11-openjdk</code></pre><p>For macOS:</p><pre><code>brew tap AdoptOpenJDK/openjdkbrew cask install adoptopenjdk11</code></pre><p><code>torch</code>默认情况下，不会安装TorchServe 。该项目中的大多数示例都需要使用Torch。</p><ul><li>For virtualenv</li></ul><pre><code>#For CPU/GPUpip install torch torchvision torchtext</code></pre><ul><li>For conda</li></ul><pre><code>#For CPUconda install -c pytorch -c powerai pytorch torchtext torchvision#For GPUconda install -c pytorch -c powerai pytorch torchtext torchvision cudatoolkit=10.1</code></pre><h3 id="pip-安装"><a href="#pip-安装" class="headerlink" title="pip 安装"></a>pip 安装</h3><p><strong>From PyTorch official repositories</strong></p><pre><code>pip install -f https://download.pytorch.org/whl/torch_stable.html torchserve torch-model-archiver</code></pre><h3 id="conda安装"><a href="#conda安装" class="headerlink" title="conda安装"></a>conda安装</h3><pre><code>conda install -c pytorch torchserve torch-model-archiver</code></pre><h3 id="Docker安装"><a href="#Docker安装" class="headerlink" title="Docker安装"></a>Docker安装</h3><h3 id="CPU-only"><a href="#CPU-only" class="headerlink" title="CPU only"></a>CPU only</h3><p>For specific versions use:</p><pre><code>docker run --rm -it -p 8080:8080 -p 8081:8081 pytorch/torchserve:0.1-cpu</code></pre><p>For the latest version you can use the <code>latest</code> tag:</p><pre><code>docker run --rm -it -p 8080:8080 -p 8081:8081 pytorch/torchserve:latest</code></pre><h3 id="GPU"><a href="#GPU" class="headerlink" title="GPU"></a>GPU</h3><p>For specific versions use:</p><pre><code>docker run --rm -it --gpus all -p 8080:8080 -p 8081:8081 pytorch/torchserve:0.1-cuda10.1-cudnn7-runtime</code></pre><p>For the latest version you can use the <code>gpu-latest</code> tag:</p><pre><code>docker run --rm -it --gpus all -p 8080:8080 -p 8081:8081 pytorch/torchserve:latest-gpu</code></pre><blockquote><p>个人推荐使用Docker安装，省去安装环境的麻烦！</p></blockquote><h2 id="示例一"><a href="#示例一" class="headerlink" title="示例一"></a>示例一</h2><p>安装TorchServe，请按照<a href="https://github.com/pytorch/serve/blob/master/README.md#installing-torchserve-with-pip">GitHub上</a>的说明进行操作</p><ol><li>下载TorchServe存储库以访问示例。运行以下代码：</li></ol><pre class=" language-bash"><code class="language-bash"><span class="token function">mkdir</span> torchserve-examples<span class="token function">cd</span> torchserve-examples<span class="token function">git</span> clone https://github.com/pytorch/serve.git</code></pre><ol start="2"><li>从官方PyTorch模型库中下载DenseNet图像分类模型。运行以下代码：</li></ol><pre class=" language-bash"><code class="language-bash"><span class="token function">wget</span> https://download.pytorch.org/models/densenet161-8d451a50.pth</code></pre><ol start="3"><li>将模型从PyTorch转换为TorchServe格式。TorchServe使用扩展名为.mar的模型存档格式。.mar文件使用<code>state_dict</code>（将每个层映射到其参数张量的字典对象）打包模型检查点或模型定义文件。您可以使用<code>torch-model-archiver</code>TorchServe中的工具来创建.mar文件。您无需创建自定义处理程序，只需指定即可<code>--handler image_classifier</code>，它会自动为您设置处理程序。现在您已经有了.mar文件，请使用TorchServe托管它。运行以下代码：</li></ol><pre class=" language-bash"><code class="language-bash">torch-model-archiver --model-name densenet161 \--version 1.0 --model-file serve/examples/image_classifier/densenet_161/model.py \--serialized-file densenet161-8d451a50.pth \--extra-files serve/examples/image_classifier/index_to_name.json \--handler image_classifier<span class="token function">ls</span> *.mar</code></pre><p><strong>handler</strong> 自定义</p><p>您可以通过使类具有任何名称来创建自定义处理程序，但是该类必须具有<code>initialize</code>和<code>handle</code>方法。</p><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">ModelHandler</span><span class="token punctuation">(</span>object<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""    A custom model handler implementation.    """</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>_context <span class="token operator">=</span> None        self<span class="token punctuation">.</span>initialized <span class="token operator">=</span> <span class="token boolean">False</span>        self<span class="token punctuation">.</span>model <span class="token operator">=</span> None        self<span class="token punctuation">.</span>device <span class="token operator">=</span> None    <span class="token keyword">def</span> <span class="token function">initialize</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> context<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""        Invoke by torchserve for loading a model        :param context: context contains model server system properties        :return:        """</span>        <span class="token comment" spellcheck="true">#  load the model</span>        self<span class="token punctuation">.</span>manifest <span class="token operator">=</span> context<span class="token punctuation">.</span>manifest        properties <span class="token operator">=</span> context<span class="token punctuation">.</span>system_properties        model_dir <span class="token operator">=</span> properties<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">"model_dir"</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cuda:"</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>properties<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">"gpu_id"</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">"cpu"</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Read model serialize/pt file</span>        serialized_file <span class="token operator">=</span> self<span class="token punctuation">.</span>manifest<span class="token punctuation">[</span><span class="token string">'model'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'serializedFile'</span><span class="token punctuation">]</span>        model_pt_path <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>model_dir<span class="token punctuation">,</span> serialized_file<span class="token punctuation">)</span>        <span class="token keyword">if</span> <span class="token operator">not</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>isfile<span class="token punctuation">(</span>model_pt_path<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">raise</span> RuntimeError<span class="token punctuation">(</span><span class="token string">"Missing the model.pt file"</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>model <span class="token operator">=</span> torch<span class="token punctuation">.</span>jit<span class="token punctuation">.</span>load<span class="token punctuation">(</span>model_pt_path<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>initialized <span class="token operator">=</span> <span class="token boolean">True</span>    <span class="token keyword">def</span> <span class="token function">handle</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> data<span class="token punctuation">,</span> context<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""        Invoke by TorchServe for prediction request.        Do pre-processing of data, prediction using model and postprocessing of prediciton output        :param data: Input data for prediction        :param context: Initial context contains model server system properties.        :return: prediction output        """</span>        pred_out <span class="token operator">=</span> self<span class="token punctuation">.</span>model<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>data<span class="token punctuation">)</span>        <span class="token keyword">return</span> pred_out</code></pre><p><strong>handler高级自定义处理程序</strong></p><p>从头开始编写自定义处理程序</p><p><em>通常，您应该从BaseHandler派生，并且只重写行为需要更改的方法！</em>如您在示例中所见，大多数情况下，您只需要覆盖<code>preprocess</code>或<code>postprocess</code></p><p>尽管如此，您仍然可以从头开始编写一个类。下面是一个例子。基本上，它遵循典型的Init-Pre-Infer-Post模式创建可维护的自定义处理程序。</p><pre class=" language-python"><code class="language-python"> custom handler file<span class="token comment" spellcheck="true"># model_handler.py</span><span class="token triple-quoted-string string">"""ModelHandler defines a custom model handler."""</span><span class="token keyword">from</span> ts<span class="token punctuation">.</span>torch_handler<span class="token punctuation">.</span>base_handler <span class="token keyword">import</span> BaseHandler<span class="token keyword">class</span> <span class="token class-name">ModelHandler</span><span class="token punctuation">(</span>BaseHandler<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""    A custom model handler implementation.    """</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>_context <span class="token operator">=</span> None        self<span class="token punctuation">.</span>initialized <span class="token operator">=</span> <span class="token boolean">False</span>    <span class="token keyword">def</span> <span class="token function">initialize</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> context<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""        Initialize model. This will be called during model loading time        :param context: Initial context contains model server system properties.        :return:        """</span>        self<span class="token punctuation">.</span>_context <span class="token operator">=</span> context        self<span class="token punctuation">.</span>initialized <span class="token operator">=</span> <span class="token boolean">True</span>        <span class="token comment" spellcheck="true">#  load the model, refer 'custom handler class' above for details</span>    <span class="token keyword">def</span> <span class="token function">preprocess</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> data<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""        Transform raw input into model input data.        :param batch: list of raw requests, should match batch size        :return: list of preprocessed model input data        """</span>        <span class="token comment" spellcheck="true"># Take the input data and make it inference ready</span>        preprocessed_data <span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">"data"</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> preprocessed_data <span class="token keyword">is</span> None<span class="token punctuation">:</span>            preprocessed_data <span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">"body"</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> preprocessed_data    <span class="token keyword">def</span> <span class="token function">inference</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> model_input<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""        Internal inference methods        :param model_input: transformed model input data        :return: list of inference output in NDArray        """</span>        <span class="token comment" spellcheck="true"># Do some inference call to engine here and return output</span>        model_output <span class="token operator">=</span> self<span class="token punctuation">.</span>model<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>model_input<span class="token punctuation">)</span>        <span class="token keyword">return</span> model_output    <span class="token keyword">def</span> <span class="token function">postprocess</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> inference_output<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""        Return inference result.        :param inference_output: list of inference output        :return: list of predict results        """</span>        <span class="token comment" spellcheck="true"># Take output from network and post-process to desired format</span>        postprocess_output <span class="token operator">=</span> inference_output        <span class="token keyword">return</span> postprocess_output    <span class="token keyword">def</span> <span class="token function">handle</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> data<span class="token punctuation">,</span> context<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""        Invoke by TorchServe for prediction request.        Do pre-processing of data, prediction using model and postprocessing of prediciton output        :param data: Input data for prediction        :param context: Initial context contains model server system properties.        :return: prediction output        """</span>        model_input <span class="token operator">=</span> self<span class="token punctuation">.</span>preprocess<span class="token punctuation">(</span>data<span class="token punctuation">)</span>        model_output <span class="token operator">=</span> self<span class="token punctuation">.</span>inference<span class="token punctuation">(</span>model_input<span class="token punctuation">)</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>postprocess<span class="token punctuation">(</span>model_output<span class="token punctuation">)</span></code></pre><ol><li>您收到以下输出：</li></ol><pre class=" language-bash"><code class="language-bash">densenet161.mar serve</code></pre><p>​    主模型。运行以下代码：</p><pre class=" language-bash"><code class="language-bash"><span class="token function">mkdir</span> model_store<span class="token function">mv</span> densenet161.mar model_store/torchserve --start --model-store model_store --models densenet161<span class="token operator">=</span>densenet161.mar <span class="token comment" spellcheck="true">#非docker安装</span><span class="token comment" spellcheck="true">#docker安装　推荐</span>docker run --rm -it -p 3000:8080 -p 3001:8081 -v <span class="token variable"><span class="token variable">$(</span><span class="token function">pwd</span><span class="token variable">)</span></span>/model_store:/home/model-server/model-store pytorch/torchserve:0.1-cpu  torchserve --start --model-store model_store --models densenet161<span class="token operator">=</span>densenet161.mar <span class="token operator">&amp;</span><span class="token operator">></span>server.log 2<span class="token operator">></span><span class="token operator">&amp;</span>1</code></pre><blockquote><p> 扩展：<code>torch-model-archiver --model-name resnet34 \--version 1.0 \--serialized-file resnet34.pt \--extra-files ./index_to_name.json,./MyHandler.py \--handler my_handler.py \--export-path model-store -f</code></p><p>变量<code>--model-name</code>定义了模型的最终名称。这是非常重要的，因为它将是endpoint的名称空间，负责进行预测。你还可以指定一个<code>--version</code>。<code>--serialized-file</code>指向我们之前创建的存储的 <code>.pt</code>模型。<code>--handler</code> 是一个python文件，我们在其中调用我们的自定义handler。</p><p>它暴露了一个<code>handle</code>函数，我们从该函数调用自定义handler中的方法。你可以使用默认名称来使用默认handler(例如，<code>--handler image_classifier</code>)。</p><p>在<code>--extra-files</code>中，你需要将路径传递给你的handlers正在使用的所有文件。在本例中，我们必须向<code>.json</code>文件中添加路径。使用所有人类可读标签名称，并在<code>MyHandler.py</code> 中定义每个类别。</p><p>如果你传递一个<code>index_to_name.json</code>文件，它将自动加载到handler ，并通过<code>self.mapping</code>访问。</p><p><code>--export-path</code>就是 <code>.mar</code>存放的地方，我还添加了<code>-f</code>来覆盖原有的文件。</p><p>如果一切顺利的话，你可以看到<code>resnet34.mar</code>存放在<code>./model-store</code>路径中。</p></blockquote><p>参考：<a href="https://jishuin.proginn.com/p/763bfbd2c54d">https://jishuin.proginn.com/p/763bfbd2c54d</a></p><p>注意：(1)docker rm只是删除容器，rm -v 不仅删除容器（如果容器有使用卷，卷也会进行相应的删除）。不能与-d一起使用,注意调用内部接口3001,外部接口3000</p><ol><li>调用</li></ol><pre class=" language-python"><code class="language-python">curl <span class="token operator">-</span>X POST http<span class="token punctuation">:</span><span class="token operator">//</span><span class="token number">127.0</span><span class="token punctuation">.</span><span class="token number">0.1</span><span class="token punctuation">:</span><span class="token number">3000</span><span class="token operator">/</span>predictions<span class="token operator">/</span>densenet161 <span class="token operator">-</span>T <span class="token punctuation">.</span><span class="token operator">/</span>examples<span class="token operator">/</span>image_classifier<span class="token operator">/</span>kitten<span class="token punctuation">.</span>jpg</code></pre><p>curl知识　请参考<a href="/2020/09/15/curl-ming-ling/" title="curl命令">curl命令</a></p><p>您收到以下输出：</p><pre class=" language-json"><code class="language-json"><span class="token punctuation">[</span> &amp;#<span class="token number">123</span><span class="token punctuation">;</span> <span class="token property">"tiger_cat"</span><span class="token operator">:</span> <span class="token number">0.4693356156349182</span> &amp;#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">,</span> &amp;#<span class="token number">123</span><span class="token punctuation">;</span> <span class="token property">"tabby"</span><span class="token operator">:</span> <span class="token number">0.46338796615600586</span> &amp;#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">,</span> &amp;#<span class="token number">123</span><span class="token punctuation">;</span> <span class="token property">"Egyptian_cat"</span><span class="token operator">:</span> <span class="token number">0.06456131488084793</span> &amp;#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">,</span> &amp;#<span class="token number">123</span><span class="token punctuation">;</span> <span class="token property">"lynx"</span><span class="token operator">:</span> <span class="token number">0.0012828155886381865</span> &amp;#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">,</span> &amp;#<span class="token number">123</span><span class="token punctuation">;</span> <span class="token property">"plastic_bag"</span><span class="token operator">:</span> <span class="token number">0.00023323005007114261</span> &amp;#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">]</span></code></pre><h2 id="托管多个模型并扩展工作"><a href="#托管多个模型并扩展工作" class="headerlink" title="托管多个模型并扩展工作"></a>托管多个模型并扩展工作</h2><p>TorchServe提供了一个管理API，用于列出已注册的模型，将新模型注册到现有服务器，取消注册当前模型，增加或减少每个模型的工作人员数量，描述模型的状态，添加版本以及设置默认版本。Management API正在端口8081上侦听，并且默认情况下只能从localhost访问，但是您可以更改默认行为。</p><p>要注册新模型，请完成以下步骤：</p><ol><li><p>使用以下代码下载新模型：</p><pre class=" language-bash"><code class="language-bash"><span class="token function">wget</span> https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pthtorch-model-archiver --model-name fastrcnn --version 1.0 \--model-file serve/examples/object_detector/fast-rcnn/model.py \--serialized-file fasterrcnn_resnet50_fpn_coco-258fb6c6.pth \--handler object_detector \--extra-files serve/examples/object_detector/index_to_name.json<span class="token function">mv</span> fastrcnn.mar model-store/</code></pre></li><li><p>使用以下代码注册新模型：</p><pre class=" language-bash"><code class="language-bash">curl -X POST <span class="token string">"http://localhost:3001/models?url=fastrcnn.mar"</span></code></pre><p>您收到以下输出：</p><pre class=" language-json"><code class="language-json">&amp;#<span class="token number">123</span><span class="token punctuation">;</span> <span class="token property">"status"</span><span class="token operator">:</span> <span class="token string">"Model \"fastrcnn\" registered"</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span></code></pre><p>您还可以使用以下代码查询已注册模型的列表：</p><pre class=" language-bash"><code class="language-bash">curl <span class="token string">"http://localhost:3001/models"</span></code></pre><p>您收到以下输出：</p><pre class=" language-json"><code class="language-json">&amp;#<span class="token number">123</span><span class="token punctuation">;</span>    <span class="token property">"models"</span><span class="token operator">:</span> <span class="token punctuation">[</span>        &amp;#<span class="token number">123</span><span class="token punctuation">;</span>            <span class="token property">"modelName"</span><span class="token operator">:</span> <span class="token string">"densenet161"</span><span class="token punctuation">,</span>            <span class="token property">"modelUrl"</span><span class="token operator">:</span> <span class="token string">"densenet161.mar"</span>        &amp;#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">,</span>        &amp;#<span class="token number">123</span><span class="token punctuation">;</span>            <span class="token property">"modelName"</span><span class="token operator">:</span> <span class="token string">"fastrcnn"</span><span class="token punctuation">,</span>            <span class="token property">"modelUrl"</span><span class="token operator">:</span> <span class="token string">"fastrcnn.mar"</span>        &amp;#<span class="token number">125</span><span class="token punctuation">;</span>    <span class="token punctuation">]</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span></code></pre></li><li><p>缩放模型工人。没有为新模型分配工人，因此请使用以下代码设置最小工人数：</p><pre class=" language-bash"><code class="language-bash">curl -v -X PUT <span class="token string">"http://localhost:3000/models/fastrcnn?min_worker=2"</span>curl <span class="token string">"http://localhost:3000/models/fastrcnn"</span></code></pre><p>您收到以下输出：</p><pre class=" language-json"><code class="language-json"><span class="token punctuation">[</span>  &amp;#<span class="token number">123</span><span class="token punctuation">;</span>    <span class="token property">"modelName"</span><span class="token operator">:</span> <span class="token string">"fastrcnn"</span><span class="token punctuation">,</span>    <span class="token property">"modelVersion"</span><span class="token operator">:</span> <span class="token string">"1.0"</span><span class="token punctuation">,</span>    <span class="token property">"modelUrl"</span><span class="token operator">:</span> <span class="token string">"fastrcnn.mar"</span><span class="token punctuation">,</span>    <span class="token property">"runtime"</span><span class="token operator">:</span> <span class="token string">"python"</span><span class="token punctuation">,</span>    <span class="token property">"minWorkers"</span><span class="token operator">:</span> <span class="token number">2</span><span class="token punctuation">,</span>    <span class="token property">"maxWorkers"</span><span class="token operator">:</span> <span class="token number">2</span><span class="token punctuation">,</span>    <span class="token property">"batchSize"</span><span class="token operator">:</span> <span class="token number">1</span><span class="token punctuation">,</span>    <span class="token property">"maxBatchDelay"</span><span class="token operator">:</span> <span class="token number">100</span><span class="token punctuation">,</span>……</code></pre><p>如果您的模型托管在具有许多内核的CPU上，例如具有96个vCPU的c5.24xlarge EC2实例，则可以使用上述方法轻松扩展线程数。</p></li><li><p>使用以下代码注销模型：</p><pre class=" language-bash"><code class="language-bash">curl -X DELETE http://localhost:3000/models/fastrcnn/</code></pre></li><li><p>要对模型进行版本控制，请在调用时<code>torch-model-archiver</code>，将版本号传递给<code>--version</code>请参见以下代码：</p><p>请参见以下代码：</p><pre class=" language-bash"><code class="language-bash">torch-model-archiver --model-name fastrcnn --version 1.0 <span class="token punctuation">..</span>.</code></pre></li></ol><h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><p>​        TorchServe是一种用于服务PyTorch模型的灵活易用的工具。提供的功能以及如何利用其实用程序通过REST端点为PyTorch模型提供服务。通过MNIST示例对其进行了测试。为了进一步了解，请深入阅读<a href="https://pytorch.org/serve/">文档</a>和<a href="https://github.com/pytorch/serve/tree/master/examples">官方示例</a>。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 运维 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 部署 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>BERT for Joint Intent Classification and Slot Filling</title>
      <link href="/2020/09/14/bert-for-joint-intent-classification-and-slot-filling/"/>
      <url>/2020/09/14/bert-for-joint-intent-classification-and-slot-filling/</url>
      
        <content type="html"><![CDATA[<h1 id="BERT-for-Joint-Intent-Classification-and-Slot-Filling"><a href="#BERT-for-Joint-Intent-Classification-and-Slot-Filling" class="headerlink" title="BERT for Joint Intent Classification and Slot Filling"></a>BERT for Joint Intent Classification and Slot Filling</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>​      意图分类和槽位填充是自然语言理解的两个基本任务。 他们经常遭受小规模的人类标签训练数据的折磨，导致泛化能力差，尤其是对于稀有单词。 最近，一种新的语言表示模型BERT（来自Transformers的双向编码器表示）有助于在大型未标记的语料库上进行预训练深层的双向表示，并在简单的操作后创建了用于各种自然语言处理任务的最新模型 微调。 但是，在探索BERT以获得自然语言理解方面并没有付出太多努力。 在这项工作中，提出了一种基于BERT的联合意图分类和槽位填充模型。 实验结果表明，与基于注意力的递归神经网络模型和槽位门控模型相比，我们提出的模型在多个公共基准数据集上的意图分类准确性，槽位填充F1和句子级语义框架准确性均得到了显着提高。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>​     近年来，谷歌Home、Amazon Echo、天猫精灵等多种智能音箱被部署并取得了巨大成功，实现了面向目标的对话，通过语音交互帮助用户完成任务。自然语言理解(NLU)对目标导向的口语对话系统的性能至关重要。NLU通常包括意图分类和插槽填充任务，目的是为用户话语形成语义解析。意图分类侧重于预测查询的意图，而插槽填充则提取语义概念。表1显示了一个用于用户查询的意图分类和插槽填充的例子。</p><p><img src="/2020/09/14/bert-for-joint-intent-classification-and-slot-filling/1.png"></p><p>​    意图分类是一个预测意图标签yi的分类问题，而插槽填充是一个序列标记任务，用插槽标签序列$ys= (y ^s _1, y ^s _2，···，y ^s _T)$标记输入单词序列$x= (x _1, x _2，···x _T)$。基于递归神经网络(RNN)的方法，特别是递归单元(GRU)和长短期记忆(LSTM)模型，已经在意图分类和槽填充方面取得了最先进的性能。最近，人们提出了几种用于意图分类和slot填充的联合学习方法，以开发和建模两个任务之间的依赖关系，改进独立模型的性能。之前的研究表明，注意力机制有助于rnns处理长期依赖关系。为此，提出了基于注意的联合学习方法，并在联合意图分类和缝隙填充方面取得了较好的效果。</p><p>​       缺少用于NLU和其他自然语言处理（NLP）任务的人类标签数据会导致较差的泛化能力。 为了解决数据稀疏性挑战，建议使用大量技术来使用大量未注释的文本来训练通用语言表示模型，例如ELMo和生成式预训练Transformer (GPT)。 预训练的模型可以在NLP任务上进行微调，并且在特定于任务的注释数据已经取得了显著的改进。最近，(BERT)的双向编码器代表提出了一种预训练技术，并为各种NLP任务创建了最先进的模型，包括问题回答(SQuAD v1.1)、自然语言推断等。</p><p>　　然而，在探索BERT这项工作的技术贡献有两方面:1)探索BERT预训练模型，解决NLU泛化能力较差的问题;2)提出了一个基于BERT的联合意图分类和插槽填充模型，并证明了所提模型在多个公共基准数据集上，与基于注意力的RNN模型和槽门模型相比，在意图分类精度、插槽填充F1和句子级语义框架精度方面都有显著提高。</p><h2 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h2><p>​        根据意图类化和插槽填充是否分别联合建模，将NLU模型分为独立建模方法和联合建模方法</p><h2 id="Proposed-Approach"><a href="#Proposed-Approach" class="headerlink" title="Proposed Approach"></a>Proposed Approach</h2><p>​    首先简要描述了BERT模型，然后介绍了提出的基于BERT的联合模型。图1模型的高级视图。</p><p><img src="/2020/09/14/bert-for-joint-intent-classification-and-slot-filling/2.png"></p><h3 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h3><p>​        BERT的模型架构是基于原始变压器模型的多层双向变压器编码器。输入表示是WordPiece  embeddings  positional  embeddings,  和 segment  embedding的串联。特别地，对于单句分类和标记任务。插入一个特殊的嵌入分类([CLS])作为第一个token，添加一个特殊的token([SEP])作为最后的令牌。给定一个输入token序列$x= (x _1，…)$，则BERT的输出为$H= (h _1，…h _T)$。</p><p>​      对BERT模型进行了两种预处理策略的训练，分别是对大规模无标记文本进行掩模语言模型和下一句预测。预先训练的BERT模型提供了一个强有力的上下文相关的句子表示，通过微调过程，可以用于各种目标任务，例如意图分类和插槽填充，类似于它在其他NLP任务中的使用。</p><h3 id="Joint-Intent-Classification-and-Slot-Filling"><a href="#Joint-Intent-Classification-and-Slot-Filling" class="headerlink" title="Joint Intent Classification and Slot Filling"></a>Joint Intent Classification and Slot Filling</h3><p>​    BERT可以很容易地扩展为联合意图类化和槽填充模型。根据第一个特殊标记([CLS])的隐藏状态，表示h1，意图被预测为</p><p><img src="/2020/09/14/bert-for-joint-intent-classification-and-slot-filling/3.png"></p><p>​      对于插槽填充，其他标记$h _2…，h _T$的最终隐藏状态放入softmax层上分类槽填充标签。为了使该过程与WordPiece  tokenization一致，我们将每个tokenization的输入词输入到WordPiece tokenizer 中，并使用与第一个sub-token对应的隐藏状态作为softmax分类器的输入。</p><p><img src="/2020/09/14/bert-for-joint-intent-classification-and-slot-filling/5.png"></p><p>其中$h _n$为单词$x _n$的第一个子令牌对应的隐藏状态。</p><p><img src="/2020/09/14/bert-for-joint-intent-classification-and-slot-filling/6.png"></p><p>为了联合建模意图分类和填槽，目标被表述为</p><p><img src="/2020/09/14/bert-for-joint-intent-classification-and-slot-filling/7.png"></p><p>学习目标是使条件概率$(y _i, y _s|x)$最大化。该模型通过最小化交叉熵损失进行了端到端的微调。</p><h2 id="Conditional-Random-Field"><a href="#Conditional-Random-Field" class="headerlink" title="Conditional Random Field"></a>Conditional Random Field</h2><p>​       槽标签的预测依赖于对周围单词的预测。研究表明，采用条件随机域(CRF)等结构化预测模型可以提高填槽性能。通过为BiLSTM编码器添加CRF层改进了语义角色标记。这里，在联合BERT模型的基础上，研究了添加CRF来建模槽标签依赖的有效性。</p><h2 id=""><a href="#" class="headerlink" title=""></a></h2><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 论文 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Bert </tag>
            
            <tag> 分类 </tag>
            
            <tag> 槽填充 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Joint Extraction of Entities and Relations Based on a Novel Decomposition Strategy</title>
      <link href="/2020/09/13/joint-extraction-of-entities-and-relations-based-on-a-novel-decomposition-strategy/"/>
      <url>/2020/09/13/joint-extraction-of-entities-and-relations-based-on-a-novel-decomposition-strategy/</url>
      
        <content type="html"><![CDATA[<h1 id="基于一种新的分解策略的实体和关系的联合提取"><a href="#基于一种新的分解策略的实体和关系的联合提取" class="headerlink" title="基于一种新的分解策略的实体和关系的联合提取"></a>基于一种新的分解策略的实体和关系的联合提取</h1><p>Paper:  <a href="https://arxiv.org/pdf/1909.04273v3.pdf">https://arxiv.org/pdf/1909.04273v3.pdf</a></p><p>Code Torch <a href="https://github.com/yubowen-ph/JointER">https://github.com/yubowen-ph/JointER</a></p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>​        联合提取实体和关系的目的是通过单一的模型来检测实体对及其关系。以往的工作通常采用先提取后分类或统一标注的方式来解决这个问题。但是，这些方法在提取实体和关系的过程中要么存在冗余实体对，要么忽略了重要的内部结构。针对这些局限性，本文首先将联合抽取任务分解为两个相互关联的子任务，即HE抽取和TER抽 取。前者的子任务是区分所有可能涉及到目标关系的头实体，后者是识别每个提取的头实体对应的尾实体和关系。然后，基于我们提出的基于跳转的标记方案，将这两个子任务进一步分解为多个序列标记问题，并采用分层边界标记和多跨解码算法方便地解决这些问题。由于采用了合理的分解策略，我们的模型可以完全捕获不同步骤之间的语义相互依赖关系，并减少不相关实体对的干扰。实验结果表明，我们的方法比以前的工作性能提高了5.2%、5.9%和21.5% (F1得分)，在三个公共数据集上达到了新的水平。</p><h2 id="NTRODUCTION"><a href="#NTRODUCTION" class="headerlink" title="NTRODUCTION"></a>NTRODUCTION</h2><p>​       传统的流水线方法首先 识别实体，然后为每个可能的提取实体对选择一个关系。 这样的框架使任务易于执行，但却忽略了这两个子任务之间的潜在交互作用。 一种改进的方法是通过参数共享共同训练它们尽管显示出令人鼓舞的结果，但是这些“提取然后分类”方法仍然需要用于实体提取和关系分类的显式单独组件。 结果，它们的关系分类器可能会被冗余实体对所误导。这些模型总是忽略内部结构，例如头部实体中包含的依赖性，尾部性和关系。 众所周知，尾部实体和关系应取决于特定的实体。 换句话说，如果一个模型不能完全理解头实体的语义，那么提取相应的尾部实体和关系将是不可靠的。</p><p>​        联合任务分层分解为几个序列标记问题，其中部分标记捕获了不同方面的信息。（参见图1）</p><p><img src="/2020/09/13/joint-extraction-of-entities-and-relations-based-on-a-novel-decomposition-strategy/1.png"></p><p>​       从一句话开始，首先区分所有可能涉及到目标关系的候选头实体，然后为每个提取的头实体标注对应的尾实体和关系。我们将前一个子任务称为前端实体(HE)提取，后一个子任务称为尾部实体关系(TER)提取。这种 extract-then-label（ETL）可以通过将提取三元组的联合概率分解为条件概率$(h, r, t|S) =p(h|S)p(r, t|h, S)$来理解，其中$(h, r, t)$为句子$ S $中的三元组。以这种方式，TER提取器能够在标记尾实体和关系时考虑给定首实体的语义和位置信息，并且自然地，一个首实体可以与多个尾实体进行交互以形成 重叠的关系。 此外，与提取然后分类的方法相比，不再在第一步提取所有实体，仅识别可能参与目标三元组的头实体，从而减轻了冗余实体对的影响。</p><p>​     受提取性问题解答的启发，该问题通过预测其开始和结束索引来识别答案范围，进一步使用基于范围的标记方案分解HE和TER提取。 具体来说，对于HE提取，在每个首部实体的开始和结束位置标记实体类型。 对于TER提取，我们在与给定头部实体有关系的所有尾部实体的开始和结束位置处注释关系类型。为了增强边界位置之间的关联，提出了一个分层的边界标记器, 标签开始和结束位置用级联结构分别解码，并用多跨译码算法将它们解码在一起。通过这种方式，可以在统一的基于跨度的提取框架中对HE和TER提取进行建模，仅根据它们的先验知识和输出标签集进行区分。 总体而言，对于具有m个头部实体的句子，整个任务被分解为2 + 2m个序列标记子任务，前2 用于HE标签，另一个2m用于TER。从直觉上讲，与整个提取任务相比，单个子任务非常易于学习，这表明通过与共享的基础表示形式进行协作训练，它们可以约束学习问题并获得更好的总体结果。</p><p>​       三个公共数据集上评估了我们的方法：NYT-single，NYT-multi和WebNLG。提出的方法明显优于以前的正常，重叠和多关系提取方面的工作，将SOTA F1得分提高到59.0％（+5.2％），78.0％（+ 5.9％）和83.1％（+ 21.5％）。</p><h2 id="METHODOLOGY"><a href="#METHODOLOGY" class="headerlink" title="METHODOLOGY"></a>METHODOLOGY</h2><h3 id="Tagging-Scheme"><a href="#Tagging-Scheme" class="headerlink" title="Tagging Scheme"></a>Tagging Scheme</h3><p>​       首先考虑头部实体（HE）的提取。 如前一节所述，它被分解为两个序列标记子任务。 第一个子任务主要集中在识别一个头部实体的起始位置。 如果一个token是起始词，则将其标记为相应的实体类型，否则将其分配为标签“ O”（外部）。 相比之下，第二个子任务旨在标识一个头部实体的终止位置，并且具有类似的标记过程，只是实体类型被标记为作为结束词的token。对于每个识别出的头实体，TER提取还分解为两个序列标记子任务，利用跨度边界提取尾实体并同时预测关系。第一个序列标记子任务主要标记尾实体的开始字标记的关系类型，第二个子任务标记尾实体的结束字标记的关系类型。</p><p>​        图1举例说明了标记方案，其中“ United”，“ States”，“ Trump”，“ Queens”，“ New”和“ City”这三个词均与最终提取结果相关，因此用特殊标记 标签。 例如，单词“ Trump”是头部实体“ Trump”的第一个也是最后一个单词，因此在标记HE时，标记在开始和结束标记序列中都是PERSON。 对于TER提取，当给定的首实体为“特朗普”时，与想要的关系有关的有两个尾实体，即（”Trump”,PresidentOf, “United States”）和（(“Trump”, BornIn, “New YorkCity”），因此在开始标签序列中，“ United”和“ New”分别标记为President_Of和Born_In。 类似地，可以获得标记有 “States”和 “City” 的结束标签序列。 除此之外，与最终结果无关的其他单词都标记为“ O”。</p><p>​       标记方案与PA-LSTM完全不同。 对于单词词句子，PA-LSTM根据不同的查询位置构建不同的标签序列，而该模型将同一句子标记为　2 + 2×m　以识别所有重叠关系，其中头实体的数量和m &lt;&lt; n。 这意味着我们的模型更加节省时间和效率。 此外，它使用“ BIES”符号表示实体中token的位置，而我们仅预测开始和结束位置，而不会损失提取多词实体提及的能力。</p><h3 id="Hierarchical-Boundary-Tagger"><a href="#Hierarchical-Boundary-Tagger" class="headerlink" title="Hierarchical Boundary Tagger"></a>Hierarchical Boundary Tagger</h3><p>​    标记方案，利用统一的架构来提取HE和TER。 在本文中，将这种提取器包装到一个名为“层次边界标记器”（缩写为HBT）的通用模块中。 为了通用起见，不区分头和尾实体，在本小节中将它们统称为目标。 形式上，从句子S中提取带有标签l（目标实体的类型为头实体或关系类型为尾部实体）的目标t的概率被普遍建模为</p><p><img src="/2020/09/13/joint-extraction-of-entities-and-relations-based-on-a-novel-decomposition-strategy/4.png"></p><p>$s ^l _t$是标记关于$l$的$t$（tail）的起始索引，$e ^l _t$是结束索引．这种分解表明任务之间存在自然顺序：预测结束位置可能会受益于起始位置的预测结果，这促使采用分层标记结构。<img src="/2020/09/13/joint-extraction-of-entities-and-relations-based-on-a-novel-decomposition-strategy/2.png"></p><p>如图2的右面板所示，将每个层与一个任务关联起来，并将低级任务的标记结果以及隐藏状态作为高级任务的输入。在本工作中，我们选择了BiLSTM作为基本的编码器。形式上，在标记开始位置时，单词$x _i$的标记被预测为。</p><p><img src="/2020/09/13/joint-extraction-of-entities-and-relations-based-on-a-novel-decomposition-strategy/5.png"></p><p>​       其中$h _i$表示标记tokem，$a _i$是辅助词向量。对于HE提取，$a _i$是从整个句子中学习的全局表示。 从全局角度进行更准确的预测是有益的。 对于TER提取，$a _i$是全局表示和与头实体相关的向量的串联，以指示给定头实体的位置和语义信息。 在这里，采用$BiLSTM _{sta}$将$h _i$与$a_i$融合为单个向量$h ^{sta} _i$。类似地，$x _i$的结束标记可以计算。</p><p><img src="/2020/09/13/joint-extraction-of-entities-and-relations-based-on-a-novel-decomposition-strategy/6.png"></p><p>　　公式2-4和公式5-7的区别是双重的。首先，我们将公式2中的$h _i$替换为$h ^{sta} _i$，使模型在预测结束位置时能够感知起始位置的隐藏状态。其次，在其他文献中使用的位置编码向量的启发，将嵌入的位置$p ^{se} _i$作为$BiLSTM _{end}$层的附加输入。通过在可训练的位置嵌入矩阵中查找$p ^{se} _i$，可以得到$p ^{se} _i$</p><p><img src="/2020/09/13/joint-extraction-of-entities-and-relations-based-on-a-novel-decomposition-strategy/7.png"></p><p>​        这里$s ^* $是当前索引之前最近的起始位置，并计算$x _i$和$s ^* $的相对距离。 在$x _i$，$s ^∗ $之前没有起始位置，则$p ^s _i$被指定为常数C，该常数通常设置为最大句子长度。 通过这种方式，我们显式地限制了提取的实体的长度，并告诉模型结束位置不可能在开始位置的前面。为了防止错误传播，我们在训练过程中使用gold $p ^se$(到最近的正确起始位置的距离)。将HBT的训练损失(待最小化)定义为开始和结束标签的负对数概率之和。</p><p><img src="/2020/09/13/joint-extraction-of-entities-and-relations-based-on-a-novel-decomposition-strategy/8.png"></p><p>其中$y ^{sta} _i$ 和 $y ^{end} _i$分别是第i个单词的真正的开始标记和结束标记，n是输入句子的长度。</p><p>​         在推理时，为了适应多目标提取任务，我们提出了多跨度译码算法，如算法1所示。对于每个输入句子$S$，我们首先初始化几个变量(line1-4)来辅助解码:</p><p>（１）n被定义为$S$的长度</p><p>（２）$R$被初始化为一个空集，用来记录所提取的目标和类型标签。</p><p>（３）引入$s ^*$保持当前指数前最近的起始位置。</p><p>（４）将$p ^se$初始化为长度为$n$的列表，默认值为$C$，保存位置序列$[p ^{se} _1，··，p ^{se} _n]$。</p><p>接下来，通过公式. 4(line5)获得开始标签序列，通过公式. 8(第6-10行)计算每个标记的$p ^{sei}$。在$p ^{se}$的基础上，通过查找位置嵌入矩阵得到$p ^{se}$((第11行)。那么根据公式. 7(第12行)计算结束位置的标签序列。</p><p><img src="/2020/09/13/joint-extraction-of-entities-and-relations-based-on-a-novel-decomposition-strategy/3.png"></p><p>​       开始解码sta_tag（S）和end_tag（S）。 首先遍历sta_tag（S）来找到目标的起始位置（第13行）。 如果当前索引的标签不是“ O”，则表示该位置可能是起始单词（第14行），然后我们将从该索引遍历end_tag（S）以搜索结束位置（第15行）。 匹配标准是，如果结束位置的标签与开始位置相同（第16行），则将两个索引之间的单词视为候选目标（第17行），并将开始位置（或结束位置）的标签视为 ）被视为该目标的标签（第18行）。 然后将提取的目标及其标签添加到集合R（第19行）中，结束end_tag（S）中的搜索以继续遍历sta_tag（S）以找到下一个起始位置（第20行）。 一旦sta_tag（S）中的所有索引都经过迭代，此解码函数将通过返回记录的数据集R（第21行）。</p><h3 id="EXTRACTION-SYSTEM"><a href="#EXTRACTION-SYSTEM" class="headerlink" title="EXTRACTION SYSTEM"></a>EXTRACTION SYSTEM</h3><p>​         利用基于跳转的标记方案和分层边界标记器，我们提出了一种端到端神经结构(图2)来联合提取实体和重叠关系，首先使用共享的BiLSTM编码器对句子进行编码。在此基础上，构造了一个提取头实体的提取器。对于每一个提取出来的头实体，利用头实体的语义和位置信息触发TER提取器，检测出相应的尾实体和关系</p><h4 id="Shared-Encode"><a href="#Shared-Encode" class="headerlink" title="Shared Encode"></a>Shared Encode</h4><p>给定语句$ S={x _1，···，x _n}$，我们利用BiLSTM层合并来自向前和向后的信息</p><p><img src="/2020/09/13/joint-extraction-of-entities-and-relations-based-on-a-novel-decomposition-strategy/9.png"></p><p>​       其中$h _i$是位置$i$的隐藏状态，$x _i$是$x _i$的单词表示，其中包含预先训练过的嵌入和对$x _i$的字符序列运行CNN生成的基于字符的单词表示。之后，我们还使用part-of-speech(POS)词向量来丰富$x _i$。</p><h4 id="HE-Extractor"><a href="#HE-Extractor" class="headerlink" title="HE Extractor"></a>HE Extractor</h4><p>​       提取器的目的是区分候选的头实体，排除不相关的头实体。我们首先将$h _i$和$g$连接起来，得到特征向量$x _i= [h _i;g]$，其中$g$是通过对所有隐藏状态的最大池化计算得到的全局上下文表示。实际上，$g$作为公式2中每个token的$a _i$。此外，我们使用$HHE= ｛x _1，··，x _n｝ $表示所有用于提取HE的词表示，然后将HHE输入到一个HBT中提取头部实体。其中$RHE =｛(h _j, type _{h _j})｝^m _{j=1}$包含了S中所有的头部实体和对应的实体类型标签。</p><p><img src="/2020/09/13/joint-extraction-of-entities-and-relations-based-on-a-novel-decomposition-strategy/10.png"></p><h4 id="TER-Extractor"><a href="#TER-Extractor" class="headerlink" title="TER Extractor"></a>TER Extractor</h4><p>类似于HE提取器，TER提取器还使用基本表示形式$h _i$和全局词向量$g$作为输入特征。 但是，仅将$h _i$和$g$串联不足以检测尾部实体以及与特定头部实体的关系。 进行TER提取所需的关键信息包括：</p><p>（1）尾部实体内的单词；</p><p>（2）被依赖的头部实体； </p><p>（3）表示关系的上下文； </p><p>（4）尾部实体与头部实体之间的距离。</p><p> 基于这些考虑，提出了位置感知，头目实体感知和上下文感知表示。 给定一个主实体$h$，我们定义$\overline{x } _i$为：</p><p><img src="/2020/09/13/joint-extraction-of-entities-and-relations-based-on-a-novel-decomposition-strategy/11.png"></p><p>式中$h ^h= [h _{s _h};h _{e _h}]$表示头实体$h$的表示，其中$h _{s _h}$和$h _{e _h}$分别为$h$的开始和结束指标处的隐藏状态。$p ^{ht} _i$是对$x _i$到h的相对距离进行编码的嵌入位置。显然，$[g;h _h;p ^{ht} _i]$是用于TER提取的辅助特征向量，如公式2中的$a _i$。</p><p>形式上，我们取$HTER={x _1，···，x _n}$作为一个HBT的输入，输出$RTER=｛(t _o, rel _o)｝zo=1$，其中$t _o$为第$o$个提取的尾部实体，$rel _o$为其与给定头实体的关系标签。</p><p><img src="/2020/09/13/joint-extraction-of-entities-and-relations-based-on-a-novel-decomposition-strategy/12.png"></p><p>​     可以通过将$h$和$each（t _o，rel _o）$组合成$｛（h，relo，to）｝ ^{z} _{o = 1}$来组合三元组，其中包含句子$S$中所有带有头实体$h$的三元组。而在推理时，我们从RHE中逐一选择头实体以完成提取任务。</p><h4 id="Training-of-Joint-Extractor"><a href="#Training-of-Joint-Extractor" class="headerlink" title="Training of Joint Extractor"></a>Training of Joint Extractor</h4><p>提供了两种学习方式来训练模型：用于HE提取的LHE和用于TER提取的LTER，都被表示为公式9.为了在任务之间共享输入并进行联合训练，对于每个训练实例，我们随机选择一个头部实体 从全局头实体中设置为TER提取器的指定输入。 我们还可以多次重复每个句子以确保使用所有三元组，但是实验结果表明不是很有意义。最后，损失由下式给出：</p><p><img src="/2020/09/13/joint-extraction-of-entities-and-relations-based-on-a-novel-decomposition-strategy/13.png"></p><p>   然后，对模型进行随机梯度下降训练。运算式公式14能够提取头-实体、尾-实体和相互影响的关系，这样，每个组合中的错误可以互相约束。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 论文 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 关系抽取 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>关系抽取汇总</title>
      <link href="/2020/09/12/guan-xi-chou-qu-github-hui-zong/"/>
      <url>/2020/09/12/guan-xi-chou-qu-github-hui-zong/</url>
      
        <content type="html"><![CDATA[<h1 id="关系抽取汇总"><a href="#关系抽取汇总" class="headerlink" title="关系抽取汇总"></a>关系抽取汇总</h1><p> 　　目前有两大类方法，一种是使用流水线的方法（Pipelined Method）进行抽取：输入一个句子，首先进行命名实体识别，然后对识别出来的实体进行两两组合，再进行关系分类，最后把存在实体关系的三元组作为输入。流水线的方法存在的缺点有：1）错误传播，实体识别模块的错误会影响到下面的关系分类性能；2）忽视了两个子任务之间存在的关系，例如如果存在Country-President关系，那么我们可以知道前一个实体必然属于Location类型，后一个实体属于Person类型，流水线的方法没法利用这样的信息。3）产生了没必要的冗余信息，由于对识别出来的实体进行两两配对，然后再进行关系分类，那些没有关系的实体对就会带来多余信息，提升错误率。另一种是联合学习，输入一个句子，通过实体识别和关系抽取联合模型，直接得到有关系的实体三元组。这种可以克服上面流水线方法的缺点，但是可能会有更复杂的结构。</p><h2 id="流水线"><a href="#流水线" class="headerlink" title="流水线"></a>流水线</h2><h2 id="联合模型"><a href="#联合模型" class="headerlink" title="联合模型"></a>联合模型</h2><p>​      基于神经网络方法的联合学习，我把目前的工作主要分为两大类：1）参数共享（Parameter Sharing）和2）标注策略（Tagging Scheme）</p><h3 id="半指针-半标注"><a href="#半指针-半标注" class="headerlink" title="半指针-半标注"></a>半指针-半标注</h3><p>[DESC]</p><p>“半指针-半标注”方法实体的抽取器，基于苏神的<a href="https://kexue.fm/archives/7161">三元组抽取</a>方法改造，这里取消了三元组抽取模型中对s的抽取，直接抽取实体并做分类(相当于直接抽取p和o)。改造后的实体抽取方法不仅可以运用于短实体的抽取，也可以运用到长句实体的抽取。</p><p>[github] <a href="https://github.com/StanleyLsx/entity_extractor_by_binary_tagging">https://github.com/StanleyLsx/entity_extractor_by_binary_tagging</a></p><p>[github]<a href="https://github.com/zhengyima/kg-baseline-pytorch">https://github.com/zhengyima/kg-baseline-pytorch</a> (torch baseline 版本)</p><h2 id="Bert"><a href="#Bert" class="headerlink" title="Bert"></a>Bert</h2><p>[DESC] 中文关系抽取, 人物关系抽取</p><p>[github] <a href="https://github.com/Jacen789/relation-extraction">https://github.com/Jacen789/relation-extraction</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 汇总 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 关系抽取 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python argparse参数解析模块</title>
      <link href="/2020/09/12/python-argparse-can-shu-jie-xi-mo-kuai/"/>
      <url>/2020/09/12/python-argparse-can-shu-jie-xi-mo-kuai/</url>
      
        <content type="html"><![CDATA[<h1 id="argparse"><a href="#argparse" class="headerlink" title="argparse"></a>argparse</h1><h2 id="基本使用"><a href="#基本使用" class="headerlink" title="基本使用"></a>基本使用</h2><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> argparse<span class="token punctuation">(</span>导入程序参数模块<span class="token punctuation">)</span>创建argparse对象，并将产品简要说明加入show <span class="token operator">=</span> <span class="token string">'程序说明'</span> <span class="token operator">==</span><span class="token operator">=</span><span class="token operator">></span>程序简要说明<span class="token punctuation">(</span>字符串<span class="token punctuation">)</span>，输出help时会显示p <span class="token operator">=</span> argparse<span class="token punctuation">.</span>ArgumentParser<span class="token punctuation">(</span>description<span class="token operator">=</span>show<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 创建一个参数，如果参数名称前没有‘-’或‘--’则该参数为必填参数，如果程序运行时不给它赋值则程序将抛出异常(赋值时直接给值即可，不需输参数名，参考ping命令)</span>p<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'参数名称(-h时会显示)'</span><span class="token punctuation">,</span> <span class="token string">'--别名(选填，'</span><span class="token operator">-</span>参数名称 <span class="token operator">=</span> <span class="token operator">-</span><span class="token operator">-</span>别名<span class="token string">')'</span>，<span class="token operator">+</span>各种参数。。。<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 创建一个‘--’参数，如果参数前有‘--’则为可选参数。在输入‘--参数’后再赋值。</span>p<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'--参数名称'</span><span class="token punctuation">,</span><span class="token operator">+</span>各种参数。。。<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 创建一个‘-’参数，如果参数前有‘-’则为可选参数。在输入‘-参数’后再赋值。注意：如果参数有别名则实际参数值是赋给了别名。</span>p<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'-参数名称'</span><span class="token punctuation">,</span> <span class="token string">'--别名(选填，'</span><span class="token operator">-</span>参数名称 <span class="token operator">=</span> <span class="token operator">-</span><span class="token operator">-</span>别名<span class="token string">')'</span>，<span class="token operator">+</span>各种参数。。。<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 获取程序参数的值，args是一个对象，该对象里的若干属性等于参数(提取参数的值：args.参数名称)</span>args <span class="token operator">=</span> p<span class="token punctuation">.</span>parse_args<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><h2 id="add-argument方法参数说明："><a href="#add-argument方法参数说明：" class="headerlink" title="add_argument方法参数说明："></a>add_argument方法参数说明：</h2><p>help：针对该参数的说明。例：help=’this display information’</p><p>type：该参数字符类型(str,int,float等)，如果输入类型不对则会抛异常。例：type=int</p><p>action：当该参数值为”store_true”时表示创建的是一个机关，带该参数时则表示开启该机关(创建的参数(机关)此时的值为True)，不带该参数时则表示关闭该机关(创建的参数(机关)此时的值为False)。<br>(例：p.add_argument(‘–v’,action=’store_true’） ===&gt;如果带该参数则‘v’的值为true)</p><p>choices：限定参数值范围(规定只接收规定列表中的值)，值为列表。例：choices=[1,2,3]</p><p>default：指定参数默认值。例：default=100</p><h2 id="创建两个互斥的参数"><a href="#创建两个互斥的参数" class="headerlink" title="创建两个互斥的参数"></a>创建两个互斥的参数</h2><pre class=" language-python"><code class="language-python">group <span class="token operator">=</span> parser <span class="token punctuation">.</span> add_mutually_exclusive_group <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">></span> 创建一个互斥组group<span class="token punctuation">.</span>add_argument <span class="token punctuation">(</span> <span class="token string">"-v"</span> <span class="token punctuation">,</span> <span class="token string">"--verbose"</span> <span class="token punctuation">,</span> action <span class="token operator">=</span> <span class="token string">"store_true"</span> <span class="token punctuation">)</span> <span class="token operator">==</span><span class="token operator">==</span><span class="token operator">></span>互斥组内的参数不可同时出现，否则抛异常group<span class="token punctuation">.</span>add_argument <span class="token punctuation">(</span> <span class="token string">"-q"</span> <span class="token punctuation">,</span> <span class="token string">"--quiet"</span> <span class="token punctuation">,</span> action <span class="token operator">=</span> <span class="token string">"store_true"</span> <span class="token punctuation">)</span> <span class="token operator">==</span><span class="token operator">==</span><span class="token operator">></span>互斥组内的参数不可同时出现，否则抛异常</code></pre><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> argparse<span class="token comment" spellcheck="true"># 创建参数功能对象</span>show <span class="token operator">=</span> <span class="token string">'这是一个求长方形面积或长方体体积的程序'</span>p <span class="token operator">=</span> argparse<span class="token punctuation">.</span>ArgumentParser<span class="token punctuation">(</span>description<span class="token operator">=</span>show<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 创建各个参数</span>p<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'-x'</span><span class="token punctuation">,</span> help<span class="token operator">=</span><span class="token string">'长方形的底边长度'</span><span class="token punctuation">,</span> type<span class="token operator">=</span>int<span class="token punctuation">,</span> choices<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> default<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>p<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'-z'</span><span class="token punctuation">,</span> <span class="token string">'--kuan'</span><span class="token punctuation">,</span> help<span class="token operator">=</span><span class="token string">'另外一条边长'</span><span class="token punctuation">,</span> type<span class="token operator">=</span>int<span class="token punctuation">,</span> choices<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> default<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>p<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'-y'</span><span class="token punctuation">,</span> <span class="token string">'--gao'</span><span class="token punctuation">,</span> help<span class="token operator">=</span><span class="token string">'长方体的高度'</span><span class="token punctuation">,</span> type<span class="token operator">=</span>int<span class="token punctuation">,</span> choices<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> default<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 创建互斥参数</span>group <span class="token operator">=</span> p<span class="token punctuation">.</span>add_mutually_exclusive_group<span class="token punctuation">(</span><span class="token punctuation">)</span>group<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'-v'</span><span class="token punctuation">,</span> action<span class="token operator">=</span><span class="token string">"store_true"</span><span class="token punctuation">,</span> help<span class="token operator">=</span><span class="token string">'求长方形面积'</span><span class="token punctuation">)</span>group<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'-vv'</span><span class="token punctuation">,</span> action<span class="token operator">=</span><span class="token string">"store_true"</span><span class="token punctuation">,</span> help<span class="token operator">=</span><span class="token string">'求长方体体积'</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 提取参数的赋值</span>args <span class="token operator">=</span> p<span class="token punctuation">.</span>parse_args<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 利用参数的值进行计算</span><span class="token keyword">if</span> args<span class="token punctuation">.</span>v<span class="token punctuation">:</span>　　<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'长方形面积是%d'</span> <span class="token operator">%</span> <span class="token punctuation">(</span>args<span class="token punctuation">.</span>x <span class="token operator">*</span> args<span class="token punctuation">.</span>kuan<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">elif</span> args<span class="token punctuation">.</span>vv<span class="token punctuation">:</span>　　<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'长方体体积是%d'</span> <span class="token operator">%</span> <span class="token punctuation">(</span>args<span class="token punctuation">.</span>x <span class="token operator">*</span> args<span class="token punctuation">.</span>kuan <span class="token operator">*</span> args<span class="token punctuation">.</span>gao<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">else</span><span class="token punctuation">:</span>　　<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'请用使用参数表明你要使用的功能！'</span><span class="token punctuation">)</span></code></pre><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python基础 </tag>
            
            <tag> 参数模块 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch两种模型保存方式</title>
      <link href="/2020/09/11/pytorch-liang-chong-mo-xing-bao-cun-fang-shi/"/>
      <url>/2020/09/11/pytorch-liang-chong-mo-xing-bao-cun-fang-shi/</url>
      
        <content type="html"><![CDATA[<h1 id="Torch模型保存"><a href="#Torch模型保存" class="headerlink" title="Torch模型保存"></a>Torch模型保存</h1><h2 id="只保存模型参数"><a href="#只保存模型参数" class="headerlink" title="只保存模型参数"></a>只保存模型参数</h2><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 保存</span>torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'\parameter.pkl'</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 加载</span>model <span class="token operator">=</span> TheModelClass<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'\parameter.pkl'</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><h2 id="保存完整模型"><a href="#保存完整模型" class="headerlink" title="保存完整模型"></a>保存完整模型</h2><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 保存</span>torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">,</span> <span class="token string">'\model.pkl'</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 加载</span>model <span class="token operator">=</span> torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'\model.pkl'</span><span class="token punctuation">)</span></code></pre><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> torch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> torch基础 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CCKS2020面向中文短文本的实体链指任务</title>
      <link href="/2020/09/10/ccks2020-mian-xiang-zhong-wen-duan-wen-ben-de-shi-ti-lian-zhi-ren-wu/"/>
      <url>/2020/09/10/ccks2020-mian-xiang-zhong-wen-duan-wen-ben-de-shi-ti-lian-zhi-ren-wu/</url>
      
        <content type="html"><![CDATA[<h1 id="实体链指"><a href="#实体链指" class="headerlink" title="实体链指"></a>实体链指</h1><h2 id="任务介绍"><a href="#任务介绍" class="headerlink" title="任务介绍"></a>任务介绍</h2><p>面向中文短文本的实体链指，简称 <strong>EL（Entity Linking）</strong>。即对于给定的一个中文短文本（如搜索 Query、微博、对话内容、文章/视频/图片的标题等），EL 将其中的实体与给定知识库中对应的实体进行关联。针对中文短文本的实体链指存在很大的挑战，主要原因如下：</p><p>（1）口语化严重，导致实体歧义消解困难；<br>（2）短文本上下文语境不丰富，须对上下文语境进行精准理解；<br>（3）相比英文，中文由于语言自身的特点，在短文本的链指问题上更有挑战。</p><h2 id="任务详情"><a href="#任务详情" class="headerlink" title="任务详情"></a>任务详情</h2><p>此次任务的输入输出定义如下：</p><p><strong>输入：</strong>中文短文本以及该短文本中的实体集合。</p><p><strong>输出：</strong>输出文本此中文短文本的实体链指结果。每个结果包含：实体 mention、在中文短文本中的位置偏移、其在给定知识库中的 id，如果为 NIL 情况，需要再给出实体的上位概念类型</p><h5 id="示例输入："><a href="#示例输入：" class="headerlink" title="示例输入："></a>示例输入：</h5><pre class=" language-json"><code class="language-json">&amp;#<span class="token number">123</span><span class="token punctuation">;</span> <span class="token property">"text_id"</span><span class="token operator">:</span><span class="token string">"1"</span><span class="token punctuation">,</span> <span class="token property">"text"</span><span class="token operator">:</span>"《琅琊榜》海宴_【原创小说|权谋小说】"<span class="token punctuation">,</span> <span class="token property">"mention_data"</span><span class="token operator">:</span><span class="token punctuation">[</span> &amp;#<span class="token number">123</span><span class="token punctuation">;</span>  <span class="token property">"mention"</span><span class="token operator">:</span><span class="token string">"琅琊榜"</span><span class="token punctuation">,</span>  <span class="token property">"offset"</span><span class="token operator">:</span><span class="token string">"1"</span> &amp;#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">,</span>   &amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"mention"</span><span class="token operator">:</span><span class="token string">"海宴"</span><span class="token punctuation">,</span>  <span class="token property">"offset"</span><span class="token operator">:</span><span class="token string">"5"</span>    &amp;#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">,</span>   &amp;#<span class="token number">123</span><span class="token punctuation">;</span> <span class="token property">"mention"</span><span class="token operator">:</span><span class="token string">"原创小说"</span><span class="token punctuation">,</span>   <span class="token property">"offset"</span><span class="token operator">:</span><span class="token string">"9"</span> &amp;#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">,</span>&amp;#<span class="token number">123</span><span class="token punctuation">;</span> <span class="token property">"mention"</span><span class="token operator">:</span><span class="token string">"权谋小说"</span><span class="token punctuation">,</span>   <span class="token property">"offset"</span><span class="token operator">:</span><span class="token string">"14"</span>  &amp;#<span class="token number">125</span><span class="token punctuation">;</span> <span class="token punctuation">]</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span></code></pre><h5 id="示例输出："><a href="#示例输出：" class="headerlink" title="示例输出："></a>示例输出：</h5><pre class=" language-json"><code class="language-json">&amp;#<span class="token number">123</span><span class="token punctuation">;</span> <span class="token property">"text_id"</span><span class="token operator">:</span><span class="token string">"1"</span><span class="token punctuation">,</span>    <span class="token property">"text"</span><span class="token operator">:</span>"《琅琊榜》海宴_【原创小说|权谋小说】"<span class="token punctuation">,</span>  <span class="token property">"mention_data"</span><span class="token operator">:</span><span class="token punctuation">[</span> &amp;#<span class="token number">123</span><span class="token punctuation">;</span> <span class="token property">"kb_id"</span><span class="token operator">:</span><span class="token string">"2135131"</span><span class="token punctuation">,</span>            <span class="token property">"mention"</span><span class="token operator">:</span><span class="token string">"琅琊榜"</span><span class="token punctuation">,</span>   <span class="token property">"offset"</span><span class="token operator">:</span><span class="token string">"1"</span>   &amp;#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">,</span>&amp;#<span class="token number">123</span><span class="token punctuation">;</span> <span class="token property">"kb_id"</span><span class="token operator">:</span><span class="token string">"10572965"</span><span class="token punctuation">,</span>   <span class="token property">"mention"</span><span class="token operator">:</span><span class="token string">"海宴"</span><span class="token punctuation">,</span>     <span class="token property">"offset"</span><span class="token operator">:</span><span class="token string">"5"</span>  &amp;#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">,</span>     &amp;#<span class="token number">123</span><span class="token punctuation">;</span>  <span class="token property">"kb_id"</span><span class="token operator">:</span><span class="token string">"215143"</span><span class="token punctuation">,</span>   <span class="token property">"mention"</span><span class="token operator">:</span><span class="token string">"原创小说"</span><span class="token punctuation">,</span>  <span class="token property">"offset"</span><span class="token operator">:</span><span class="token string">"9"</span>  &amp;#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">,</span> &amp;#<span class="token number">123</span><span class="token punctuation">;</span> <span class="token property">"kb_id"</span><span class="token operator">:</span><span class="token string">" NIL_Work "</span><span class="token punctuation">,</span>  <span class="token property">"mention"</span><span class="token operator">:</span><span class="token string">"权谋小说"</span><span class="token punctuation">,</span>   <span class="token property">"offset"</span><span class="token operator">:</span><span class="token string">"14"</span> &amp;#<span class="token number">125</span><span class="token punctuation">;</span> <span class="token punctuation">]</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span></code></pre><h2 id="数据介绍"><a href="#数据介绍" class="headerlink" title="数据介绍"></a>数据介绍</h2><p><strong>知识库</strong></p><pre class=" language-json"><code class="language-json">&amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"alias"</span><span class="token operator">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token property">"subject_id"</span><span class="token operator">:</span> <span class="token string">"10001"</span><span class="token punctuation">,</span> <span class="token property">"data"</span><span class="token operator">:</span> <span class="token punctuation">[</span>&amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"predicate"</span><span class="token operator">:</span> <span class="token string">"政治面貌"</span><span class="token punctuation">,</span> <span class="token property">"object"</span><span class="token operator">:</span> <span class="token string">"中共党员"</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">,</span> &amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"predicate"</span><span class="token operator">:</span> <span class="token string">"义项描述"</span><span class="token punctuation">,</span> <span class="token property">"object"</span><span class="token operator">:</span> <span class="token string">"潜山县塔畈乡副主任科员、纪委副书记"</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">,</span> &amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"predicate"</span><span class="token operator">:</span> <span class="token string">"性别"</span><span class="token punctuation">,</span> <span class="token property">"object"</span><span class="token operator">:</span> <span class="token string">"男"</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">,</span> &amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"predicate"</span><span class="token operator">:</span> <span class="token string">"学历"</span><span class="token punctuation">,</span> <span class="token property">"object"</span><span class="token operator">:</span> <span class="token string">"大专"</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">,</span> &amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"predicate"</span><span class="token operator">:</span> <span class="token string">"中文名"</span><span class="token punctuation">,</span> <span class="token property">"object"</span><span class="token operator">:</span> <span class="token string">"张健"</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token property">"type"</span><span class="token operator">:</span> <span class="token string">"Person"</span><span class="token punctuation">,</span> <span class="token property">"subject"</span><span class="token operator">:</span> <span class="token string">"张健"</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span></code></pre><p>知识库中的每个实体都包含alias, 对应subject可能的别名一个subject_id(知识库id)，一个subject名称，实体的别名(主语)，对应的概念类型，以及与此实体相关的一系列二元组&lt; predicate，object&gt;（&lt;属性，属性值&gt;）或者叫（谓语，宾语）信息形式。知识库中每行代表知识库的一条记录（一个实体信息），每条记录为json数据格式。</p><p><strong>数据集</strong></p><p>标注数据集由训练集、验证集和测试集组成，整体标注数据大约10万条左右。<br>标注数据集中每条数据的格式为：</p><pre class=" language-json"><code class="language-json">&amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"text_id"</span><span class="token operator">:</span> <span class="token string">"1"</span><span class="token punctuation">,</span> <span class="token property">"text"</span><span class="token operator">:</span> <span class="token string">"天下没有不散的宴席 - ╰つ雲中帆╰つ"</span><span class="token punctuation">,</span> <span class="token property">"mention_data"</span><span class="token operator">:</span> <span class="token punctuation">[</span>&amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"kb_id"</span><span class="token operator">:</span> <span class="token string">"28270"</span><span class="token punctuation">,</span> <span class="token property">"mention"</span><span class="token operator">:</span> <span class="token string">"天下没有不散的宴席"</span><span class="token punctuation">,</span> <span class="token property">"offset"</span><span class="token operator">:</span> <span class="token string">"0"</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">,</span> &amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"kb_id"</span><span class="token operator">:</span> <span class="token string">"NIL_Other"</span><span class="token punctuation">,</span> <span class="token property">"mention"</span><span class="token operator">:</span> <span class="token string">"╰つ雲中帆╰つ"</span><span class="token punctuation">,</span> <span class="token property">"offset"</span><span class="token operator">:</span> <span class="token string">"12"</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">]</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span></code></pre><p>kb_id对应知识库subject_id，mention对应的是实体．</p><h2 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h2><p><img src="/2020/09/10/ccks2020-mian-xiang-zhong-wen-duan-wen-ben-de-shi-ti-lian-zhi-ren-wu/3.png"></p><h2 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h2><p><strong>知识库处理</strong></p><pre class=" language-python"><code class="language-python">logger <span class="token operator">=</span> logging<span class="token punctuation">.</span>getLogger<span class="token punctuation">(</span>__name__<span class="token punctuation">)</span>PICKLE_PATH <span class="token operator">=</span> <span class="token string">"middle_data/kb/"</span>entity_to_kbids <span class="token operator">=</span> defaultdict<span class="token punctuation">(</span>set<span class="token punctuation">)</span> <span class="token comment" spellcheck="true">## 实体名称对应KDID表</span>kbid_to_entities <span class="token operator">=</span> dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">## KBID对应的实体名称列表</span>kbid_to_text <span class="token operator">=</span> dict<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">## KDID对应属性文本</span>kbid_to_types <span class="token operator">=</span> dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">## KDID对应实体类型表</span>kbid_to_predicates <span class="token operator">=</span> dict<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># KBID对应的属性列表</span>idx_to_type <span class="token operator">=</span> list<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">## 索引类型映射列表</span>type_to_idx <span class="token operator">=</span> dict<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">##类型索引映射字典</span><span class="token keyword">with</span> open<span class="token punctuation">(</span><span class="token string">"/media/daiyizheng/SSD/data/CCKS/2020/ccks2020面向中文短文本的实体链指任务/kb.json"</span><span class="token punctuation">,</span> <span class="token string">'r'</span><span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">"utf-8"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>    <span class="token keyword">for</span> line <span class="token keyword">in</span> tqdm<span class="token punctuation">(</span>f<span class="token punctuation">.</span>readlines<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        line <span class="token operator">=</span> json<span class="token punctuation">.</span>loads<span class="token punctuation">(</span>line<span class="token punctuation">)</span>        entities <span class="token operator">=</span> set<span class="token punctuation">(</span>line<span class="token punctuation">[</span><span class="token string">'alias'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        kbid <span class="token operator">=</span> line<span class="token punctuation">[</span><span class="token string">'subject_id'</span><span class="token punctuation">]</span>        type <span class="token operator">=</span> line<span class="token punctuation">[</span><span class="token string">'type'</span><span class="token punctuation">]</span>        subject <span class="token operator">=</span> line<span class="token punctuation">[</span><span class="token string">'subject'</span><span class="token punctuation">]</span>        entities<span class="token punctuation">.</span>add<span class="token punctuation">(</span>subject<span class="token punctuation">)</span>        <span class="token keyword">for</span> entity <span class="token keyword">in</span> entities<span class="token punctuation">:</span>            entity_to_kbids<span class="token punctuation">[</span>entity<span class="token punctuation">]</span><span class="token punctuation">.</span>add<span class="token punctuation">(</span>kbid<span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#实体名称对应KDID表</span>        kbid_to_entities<span class="token punctuation">[</span>kbid<span class="token punctuation">]</span> <span class="token operator">=</span> entities        text_list<span class="token punctuation">,</span> predicate_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        <span class="token keyword">for</span> x <span class="token keyword">in</span> line<span class="token punctuation">[</span><span class="token string">'data'</span><span class="token punctuation">]</span><span class="token punctuation">:</span>            <span class="token comment" spellcheck="true"># data中的predicate和object拼接</span>            text_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token string">":"</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token punctuation">[</span>x<span class="token punctuation">[</span><span class="token string">'predicate'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> x<span class="token punctuation">[</span><span class="token string">'object'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>            predicate_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>x<span class="token punctuation">[</span><span class="token string">'predicate'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        random<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>text_list<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 对属性文本随机打乱顺序</span>        kbid_to_text<span class="token punctuation">[</span>kbid<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">' '</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>text_list<span class="token punctuation">)</span>        kbid_to_predicates<span class="token punctuation">[</span>kbid<span class="token punctuation">]</span> <span class="token operator">=</span> predicate_list        <span class="token keyword">for</span> c <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token string">'\r'</span><span class="token punctuation">,</span> <span class="token string">'\t'</span><span class="token punctuation">,</span> <span class="token string">'\n'</span><span class="token punctuation">]</span><span class="token punctuation">:</span>            kbid_to_text<span class="token punctuation">[</span>kbid<span class="token punctuation">]</span> <span class="token operator">=</span> kbid_to_text<span class="token punctuation">[</span>kbid<span class="token punctuation">]</span><span class="token punctuation">.</span>replace<span class="token punctuation">(</span>c<span class="token punctuation">,</span> <span class="token string">""</span><span class="token punctuation">)</span>        type_list <span class="token operator">=</span> line<span class="token punctuation">[</span><span class="token string">'type'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">'|'</span><span class="token punctuation">)</span>        kbid_to_types<span class="token punctuation">[</span>kbid<span class="token punctuation">]</span> <span class="token operator">=</span> type_list        <span class="token keyword">for</span> t <span class="token keyword">in</span> type_list<span class="token punctuation">:</span>            <span class="token keyword">if</span> t <span class="token operator">not</span> <span class="token keyword">in</span> type_to_idx<span class="token punctuation">:</span>                type_to_idx<span class="token punctuation">[</span>t<span class="token punctuation">]</span> <span class="token operator">=</span> len<span class="token punctuation">(</span>type_to_idx<span class="token punctuation">)</span>                idx_to_type<span class="token punctuation">.</span>append<span class="token punctuation">(</span>t<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 保存pickle文件</span>    pd<span class="token punctuation">.</span>to_pickle<span class="token punctuation">(</span>entity_to_kbids<span class="token punctuation">,</span> PICKLE_PATH <span class="token operator">+</span> <span class="token string">'ENTITY_TO_KBIDS.pkl'</span><span class="token punctuation">)</span>    pd<span class="token punctuation">.</span>to_pickle<span class="token punctuation">(</span>kbid_to_entities<span class="token punctuation">,</span> PICKLE_PATH <span class="token operator">+</span> <span class="token string">'KBID_TO_ENTITIES.pkl'</span><span class="token punctuation">)</span>    pd<span class="token punctuation">.</span>to_pickle<span class="token punctuation">(</span>kbid_to_text<span class="token punctuation">,</span> PICKLE_PATH <span class="token operator">+</span> <span class="token string">'KBID_TO_TEXT.pkl'</span><span class="token punctuation">)</span>    pd<span class="token punctuation">.</span>to_pickle<span class="token punctuation">(</span>kbid_to_types<span class="token punctuation">,</span> PICKLE_PATH <span class="token operator">+</span> <span class="token string">'KBID_TO_TYPES.pkl'</span><span class="token punctuation">)</span>    pd<span class="token punctuation">.</span>to_pickle<span class="token punctuation">(</span>kbid_to_predicates<span class="token punctuation">,</span> PICKLE_PATH <span class="token operator">+</span> <span class="token string">'KBID_TO_PREDICATES.pkl'</span><span class="token punctuation">)</span>    pd<span class="token punctuation">.</span>to_pickle<span class="token punctuation">(</span>idx_to_type<span class="token punctuation">,</span> PICKLE_PATH <span class="token operator">+</span> <span class="token string">'IDX_TO_TYPE.pkl'</span><span class="token punctuation">)</span>    pd<span class="token punctuation">.</span>to_pickle<span class="token punctuation">(</span>type_to_idx<span class="token punctuation">,</span> PICKLE_PATH <span class="token operator">+</span> <span class="token string">'TYPE_TO_IDX.pkl'</span><span class="token punctuation">)</span>    logger<span class="token punctuation">.</span>info<span class="token punctuation">(</span><span class="token string">'Process Pickle File Finish.'</span><span class="token punctuation">)</span></code></pre><p>最总数据保存格式</p><pre><code>ENTITY_TO_KBIDS　｛"subject":&amp;#123;subject_id...&amp;#125;...｝#一个主语对应很多id,代表不同的语义KBID_TO_ENTITIES      &amp;#123;subject_id:"subject"...｝## 每个id只对应一个主语KBID_TO_TEXT              &amp;#123;subject_id:"subject"...｝KBID_TO_TYPES           &amp;#123;subject_id:[type]...｝KBID_TO_PREDICATES   &amp;#123;subject_id:"predicate"...｝IDX_TO_TYPE               &amp;#123;idx:"type"...｝TYPE_TO_IDX              &amp;#123;"type":idx...｝</code></pre><p><strong>实体链指预处理</strong></p><p><img src="/2020/09/10/ccks2020-mian-xiang-zhong-wen-duan-wen-ben-de-shi-ti-lian-zhi-ren-wu/1.png"></p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">generate_el_dataset</span><span class="token punctuation">(</span>file_name<span class="token punctuation">,</span> max_negs<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    link_dict <span class="token operator">=</span> defaultdict<span class="token punctuation">(</span>list<span class="token punctuation">)</span>    path <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>data_dir<span class="token punctuation">,</span> file_name<span class="token punctuation">)</span>    <span class="token keyword">with</span> open<span class="token punctuation">(</span>path<span class="token punctuation">,</span> <span class="token string">'r'</span><span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">'utf-8'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>        <span class="token keyword">for</span> line <span class="token keyword">in</span> tqdm<span class="token punctuation">(</span>f<span class="token punctuation">.</span>readlines<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            line <span class="token operator">=</span> json<span class="token punctuation">.</span>loads<span class="token punctuation">(</span>line<span class="token punctuation">)</span>            <span class="token keyword">for</span> data <span class="token keyword">in</span> line<span class="token punctuation">[</span><span class="token string">'mention_data'</span><span class="token punctuation">]</span><span class="token punctuation">:</span>                <span class="token comment" spellcheck="true">## 对测试集特殊处理</span>                <span class="token keyword">if</span> <span class="token string">'kb_id'</span> <span class="token operator">not</span> <span class="token keyword">in</span> data<span class="token punctuation">:</span>                    data<span class="token punctuation">[</span><span class="token string">'kb_id'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">'0'</span>                <span class="token comment" spellcheck="true"># KB中不存在的实体不进行链接</span>                <span class="token keyword">if</span> <span class="token operator">not</span> data<span class="token punctuation">[</span><span class="token string">'kb_id'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>isdigit<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                    <span class="token keyword">continue</span>                entity <span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token string">'mention'</span><span class="token punctuation">]</span>                kbids <span class="token operator">=</span> list<span class="token punctuation">(</span>entity_to_kbids<span class="token punctuation">[</span>entity<span class="token punctuation">]</span><span class="token punctuation">)</span>                random<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>kbids<span class="token punctuation">)</span>                num_negs <span class="token operator">=</span> <span class="token number">0</span>                <span class="token keyword">for</span> kbid <span class="token keyword">in</span> kbids<span class="token punctuation">:</span>                    <span class="token keyword">if</span> num_negs <span class="token operator">>=</span> max_negs <span class="token operator">></span> <span class="token number">0</span> <span class="token operator">and</span> kbid <span class="token operator">!=</span> data<span class="token punctuation">[</span><span class="token string">'kb_id'</span><span class="token punctuation">]</span><span class="token punctuation">:</span>                        <span class="token keyword">continue</span>                    link_dict<span class="token punctuation">[</span><span class="token string">'text_id'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span>line<span class="token punctuation">[</span><span class="token string">'text_id'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>                    link_dict<span class="token punctuation">[</span><span class="token string">'entity'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span>entity<span class="token punctuation">)</span>                    link_dict<span class="token punctuation">[</span><span class="token string">'offset'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span>data<span class="token punctuation">[</span><span class="token string">'offset'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>                    link_dict<span class="token punctuation">[</span><span class="token string">'short'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span>line<span class="token punctuation">[</span><span class="token string">'text'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>                    link_dict<span class="token punctuation">[</span><span class="token string">'kb_id'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span>kbid<span class="token punctuation">)</span>                    link_dict<span class="token punctuation">[</span><span class="token string">'kb_text'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span>kbid_to_text<span class="token punctuation">[</span>kbid<span class="token punctuation">]</span><span class="token punctuation">)</span>                    link_dict<span class="token punctuation">[</span><span class="token string">'kb_predicate_num'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span>len<span class="token punctuation">(</span>kbid_to_predicates<span class="token punctuation">[</span>kbid<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>                    <span class="token keyword">if</span> kbid <span class="token operator">!=</span> data<span class="token punctuation">[</span><span class="token string">'kb_id'</span><span class="token punctuation">]</span><span class="token punctuation">:</span>                        link_dict<span class="token punctuation">[</span><span class="token string">'predict'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>                        num_negs <span class="token operator">+=</span> <span class="token number">1</span>                    <span class="token keyword">else</span><span class="token punctuation">:</span>                        link_dict<span class="token punctuation">[</span><span class="token string">'predict'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>    link_data <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span>link_dict<span class="token punctuation">)</span>    type_name <span class="token operator">=</span> file_name<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">'.'</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>    output_path <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>out_dir<span class="token punctuation">,</span> <span class="token string">"el_"</span> <span class="token operator">+</span> type_name <span class="token operator">+</span> <span class="token string">".tsv"</span><span class="token punctuation">)</span>    link_data<span class="token punctuation">.</span>to_csv<span class="token punctuation">(</span>output_path<span class="token punctuation">,</span> index<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> sep<span class="token operator">=</span><span class="token string">'\t'</span><span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">'utf_8_sig'</span><span class="token punctuation">)</span></code></pre><p><strong>负例选取</strong></p><p>通过该实体查ENTITY_TO_KBIDS所有与kid，当数据集某一条记录kid相同时正例，不相同则是负例(同一实体，不同语义)负例最多取两条．</p><p><strong>实体类型预处理</strong></p><p><img src="/2020/09/10/ccks2020-mian-xiang-zhong-wen-duan-wen-ben-de-shi-ti-lian-zhi-ren-wu/2.png"></p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">generate_et_dataset</span><span class="token punctuation">(</span>file_name<span class="token punctuation">)</span><span class="token punctuation">:</span>    type_dict <span class="token operator">=</span> defaultdict<span class="token punctuation">(</span>list<span class="token punctuation">)</span>    path <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>data_dir<span class="token punctuation">,</span> file_name<span class="token punctuation">)</span>    <span class="token keyword">with</span> open<span class="token punctuation">(</span>path<span class="token punctuation">,</span> <span class="token string">'r'</span><span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">'utf-8'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>        <span class="token keyword">for</span> line <span class="token keyword">in</span> tqdm<span class="token punctuation">(</span>f<span class="token punctuation">.</span>readlines<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            line <span class="token operator">=</span> json<span class="token punctuation">.</span>loads<span class="token punctuation">(</span>line<span class="token punctuation">)</span>            <span class="token keyword">for</span> data <span class="token keyword">in</span> line<span class="token punctuation">[</span><span class="token string">'mention_data'</span><span class="token punctuation">]</span><span class="token punctuation">:</span>                entity <span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token string">'mention'</span><span class="token punctuation">]</span>                <span class="token comment" spellcheck="true"># 测试集特殊处理</span>                <span class="token keyword">if</span> <span class="token string">'kb_id'</span> <span class="token operator">not</span> <span class="token keyword">in</span> data<span class="token punctuation">:</span>                    entity_type <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'Other'</span><span class="token punctuation">]</span>                <span class="token keyword">elif</span> data<span class="token punctuation">[</span><span class="token string">'kb_id'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>isdigit<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                    entity_type <span class="token operator">=</span> kbid_to_types<span class="token punctuation">[</span>data<span class="token punctuation">[</span><span class="token string">'kb_id'</span><span class="token punctuation">]</span><span class="token punctuation">]</span>                <span class="token keyword">else</span><span class="token punctuation">:</span>                    entity_type <span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token string">'kb_id'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">"|"</span><span class="token punctuation">)</span>                    <span class="token keyword">for</span> x <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>entity_type<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                        entity_type<span class="token punctuation">[</span>x<span class="token punctuation">]</span> <span class="token operator">=</span> entity_type<span class="token punctuation">[</span>x<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">:</span><span class="token punctuation">]</span>                <span class="token keyword">for</span> e <span class="token keyword">in</span> entity_type<span class="token punctuation">:</span>                    type_dict<span class="token punctuation">[</span><span class="token string">'text_id'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span>line<span class="token punctuation">[</span><span class="token string">'text_id'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>                    type_dict<span class="token punctuation">[</span><span class="token string">'entity'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span>entity<span class="token punctuation">)</span>                    type_dict<span class="token punctuation">[</span><span class="token string">'offset'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span>data<span class="token punctuation">[</span><span class="token string">'offset'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>                    type_dict<span class="token punctuation">[</span><span class="token string">'short_text'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span>line<span class="token punctuation">[</span><span class="token string">'text'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>                    type_dict<span class="token punctuation">[</span><span class="token string">'type'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span>e<span class="token punctuation">)</span>    type_data <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span>type_dict<span class="token punctuation">)</span>    output_path <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>out_dir<span class="token punctuation">,</span> <span class="token string">"et_"</span><span class="token operator">+</span>file_name<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">'.'</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token string">".tsv"</span><span class="token punctuation">)</span>    type_data<span class="token punctuation">.</span>to_csv<span class="token punctuation">(</span>output_path<span class="token punctuation">,</span> index<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> sep<span class="token operator">=</span><span class="token string">"\t"</span><span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">"utf-8"</span><span class="token punctuation">)</span></code></pre><h2 id="数据输入"><a href="#数据输入" class="headerlink" title="数据输入"></a>数据输入</h2><p><strong>实体链指</strong></p><p>cls+text+sep+text+sep</p><pre class=" language-python"><code class="language-python">    <span class="token keyword">def</span> <span class="token function">_create_examples</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>lines<span class="token punctuation">,</span> set_type<span class="token punctuation">)</span><span class="token punctuation">:</span>        examples <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        <span class="token keyword">for</span> i<span class="token punctuation">,</span> line <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>lines<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">if</span> i <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>                <span class="token keyword">continue</span>            guid <span class="token operator">=</span> f<span class="token string">'&amp;#123;set_type&amp;#125;-&amp;#123;i&amp;#125;'</span>            text_a <span class="token operator">=</span> line<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token string">" "</span> <span class="token operator">+</span> line<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># entity+text</span>            text_b <span class="token operator">=</span> line<span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># kb中的predicate+object</span>            label <span class="token operator">=</span> line<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token comment" spellcheck="true"># 0负例 1正例</span>            examples<span class="token punctuation">.</span>append<span class="token punctuation">(</span>InputExample<span class="token punctuation">(</span>guid<span class="token operator">=</span>guid<span class="token punctuation">,</span> text_a<span class="token operator">=</span>text_a<span class="token punctuation">,</span> text_b<span class="token operator">=</span>text_b<span class="token punctuation">,</span> label<span class="token operator">=</span>label<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> examples</code></pre><p><strong>实体分类</strong></p><pre class=" language-python"><code class="language-python">  text_a <span class="token operator">=</span> line<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>        <span class="token keyword">for</span> i<span class="token punctuation">,</span> line <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>lines<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">if</span> i <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>                <span class="token keyword">continue</span>            guid <span class="token operator">=</span> f<span class="token string">'&amp;#123;set_type&amp;#125;-&amp;#123;i&amp;#125;'</span>            text_a <span class="token operator">=</span> line<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>            text_b <span class="token operator">=</span> line<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span>            label <span class="token operator">=</span> line<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>            examples<span class="token punctuation">.</span>append<span class="token punctuation">(</span>InputExample<span class="token punctuation">(</span>                guid<span class="token operator">=</span>guid<span class="token punctuation">,</span>                text_a<span class="token operator">=</span>text_a<span class="token punctuation">,</span> <span class="token comment" spellcheck="true">#entity</span>                text_b<span class="token operator">=</span>text_b<span class="token punctuation">,</span><span class="token comment" spellcheck="true"># text</span>                label<span class="token operator">=</span>label<span class="token punctuation">,</span><span class="token comment" spellcheck="true"># classify</span>            <span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p><strong>实体链指</strong>(模型做语义相似性)</p><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">EntityLinkingModel</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> args<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>args <span class="token operator">=</span> args        bert_config <span class="token operator">=</span> BertConfig<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>args<span class="token punctuation">.</span>model_name_or_path<span class="token punctuation">)</span>        bert_config<span class="token punctuation">.</span>num_labels <span class="token operator">=</span> <span class="token number">1</span>        config <span class="token operator">=</span> <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;"config": bert_config&amp;#125;</span>        self<span class="token punctuation">.</span>bert <span class="token operator">=</span> BertForSequenceClassification<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>args<span class="token punctuation">.</span>model_name_or_path<span class="token punctuation">,</span> <span class="token operator">**</span>config<span class="token punctuation">)</span> <span class="token comment" spellcheck="true">## bert模型</span>        self<span class="token punctuation">.</span>criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>BCEWithLogitsLoss    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_ids<span class="token punctuation">,</span> attention_mask<span class="token punctuation">,</span> token_type_ids<span class="token punctuation">)</span><span class="token punctuation">:</span>        output <span class="token operator">=</span> self<span class="token punctuation">.</span>bert<span class="token punctuation">(</span>input_ids<span class="token operator">=</span>input_ids<span class="token punctuation">,</span> attention_mask<span class="token operator">=</span>attention_mask<span class="token punctuation">,</span> token_type_ids<span class="token operator">=</span>token_type_ids<span class="token punctuation">)</span>        <span class="token keyword">return</span> output</code></pre><p><strong>实体分类</strong>(模型分类)</p><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">EntityTypingModel</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> args<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>args <span class="token operator">=</span> args        bert_config <span class="token operator">=</span> BertConfig<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>args<span class="token punctuation">.</span>model_name_or_path<span class="token punctuation">)</span>        bert_config<span class="token punctuation">.</span>num_labels <span class="token operator">=</span> <span class="token number">24</span>        config <span class="token operator">=</span> <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;"config":bert_config&amp;#125;</span>        self<span class="token punctuation">.</span>bert <span class="token operator">=</span> BertForSequenceClassification<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>args<span class="token punctuation">.</span>model_name_or_path<span class="token punctuation">,</span> <span class="token operator">**</span>config<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_ids<span class="token punctuation">,</span> attention_mask<span class="token punctuation">,</span> token_type_ids<span class="token punctuation">)</span><span class="token punctuation">:</span>        outputs <span class="token operator">=</span> self<span class="token punctuation">.</span>bert<span class="token punctuation">(</span>input_ids<span class="token operator">=</span>input_ids<span class="token punctuation">,</span> attention_mask<span class="token operator">=</span>attention_mask<span class="token punctuation">,</span> token_type_ids<span class="token operator">=</span>token_type_ids<span class="token punctuation">)</span>        <span class="token keyword">return</span> outputs</code></pre><h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><pre><code>text_id entity offset short_text type   result1  林平之    0  林平之答应岳灵珊报仇之事，从长计议，师娘想令狐冲了  Other  Person1  岳灵珊    5  林平之答应岳灵珊报仇之事，从长计议，师娘想令狐冲了  Other  Person1  师娘 18 林平之答应岳灵珊报仇之事，从长计议，师娘想令狐冲了  Other  Other1  令狐冲    21 林平之答应岳灵珊报仇之事，从长计议，师娘想令狐冲了  Other  Person2  思追 0  思追原来是个超级妹控，不愿妹妹嫁人，然而妹妹却喜欢一博老师  Other  Person2  妹控 8  思追原来是个超级妹控，不愿妹妹嫁人，然而妹妹却喜欢一博老师  Other  Other2  妹妹 13 思追原来是个超级妹控，不愿妹妹嫁人，然而妹妹却喜欢一博老师  Other  Other2  妹妹 20 思追原来是个超级妹控，不愿妹妹嫁人，然而妹妹却喜欢一博老师  Other  Other</code></pre><p>最终模型评估：</p><p>f1:0.828 0.837</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 比赛 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 实体歧义消解 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>图神经网络开放数据集</title>
      <link href="/2020/09/10/tu-shen-jing-wang-luo-kai-fang-shu-ju-ji/"/>
      <url>/2020/09/10/tu-shen-jing-wang-luo-kai-fang-shu-ju-ji/</url>
      
        <content type="html"><![CDATA[<h1 id="图神经网络开放数据集"><a href="#图神经网络开放数据集" class="headerlink" title="图神经网络开放数据集"></a>图神经网络开放数据集</h1><p>很多学者和机构发布了许多与图相关的任务，以测试各种GNN的性能。这些任务一般都会提供数据集。</p><p>按照任务分类，可以把数据集分成以下几类：</p><ul><li><strong>引文网络</strong></li><li><strong>生化图</strong></li><li><strong>社交网络</strong></li><li><strong>知识图谱</strong></li><li><strong>开源数据集仓库</strong></li></ul><h2 id="引文网络"><a href="#引文网络" class="headerlink" title="引文网络"></a>引文网络</h2><h3 id="Pubmed-Cora-Citeseer"><a href="#Pubmed-Cora-Citeseer" class="headerlink" title="Pubmed/Cora/Citeseer"></a><strong>Pubmed/Cora/Citeseer</strong></h3><p>引文网络，节点为论文、边为论文间的引用关系。这三个数据集通常用于链路预测或节点分类。</p><p>这三个数据集均来自于：</p><p>《Collective classification in network data》</p><p><strong>下载链接可从以下网址找到：</strong><a href="https://linqs.soe.ucsc.edu/data">https://linqs.soe.ucsc.edu/data</a></p><h3 id="DBLP"><a href="#DBLP" class="headerlink" title="DBLP"></a><strong>DBLP</strong></h3><p>DBLP是大型的计算机类文献索引库。原始的DBLP只是XML格式，清华唐杰教授的一篇论文将其进行处理并获得引文网络数据集。到目前为止已经发展到了第12个版本。</p><p><strong>DBLP引用网络论文</strong>：<br>《ArnetMiner: Extraction and Mining of Academic Social Networks》</p><ul><li><p>原始数据可以从这里获得：<a href="https://dblp.uni-trier.de/xml/">https://dblp.uni-trier.de/xml/</a></p></li><li><p>如果是想找处理过的DBLP引文网络数据集，可以从这里获得：<a href="https://www.aminer.cn/citation">https://www.aminer.cn/citation</a></p></li></ul><table><thead><tr><th align="left"><strong>数据集</strong></th><th align="left"><strong>节点数</strong></th><th align="left"><strong>边数</strong></th><th align="left"><strong>特征</strong></th><th align="left"><strong>标签</strong></th></tr></thead><tbody><tr><td align="left"><strong>Cora</strong></td><td align="left">2,708</td><td align="left">5,429</td><td align="left">1,433</td><td align="left">7</td></tr><tr><td align="left"><strong>Citeseer</strong></td><td align="left">3,327</td><td align="left">4,732</td><td align="left">3,703</td><td align="left">6</td></tr><tr><td align="left"><strong>Pubmed</strong></td><td align="left">19,717</td><td align="left">44,338</td><td align="left">500</td><td align="left">3</td></tr><tr><td align="left"><strong>DBLP_v12</strong></td><td align="left">4,894,081</td><td align="left">45,564,149</td><td align="left">-</td><td align="left">-</td></tr></tbody></table><h2 id="生化图"><a href="#生化图" class="headerlink" title="生化图"></a><strong>生化图</strong></h2><h3 id="PPI"><a href="#PPI" class="headerlink" title="PPI"></a><strong>PPI</strong></h3><p>蛋白质-蛋白质相互作用（protein-protein interaction, PPI）是指两个或两个以上的蛋白质分子通过非共价键形成 蛋白质复合体（protein complex）的过程。</p><p>PPI数据集中共有24张图，其中训练用20张，验证/测试分别2张。</p><p>节点最多可以有121种标签（比如蛋白质的一些性质、所处位置等）。每个节点有50个特征，包含定位基因集合、特征基因集合以及免疫特征。</p><p><strong>PPI论文</strong>：《Predicting multicellular function through multi-layer tissue networks》<br><strong>PPI下载链接</strong>：<a href="http://snap.stanford.edu/graphsage/ppi.zip">http://snap.stanford.edu/graphsage/ppi.zip</a></p><h3 id="NCI-1"><a href="#NCI-1" class="headerlink" title="NCI-1"></a><strong>NCI-1</strong></h3><p>NCI-1是关于化学分子和化合物的数据集，节点代表原子，边代表化学键。NCI-1包含4100个化合物，任务是判断该化合物是否有阻碍癌细胞增长的性质。</p><p><strong>NCI-1论文</strong>：《Comparison of descriptor spaces for chemical compound retrieval and classiﬁcation》</p><p><strong>Graph Kernel Datasets提供下载</strong></p><h3 id="MUTAG"><a href="#MUTAG" class="headerlink" title="MUTAG"></a><strong>MUTAG</strong></h3><p>MUTAG数据集包含188个硝基化合物，任务是判断化合物是芳香族还是杂芳族。</p><p><strong>MUTAG论文</strong>：《Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlation with molecular orbital energies and hydrophobicity》</p><p><strong>Graph Kernel Datasets提供下载</strong></p><h3 id="D-amp-D-PROTEIN"><a href="#D-amp-D-PROTEIN" class="headerlink" title="D&amp;D/PROTEIN"></a><strong>D&amp;D/PROTEIN</strong></h3><p>D&amp;D在蛋白质数据库的非冗余子集中抽取了了1178个高分辨率蛋白质，使用简单的特征，如二次结构含量、氨基酸倾向、表面性质和配体；其中节点是氨基酸，如果两个节点之间的距离少于6埃（Angstroms），则用一条边连接。</p><p>PROTEIN则是另一个蛋白质网络。任务是判断这类分子是否酶类。<br><strong>D&amp;D论文：</strong>《Distinguishing enzyme structures from non-enzymes without alignments》<br><strong>D&amp;D下载链接：</strong><a href="https://github.com/snap-stanford/GraphRNN/tree/master/dataset/DD">https://github.com/snap-stanford/GraphRNN/tree/master/dataset/DD</a></p><p><strong>PROTEIN论文：</strong>《Protein function prediction via graph kernels》</p><p><strong>Graph Kernel Datasets提供下载</strong></p><h3 id="PTC"><a href="#PTC" class="headerlink" title="PTC"></a><strong>PTC</strong></h3><p>PTC全称是预测毒理学挑战，用来发展先进的SAR技术预测毒理学模型。这个数据集包含了针对啮齿动物的致癌性标记的化合物。</p><p>根据实验的啮齿动物种类，一共有4个数据集：</p><ul><li>PTC_FM（雌性小鼠）</li><li>PTC_FR（雌性大鼠）</li><li>PTC_MM（雄性小鼠）</li><li>PTC_MR（雄性大鼠）</li></ul><p><strong>PTC论文：</strong>《Statistical evaluation of the predictive toxicology challenge 2000-2001》</p><p><strong>Graph Kernel Datasets提供下载</strong></p><h3 id="QM9"><a href="#QM9" class="headerlink" title="QM9"></a><strong>QM9</strong></h3><p>这个数据集有133,885个有机分子，包含几何、能量、电子等13个特征，最多有9个非氢原子（重原子）。来自GDB-17数据库。</p><p><strong>QM9论文：</strong>《Quantum chemistry structures and properties of 134 kilo molecules》<br><strong>QM9下载链接：</strong><a href="http://quantum-machine.org/datasets/">http://quantum-machine.org/datasets/</a></p><h3 id="Alchemy"><a href="#Alchemy" class="headerlink" title="Alchemy"></a><strong>Alchemy</strong></h3><p>Alchemy包含119,487个有机分子，其有12个量子力学特征（quantum mechanical properties），最多14个重原子（heavy atoms），从GDB MedChem数据库中取样。扩展了现有分子数据集多样性和容量。</p><p><strong>Alchemy论文：</strong>《Alchemy: A quantum chemistry dataset for benchmarking ai models》<br><strong>Alchemy下载链接：</strong><a href="https://alchemy.tencent.com/">https://alchemy.tencent.com/</a></p><table><thead><tr><th align="left"><strong>数据集</strong></th><th align="left"><strong>图数</strong></th><th align="left"><strong>节点数</strong></th><th align="left"><strong>边数</strong></th><th align="left"><strong>特征</strong></th><th align="left"><strong>标签</strong></th></tr></thead><tbody><tr><td align="left"><strong>PPI</strong></td><td align="left">24</td><td align="left">56,944</td><td align="left">818,716</td><td align="left">50</td><td align="left">121</td></tr><tr><td align="left"><strong>NCI-1</strong></td><td align="left">4110</td><td align="left">29.87</td><td align="left">32.30</td><td align="left">37</td><td align="left">2</td></tr><tr><td align="left"><strong>MUTAG</strong></td><td align="left">188</td><td align="left">17.93</td><td align="left">19.79</td><td align="left">7</td><td align="left">2</td></tr><tr><td align="left"><strong>D&amp;D</strong></td><td align="left">1178</td><td align="left">284.31</td><td align="left">715.65</td><td align="left">82</td><td align="left">2</td></tr><tr><td align="left"><strong>PROTEIN</strong></td><td align="left">1,113</td><td align="left">39.06</td><td align="left">72.81</td><td align="left">4</td><td align="left">2</td></tr><tr><td align="left"><strong>PTC_MR</strong></td><td align="left">344</td><td align="left">14.29</td><td align="left">14.69</td><td align="left">-</td><td align="left">2</td></tr><tr><td align="left"><strong>QM9</strong></td><td align="left">133,885</td><td align="left">-</td><td align="left">-</td><td align="left">-</td><td align="left">-</td></tr><tr><td align="left"><strong>Alchemy</strong></td><td align="left">119,487</td><td align="left">-</td><td align="left">-</td><td align="left">-</td><td align="left">-</td></tr></tbody></table><h2 id="社交网络"><a href="#社交网络" class="headerlink" title="社交网络"></a><strong>社交网络</strong></h2><h3 id="Reddit"><a href="#Reddit" class="headerlink" title="Reddit"></a><strong>Reddit</strong></h3><p>Reddit数据集是由来自Reddit论坛的帖子组成，如果两个帖子被同一人评论，那么在构图的时候，就认为这两个帖子是相关联的，标签是每个帖子对应的社区分类。</p><p><strong>Reddit论文：</strong>《Inductive representation learning on large graphs》<br><strong>Reddit下载链接：</strong><a href="https://github.com/linanqiu/reddit-dataset">https://github.com/linanqiu/reddit-dataset</a></p><h3 id="BlogCatalog"><a href="#BlogCatalog" class="headerlink" title="BlogCatalog"></a><strong>BlogCatalog</strong></h3><p>BlogCatalog数据集是一个社会关系网络，图是由博主及其社会关系（比如好友）组成，标签是博主的兴趣爱好。</p><p><strong>BlogCatalog论文：</strong>《Relational learning via latent social dimensions》<br><strong>BlogCatalog下载链接：</strong><a href="http://socialcomputing.asu.edu/datasets/BlogCatalog">http://socialcomputing.asu.edu/datasets/BlogCatalog</a></p><table><thead><tr><th align="left"><strong>数据集</strong></th><th align="left"><strong>节点数</strong></th><th align="left"><strong>边数</strong></th><th align="left"><strong>特征</strong></th><th align="left"><strong>标签</strong></th></tr></thead><tbody><tr><td align="left"><strong>Reddit</strong></td><td align="left">232965</td><td align="left">11606919</td><td align="left">602</td><td align="left">41</td></tr><tr><td align="left"><strong>BlogCatalog</strong></td><td align="left">10312</td><td align="left">333983</td><td align="left">-</td><td align="left">39</td></tr></tbody></table><h2 id="知识图谱"><a href="#知识图谱" class="headerlink" title="知识图谱"></a><strong>知识图谱</strong></h2><h3 id="FB13-FB15K-FB15K237"><a href="#FB13-FB15K-FB15K237" class="headerlink" title="FB13/FB15K/FB15K237"></a><strong>FB13/FB15K/FB15K237</strong></h3><p>这三个数据集是Freebase的子集。其中：</p><ul><li><strong>FB13</strong>：包含13种关系、75043个实体。</li><li><strong>FB15K</strong>：包含1345种关系、14951个实体</li><li><strong>FB15K237</strong>：包含237种关系、14951个实体</li></ul><p>如果希望找到entity id对应的实体数据，可以通过以下渠道（并不是所有的实体都能找到）：</p><ul><li><a href="https://developers.google.com/freebase/#freebase-wikidata-mappings">https://developers.google.com/freebase/#freebase-wikidata-mappings</a></li><li><a href="http://sameas.org/">http://sameas.org/</a></li></ul><h3 id="WN11-WN18-WN18RR"><a href="#WN11-WN18-WN18RR" class="headerlink" title="WN11/WN18/WN18RR"></a><strong>WN11/WN18/WN18RR</strong></h3><p>这三个是WordNet的子集：</p><ul><li><strong>WN11</strong>：包含11种关系、38696个实体</li><li><strong>WN18</strong>：包含18种关系、40943个实体</li><li><strong>WN18RR</strong>：包含11种关系、40943个实体</li></ul><blockquote><p>为了避免在评估模型时出现inverse relation test leakage，<strong>建议使用FB15K237/WN18RR</strong> 来替代FB15K/WN18。更多建议阅读《Convolutional 2D Knowledge Graph Embeddings》</p></blockquote><p><strong>FB15K/WN8论文：</strong>《Translating Embeddings for Modeling Multi-relational Data》<br><strong>FB13/WN11论文：</strong>《Reasoning With Neural Tensor Networks for Knowledge Base Completion》<br><strong>WN18RR论文：</strong>《Convolutional 2D Knowledge Graph Embeddings》</p><p>以上6个知识图谱数据集均可从这里下载：<a href="https://github.com/thunlp/OpenKE/tree/master/benchmarks">https://github.com/thunlp/OpenKE/tree/master/benchmarks</a></p><table><thead><tr><th align="left"><strong>数据集</strong></th><th align="left"><strong>关系</strong></th><th align="left"><strong>实体数</strong></th></tr></thead><tbody><tr><td align="left"><strong>FB13</strong></td><td align="left">13</td><td align="left">75043</td></tr><tr><td align="left"><strong>FB15K</strong></td><td align="left">1345</td><td align="left">14951</td></tr><tr><td align="left"><strong>FB15K237</strong></td><td align="left">237</td><td align="left">14951</td></tr><tr><td align="left"><strong>WN11</strong></td><td align="left">11</td><td align="left">38696</td></tr><tr><td align="left"><strong>WN18</strong></td><td align="left">18</td><td align="left">40943</td></tr><tr><td align="left"><strong>WN18RR</strong></td><td align="left">11</td><td align="left">40943</td></tr></tbody></table><h2 id="开源的数据仓库"><a href="#开源的数据仓库" class="headerlink" title="开源的数据仓库"></a>开源的数据仓库</h2><h3 id="Network-Repository"><a href="#Network-Repository" class="headerlink" title="Network Repository"></a><strong>Network Repository</strong></h3><p>具有交互式可视化和挖掘工具的图数据仓库。具有以下特点：</p><ul><li>用表格的形式展示每一个图数据集的节点数、遍数、平均度数、最大度数等。</li><li>可视化对比图数据集之间的参数。</li><li>在线GraphVis，可视化图结构和详细参数。</li></ul><p><strong>链接：</strong><a href="http://networkrepository.com/">http://networkrepository.com</a></p><h3 id="Graph-Kernel-Datasets"><a href="#Graph-Kernel-Datasets" class="headerlink" title="Graph Kernel Datasets"></a><strong>Graph Kernel Datasets</strong></h3><p>图核的基准数据集。提供了一个表格，可以快速得到每个数据集的节点数量、类别数量、是否有节点/边标签、节点/边特征。</p><p><strong>链接：</strong><a href="https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets">https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets</a></p><h3 id="Relational-Dataset-Repository"><a href="#Relational-Dataset-Repository" class="headerlink" title="Relational Dataset Repository"></a><strong>Relational Dataset Repository</strong></h3><p>关系机器学习的数据集集合。能够以数据集大小、领域、数据类型等条件来检索数据集。</p><p><strong>链接：</strong><a href="https://relational.fit.cvut.cz/">https://relational.fit.cvut.cz</a></p><h3 id="Stanford-Large-Network-Dataset-Collection"><a href="#Stanford-Large-Network-Dataset-Collection" class="headerlink" title="Stanford Large Network Dataset Collection"></a><strong>Stanford Large Network Dataset Collection</strong></h3><p>SNAP库包含了一个大型图网络数据集集合，拥有大型社交、信息网络。包括：图分类数据库、社交网络、引用网络、亚马逊网络等等，非常丰富。</p><p><strong>链接：</strong><a href="https://snap.stanford.edu/data/">https://snap.stanford.edu/data/</a></p><h3 id="Open-Graph-Benchmark"><a href="#Open-Graph-Benchmark" class="headerlink" title="Open Graph Benchmark"></a><strong>Open Graph Benchmark</strong></h3><p>OGB是真实基准数据集的集合，同时提供数据加载器和评估器（PyTorch）。可以自动下载、处理和切割；完全兼容PyG和DGL。</p><p><strong>链接：</strong><a href="https://ogb.stanford.edu/">https://ogb.stanford.edu/</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 数据集 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 开放数据 </tag>
            
            <tag> 图神经 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>几种常见激活函数</title>
      <link href="/2020/09/10/ji-huo-han-shu/"/>
      <url>/2020/09/10/ji-huo-han-shu/</url>
      
        <content type="html"><![CDATA[<h1 id="几种常见激活函数"><a href="#几种常见激活函数" class="headerlink" title="几种常见激活函数"></a>几种常见激活函数</h1><h2 id="为什么需要激活函数"><a href="#为什么需要激活函数" class="headerlink" title="为什么需要激活函数"></a>为什么需要激活函数</h2><ul><li><strong>非线性:</strong> 首先,线性函数可以高效可靠对数据进行拟合, 但是现实生活中往往存在一些非线性的问题(如XOR), 这个时候, 我们就需要借助激活函数的非线性来对数据的分布进行重新映射, 从而获得更强大的拟合能力. (这个是最主要的原因, 其他还有下面这些性质也使得我们选择激活函数作为网络常用层)</li><li><strong>可微性:</strong> 这一点有助于我们使用梯度下降发来对网络进行优化</li><li><strong>单调性:</strong> 激活函数的单调性在可以使单层网络保证网络是凸优化的</li><li><strong>f(x)≈x</strong>: 当激活满足这个性质的时候, 如果参数初值是很小的值, 那么神经网络的训练将会很高效(参考ResNet训练残差模块的恒等映射); 如果不满足这个性质, 那么就需要用心的设值初始值( <strong>这一条有待商榷</strong> )</li></ul><h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><h3 id="Sigmoid"><a href="#Sigmoid" class="headerlink" title="Sigmoid"></a>Sigmoid</h3><p><strong>优点：</strong></p><ol><li>Sigmoid函数的输出映射在**(0,1)**之间，单调连续，输出范围有限，如果是非常大的负数，那么输出就是0；如果是非常大的正数，输出就是1。优化稳定，可以用作输出层。</li><li>求导容易。</li><li>sigmoid 函数曾经被使用的很多，不过近年来，用它的人越来越少了。</li></ol><p><strong>缺点：</strong></p><ol><li>容易饱和和终止梯度传递(“死神经元”)；</li><li>sigmoid函数的输出没有0中心化。</li></ol><h3 id="双曲正切函数（Tanh）"><a href="#双曲正切函数（Tanh）" class="headerlink" title="双曲正切函数（Tanh）"></a>双曲正切函数（Tanh）</h3><p><strong>优点</strong>：</p><ol><li>比Sigmoid函数收敛速度更快。</li><li>相比Sigmoid函数，其输出以0为中心。</li></ol><p><strong>缺点</strong>：</p><p>还是没有改变Sigmoid函数的最大问题——由于饱和性产生的梯度消失。</p><h3 id="修正线性单元（Rectified-linear-unit，ReLU）"><a href="#修正线性单元（Rectified-linear-unit，ReLU）" class="headerlink" title="修正线性单元（Rectified linear unit，ReLU）"></a>修正线性单元（Rectified linear unit，ReLU）</h3><p><strong>点</strong>：</p><p>1.相比起Sigmoid和tanh，ReLU在SGD中能够快速收敛，这是因为它线性（linear）、非饱和（non-saturating）的形式。</p><p>2.Sigmoid和tanh涉及了很多很expensive的操作（比如指数），ReLU可以更加简单的实现。</p><p>3.有效缓解了梯度消失的问题。</p><p>4.在没有无监督预训练的时候也能有较好的表现。</p><p><strong>缺点</strong>：</p><ol><li>没有边界，可以使用变种ReLU: min(max(0,x), 6)</li><li>比较脆弱，比较容易陷入出现”死神经元”的情况</li></ol><p>• 解决方案： 较小的学习率</p><h2 id="激活函数的使用原则"><a href="#激活函数的使用原则" class="headerlink" title="激活函数的使用原则"></a>激活函数的使用原则</h2><ol><li>优先使用ReLU, 同时要谨慎设置初值和学习率 ( 实际操作中，如果你的learning rate 很大，那么很有可能你网络中的40%的神经元都 “dead” 了。 当然，如果你设置了一个合适的较小的learning rate，这个问题发生的情况其实也不会太频繁 )</li><li>尝试使用LeakyReLU/PReLU/Maxout/ELU等激活函数</li><li>可以试下tanh, 但是一般不会有太好的结果</li><li><strong>不要使用sigmoid</strong></li></ol><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>标注精灵</title>
      <link href="/2020/09/07/biao-zhu-jing-ling/"/>
      <url>/2020/09/07/biao-zhu-jing-ling/</url>
      
        <content type="html"><![CDATA[<h1 id="标注精灵（ubantu）"><a href="#标注精灵（ubantu）" class="headerlink" title="标注精灵（ubantu）"></a>标注精灵（ubantu）</h1><ul><li><p>下载标注精灵</p><p><a href="http://www.jinglingbiaozhu.com/">http://www.jinglingbiaozhu.com/</a></p></li><li><p>在命令行上</p><p>chmod a+x “软件”</p></li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 脚本 </tag>
            
            <tag> lniux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ngnix Flask 高并发部署方案详细教程！</title>
      <link href="/2020/09/06/ngnix/"/>
      <url>/2020/09/06/ngnix/</url>
      
        <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a><strong>前言</strong></h3><p>虽然标题写的是 Flask，但是下面这个教程不仅仅只适用于 Flask, 还适用于其他Python web 框架，记得帮忙点赞！</p><p>众所周知 Flask 是一个同步的框架，处理请求的时候是以单进程的方式，当同时访问的人数过多时，Flask 服务就会出现阻塞的情况。</p><p>就像我们买火车票一样，当买火车票的人多的时候，排队的人就会很多，队伍就会很长，相应的等待的时间会变得很长！</p><p>因此 Flask, Django，webpy 等框架自带的 web server 性能都很差，只能用来做测试用途，线上发布则需要选择更高性能的 wsgi server 。这里推荐的部署方式：nginx + gunicorn + flask + supervisor</p><p>其中每个服务代表的含义如下：</p><ul><li>Nginx：高性能 Web 服务器+负载均衡；</li><li>gunicorn：高性能 WSGI 服务器；</li><li>gevent：把 Python 同步代码变成异步协程的库；</li><li>Supervisor：监控服务进程的工具；</li></ul><h3 id="Gunicorn"><a href="#Gunicorn" class="headerlink" title="Gunicorn"></a><strong>Gunicorn</strong></h3><p>Gunicorn 可以指定多个工作进程，有多种工作模式可以供你选择。默认是同步的 sync 工作模式，除此之外还有 gevent, tronado, gthread, gaiohttp 等。</p><p>这里推荐 gevent, gevent 是一个基于 Greenlet 库，利用 python 协程来实现，这样你的 web 服务才能实现并发的功能！</p><p>gunicorn 是一个 python Wsgi http server，只支持在 Unix 系统上运行，下面我们来熟悉一下以 gunicorn 的配置与使用。</p><h3 id="gunicorn-的安装"><a href="#gunicorn-的安装" class="headerlink" title="gunicorn 的安装"></a>gunicorn 的安装</h3><p>注意 gunicorn 不能在 windows 环境下使用</p><blockquote><p>pip install gunicorn</p></blockquote><p>比如有以下一个 app 的 py 文件</p><pre><code> from flask import Flask   app = Flask(__name__)   @app.route('/')   def index():       return 'hello world!' if __name__ == '__main__':      app.run()   </code></pre><p>那么我们在 flask 的项目的目录下如何使用 gunicorn 来启动呢?<br>命令如下：</p><pre><code>gunicorn -w 4 -b 0.0.0.0:8000 app:app</code></pre><p>其中：<br>第一个 app 指的是 app.py 文件；<br>第二个指的是第三行代码 flask 应用的名字。</p><h3 id="gunicorn-的参数详解"><a href="#gunicorn-的参数详解" class="headerlink" title="gunicorn 的参数详解"></a>gunicorn 的参数详解</h3><p>通过 gunicorn -h 我们可以看到 gunicorn 有非常多的配置项，我将他们的参数配置项都列出来了，供大家参考：</p><pre><code> -c CONFIG : CONFIG,配置文件的路径，通过配置文件启动；生产环境使用； -b ADDRESS : ADDRESS，ip加端口，绑定运行的主机； -w INT,  --workers INT：用于处理工作进程的数量，为正整数，默认为1； -k STRTING, --worker-class STRTING：要使用的工作模式，默认为sync异步，可以下载eventlet和gevent并指定 --threads INT：处理请求的工作线程数，使用指定数量的线程运行每个worker。为正整数，默认为1。--worker-connections INT：最大客户端并发数量，默认情况下这个值为1000。--backlog int：未决连接的最大数量，即等待服务的客户的数量。默认2048个，一般不修改；-p FILE, --pid FILE：设置pid文件的文件名，如果不设置将不会创建pid文件--access-logfile FILE ： 要写入的访问日志目录--access-logformat STRING：要写入的访问日志格式--error-logfile FILE, --log-file FILE ：  要写入错误日志的文件目录。--log-level LEVEL ：  错误日志输出等级。--limit-request-line INT ： HTTP请求头的行数的最大大小，此参数用于限制HTTP请求行的允许大小，默认情况下，这个值为4094。值是0~8190的数字。--limit-request-fields INT ： 限制HTTP请求中请求头字段的数量。此字段用于限制请求头字段的数量以防止DDOS攻击，默认情况下，这个值为100，这个值不能超过32768--limit-request-field-size INT ： 限制HTTP请求中请求头的大小，默认情况下这个值为8190字节。值是一个整数或者0，当该值为0时，表示将对请求头大小不做限制-t INT, --timeout INT：超过这么多秒后工作将被杀掉，并重新启动。一般设定为30秒；--daemon：是否以守护进程启动，默认false；--chdir ：在加载应用程序之前切换目录；--graceful-timeout INT：默认情况下，这个值为30，在超时(从接收到重启信号开始)之后仍然活着的工作将被强行杀死；一般使用默认；--keep-alive INT：在keep-alive连接上等待请求的秒数，默认情况下值为2。一般设定在1~5秒之间。--reload：默认为False。此设置用于开发，每当应用程序发生更改时，都会导致工作重新启动。--spew：打印服务器执行过的每一条语句，默认False。此选择为原子性的，即要么全部打印，要么全部不打印；--check-config ：显示现在的配置，默认值为False，即显示。-e ENV,  --env ENV： 设置环境变量；</code></pre><p>是不是觉得 gunicorn 的参数很多？在部署的时候如果输入这么大一串命令，我相信就算是对参数很熟悉的人，也有可能会输错！</p><p>那么，有没有更方便的方法呢？当然！那就是用以配置文件的方式启动。</p><h3 id="以配置文件的方式启动"><a href="#以配置文件的方式启动" class="headerlink" title="以配置文件的方式启动"></a>以配置文件的方式启动</h3><p>一般这个配置文件名是 gunicorn.conf.py 或者 config.py。其中部分代码如下:</p><pre><code> # 并行工作进程数 workers = 4 # 指定每个工作者的线程数 threads = 2 # 端口 5000 bind = '0.0.0.0:5000'# 设置守护进程,将进程交给supervisor管理daemon = 'false'# 工作模式协程worker_class = 'gevent'# 设置最大并发量worker_connections = 2000# 设置进程文件目录pidfile = '/var/run/gunicorn.pid'# 设置访问日志和错误信息日志路径accesslog = "log/access.log"errorlog = "log/debug.log"loglevel = "debug"# 设置日志记录水平loglevel = 'warning'</code></pre><p>代码里面的注释都很详细了，但是还是有几点需要说明的：</p><h4 id="1-日志"><a href="#1-日志" class="headerlink" title="1.日志"></a>1.日志</h4><p>配置文件中指定了日志文件的输出目录，需要注意的是：</p><ul><li>需要log目录存在，如果不存在，启动会报错</li><li>accesslog 是访问日志，可以通过 access_log_format 设置访问日志格式</li><li>loglevel 用于控制 errorlog 的信息级别</li></ul><p>不过更建议使用 logging 模块来管理日志</p><h4 id="2-workers"><a href="#2-workers" class="headerlink" title="2. workers"></a>2. workers</h4><ul><li>worker_class 是指开启的每个工作进程的模式类型，默认为 sync 模式，这个使用 gevent 模式，gevent 是 python 的一个高并发库</li><li>workers 是并行工作进程数 ，在上述配置文件中，取的是服务器的 CPU的数量。</li><li>需要注意的是，这个数字不是越大越好，因为我们还要注意部署机器的性能，不能无限制多开。一般是根据服务器的 CPU核心数来确定的！</li><li>workers = multiprocessing * cpu_count() * 2 + 1</li></ul><p>配置文件搞定之后，启动命令就很简单了。启动 gunicorn 命令如下：</p><pre><code>gunicorn -c gunicorn.conf app:app</code></pre><h3 id="Nginx"><a href="#Nginx" class="headerlink" title="Nginx"></a><strong>Nginx</strong></h3><p>Nginx 实际上只能处理静态资源请求，那么对于动态请求怎么做呢。这就需要用到 Nginx 的 <code>upstream</code> 模块对这些请求进行转发，即反向代理。Nginx 在这里主要是用来做负载均衡，同时它能缓存一些动态内容</p><h3 id="安装-nginx"><a href="#安装-nginx" class="headerlink" title="安装 nginx"></a><strong>安装 nginx</strong></h3><p>安装命令如下：</p><pre class=" language-text"><code class="language-text">sudo apt-get install nginx</code></pre><p>nginx 安装完后，我们可以通过以下命令控制 nginx 的开启和关闭</p><pre class=" language-text"><code class="language-text">sudo /etc/init.d/nginx restart // 重启sudo /etc/init.d/nginx start 开启sudo /etc/init.d/nginx stop 关闭</code></pre><h3 id="centos安装"><a href="#centos安装" class="headerlink" title="centos安装"></a>centos安装</h3><pre class=" language-shell"><code class="language-shell">#安装nginx前首先要确认系统中安装了gcc、pcre-devel、zlib-devel、openssl-develyum -y install gcc pcre-devel zlib-devel openssl openssl-devel</code></pre><p>nginx下载地址：<a href="https://nginx.org/download/">https://nginx.org/download/</a></p><pre class=" language-shell"><code class="language-shell">## 解压tar -zxvf nginx-1.9.9.tar.gz##进入nginx目录cd nginx-1.9.9## 配置./configure --prefix=/usr/local/nginx# makemakemake install# 测试是否安装成功# cd到刚才配置的安装目录/usr/loca/nginx/./sbin/nginx -t</code></pre><p>6996#0: open() “/usr/local/nginx/logs/access.log” failed (2: No such file or directory)</p><p>原因分析：nginx/目录下没有logs文件夹</p><p>解决方法：</p><pre class=" language-shell"><code class="language-shell">mkdir logschmod 700 logs</code></pre><p>正常情况的信息输出：</p><p>nginx: the configuration file /usr/local/nginx/conf/nginx.conf syntax is ok<br>nginx: configuration file /usr/local/nginx/conf/nginx.conf test is successful</p><p>启动nginx </p><pre class=" language-shell"><code class="language-shell">cd /usr/local/nginx/sbin./nginx //启动nginx</code></pre><p>在浏览器中输入服务器的ip地址，如：192.168.1.12</p><pre class=" language-shell"><code class="language-shell">ping 192.168.1.12</code></pre><p>[error] invalid PID number “” in “/usr/local/nginx/logs/nginx.pid”</p><p>解决方法：用指定文件加载nginx配置文件<br>[root@localhost nginx]/usr/local/nginx/sbin/nginx -c /usr/local/nginx/conf/nginx.conf</p><h3 id="配置-nginx"><a href="#配置-nginx" class="headerlink" title="配置 nginx"></a><strong>配置 nginx</strong></h3><p>Nginx 配置文件位于 /usr/local/nginx/conf/nginx.conf</p><h4 id="部署单个应用的配置文件"><a href="#部署单个应用的配置文件" class="headerlink" title="部署单个应用的配置文件"></a>部署单个应用的配置文件</h4><pre class=" language-nginx"><code class="language-nginx"><span class="token keyword">server</span> <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;</span>    <span class="token keyword">listen</span>     <span class="token number">8080</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true"># 监听8080端口,可以自行配置</span>    <span class="token keyword">server_name</span> localhost<span class="token punctuation">;</span> <span class="token comment" spellcheck="true"># 配置域名</span>    <span class="token comment" spellcheck="true"># 动态请求转发到 9600 端口(gunicorn):</span>    <span class="token keyword">location</span> <span class="token operator">/</span> <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;</span>        <span class="token keyword">proxy_pass</span>   <span class="token keyword">http</span><span class="token punctuation">:</span><span class="token operator">/</span><span class="token operator">/</span><span class="token number">127.0</span><span class="token punctuation">.</span><span class="token number">0.1</span><span class="token punctuation">:</span><span class="token number">9600</span><span class="token punctuation">;</span>        <span class="token keyword">proxy_redirect</span> off<span class="token punctuation">;</span>        <span class="token keyword">proxy_set_header</span> X<span class="token operator">-</span>Real<span class="token operator">-</span>IP <span class="token variable">$remote_addr</span><span class="token punctuation">;</span>        <span class="token keyword">proxy_set_header</span> Host <span class="token variable">$host</span><span class="token punctuation">;</span>        <span class="token keyword">proxy_set_header</span> X<span class="token operator">-</span>Forwarded<span class="token operator">-</span>For <span class="token variable">$proxy_add_x_forwarded_for</span><span class="token punctuation">;</span>       <span class="token keyword">proxy_read_timeout</span> <span class="token number">300</span><span class="token punctuation">;</span>       <span class="token keyword">proxy_send_timeout</span> <span class="token number">300</span><span class="token punctuation">;</span>    <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#125;</span><span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#125;</span></code></pre><h4 id="配置多个应用的配置信息"><a href="#配置多个应用的配置信息" class="headerlink" title="配置多个应用的配置信息"></a>配置多个应用的配置信息</h4><pre class=" language-nginx"><code class="language-nginx"><span class="token keyword">server</span> <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;</span>    <span class="token keyword">listen</span>     <span class="token number">8080</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true"># 监听8080端口,可以自行配置</span>    <span class="token keyword">server_name</span> localhost<span class="token punctuation">;</span> <span class="token comment" spellcheck="true"># 配置域名</span>    <span class="token comment" spellcheck="true"># 动态请求转发到 9600 端口(gunicorn):</span>    <span class="token keyword">location</span> <span class="token operator">/</span> app1<span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;</span>        <span class="token keyword">proxy_pass</span>   <span class="token keyword">http</span><span class="token punctuation">:</span><span class="token operator">/</span><span class="token operator">/</span><span class="token number">127.0</span><span class="token punctuation">.</span><span class="token number">0.1</span><span class="token punctuation">:</span><span class="token number">9600</span><span class="token punctuation">;</span>        <span class="token keyword">proxy_redirect</span> off<span class="token punctuation">;</span>        <span class="token keyword">proxy_set_header</span> X<span class="token operator">-</span>Real<span class="token operator">-</span>IP <span class="token variable">$remote_addr</span><span class="token punctuation">;</span>        <span class="token keyword">proxy_set_header</span> Host <span class="token variable">$host</span><span class="token punctuation">;</span>        <span class="token keyword">proxy_set_header</span> X<span class="token operator">-</span>Forwarded<span class="token operator">-</span>For <span class="token variable">$proxy_add_x_forwarded_for</span><span class="token punctuation">;</span>       <span class="token keyword">proxy_read_timeout</span> <span class="token number">300</span><span class="token punctuation">;</span>       <span class="token keyword">proxy_send_timeout</span> <span class="token number">300</span><span class="token punctuation">;</span>    <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#125;</span>    <span class="token keyword">location</span> <span class="token operator">/</span> app2<span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;</span>        <span class="token keyword">proxy_pass</span>   <span class="token keyword">http</span><span class="token punctuation">:</span><span class="token operator">/</span><span class="token operator">/</span><span class="token number">127.0</span><span class="token punctuation">.</span><span class="token number">0.1</span><span class="token punctuation">:</span><span class="token number">9601</span><span class="token punctuation">;</span>        <span class="token keyword">proxy_redirect</span> off<span class="token punctuation">;</span>        <span class="token keyword">proxy_set_header</span> X<span class="token operator">-</span>Real<span class="token operator">-</span>IP <span class="token variable">$remote_addr</span><span class="token punctuation">;</span>        <span class="token keyword">proxy_set_header</span> Host <span class="token variable">$host</span><span class="token punctuation">;</span>        <span class="token keyword">proxy_set_header</span> X<span class="token operator">-</span>Forwarded<span class="token operator">-</span>For <span class="token variable">$proxy_add_x_forwarded_for</span><span class="token punctuation">;</span>       <span class="token keyword">proxy_read_timeout</span> <span class="token number">300</span><span class="token punctuation">;</span>       <span class="token keyword">proxy_send_timeout</span> <span class="token number">300</span><span class="token punctuation">;</span>    <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#125;</span><span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#125;</span></code></pre><p>修改完之后保存，重启 nginx.</p><h3 id="Supervisor"><a href="#Supervisor" class="headerlink" title="Supervisor"></a><strong>Supervisor</strong></h3><h3 id="安装-supervisor"><a href="#安装-supervisor" class="headerlink" title="安装 supervisor"></a><strong>安装 supervisor</strong></h3><p>命令如下:</p><pre class=" language-text"><code class="language-text">pip install supervisor</code></pre><p>初始化配置文件：</p><pre class=" language-text"><code class="language-text">echo_supervisord_conf > supervisor.conf</code></pre><p>修改配置文件，在配置文件最底部添加相应配置</p><pre class=" language-text"><code class="language-text">[include] 自己的项目配置  [program:project]  directory = /home/jerry/Code/project       ; 程序的启动目录  command = /home/jerry/.virtualenvs/parsing/bin/gunicorn -w 4 -worker-class gevent -bind 0.0.0.0:9600 app:app  ; 启动命令numprocs=1           ; number of processes copies to start (def 1)   autostart = true     ; 在 supervisord 启动的时候也自动启动   startsecs = 1        ; 启动 1 秒后没有异常退出，就当作已经正常启动了   autorestart = true   ; 程序异常退出后自动重启   startretries = 3     ; 启动失败自动重试次数，默认是 3   user = root          ; 用哪个用户启动   redirect_stderr = true          ; 把 stderr 重定向到 stdout，默认 false   stdout_logfile_maxbytes = 20MB  ; stdout 日志文件大小，默认 50MB   stdout_logfile_backups = 10     ; stdout 日志文件备份数 stdout_logfile=/home/jerry/Code/project/log/gunicorn.log       ; log 日志stderr_logfile=/home/jerry/Code/project/log/gunicorn.error     ; 错误日志</code></pre><p>编辑完之后保存，启动 supervisor。这里的启动命令和在命令行用 gunicorn 启动的命令是一致的，其中 -w 是指服务的进程数，详细命令查看我之前写的那篇文章哈。</p><h3 id="基本命令"><a href="#基本命令" class="headerlink" title="基本命令"></a><strong>基本命令</strong></h3><p>通过配置文件启动 supervisor</p><pre class=" language-text"><code class="language-text">supervisord -c supervisor.conf </code></pre><p>查看 supervisor 的状态</p><pre class=" language-text"><code class="language-text">supervisorctl -c supervisor.conf status </code></pre><p>重新载入配置文件,每次修改之后记得重新载入</p><pre class=" language-text"><code class="language-text">supervisorctl -c supervisor.conf reload</code></pre><p>启动指定/所有 supervisor 管理的程序进程</p><pre class=" language-text"><code class="language-text">supervisorctl -c supervisor.conf start [all]|[appname]</code></pre><p>关闭指定/所有 supervisor管理的程序进程</p><pre class=" language-text"><code class="language-text">supervisorctl -c supervisor.conf stop [all]|[appname]</code></pre><p>这时候通过 <a href="https://link.zhihu.com/?target=http://127.0.0.1:8080">http://127.0.0.1:8080</a> 就能访问你的应用了! 想知道效果如何，可以自己测试一下，比如在代码中增加 sleep,或者自己动手写个脚本测试！</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="http://cocofe.cn/2017/07/25/%E5%90%8C%E6%97%B6%E5%9C%A8%E4%B8%80%E4%B8%AA%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8A%E9%83%A8%E7%BD%B2%E5%A4%9A%E4%B8%AAFlask%E5%BA%94%E7%94%A8/">http://cocofe.cn/2017/07/25/%E5%90%8C%E6%97%B6%E5%9C%A8%E4%B8%80%E4%B8%AA%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8A%E9%83%A8%E7%BD%B2%E5%A4%9A%E4%B8%AAFlask%E5%BA%94%E7%94%A8/</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 运维 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> falsk </tag>
            
            <tag> ngnix </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Bahdanau 和 Luong 两种 Attention 机制的区别</title>
      <link href="/2020/09/05/bahdanau-he-luong-liang-chong-attention-ji-zhi-de-qu-bie/"/>
      <url>/2020/09/05/bahdanau-he-luong-liang-chong-attention-ji-zhi-de-qu-bie/</url>
      
        <content type="html"><![CDATA[<h1 id="Bahdanau-和-Luong-两种-Attention-机制的区别"><a href="#Bahdanau-和-Luong-两种-Attention-机制的区别" class="headerlink" title="Bahdanau 和 Luong 两种 Attention 机制的区别"></a>Bahdanau 和 Luong 两种 Attention 机制的区别</h1><p>转载至<a href="https://zhuanlan.zhihu.com/p/129316415">https://zhuanlan.zhihu.com/p/129316415</a></p><p><img src="/2020/09/05/bahdanau-he-luong-liang-chong-attention-ji-zhi-de-qu-bie/1.jpg"></p><ul><li><strong>注意力的计算方式不同</strong><br>在 Luong Attention 机制中，第 t 步的注意力 $c  _t$是由 decoder 第 t 步的 hidden state$h _t$ 与 encoder 中的每一个 hidden state $\overline{h} _s$ 加权计算得出的。而在 Bahdanau Attention 机制中，第 t 步的注意力$c  _t$  是由 decoder 第 t-1 步的 hidden state $h _{t-1}$ 与 encoder 中的每一个 hidden state $\overline {h} _s$ 加权计算得出的。</li><li><strong>decoder 的输入输出不同</strong><br>在 Bahdanau Attention 机制中，decoder 在第 t 步时，输入是由注意力  $c  _t$ 与前一步的 hidden state $h _{t-1}$  拼接（concatenate）得出的，得到第 t 步的 hidden state $ h _t $  并直接输出 $ \widehat{y} _t$ 。而 Luong Attention 机制在 decoder 部分建立了一层额外的网络结构，以注意力 $c _t $ 与原 decoder 第 t 步的 hidden state $h _t$  拼接作为输入，得到第 t 步的 hidden state  $ \widetilde{h} _t$  并输出 $\widehat{y} _t$  。</li></ul><p><strong>总结一下</strong>，Bahdanau Attention 机制的计算流程为 $h _{t-1} \rightarrow a _t \rightarrow c _t \rightarrow h _t$ 。而 Luong attention 机制的计算流程为  $h _{t} \rightarrow a _t \rightarrow c _t \rightarrow h _t$ 。相较而言， Luong attention 机制中的 decoder 在每一步使用当前步（而非前一步）的 hidden state 来计算注意力，从逻辑上更自然，但需要使用一层额外的 RNN decoder 来计算输出。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> nlp </category>
          
      </categories>
      
      
        <tags>
            
            <tag> attention </tag>
            
            <tag> 注意力机制 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GRAPH-BERT: Only Attention is Needed for Learning Graph Representations</title>
      <link href="/2020/09/03/graph-bert-only-attention-is-needed-for-learning-graph-representations/"/>
      <url>/2020/09/03/graph-bert-only-attention-is-needed-for-learning-graph-representations/</url>
      
        <content type="html"><![CDATA[<h1 id="GRAPH-BERT-Only-Attention-is-Needed-for-Learning-Graph-Representations"><a href="#GRAPH-BERT-Only-Attention-is-Needed-for-Learning-Graph-Representations" class="headerlink" title="GRAPH-BERT: Only Attention is Needed for Learning Graph Representations"></a>GRAPH-BERT: Only Attention is Needed for Learning Graph Representations</h1><p>Paper<a href="https://link.zhihu.com/?target=https://github.com/jwzhanggy/Graph-Bert">GRAPH-BERT: Only Attention is Needed for Learning Graph Representations</a></p><p>Code<a href="https://link.zhihu.com/?target=https://github.com/jwzhanggy/Graph-Bert">torch</a></p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul><li><p>模型假死(suspended animation)：随着训练层数加深，模型不再对输入数据响应；</p></li><li><p>过度平滑(over-smoothing)：随着训练层数加深，所有结点的embedding会越来越相似；</p></li><li><p>并行化处理(paralelization)：在大图上无法并行化处理</p></li></ul><p>针对以上问题，提出；额新的神经网络（Graph-Bert），他完全基于注意力机制去学习节点特征，没有使用聚类算子.主要通过子图来训练，</p><p><img src="/2020/09/03/graph-bert-only-attention-is-needed-for-learning-graph-representations/1.png"></p><p>GRAPH-BERT涉及几个部分：</p><p>从原始大图中采样无连接子图（linkless graph）</p><p>输入结点embedding</p><p>处理基于图的transformer-encoder</p><p>基于图的transformer-decoder</p><h4 id="子图采样"><a href="#子图采样" class="headerlink" title="子图采样"></a><strong>子图采样</strong></h4><p>​        为了更好地处理大图（并行化），graph-bert选择在采样子图上进行训练。使用了一种基于图亲密度矩阵 的采样方案：<strong>「top-k intimacy」</strong>，其中$S(i,j)$表示节点$v _i$和$v -j$结点 之间的亲密度，计算公式为：</p><p>$S = \alpha \cdot (I - (1- \alpha) \cdot A) ^{-1}$</p><p>​      其中，$\alpha$ 是一个[0, 1]之间的超参数（通常取0.15），$A = AD ^-1$ 表示列规范化邻接矩阵， $A$为图的邻接矩阵， $D$为对应的对角矩阵 $D(i,i) = \sum _{j} A(i,j)$。</p><p>​      那么对于一个给定的目标结点，就可以利用上面定义的亲密度来找出其上下文结点</p><p>​     将输入向量嵌入到graph-transformer模型中实际上包括四个部分</p><ul><li><p><strong>原始特征embedding</strong></p><p>就是使用一个映射操作将原始特征表示到新的共享的特征空间，记为$e _j ^{(x)} = Embed(x _j) \in R ^{d _n \times 1}$</p><p>对于不同的输入可以有不同的映射函数，如CNN/LSTM/BERT等</p></li><li><p><strong>Weisfeiler-Lehman 绝对角色embedding</strong></p><p>Weisfeiler-Lehman算法是用来确定两个图是否是同构的，其基本思路是通过迭代式地聚合邻居节点的信息来判断当前中心节点的独立性(Identity)，从而更新整张图的编码表示。</p><p>将节点WL绝对角色嵌入向量定义为</p><p><img src="/2020/09/03/graph-bert-only-attention-is-needed-for-learning-graph-representations/2.png"></p><p>有关更多WL算法的细节可以参考这个slides：Graph Kernel</p></li><li><p><strong>基于亲密度的相对位置embedding</strong></p><p>上一节计算的嵌入可以表示全局的信息，而这一步主要是获取局部信息。</p><p>形式上，对于node $v _j$，我们还可以使用上面定义的position - embed()函数提取其基于时间复杂度的相对位置嵌入，如下所示</p><img src="/2020/09/03/graph-bert-only-attention-is-needed-for-learning-graph-representations/3.png"></li><li><p><strong>基于相对距离embedding</strong></p><p>​    对两个结点在原始大图中的距离（间隔边的数量）进行embedding表示，主要是为了平衡上述两步的embedding</p><p>​     形式上，对于子图中的node$v _j ∈V _i$，我们可以将其相对于原始输入图的跳数表示为$H（v _j; v _i）$，可用于将其嵌入向量定义为</p><img src="/2020/09/03/graph-bert-only-attention-is-needed-for-learning-graph-representations/4.png"></li><li><h4 id="Transfomer编码器"><a href="#Transfomer编码器" class="headerlink" title="Transfomer编码器"></a><strong>Transfomer编码器</strong></h4><p>首先是对前面得到的四个embeeding进行聚合，</p><p>聚合的函数有很多可以选择，文章里作者就使用了最简单的加和操作。聚合之后就可以得到所有结点的输入表示</p></li></ul><img src="/2020/09/03/graph-bert-only-attention-is-needed-for-learning-graph-representations/5.png"><ul><li><p><strong>更新</strong></p><p>$g _i$中所有节点的初始输入矢量可以组织成一个矩阵$H ^{(0)} =[h _i ^{(0)}, h _{i,1} ^{(0)},…,h _{i,k} ^{(0)}] ^T \in R ^{(k+1) \cdot d _n} $</p><p>然后就是进行N层的transformer encoder的迭代更新</p><img src="/2020/09/03/graph-bert-only-attention-is-needed-for-learning-graph-representations/6.png"><p>符号$G-Res H ^{(l-1)} , X _i$，表示图残差项，$X _i \in R ^{(k+1) \cdot d _n}$ 是子图gi中所有节点的原始特征。</p></li><li><p><strong>输出</strong></p><p>经过D层的编码之后，我们就可以得到对应每个结点的表示， 之后就可以根据具体的下游任务来使用这些向量表示。</p><img src="/2020/09/03/graph-bert-only-attention-is-needed-for-learning-graph-representations/7.png"></li></ul><h2 id="预训练"><a href="#预训练" class="headerlink" title="预训练"></a><strong>预训练</strong></h2><p>预训练包括两项任务:</p><ul><li><p><strong>节点属性重建</strong></p><p>对于目标结点 $v _i$，原始特征为 $x _i$，我们通过GRAPH-BERT编码层可以得到其隐藏表示 $z _i$， 然后经过一层FC映射后$\widehat{x } _i = FC(z _i)$重建原始特征。</p><p>我们可以如下定义基于节点原始属性重构的损失项</p><img src="/2020/09/03/graph-bert-only-attention-is-needed-for-learning-graph-representations/8.png"></li><li><p><strong>图结构重建</strong></p><p>这一部分其实是考虑了图的结构信息，利用两个节点表示的cos距离与之前提前算好的亲密度打分，来做损失。</p><img src="/2020/09/03/graph-bert-only-attention-is-needed-for-learning-graph-representations/9.png"><p>其中，$\widehat{S}(i,j) = \widehat{s}  _{i,j}$</p></li></ul><h2 id="模型转移和微调"><a href="#模型转移和微调" class="headerlink" title="模型转移和微调"></a>模型转移和微调</h2><p>​       以节点分类和图聚类任务为例，其中图聚类可以直接使用学习的表示形式，但是对节点分类任务必须进行微调</p><ul><li><p><strong>节点分</strong></p><p>定义训练批处理中引入的节点分类丢失项</p><img src="/2020/09/03/graph-bert-only-attention-is-needed-for-learning-graph-representations/10.png"></li><li><p><strong>图聚类</strong></p><p>图聚类的目标函数定义如下</p></li></ul><img src="/2020/09/03/graph-bert-only-attention-is-needed-for-learning-graph-representations/11.png"><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 论文 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Bert </tag>
            
            <tag> GNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>beam search 算法</title>
      <link href="/2020/09/03/beam-search-suan-fa/"/>
      <url>/2020/09/03/beam-search-suan-fa/</url>
      
        <content type="html"><![CDATA[<p>转载至<a href="https://www.zhihu.com/question/54356960">https://www.zhihu.com/question/54356960</a></p><h1 id="beam-search-原理"><a href="#beam-search-原理" class="headerlink" title="beam search 原理"></a>beam search 原理</h1><p>　　seq2seq作为一个条件语言模型，形式化来说，它直接对P(y|x)P(y|x)进行建模，在生成yy的过程中，始终有xx作为条件，正如下图的条件概率所示。</p><p><img src="/2020/09/03/beam-search-suan-fa/1.jpg"></p><p>​       seq2seq的预测过程，一般是一个贪心的预测过程，即在Decoder RNN的每一步都贪心选择$\widehat{y}^t$概率最大的那个词。</p><ul><li>greedy search(贪心算法)问题：<ul><li>贪心只能保证每一步是最优的，无法保证预测出来的句子整体是最优的。</li><li>如果在t时刻贪心选择的词不是全局最优，会导致t时刻往后的所有预测词都是错误的。</li><li>如果每个时间步都穷举所有可能的情况的话，时间复杂度$O(V^T)$又太高了。</li></ul></li></ul><p>​       Beam search搜索策略是贪心策略和穷举策略的一个折中方案，它在预测的每一步，都保留Top-k高概率的词，作为下一个时间步的输入。k称为beam size，k越大，得到更好结果的可能性更大，但计算消耗也越大。请注意，这里的Top-k高概率不仅仅指当前时刻的$\widehat{y} ^t$的最高概率，而是截止目前这条路径上的累计概率之和，如下图的公式所示。</p><p><img src="/2020/09/03/beam-search-suan-fa/2.jpg"></p><p>​       举例如下，假设k=2，第一个时间步保留Top-2的词为”he”和”I”，他们分别作为下一个时间步的输入。”he”输入预测输出前两名是”hit”和”struck”，则”hit”这条路的累加概率是”he”的概率加上”hit”的概率=-1.7，类似的可以算出其他几个词对应路径的概率打分。最后在这4条路上保留k=2条路，所以”hit”和”was”对应路径保留，作为下一个时间步的输入；”struck”和”got”对应路径被剪枝。</p><p><img src="/2020/09/03/beam-search-suan-fa/3.jpg"></p><p>​      最终的搜索树如下图所示，可以看到在每个时间步都只保留了k=2个节点往下继续搜索。最后”pie”对应的路径打分最高，通过回溯法得到概率最高的翻译句子。请注意，beam search作为一种剪枝策略，并不能保证得到全局最优解，但它能以较大的概率得到全局最优解，同时相比于穷举搜索极大的提高了搜索效率。</p><p><img src="/2020/09/03/beam-search-suan-fa/4.jpg"></p><p>在beam search的过程中，不同路径预测输出结束标志符&lt; END &gt;的时间点可能不一样，有些路径可能提前结束了，称为完全路径，暂时把这些完全路径放一边，其他路径接着beam search。beam search的停止条件有很多种，可以设置一个最大的搜索时间步数，也可以设置收集到的最多的完全路径数。</p><p>​    当beam search结束时，需要从n条完全路径中选一个打分最高的路径作为最终结果。由于不同路径的长度不一样，而beam search打分是累加项，累加越多打分越低，所以需要用长度对打分进行归一化，如下图所示。那么，为什么不在beam search的过程中就直接用下面的归一化打分来比较呢？因为在树搜索的过程中，每一时刻比较的两条路径的长度是一样的，即分母是一样的，所以归一化打分和非归一化打分的大小关系是一样的，即在beam search的过程中就没必要对打分进行归一化了。</p><p><img src="/2020/09/03/beam-search-suan-fa/5.jpg"></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 序列 </tag>
            
            <tag> Seq2Seq </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GNN</title>
      <link href="/2020/09/03/gnn/"/>
      <url>/2020/09/03/gnn/</url>
      
        <content type="html"><![CDATA[<script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>markdown数学公式</title>
      <link href="/2020/09/03/markdown-shu-xue-gong-shi/"/>
      <url>/2020/09/03/markdown-shu-xue-gong-shi/</url>
      
        <content type="html"><![CDATA[<p>markdown支持完整LaTex数学公式语法</p><h4 id="1-上标和下标"><a href="#1-上标和下标" class="headerlink" title="1.上标和下标"></a>1.上标和下标</h4><ul><li>下标使用符号<code>Y_1</code>表达,公式表达结果为: $Y _1$</li><li>上标使用 <code>X^{2m}</code>或者<code>X^2</code>, 其中较为复杂的表达式用<code>{}</code>包含如$X ^{2m}$</li></ul><h4 id="2-常见运算符"><a href="#2-常见运算符" class="headerlink" title="2.常见运算符"></a>2.常见运算符</h4><table><thead><tr><th>符号</th><th>表达式</th><th>符号</th><th>表达式</th><th>符号</th><th>表达式</th></tr></thead><tbody><tr><td>$\pm$</td><td>\pm</td><td>$\times$</td><td>\times</td><td>$\div$</td><td>\div</td></tr><tr><td>$\cdot$</td><td>\cdot</td><td>$\leq$</td><td>\leq</td><td>$\geq$</td><td>\geq</td></tr><tr><td>$\neq$</td><td>\neq</td><td>$\approx$</td><td>\approx</td><td>$\equiv$</td><td>\equiv</td></tr><tr><td>$\in$</td><td>\in</td><td>$\notin$</td><td>\notin</td><td>$\subset$</td><td>\subset</td></tr><tr><td>$ \supset $</td><td>\supset</td><td>$\bigcap$</td><td>\bigcap</td><td>$\bigcup$</td><td>\bigcup</td></tr></tbody></table><h4 id="3-特殊符号"><a href="#3-特殊符号" class="headerlink" title="3.特殊符号"></a>3.特殊符号</h4><ul><li>求和符号: 使用<code>\sum</code> 转义表达求和符号,下限符号使用<code>_{m}</code>,上限符号使用<code>^\infty</code>, 如:  $\sum _{m} ^\infty$</li><li>积分符号: <code>\int_0^1</code>转义表达积分符号, 如: $\int _0 ^1$</li><li>极限符号: <code>\lim_{变量 \to 表达式} 表达式</code>, 如 $\lim _{1 \to 5} 表达式$</li><li>向量符号: <code>\vec{a}</code>转义表达向量, 如: $\vec{a}$</li><li>空格符号: markdown支持四种空格<code>\, 、\;、\quad 和 \qquad</code></li></ul><h4 id="4-希腊字母的大小写"><a href="#4-希腊字母的大小写" class="headerlink" title="4.希腊字母的大小写"></a>4.希腊字母的大小写</h4><p>希腊字母的大写使用首字母大写<code>\Gamma</code>转义,小写使用首字母小写<code>\gamma</code>转义, 公式表达结果为: $\Gamma$  和   $\gamma$</p><h4 id="5-关于注释"><a href="#5-关于注释" class="headerlink" title="5.关于注释"></a>5.关于注释</h4><ul><li>公式注释: 使用<code>\text{内容}</code>转义注释,公式表达结果为: $\text{公式注释}$</li><li>文字颜色: 使用<code>\color{颜色}{文字}</code>转义表达带颜色的文字,公式表达结果为:  $\color{green}{文字}$</li></ul><h4 id="6-分数"><a href="#6-分数" class="headerlink" title="6.分数"></a>6.分数</h4><p>分数使用<code>\frac{part I}{part II}</code>转义表达,公式表达结果为: $\frac{part I}{part II}$</p><h4 id="7-公式中的大括号或大分隔符"><a href="#7-公式中的大括号或大分隔符" class="headerlink" title="7.公式中的大括号或大分隔符"></a>7.公式中的大括号或大分隔符</h4><ul><li>在配对符号中.如公式中的括号使用<code>\left(</code>表达左括号,使用<code>\right)</code>转义表达向右的括号;如: $\left( x _i \right)$</li><li>在非配对符号中以<code>\left. 其他 \right符号</code> 或 <code>\left符号 其他 \right.</code> 表非配对的较大符号.如: $\left. \frac{du}{dx}\right| _{x=0}$</li><li>注: 转义表达不需要<code>{ 和 }</code></li></ul><h4 id="8-上划线与下划线"><a href="#8-上划线与下划线" class="headerlink" title="8.上划线与下划线"></a>8.上划线与下划线</h4><p>上划线使用<code>\overline{}</code>转义表达,下划线使用<code>\underline{}</code>转义表达,公式表达结果为: $\overline{a+b+c+d}$和$\underline{}$</p><h4 id="9-开根号"><a href="#9-开根号" class="headerlink" title="9.开根号"></a>9.开根号</h4><p>多次方根使用<code>\sqrt[n]{x}</code>,其中<code>n</code>表示根的次数,<code>x</code>表示被开方项,公式表达结果为: $\sqrt[n]{x}$</p><h4 id="10-使用HTML语法表达数学公式"><a href="#10-使用HTML语法表达数学公式" class="headerlink" title="10.使用HTML语法表达数学公式"></a>10.使用HTML语法表达数学公式</h4><ul><li>上标与下标的表达: 使用标签<code>&lt;sub&gt;</code>和<code>&lt;sup&gt;</code>表达, 表达结果为: text 与text</li><li>上划线表达: 使用标签<code>&lt;SPAN style="TEXT-DECORATION: overline"&gt;X&lt;/SPAN&gt;</code>表达,表达结果为: <span style="TEXT-DECORATION: overline">X</span></li></ul><h4 id="11-补充"><a href="#11-补充" class="headerlink" title="11.补充"></a>11.补充</h4><p>对于部分符号无法用 markdown 的公式表达,可以使用 <code>\符号</code> 转义表达,如$ % $等</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://zhuanlan.zhihu.com/p/95886235">数学公式</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> markdown </tag>
            
            <tag> 数学公式 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>XLNet</title>
      <link href="/2020/08/31/xlnet/"/>
      <url>/2020/08/31/xlnet/</url>
      
        <content type="html"><![CDATA[<h1 id="XLNet"><a href="#XLNet" class="headerlink" title="XLNet"></a>XLNet</h1><p>Paper: <a href="https://arxiv.org/abs/1906.08237v2">XLNet: Generalized Autoregressive Pretraining for Language Understanding</a></p><p>Code:<a href="https://github.com/graykode/xlnet-Pytorch">torch</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>BART</title>
      <link href="/2020/08/28/bart/"/>
      <url>/2020/08/28/bart/</url>
      
        <content type="html"><![CDATA[<h1 id="BART"><a href="#BART" class="headerlink" title="BART"></a>BART</h1><p>brat是linux下的一款应用于webserver端的文本标注工具，可用于标注如下类型信息：<br>（1）实体：命名实体，可用于NER；<br>（2）关系：实体间关系，可用于关系抽取；<br>（3）事件：实体参与的事件；<br>（4）属性：事件或实体的属性，常用于知识图谱。</p><p>本文将依次介绍brat的安装、配置和基本使用。</p><h2 id="brat的安装"><a href="#brat的安装" class="headerlink" title="brat的安装"></a>brat的安装</h2><h3 id="基本环境配置"><a href="#基本环境配置" class="headerlink" title="基本环境配置"></a>基本环境配置</h3><p>【系统环境】<br><code>brat只能用于linux下，若为windows系统，请安装于linux虚拟机上。</code></p><p>【web环境】<br>官方建议使用使用Apache2进行web配置，用如下命令安装apache2：</p><pre class=" language-shell"><code class="language-shell">sudo apt-get upgradesudo apt-get updatesudo apt-get install apache2</code></pre><p>安装完毕后，会在 /var 目录下生成一个www/html目录，下面bart的安装路径均在其中：</p><pre class=" language-she"><code class="language-she">cd /var/www/html</code></pre><h3 id="安装流程"><a href="#安装流程" class="headerlink" title="安装流程"></a>安装流程</h3><p>（1）下载brat的release版本(<a href="https://github.com/nlplab/brat/releases/tag/v1.3p1">https://github.com/nlplab/brat/releases/tag/v1.3p1</a>)</p><p>（2）将其挪至/var/www/html路径下，解压并重命名为brat</p><pre class=" language-shell"><code class="language-shell">unzip brat-1.3p1.zipmv brat-1.3p1 brat</code></pre><p>（3）修改brat路径权限</p><pre class=" language-shell"><code class="language-shell">sudo chmod 777 -R /var/www/html/brat</code></pre><p>（4）进入brat目录，进行安装，并根据提示输入用户名、密码和邮箱等信息</p><pre class=" language-shell"><code class="language-shell">cd /var/www/html/bratsudo ./install.sh</code></pre><p>（5）配置apache2.conf文件</p><pre class=" language-shell"><code class="language-shell">sudo vim /etc/apache2/apache2.conf</code></pre><p>在文件末尾加入如下内容（注意缩进，若启动apache2服务时候报错“…the control process exited with error code…”，建议手动缩进（4下空格键））：</p><pre class=" language-shell"><code class="language-shell"><Directory /var/www/html/brat>　　AllowOverride Options Indexes FileInfo Limit　　Require all granted　　AddType application/xhtml+xml .xhtml　　AddType font/ttf .ttf　　Options +ExecCGI　　AddHandler cgi-script .cgi</Directory></code></pre><p>（6）建立Apache2和cgi模块的软链接，其具体说明可见issue #1141</p><pre class=" language-shell"><code class="language-shell">cd /etc/apache2/mods-enabledsudo ln -s ../mods-available/cgi.load</code></pre><p>（7）brat默认不支持中文标注，因此需手动修改brat主目录/server/src路径下的projectconfig.py文件的第162行代码</p><pre class=" language-shell"><code class="language-shell">sudo vim /var/www/html/brat/server/src/projectconfig.py</code></pre><p>将其改为：</p><pre class=" language-shell"><code class="language-shell">n  = re.sub(r'[^a-zA-Z0-9_-]', '_', n)改为n = re.sub(u'[^a-zA-Z\u4e00-\u9fa5<>,0-9_-]', '_', n)</code></pre><p>​    如果已经配置了支持中文，但是仍然报不支持的字符的问题，可能是你的配置里面有中文标点符号，要么全部改成英文标点，要么修改上面的正则，如下（修改之后记得重启apache2服务）：</p><pre class=" language-shell"><code class="language-shell"># n  = re.sub(r'[^a-zA-Z0-9_-]', '_', n)n = re.sub(u'[^a-zA-Z\u4e00-\u9fa5<>\u2014-\uff1b,0-9_-]', '_', n)</code></pre><p>​    如果修改之后，仍然有字符问题，请检查配置文件的编码格式是否是UTF-8，且是标准的UTF-8，而不是带BOM的UTF-8，具体可以使用Notepad++打开文件再编码选项中查看和转换</p><ul><li><p>如果使用brat加载本地文件的时候报错“unable to read text file xxx/xxx/xxx/xxx.txt ”，请确保你的txt文件和ann文件已经授权给当前linux系统的登录用户，授权方式可以参考如下方式：</p><p><code>sudo chmod 777 xxx.txt</code></p><p><code>sudo chmod 777 xxx.ann</code></p></li></ul><p>（8）重启Apache2</p><pre class=" language-shell"><code class="language-shell">sudo service apache2 restart（9）访问本地brat地址，若页面正常，即安装配置无误。</code></pre><h2 id="brat的基本使用"><a href="#brat的基本使用" class="headerlink" title="brat的基本使用"></a>brat的基本使用</h2><h3 id="项目文件的配置"><a href="#项目文件的配置" class="headerlink" title="项目文件的配置"></a>项目文件的配置</h3><p>项目文件建议放在如下路径（若无，自行创建）：</p><pre class=" language-shell"><code class="language-shell">cd /var/www/html/brat/data/projects</code></pre><p>每个项目下总共有4个文件，分别为：<br>（1）annotation.conf 规定了项目中的所有实体、事件、关系和属性类型<br>（2）visual.conf 规定了annotation.conf 所有实体、事件、关系和属性类型的显示文字和颜色等配置<br>（3）project_name.txt 项目的语聊文本<br>（4）project_name.ann 标注后的结果文件</p><h3 id="annotation-conf-文件的配置"><a href="#annotation-conf-文件的配置" class="headerlink" title="annotation.conf 文件的配置"></a>annotation.conf 文件的配置</h3><p>（1）定义实体entities</p><p>每一行表示一个实体名称，若需要定义层次化的实体，则在下一层实体前加入Tab键。</p><pre class=" language-shell"><code class="language-shell">[entities]# Definition of entities.# Format is a simple list with one type per line.Person    Male    FemaleOrganizationGPE</code></pre><p>（2）定义关系relations</p><p>每一行表示一条关系，定义了关系名称，以及关系双方的实体。</p><p>其基本的配置规则为：<br>a). 每一行开头为关系名称；<br>b). 关系名称后为空格，空格后为相关的实体信息，其格式为Arg1:实体1，Arg2:实体2，表示从实体1—&gt;实体2的关系；<br>c) 若需要定义双向关系，则补充<rel-type>:symmetric-transitive。</rel-type></p><pre class=" language-shell"><code class="language-shell">[relations]# Definition of (binary) relations.# Format in brief: one relation per line, with first space-separated# field giving the relation type and the rest of the line the# comma-separated arguments in ROLE:TYPE format. The roles are# typically "Arg1" and "Arg2".Located            Arg1:Person, Arg2:GPEGeographical_part  Arg1:GPE,    Arg2:GPEFamily             Arg1:Person, Arg2:PersonEmployment         Arg1:Person, Arg2:GPEOwnership          Arg1:Person, Arg2:OrganizationOrigin             Arg1:Organization, Arg2:GPEAlias              Arg1:Person, Arg2:Person, <REL-TYPE>:symmetric-transitive</code></pre><p>（3）定义事件events</p><p>每一行表示一个事件，定义了事件名称，以及事件参与方的实体信息。</p><p>其基本的配置规则为：<br>a) 每一行开头为事件名；<br>b) 事件名称后为空格，空格后为相关的实体信息，其格式为Arg1:实体1，Arg2:实体2<br>c) 可通过定义<name>=实体1|实体2来将实体进行组合;<br>d) 每个事件，可通过正则表达式中适配符来限制某个实体的参与与否以及参与个数：？表示可参与，可不参与；* 表示可参与0次或多次；+表示至少参与一次；{2}表示必须为2个实体，其他正则表达式类似。<br>e) 若为层次性的事件，则可通过在上一层事件名称前添加!，以使其不参与具体标注（因此，其仅做注释性的层次化）</name></p><pre class=" language-shell"><code class="language-shell">[events]# Definition of events.# Format in brief: one event per line, with first space-separated# field giving the event type and the rest of the line the# comma-separated arguments in ROLE:TYPE format. Arguments may be# specified as either optional (by appending "?" to role) or repeated# (by appending either "*" for "0 or more" or "+" for "1 or more").# this is a macro definition, used for brevity<POG>=Person|Organization|GPE# the "!" before a type specifies that it cannot be used for annotation# (hierarchy structure only.)!Life    Be-born   Person-Arg:Person, Place-Arg?:GPE    Marry     Person-Arg&#123;2&#125;:Person, Place-Arg?:GPE    Divorce   Person-Arg&#123;2&#125;:Person, Place-Arg?:GPE    Die       Person-Arg:Person, Agent-Arg?:<POG>, Place-Arg?:GPE!Transaction    Transfer-ownership  Buyer-Arg:<POG>, Seller-Arg:<POG>, Artifact-Arg:Organization    Transfer-money    Giver-Arg:<POG>, Recipient-Arg:<POG>, Beneficiary-Arg:<POG>!Business    Start-org  Agent-Arg?:<POG>, Org-Arg:Organization    Merge-org  Org-Arg&#123;2&#125;:Organization    End-org    Org-Arg:Organization</code></pre><p>（4）定义属性attributes</p><p>每一行表示一个属性，可依附于事件或实体对象。</p><p>其基本的配置规则为：<br>a) 每一行开头为属性名；<br>b) 事件名称后为Tab符，Tab符后为具体的属性定义，其格式一般为Arg:<event>，表示为事件属性，EVENT可改为ENTITY表示实体属性。<br>c）若改行定义直接结束，表示这是个2元属性；若为多值属性，则在Arg:<event>后需进一步补充,Value:值1|值2|值3。</event></event></p><pre class=" language-shell"><code class="language-shell">[attributes]# Definition of entity and event attributes.# Format in brief: first tab-separated field is attribute name, second# a set of key-value pairs. The latter must define "Arg:" which# specifies what the attribute can attach to (typically "<EVENT>").# If no other keys are defined, the attribute is binary (present or# absent). If "Value:" with multiple alternatives is defined, the# attribute can have one of the given values.Negation     Arg:<EVENT>Confidence   Arg:<EVENT>, Value:High|Neutral|Low</code></pre><p>2.1.2 visual.conf 文件的配置<br>（1）定义显示标签labels</p><p>用于定义实体、关系、事件和属性等对象的显示标签，常用于显示中文标签（原始annotation.conf 文件为英文时）或防止原始各对象的名称过程而难以显示。</p><p>其定义非常简单：</p><pre class=" language-shell"><code class="language-shell">[labels]### Entity typesPerson | PersonOrganization | Organization | OrgGPE | Geo-political entity | GPE</code></pre><p>（2）定义显示颜色drawing<br>用于定义实体、关系、事件和属性等对象的显示颜色，以在webserver标注时予以区分。</p><p>其定义非常简单：</p><pre class=" language-shell"><code class="language-shell">Person    bgColor:#ffccaaOrganization    bgColor:#8fb2ffGPE    bgColor:#7fe2ff</code></pre><p>2.1.3 project_name.txt 文件的配置<br>该文件即为需要进行标注的原始语料文件，为方便标注，一般在放入标注项目文件夹之前，建议进行文本预处理。</p><p>2.1.4 project_name.ana 文件的配置<br>该文件即为标注后的结果文件，随着标注过程其同步自动更新。在最开始，其为空文件。</p><p>2.2 基于webserver的文本标注<br>（1）访问本地brat地址；(<a href="http://localhost/brat">http://localhost/brat</a>)<br>（2）登录账号和密码；<br>（3）选择<code>/var/www/html/brat/data/projects</code>文件夹内的具体项目下的<code>project_name.txt</code>语料文件，开始标注；<br>（4）选择文本字段，弹出【实体候选框】，即可定义实体；<br>（5）拖动标注好的实体即可定义连接关系；<br>（6）双击标签，即可对标签进行修改或删除。</p><p>注意: 若显示无法加载本地文件或无法修改静态文件，一般为权限问题，修改该项目文件夹权限即可。</p><p>2.3 ana文件的基本解读<br>一个典型的标注好的ana结果文件如下：</p><p>T1    人名 11 13    马云<br>T2    地点 16 18    杭州<br>R1    来自 Arg1:T1 Arg2:T2<br>T3    人名 23 25    马云<br>T4    时间 0 10    1964年9月10日<br>T5    人名 61 63    父亲<br>T6    人名 81 83    马云<br>T7    地点 69 71    江南<br>T8    人名 99 101    金庸<br>T9    人名 129 131    马云</p><ul><li>家属 T3 T5<br>T10    人名 86 88    父亲<br>R2    来自 Arg1:T5 Arg2:T7    </li><li>家属 T6 T10</li></ul><p>每一行表示标注好的实体、关系、事件或属性对象。</p><p>对于实体对象，其共有5个字段，各字段间用空格区分，其意义依次为：<br>（1）编号，如T1表示编号为1的实体；<br>（2）实体类型，如人名;<br>（3）该实体在整个文本中的起始位置；<br>（4）该实体在整个文本中的终止位置；<br>（5）实体对应的文本。</p><p>值得注意的是：整个文本的顺序为各行文本的拼接（保留\n等符号），整个文本的起始位置编号为0，起始位置-&gt;终止位置采用左开右闭的对应关系（可理解为切片），如[11, 13]表示占用11、12两个位置。</p><p>对于单向关系对象，包括4个字段，其意义依次为：<br>（1）编号，如R1表示编号为1的关系；<br>（2）关系类型，如地点;<br>（3）关系起始实体编码，如Arg1:T1<br>（4）关系指向实体编码，如Arg2:T2</p><p>对于双向关系对象，也包括4个字段，其意义依次为：<br>（1）*号<br>（2）关系类型，如家属;<br>（3）关系起始实体编码，如T3<br>（4）关系指向实体编码，如T5</p><p>【Reference】：</p><ol><li><a href="https://blog.csdn.net/p_jinsan/article/details/96152078">Brat标注工具（本地）安装及使用</a></li><li>[文本标注工具brat</li></ol><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> nlp </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 命名实体识别 </tag>
            
            <tag> 标注工具 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TextCNN</title>
      <link href="/2020/08/27/textcnn/"/>
      <url>/2020/08/27/textcnn/</url>
      
        <content type="html"><![CDATA[<h1 id="TextCNN"><a href="#TextCNN" class="headerlink" title="TextCNN"></a>TextCNN</h1><p>Paper <a href="http://www.aclweb.org/anthology/D14-1181">Convolutional Neural Networks for Sentence Classification(2014)</a></p><p>code<a href=""></a></p><h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p> 将CNN应用到文本分类任务，多个不同kernel来提取句子中的信息（类似于ngram），从而捕捉局部相关性。</p><h3 id="网络架构"><a href="#网络架构" class="headerlink" title="网络架构"></a>网络架构</h3><p><img src="/2020/08/27/textcnn/1.png"></p><p><img src="/2020/08/27/textcnn/2.png"></p><p>TextCNN详细过程：</p><ul><li><strong>Embedding</strong>：第一层是图中最左边的7乘5的句子矩阵，每行是词向量，维度=5，这个可以类比为图像中的原始像素点。</li><li><strong>Convolution</strong>：然后经过 kernel_sizes=(2,3,4) 的一维卷积层，每个kernel_size 有两个输出 channel。</li><li><strong>MaxPolling</strong>：第三层是一个1-max pooling层，这样不同长度句子经过pooling层之后都能变成定长的表示。</li><li><strong>FullConnection and Softmax</strong>：最后接一层全连接的 softmax 层，输出每个类别的概率。</li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> nlp </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 文本分类 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Bert原理</title>
      <link href="/2020/08/27/bert-yuan-li/"/>
      <url>/2020/08/27/bert-yuan-li/</url>
      
        <content type="html"><![CDATA[<h2 id="Bert原理"><a href="#Bert原理" class="headerlink" title="Bert原理"></a>Bert原理</h2><p>Paper <a href="https://arxiv.org/abs/1810.04805">Pre-training of Deep Bidirectional Transformers for Language Understanding</a></p><p>Code <a href="https://github.com/codertimo/BERT-pytorch">torch</a>   <a href="https://github.com/google-research/bert">tensorflow</a>   </p><h2 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h2><p>​       BERT 全称为 <strong>Bidirectional Encoder Representation from Transformer</strong>，是 Google 以无监督的方式利用大量<strong>无标注</strong>文本「炼成」的语言模型，其架构为 <a href="/2020/08/26/transformer-yuan-li/" title="Transformer">Transformer</a> 中的 Encoder（BERT=Encoder of Transformer）</p><p><img src="/2020/08/27/bert-yuan-li/1.png"></p><h3 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h3><ul><li><code>L</code>：Transformer blocks；</li><li><code>H</code>：hidden size；</li><li><code>A</code>：self-attention heads；</li></ul><p>$ {BERT}_\textbf{BASE: } L=12, H=768, A=12, \text {Total Parameters=} 110 M $</p><p>${BERT}_\textbf{LARGE: } L=24, H=1024, A=16, \text {Total Parameters=} 340 M$</p><h3 id="无监督预训练深度双向语言模型"><a href="#无监督预训练深度双向语言模型" class="headerlink" title="无监督预训练深度双向语言模型"></a>无监督预训练深度双向语言模型</h3><p><img src="/2020/08/27/bert-yuan-li/2.png"></p><h4 id="模型输入"><a href="#模型输入" class="headerlink" title="模型输入"></a>模型输入</h4><ul><li>最大支持长度为 512；</li><li>每个句子第一个 token 总是 “special classiﬁcation embedding”，用 <code>[CLS]</code> 表示；</li><li>如果输入的是两个句子，则用 <code>[SEP]</code> 分隔；</li><li>Segment Embeddings：使用可学习的 A 和 B 表示两个句子的嵌入。当只有一个句子时，全部都是 A。</li></ul><h4 id="预训练任务"><a href="#预训练任务" class="headerlink" title="预训练任务"></a>预训练任务</h4><h5 id="Task1-Masked-LM"><a href="#Task1-Masked-LM" class="headerlink" title="Task1: Masked LM"></a>Task1: Masked LM</h5><p>为了构建一个深层的真双向语言模型，但是标准的语言模型是使用了马尔可夫链进行的单向编码，即使使用 LTR 与 RTL，但也是假的双向编码，性能会受到极大的影响。使用完形填空机制可以避免标准的语言模型的编码瓶颈。<br><strong>完形填空策略</strong>：随机的 mask 掉 15% 的单词，然后使用编码器最后的 hidden state 过一层 softmax 进行完形填空预测。<br>但是这种策略会有两个缺点，以下是内容和解决方案：</p><h6 id="Downside-1-mismatch"><a href="#Downside-1-mismatch" class="headerlink" title="Downside 1: mismatch"></a>Downside 1: mismatch</h6><p>这样做构造了一种 <strong>mismatch</strong>：因为 <code>[MASK]</code> 永运不会出现在 fine-tuning 阶段，所以 pre-training 与 fine-tuning 出现了 mismatch。<br>缓解方案：对于随机选择的 15% 待 mask 单词，不是直接将它替换为 <code>[MASK]</code>，而是再做一次随机：</p><ul><li>80%：将该词替换为 <code>[MASK]</code></li><li>10%：将该词替换为一个随机的词语</li><li>10%：不替换</li></ul><p>原因：Transformer Encoder 不知道哪个单词被要求做预测，哪个单词被随机替换掉了，所以对于每个输入的单词，它都必须保持上下文嵌入；而且，现在这种策略下随机替换掉的单词只有 1.5%，几乎不会影响模型的语言建模能力。（其实这部分解释有点牵强，有待深刻理解）</p><h6 id="Downside-2-slower"><a href="#Downside-2-slower" class="headerlink" title="Downside 2: slower"></a>Downside 2: slower</h6><p>现在使用的 MLM 模型，每个 batch 只有 15% 的单词被预测，所以收敛速度确实慢了。但是效果带来的提升却很大。</p><h5 id="Task2-Next-Sentence-Prediction"><a href="#Task2-Next-Sentence-Prediction" class="headerlink" title="Task2: Next Sentence Prediction"></a>Task2: Next Sentence Prediction</h5><p>NLP 中有很多<strong>句子关系</strong>性的任务，这部分的能力不能通过 Task1 的 MLM 来俘获到，所以加入了一个二分类任务进行多任务学习。<br>策略：50% 的句子对，将第二句替换为随机的句子来构建负样本。</p><h4 id="其他细节"><a href="#其他细节" class="headerlink" title="其他细节"></a>其他细节</h4><ul><li>训练语料：BooksCorpus (800M words) + English Wikipedia (2,500M words)</li><li>batch size：256</li><li>Adam：γ=1e−4,β1=0.9,β2=−.999，warmup</li><li>dropout：0.1</li><li>GELU</li><li>loss：两个任务的 loss 和</li></ul><h3 id="下游监督任务微调"><a href="#下游监督任务微调" class="headerlink" title="下游监督任务微调"></a>下游监督任务微调</h3><p><img src="/2020/08/27/bert-yuan-li/3.png"></p><ul><li><p>单句/句子对分类任务：直接使用 <code>[CLS]</code> 的 hidden state 过一层 softmax 进行预测；</p></li><li><p>QA 任务：将问题和答案所在的段拼接起来，使用最后的答案段的 hidden state 向量来计算某个单词是答案开始单词和结束单词的概率，进而进行预测。</p><p>$\begin{align*}<br>P^S_i&amp;=\frac{e^{S\cdot T_i}}{\sum_j{e^{S\cdot T_j}}} \<br>P^E_i&amp;=\frac{e^{E\cdot T_i}}{\sum_j{e^{E\cdot T_j}}}<br>\end{align*}$</p></li><li><p>其中，S 和 E 是需要下游 fine-tuning 阶段训练的开始向量和结束向量。在推断阶段，会强行限制结束的位置必须在开始的位置之后。</p></li><li><p>序列标注任务：直接将序列所有 token 的最后一层 hidden state 喂进一个分类层（没有使用自回归、CRF）</p></li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> nlp </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 预训练模型 </tag>
            
            <tag> Encoder </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transformer原理</title>
      <link href="/2020/08/26/transformer-yuan-li/"/>
      <url>/2020/08/26/transformer-yuan-li/</url>
      
        <content type="html"><![CDATA[<p>转载至　<a href="https://zhuanlan.zhihu.com/p/44121378">https://zhuanlan.zhihu.com/p/44121378</a></p><p>　　　　<a href="https://zhuanlan.zhihu.com/p/43493999">https://zhuanlan.zhihu.com/p/43493999</a></p><h1 id="Transformer原理"><a href="#Transformer原理" class="headerlink" title="Transformer原理"></a>Transformer原理</h1><p>Paper <a href="https://arxiv.org/abs/1706.03762">Attention is All You Need</a></p><p>Code <a href="https://github.com/harvardnlp/annotated-transformer">torch</a></p><h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><p><img src="/2020/08/26/transformer-yuan-li/1.jpg"></p><p>​         和大多数seq2seq模型一样，transformer的结构也是由encoder和decoder组成。在Encoder方面，6个编码器组件协同工作，组成一个大的编码器，解码器同样由6个解码器组件组成。我们先看Encoder。6个编码器组件依次排列，每个组件内部都是由一个多头attention加上一个前馈网络，attenion和前馈的输出都经过层归一化（LayerNormalization），并且都有各自的残差网络 。Decoder呢，组件的配置基本相同， 不同的是Decoder有两个多头attention机制，一个是其自身的mask自注意力机制，另一个则是从Encoder到Decoder的注意力机制，而且是Decoder内部先做一次attention后再接收Encoder的输出。</p><h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><p>Encoder由N=6个相同的layer组成，layer指的就是上图左侧的单元，最左边有个“Nx”，这里是x6个。每个Layer由两个sub-layer组成，分别是multi-head self-attention mechanism和fully connected feed-forward network。其中每个sub-layer都加了residual connection和normalisation，因此可以将sub-layer的输出表示为：</p><p>sub_layer_output = LayerNorm( + (subLayer(x)))</p><ul><li><strong>Self-Attention</strong></li></ul><p><img src="/2020/08/26/transformer-yuan-li/2.jpg"></p><p>上图中左边是一个RNN模型，Transformer的核心就是将需要按照顺序依次进行计算的RNN变为右边可以并行化的Self-Attention。</p><p>接下来详细说明Self-Attention是怎么运作的。</p><p><img src="/2020/08/26/transformer-yuan-li/3.jpg"></p><p>上图中x1、x2、x3、x4是单词的one-hot形式，处理后得到Embedding形式的a1、a2、a3、a4作为模型的输入，这一步实际上就是word2vec过程。</p><p>a1、a2、a3、a4分别乘上一些矩阵（就是对两层神经元进行全连接操作，计算方式如下）得到各自的q、k、v，q相当于<strong>query</strong>用来匹配k，k相当于<strong>key</strong>用来被q匹配，v相当于<strong>value</strong>指被用来提取的信息，具体怎么用后边会说。这里看不明白没有关系，只需要知道有这么一个东西就可以了，下面用到的时候再返回来看一下就好。</p><img src="/2020/08/26/transformer-yuan-li/4.jpg" style="zoom:50%;"><p>然后要做的就是<strong>用每个 query q 去对每个 key k 做 attention</strong>，这个attention的核心就是匹配q和k然后<strong>输出得分α</strong>（可将此步看作是计算二者的相似度）。首先用q1分别去attention k1、k2、k3、k4得到α1,1、α1,2、α1,3、α1,4，如下：</p><p><img src="/2020/08/26/transformer-yuan-li/5.jpg"></p><p>α的计算公式如下</p><img src="/2020/08/26/transformer-yuan-li/6.jpg" style="zoom:50%;"><p>这里分母上的$\sqrt(d)$相当于对计算结果进行了一次缩放，至于为什么是$\sqrt(d)$ 原论文作者也没有详细解释，一般就直接默认使用这个值了。</p><p>得到α1,1、α1,2、α1,3、α1,4后进行softmax处理获得$\widehat{\alpha} _{1,1} 　 \widehat{\alpha} _{1,2}  　\widehat{\alpha} _{1,3}　  \widehat{\alpha} _{1,4}$如下：</p><p><img src="/2020/08/26/transformer-yuan-li/7.jpg"></p><p>还记不记得我们前面讲过α是q1对每个k做的attention匹配，那么这里的$\widehat{\alpha} _{1,1} 　 \widehat{\alpha} _{1,2}  　\widehat{\alpha} _{1,3}　  \widehat{\alpha} _{1,4}$ 就<strong>相当于获得了一系列针对v1、v2、v3、v4的权重</strong>，权重和对应的值都有了就自然而然想到了<strong>加权平均</strong>，所以我们就得到了输出seq的第一个结果b1，过程如下：</p><p><img src="/2020/08/26/transformer-yuan-li/8.jpg"></p><p>从过程里不难发现，这里<strong>b1的产生其实attend到了整个句子的所有信息</strong>，这就是Self-Attention的妙用，完美解决了之前RNN、LSTM等对于长距离单词无法attend到的问题。</p><p>刚才我们只计算了b1，对于后面的b2（如下）、b3、……类似，而且不难发现，这些计算过程完全可以同时进行，不像之前的RNN必须严格按照给定序列依次计算。</p><p><img src="/2020/08/26/transformer-yuan-li/9.jpg"></p><p>那么我们再回到Self-Attention Layer中，把所有参数考虑进来整个流程写成矩阵的形式就是下面这个样子：</p><p><img src="/2020/08/26/transformer-yuan-li/10.jpg"></p><p>反正就是一堆矩阵乘法，而这恰恰是GPU非常擅长的运算。</p><ul><li><strong>Multi-head Self-attention</strong></li></ul><p>将单个Self-attention进行分裂就得到Multi-head Self-attention，这里以2个heads为例进行说明（实际使用时head数目也是需要作为参数进行调的）。</p><p><img src="/2020/08/26/transformer-yuan-li/11.jpg"></p><p>上图将 $q ^i$ 分别乘2个不同的矩阵就得到将 $q ^{i,1}$ 和 $q ^{i,2}$ ，k和v的处理过程类似，然后按照Self-attention的流程最终得到将 $b ^{i,1}$ ，同理获得 $b ^{i,2}$ ，</p><p><img src="/2020/08/26/transformer-yuan-li/12.jpg"></p><p>得到的 $b ^{i,1}$ 和 $b ^{i,2}$ 可以直接连接形成 $b ^{i}$ ，当然如果对连接产生的维度不满意也可以继续乘上个矩阵进行降维，过程如下：</p><img src="/2020/08/26/transformer-yuan-li/13.jpg" style="zoom:67%;"><p>为什么需要多heads呢？因为<strong>不同head之间attention的点可能不太一样</strong>，比如有的head更重视和自己邻近上下文的信息，而其他的head可能更容易注意到全局信息，这样许多heads就可以各司其职进行更复杂的任务。</p><ul><li><p><strong>The Residual Connection 残差连接</strong></p><p>​       sublayer有两个，一个是多头self-attention层，另一个是前馈网络（feed_forward）。输入x先进入多头self-attention，用一个残差网络加成，接着通过前馈网络， 再用一个残差网络加成。</p><p>让我们从输入x开始，再从头理一遍这个过程:</p><img src="/2020/08/26/transformer-yuan-li/16.png" style="zoom:67%;"><img src="/2020/08/26/transformer-yuan-li/18.png" style="zoom:67%;"></li></ul><ul><li>输入x</li><li>x做一个层归一化： x1 = norm(x)</li><li>进入多头self-attention: x2 = self_attn(x1)</li><li>残差加成：x3 = x + x2</li><li>再做个层归一化：x4 = norm(x3)</li><li>经过前馈网络: x5 = feed_forward(x4)</li><li>残差加成: x6 = x3 + x5</li><li>输出x6</li></ul><ul><li><strong>层归一化</strong></li></ul><p><strong>BatchNormalization</strong></p><p>​       BatchNormalization的出现无疑是广大AI调参侠的福音，将大家从繁琐的权重初始化、学习率调节中释放出来。它不仅能够大大加快收敛速度，还自带正则化功能，是Google 2015年提出的。</p><p>机器学习的一个重要的假设是：<strong>数据是独立同分布的</strong>。训练集合测试集的数据是同分布的，这样模型才有好的泛化效果。神经网络其实也是在学习这个分布。在这个假设前提下，一旦我们知道了（x，y）的联合分布，很多问题就能通过条件概率P(x∣y)P(x|y)P(x∣y)计算出来了。但是在实际训练过程中，数据经过前一个隐藏层向后一个隐藏层传播（线性+非线性运算），分布通常会发生变化（作者称之为<strong>Internal Covariate Shift</strong>），这会导致网络学习变慢。</p><p>​         对每一个Mini-Batch的所有样本的每一维特征，计算两个统计量：均值和方差，然后做一个归一化操作,这样就变成了正态分布了。但是只这样做也有问题，首先，谁说数据一定是正态分布的，偏正态不行吗？第二，把数据全部拉到接近0的位置，sigmoid不就接近于一个线性函数了吗，没有起到激活的作用啊（线性激活函数+线性操作等价于一层线性操作）。</p><img src="/2020/08/26/transformer-yuan-li/19.png" style="zoom:67%;"><p><strong>LayerNormalization</strong></p><ul><li>对batch_size非常敏感。BatchNormalization的一个重要出发点是保持每层输入的数据同分布。回想下开始那个独立同分布的假设。假如取的batch_size很小，那显然有些Mini-Batch的数据分布就很可能与整个数据集的分布不一致了，又出现了那个问题，数据分布不一致，这就等于说没起到同分布的作用了，或者说同分布得不充分。实验也证明，batch_size取得大一点， 数据shuffle的好一点，BatchNormalization的效果就越好。</li><li>不能很方便地用于RNN。这其实是第一个问题的引申。我们再来看一下<strong>Figure 6</strong>中的均值和方差的计算公式。对所有样本求均值。对于图片这类等长的输入来说，这很容易操作，在每个维度加加除除就可以了，因为维度总是一致的。而对于不等长的文本来说，RNN中的每个time step共享了同一组权重。在应用BatchNormalization时，这就要求对每个time step的batch_size个输入计算一个均值和方差。那么问题就来了，假如有一个句子S非常长，那就意味着对S而言，总会有个time_step的batch_size为1，均值方差没意义，这就导致了BatchNormalization在RNN上无用武之地了</li></ul><p>LayerNormalization的主要变化在于：</p><ul><li><p>不再对Mini-Batch中的N的样本在各个维度做归一化，而是针对同一层的所有神经元做归一化。归一化公式为：$\mu ^1 =  \frac {1}{H} \sum _{H} ^{1}{\alpha _i ^1}$</p><p>$\sigma ^1 = \sqrt{\frac {1} {H} { \sum _{H} ^{1} (\alpha _i ^1 - \mu _i ^1)}} $</p><p>其中，H指的是一层神经网络的神经元个数。我们再回想下BatchNormalization，其实它是在每个神经元上对batch_size个数据做归一化，每个神经元的均值和方差均不相同。而LayerNormalization则是对<strong>所有神经元</strong>做一个归一化，这就跟batch_size无关了。哪怕batch_size为1，这里的均值和方差只和神经元的个数有关系</p></li><li><p>测试的时候可以直接利用LN，所以训练时不用保存均值和方差，这节省了内存空间</p></li></ul><img src="/2020/08/26/transformer-yuan-li/20.png" style="zoom:67%;"><p>​         示意了两种方式的区别。假设有N个样本，每个样本的特征维度为4，图中每个小圆代表一个特征，特征1，特征2等等，特征4。BatchNormalization是在N个同一特征（如特征1）上求均值和方差，这里要对每个特征求1次，共4次。对照一下上面说的，万一有个样本有5个特征，是不是就没法玩了。LayerNormalization呢，别的样本都和我没啥关系，有多少个特征我把这些特征求个均值方差就好了。这也就是为什么一个叫”批归一化“，另一个叫”层归一化“了。理解了这一点，也就理解了为什么Transformer中使用LN而不是BN。</p><p>​        当然BatchNormalization也不是吃素的，虽然它在处理不等长序列上存在天生的缺陷，但是除此之外，它的效果都要好于其他Normalization方式（比如LN，WN，IN）。直觉上，BN貌似更好理解一点，</p><ul><li><p><strong>前馈网络</strong></p><p>每个encoderLayer中，多头attention后会接一个前馈网络。这个前馈网络其实是两个全连接层，进行了如下操作:</p><p>​     $FFN(x) = max(0,xW _1 + b _1)W _2 + b _2$</p><p>这两层的作用等价于两个 kenerl_size=1的一维卷积操作。</p></li></ul><ul><li><p><strong>词向量</strong></p><p>词向量，将词语变成d_model维的向量。</p></li></ul><ul><li><strong>Positional Encoding</strong></li></ul><img src="/2020/08/26/transformer-yuan-li/14.jpg" style="zoom:67%;"><p>首先 $p ^i$ 采用one-hot形式表示给定词出现的位置，出现在第i个位置就将对应位置设为1其他维度设为0，然后通过矩阵运算得到与 $\alpha ^i$ 相同维度的 $e ^i$ 的，这个  $e ^i$  就包含了单词的位置信息，接下来只需要在之前  $W ^p$  的基础上直接将 $e ^i$ 加上进行运算即可。这里比较诡异的地方是产生 $e ^i$ 的  $W ^p$  矩阵并不是训练得来的，而是通过人工手动给定的，这也是原论文比较匪夷所思的一个点了。下图是有人将这个  $W ^p$  矩阵可视化后的样子:</p><img src="/2020/08/26/transformer-yuan-li/15.jpg" style="zoom:67%;"><p>​        由于Transformer没有用到CNN和RNN，因此，句子单词之间的位置信息就没有利用到。显然，这些信息对于翻译来说是非常有用的，同样一句话，每个单词的意思能够准确的翻译出来，但如果顺序不对，表达出来的意思就截然不同了。举个栗子感受一下，原句：”A man went through the Big Buddhist Temple“, 翻译成：”人过大佛寺“和”寺佛大过人“，意思就完全不同了。</p><p>​      那么如何表达一个序列的位置信息呢？对于某一个单词来说，他的位置信息主要有两个方面：一是绝对位置，二是相对位置。绝对位置决定了单词在一个序列中的第几个位置，相对位置决定了序列的流向。作者利用了正弦函数和余弦函数来进行位置编码：</p><p>$PE_{(pos,2i)} = sin(pos / 10000^{2i/d_{\text{model}}})$</p><p>$PE_{(pos,2i+1)} = cos(pos / 10000^{2i/d_{\text{model}}}) $</p><p>其中pos是单词处于句子的第几个位置。我们来考察一下第一个公式，看是否每个位置都能得到一个唯一的值作为编码。为简单起见，不妨令i=0，那么：</p><p>$PE(pos,0)=sin(pos)$</p><p>我们反过来想，假如存在位置j和k的编码值相同，那么就有：</p><p>$sin(i)=sin(j)$</p><p>$i, j 为非负整数且为非负整数且为非负整数且i不等于j$</p><p>以上两式需要同时满足，可等价为：</p><p>$i=(−1)k⋅j+k⋅π$</p><p>$i, j $为非负整数且为非负整数且为非负整数且i不等于j且且且$k$为整数</p><p>同时成立，这就意味着：</p><p>$π=\frac{[i−(−1)k⋅j]}{k}$</p><p>​     这显然是不可能的，因为左边是个无理数（无限不循环小数），而右边是个有理数。通过反证法就证明了在这种表示下，每个位置确实有唯一的编码。</p><p>​    上面的讨论并未考虑i的作用。i决定了频率的大小，不同的i可以看成是不同的频率空间中的编码，是相互正交的，通过改变i的值，就能得到多维度的编码，类似于词向量的维度。$2i&lt;=512$, 一共512维。想象一下，当2i大于d_model时会出现什么情况，这时sin函数的周期会变得非常大，函数值会非常接近于0，这显然不是我们希望看到的，因为这样和词向量就不在一个量级了，位置编码的作用被削弱了。另外，值得注意的是，位置编码是不参与训练的，而词向量是参与训练的。作者通过实验发现，位置编码参与训练与否对最终的结果并无影响。</p><p>​      之所以对奇偶位置分别编码，是因为编码前一个位置是可以由另一个位置线性表示的（公差为1的等差数列），在编码之后也希望能保留这种线性。我们以第1个位置和第k+1个位置为例，还是令i=0：</p><p>$PE(1,0)=cos(1)$</p><p>$PE(k,0)=sin(1+k)=sin(1)cos(k)+cos(1)sin(k)=A+B⋅PE(1,0)$</p><h3 id="Deocder"><a href="#Deocder" class="headerlink" title="Deocder"></a>Deocder</h3><p>Decoder和Encoder的结构差不多，但是多了一个attention的sub-layer，这里先明确一下decoder的输入输出和解码过程：</p><ul><li>输出：对应i位置的输出词的概率分布</li><li>输入：encoder的输出 &amp; 对应i-1位置decoder的输出。所以中间的attention不是self-attention，它的K，V来自encoder，Q来自上一位置decoder的输出</li><li>解码：<strong>这里要特别注意一下，编码可以并行计算，一次性全部encoding出来，但解码不是一次把所有序列解出来的，而是像rnn一样一个一个解出来的</strong>，因为要用上一个位置的输入当作attention的query</li></ul><p>明确了解码过程之后最上面的图就很好懂了，这里主要的不同就是新加的另外要说一下新加的attention多加了一个mask，因为训练时的output都是ground truth，这样可以确保预测第i个位置时不会接触到未来的信息。</p><p>加了mask的attention原理如图（另附multi-head attention）：</p><img src="/2020/08/26/transformer-yuan-li/21.jpg" style="zoom:67%;"><ul><li><p><strong>线性层和softmax</strong></p><p>​        整个模型的最后一步了。从Decoder拿到的输出是维度为（batch_size, max_seq_len, d_model）的浮点型张量，我们希望得到最终每个单词预测的结果，首先用一个线性层将d_model映射到vocab的维度，得到每个单词的可能性，然后送入softmax，找到最可能的单词。</p><p>​       线性层的参数个数为 d_model*vocab_size,  一般来说，vocab_size会比较大，拿20000为例,那么只这层的参数就有512 * 20000个,约为10的8次方，非常惊人。而在词向量那一层，同样也是这个数值，所以，一种比较好的做法是将这两个全连接层的参数共享，会节省不少内存，而且效果也不会差。</p></li></ul><h2 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h2><p>终于可以回过头来看一下那张非常经典却很难直接理解的Transformer结构图了。</p><img src="/2020/08/26/transformer-yuan-li/16.jpg" style="zoom:67%;"><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> nlp </category>
          
      </categories>
      
      
        <tags>
            
            <tag> attention </tag>
            
            <tag> 注意力机制 </tag>
            
            <tag> Encoder Decoder </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TensorFlow Serving</title>
      <link href="/2020/08/23/tensorflow-serving/"/>
      <url>/2020/08/23/tensorflow-serving/</url>
      
        <content type="html"><![CDATA[<h1 id="tensorflow"><a href="#tensorflow" class="headerlink" title="tensorflow"></a>tensorflow</h1><h2 id="TensorFlow-Serving模型部署"><a href="#TensorFlow-Serving模型部署" class="headerlink" title="TensorFlow Serving模型部署"></a>TensorFlow Serving模型部署</h2><h3 id="TensorFlow-Serving"><a href="#TensorFlow-Serving" class="headerlink" title="TensorFlow Serving"></a>TensorFlow Serving</h3><p>TensorFlow Serving是一种灵活的高性能服务系统，适用于机器学习模型，专为生产环境而设计。TensorFlow Serving可以轻松部署新算法和实验，同时保持相同的服务器架构和API。TensorFlow Serving提供与TensorFlow模型的开箱即用集成，但可以轻松扩展以提供其他类型的模型和数据。</p><p><img src="/2020/08/23/tensorflow-serving/serving%E5%9B%BE.png"></p><h3 id="安装Tensorflow-Serving"><a href="#安装Tensorflow-Serving" class="headerlink" title="安装Tensorflow Serving"></a>安装Tensorflow Serving</h3><p>安装过程详细参考官网</p><blockquote><p><a href="https://www.tensorflow.org/serving/setup">https://www.tensorflow.org/serving/setup</a></p></blockquote><ul><li>使用Docker安装进行，首先你的电脑当中已经安装过docker容器<ul><li>下载桌面版本：<a href="https://www.docker.com/products/docker-desktop">https://www.docker.com/products/docker-desktop</a></li></ul></li></ul><h3 id="TensorFlow-Serving-Docker"><a href="#TensorFlow-Serving-Docker" class="headerlink" title="TensorFlow Serving Docker"></a>TensorFlow Serving Docker</h3><ol><li><p>获取最新TF Serving docker镜像</p><pre class=" language-shell"><code class="language-shell">docker pull tensorflow/serving</code></pre></li><li><p>查看docker镜像</p><pre class=" language-shell"><code class="language-shell">docker images</code></pre></li><li><p>运行tf serving（即创建一个docker容器来运行）</p><pre class=" language-shell"><code class="language-shell">docker run -p 8501:8501 -p 8500:8500 --mount type=bind,source=/home/ubuntu/detectedmodel/commodity,target=/models/commodity -e MODEL_NAME=commodity -t tensorflow/serving</code></pre><p>说明：</p><ul><li><code>-p 8501:8501</code> 为端口映射，<code>-p 主机端口:docker容器程序(tf serving)使用端口</code>，访问主机8501端口就相当于访问了tf serving程序的8501端口</li><li>tf serving 使用8501端口对外提供HTTP服务，使用8500对外提供gRPC服务，这里同时开放了两个端口的使用</li><li><code>--mount type=bind,source=/home/ubuntu/detectedmodel/commodity,target=/models/commodity</code> 为文件映射，将主机(source)的模型文件映射到docker容器程序（target)的位置，以便tf serving使用模型，<code>target</code>参数为<code>/models/我的模型</code></li><li><code>-e MODEL_NAME=commodity</code>设置了一个环境变量，名为<code>MODEL_NAME</code>，此变量被tf serving读取，用来按名字寻找模型，与上面target参数中<code>我的模型</code>对应</li><li><code>-t</code> 为tf serving创建一个伪终端，供程序运行</li><li><code>tensorflow/serving</code>为镜像名</li></ul></li></ol><p>tensorflow serving是中灵活的高性能服务系统，适用于机器学习模型，专用于生产环境设计。</p><ul><li>训练模型与部署环境进行隔离</li><li>热更新<ul><li>第一版本：部署起来</li><li>第二版本，重新训练新的更好的模型，保存到原来部署的路径即可</li><li>tensorflow会根据文件夹时间戳，自动替换和使用新的模型</li></ul></li><li>docker 开起tensorflow serving 模型服务命令<ul><li>8500 grpc接口</li><li>8501 HTTP服务</li><li>type=bind,source=/home/ubantu/detectedmodel/commodity,target=/models/commodity # 模型路径文件夹<ul><li>target: /models/ 固定写法</li><li>/models/*/:    *代表模型文件夹</li></ul></li></ul></li></ul><h3 id="tensorflow-serving-不同版本模型部署"><a href="#tensorflow-serving-不同版本模型部署" class="headerlink" title="tensorflow serving 不同版本模型部署"></a>tensorflow serving 不同版本模型部署</h3><p>​        使用tfserving和docker同时部署多个模型，使用不同版本的模型，基本的流程与部署单个模型的过程类似，(关于运行tfserving容器使用单个模型进行预测的相关步骤可以参见 使用docker和tf serving搭建模型预测服务。)不同之处在于需要用到一个多模型的配置文件。首先得到多个可以用于tfserving预测的模型文件，相关步骤可以参考使用savedModel保存模型。本例中用使用savedModel保存模型中的相关代码生成三个模型，分别建立三个文件夹，将得到的模型分别放入，最后的文件结构如下图。其中100001文件夹表示模型的版本，可以在model1下放置不同版本的模型，默认情况下会加载具有较大版本号数字的模型。</p><p><img src="/2020/08/23/tensorflow-serving/%E4%B8%8D%E5%90%8C%E7%89%88%E6%9C%AC%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2.png"></p><h3 id="多模型部署"><a href="#多模型部署" class="headerlink" title="多模型部署"></a>多模型部署</h3><p>在multiModel文件夹下新建一个配置文件<code>model.config</code>，文件内容为：</p><pre class=" language-shell"><code class="language-shell">model_config_list:&#123;    config:&#123;      name:"model1",      base_path:"/models/multiModel/model1",      model_platform:"tensorflow"    &#125;,    config:&#123;      name:"model2",      base_path:"/models/multiModel/model2",      model_platform:"tensorflow"    &#125;,    config:&#123;      name:"model3",      base_path:"/models/multiModel/model3",      model_platform:"tensorflow"    &#125; &#125;</code></pre><p>配置文件定义了模型的名称和模型在容器内的路径，现在运行tfserving容器 :</p><pre class=" language-shell"><code class="language-shell">docker run -p 8501:8501 --mount type=bind,source=/home/jerry/tmp/multiModel/,target=/models/multiModel  -t tensorflow/serving --model_config_file=/models/multiModel/models.config &>server.log 2>&1</code></pre><p><strong>请求预测</strong>：</p><pre class=" language-she"><code class="language-she">import requests import numpy as np SERVER_URL = 'http://localhost:8501/v1/models/model3:predict'  #注意SERVER_URL中的‘model3’是config文件中定义的模型name,不是文件夹名称def prediction():     predict_request='&#123;"instances":%s&#125;' % str([[[10]*7]*7])     print(predict_request)     response = requests.post(SERVER_URL, data=predict_request)     print(response)    prediction = response.json()['predictions'][0]     print(prediction) if __name__ == "__main__":     prediction()</code></pre><h3 id="指定模型版本"><a href="#指定模型版本" class="headerlink" title="指定模型版本"></a>指定模型版本</h3><p>如果一个模型有多个版本，并在预测的时候希望指定模型的版本，可以通过以下方式实现。<br>修改model.config文件，增加model_version_policy：</p><pre class=" language-shell"><code class="language-shell">model_config_list:&#123;    config:&#123;      name:"model1",      base_path:"/models/multiModel/model1",      model_platform:"tensorflow",      model_version_policy:&#123;        all:&#123;&#125;      &#125;    &#125;,    config:&#123;      name:"model2",      base_path:"/models/multiModel/model2",      model_platform:"tensorflow",      model_version_policy: &#123;       specific: &#123;        versions: 101,        versions: 202       &#125;    &#125;    &#125;,    config:&#123;      name:"model3",      base_path:"/models/multiModel/model3",      model_platform:"tensorflow",       model_version_policy: &#123;       latest: &#123;        num_versions: N       &#125;    &#125;    &#125; &#125;</code></pre><p>请求预测的时候，如果要使用版本为100001的模型，只要修改SERVER_URL为：</p><p>`SERVER_URL = ‘<a href="http://localhost:8501/v1/models/model1/versions/100001:predict'">http://localhost:8501/v1/models/model1/versions/100001:predict'</a> </p><p>tfserving支持模型的Hot Plug，上述容器运行起来之后，如果在宿主机的 /home/jerry/tmp/multiModel/model1/ 文件夹下新增模型文件如100003/，tfserving会自动加载新模型；同样如果移除现有模型，tfserving也会自动卸载模型。</p><h2 id="代码demo"><a href="#代码demo" class="headerlink" title="代码demo"></a>代码demo</h2><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf<span class="token keyword">from</span> tensorflow<span class="token punctuation">.</span>keras <span class="token keyword">import</span> models<span class="token punctuation">,</span>layers<span class="token punctuation">,</span>optimizers<span class="token comment" spellcheck="true">## 样本数量</span>n <span class="token operator">=</span> <span class="token number">800</span><span class="token comment" spellcheck="true">## 生成测试用数据集</span>X <span class="token operator">=</span> tf<span class="token punctuation">.</span>random<span class="token punctuation">.</span>uniform<span class="token punctuation">(</span><span class="token punctuation">[</span>n<span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span>minval<span class="token operator">=</span><span class="token operator">-</span><span class="token number">10</span><span class="token punctuation">,</span>maxval<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span> w0 <span class="token operator">=</span> tf<span class="token punctuation">.</span>constant<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">2.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>b0 <span class="token operator">=</span> tf<span class="token punctuation">.</span>constant<span class="token punctuation">(</span><span class="token number">3.0</span><span class="token punctuation">)</span>Y <span class="token operator">=</span> X@w0 <span class="token operator">+</span> b0 <span class="token operator">+</span> tf<span class="token punctuation">.</span>random<span class="token punctuation">.</span>normal<span class="token punctuation">(</span><span class="token punctuation">[</span>n<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>    mean <span class="token operator">=</span> <span class="token number">0.0</span><span class="token punctuation">,</span>stddev<span class="token operator">=</span> <span class="token number">2.0</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># @表示矩阵乘法,增加正态扰动</span><span class="token comment" spellcheck="true">## 建立模型</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>backend<span class="token punctuation">.</span>clear_session<span class="token punctuation">(</span><span class="token punctuation">)</span>inputs <span class="token operator">=</span> layers<span class="token punctuation">.</span>Input<span class="token punctuation">(</span>shape <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span>name <span class="token operator">=</span><span class="token string">"inputs"</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#设置输入名字为inputs</span>outputs <span class="token operator">=</span> layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> name <span class="token operator">=</span> <span class="token string">"outputs"</span><span class="token punctuation">)</span><span class="token punctuation">(</span>inputs<span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#设置输出名字为outputs</span>linear <span class="token operator">=</span> models<span class="token punctuation">.</span>Model<span class="token punctuation">(</span>inputs <span class="token operator">=</span> inputs<span class="token punctuation">,</span>outputs <span class="token operator">=</span> outputs<span class="token punctuation">)</span>linear<span class="token punctuation">.</span>summary<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">## 使用fit方法进行训练</span>linear<span class="token punctuation">.</span>compile<span class="token punctuation">(</span>optimizer<span class="token operator">=</span><span class="token string">"rmsprop"</span><span class="token punctuation">,</span>loss<span class="token operator">=</span><span class="token string">"mse"</span><span class="token punctuation">,</span>metrics<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"mae"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>linear<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X<span class="token punctuation">,</span>Y<span class="token punctuation">,</span>batch_size <span class="token operator">=</span> <span class="token number">8</span><span class="token punctuation">,</span>epochs <span class="token operator">=</span> <span class="token number">100</span><span class="token punctuation">)</span>  tf<span class="token punctuation">.</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"w = "</span><span class="token punctuation">,</span>linear<span class="token punctuation">.</span>layers<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>kernel<span class="token punctuation">)</span>tf<span class="token punctuation">.</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"b = "</span><span class="token punctuation">,</span>linear<span class="token punctuation">.</span>layers<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>bias<span class="token punctuation">)</span><span class="token comment" spellcheck="true">## 将模型保存成pb格式文件</span>export_path <span class="token operator">=</span> <span class="token string">"./data/linear_model/"</span>version <span class="token operator">=</span> <span class="token string">"1"</span>       <span class="token comment" spellcheck="true">#后续可以通过版本号进行模型版本迭代与管理</span>linear<span class="token punctuation">.</span>save<span class="token punctuation">(</span>export_path<span class="token operator">+</span>version<span class="token punctuation">,</span> save_format<span class="token operator">=</span><span class="token string">"tf"</span><span class="token punctuation">)</span> </code></pre><h3 id="链接"><a href="#链接" class="headerlink" title="链接"></a>链接</h3><pre class=" language-python"><code class="language-python">!curl <span class="token operator">-</span>d <span class="token string">'&amp;#123;"instances": [[1.0, 2.0], [5.0,7.0]]&amp;#125;'</span>  <span class="token operator">-</span>X POST http<span class="token punctuation">:</span><span class="token operator">//</span>localhost<span class="token punctuation">:</span><span class="token number">8501</span><span class="token operator">/</span>v1<span class="token operator">/</span>models<span class="token operator">/</span>linear_model<span class="token punctuation">:</span>predict</code></pre><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true">##HTTP</span><span class="token keyword">import</span> json<span class="token keyword">import</span> requestsdata <span class="token operator">=</span> json<span class="token punctuation">.</span>dumps<span class="token punctuation">(</span><span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;"signature_name": "serving_default", "instances": [[1.0, 2.0], [5.0,7.0]]&amp;#125;)</span>headers <span class="token operator">=</span> <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;"content-type": "application/json"&amp;#125;</span>json_response <span class="token operator">=</span> requests<span class="token punctuation">.</span>post<span class="token punctuation">(</span><span class="token string">'http://localhost:8501/v1/models/linear_model:predict'</span><span class="token punctuation">,</span>        data<span class="token operator">=</span>data<span class="token punctuation">,</span> headers<span class="token operator">=</span>headers<span class="token punctuation">)</span>predictions <span class="token operator">=</span> json<span class="token punctuation">.</span>loads<span class="token punctuation">(</span>json_response<span class="token punctuation">.</span>text<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token string">"predictions"</span><span class="token punctuation">]</span><span class="token keyword">print</span><span class="token punctuation">(</span>predictions<span class="token punctuation">)</span></code></pre><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true">## grpc</span></code></pre><h2 id="参考内容："><a href="#参考内容：" class="headerlink" title="参考内容："></a>参考内容：</h2><p><a href="https://www.jianshu.com/p/d11a5c3dc757">TensorFlow Serving使用指南</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> tensorflow </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 运维 </tag>
            
            <tag> 部署 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Get To The Point Summarization with Pointer Generator Networks 代码解析</title>
      <link href="/2020/08/22/get-to-the-point-summarization-with-pointer-generator-networks-dai-ma-jie-xi/"/>
      <url>/2020/08/22/get-to-the-point-summarization-with-pointer-generator-networks-dai-ma-jie-xi/</url>
      
        <content type="html"><![CDATA[<h1 id="Get-To-The-Point-Summarization-with-Pointer-Generator-Networks-代码解析"><a href="#Get-To-The-Point-Summarization-with-Pointer-Generator-Networks-代码解析" class="headerlink" title="Get To The Point Summarization with Pointer Generator Networks 代码解析"></a>Get To The Point Summarization with Pointer Generator Networks 代码解析</h1><h2 id=""><a href="#" class="headerlink" title=""></a></h2><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Get To The Point  Summarization with Pointer-Generator Networks</title>
      <link href="/2020/08/21/get-to-the-point-summarization-with-pointer-generator-networks/"/>
      <url>/2020/08/21/get-to-the-point-summarization-with-pointer-generator-networks/</url>
      
        <content type="html"><![CDATA[<h1 id="Get-To-The-Point-Summarization-with-Pointer-Generator-Networks"><a href="#Get-To-The-Point-Summarization-with-Pointer-Generator-Networks" class="headerlink" title="Get To The Point: Summarization with Pointer-Generator Networks"></a>Get To The Point: Summarization with Pointer-Generator Networks</h1><p><a href="https://arxiv.org/pdf/1704.04368.pdf">paper</a></p><p><a href="https://github.com/abisee/pointer-generator">code</a></p><p><a href="https://github.com/zingp/NLP/tree/master/P007PytorchPointerGeneratorNetwork">pytorch 改动版</a></p><p><a href="https://github.com/hquzhuguofeng/New-Pointer-Generator-Networks-for-Summarization-Chinese">其他code</a></p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p> 　　神经序列到序列模型摘要提供了一种可行的新方法。首先使用了混合指针网络从原始集中复制单词，这些指针能准确的复制信息，同时保留产生单词的生成器。其次，我们使用新闻信息来记录被总结的内容，这就防止了重复。将模型应用到CNN每日邮报的摘要任务，比当前的SOTA提升至少2点。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>​        两种广泛的方法:提取和抽象。提取方法专门从直接从源文本获取的段落(通常是整个语义)中收集总结，而抽象方法则可能生成源文本中没有的新单词和短语，就像人类编写的摘要通常那样。</p><p><img src="/2020/08/21/get-to-the-point-summarization-with-pointer-generator-networks/1.png">       </p><p>​         图１ 基线注意序列模型。该模型可以考虑源文本中的相关词，生成新的词.例如，抽象化的总结中产生新颖的单词beat,</p><p>可以注意到原文中的单词victory和win。</p><p><img src="/2020/08/21/get-to-the-point-summarization-with-pointer-generator-networks/2.png"></p><p>​      图３Pointer-generator模型。每一个解码器时间步长对应一个生成概率 ${pen} _{gen} \in {[0, 1]}$计算出从词汇中生成单词和从原文中复制单词的概率。对词汇分布和注意分布进行加权求和，得到最终的分布，并据此进行预测。最终发行版中包含了词汇表之外的文章词，比如2-0。</p><p>​      RNN使文本生成成为可能，但是不准确地再现事实的细节，不能处理OOV 问题，以及重复自己。</p><p>　　hybrid pointer-generator network平衡了抽取式和抽象式，解决了细节和OOV 问题，保留生成单词的能力。</p><p>   　从神经机器翻译中提出了一种新的覆盖范围向量的变体，用于跟踪和控制源文档的覆盖范围，证明了覆盖范围对于消除重复是非常有效的。</p><h2 id="Our-Models"><a href="#Our-Models" class="headerlink" title="Our Models"></a>Our Models</h2><ul><li>sequence-to-sequence  基线模型</li><li>pointer-generator模型</li><li>添加覆盖机制的上面两个模型</li></ul><h3 id="Sequence-to-sequence-attentional-mode"><a href="#Sequence-to-sequence-attentional-mode" class="headerlink" title="Sequence-to-sequence attentional mode"></a>Sequence-to-sequence attentional mode</h3><p>sequence-to-sequence  基线模型，类似图１，toknes输入编码器(单层双向LSTM)，生成编码器隐藏状态序列$h _i$。在每$t$步中，解码器(一个单层单向LSTM)接收到前一个词向量，同时产生$s _t$,$a ^t$注意力分布</p><p><img src="/2020/08/21/get-to-the-point-summarization-with-pointer-generator-networks/3.png"></p><p>$ v, W _h, W _s , {b} _{attn} $ 是可学习的参数，注意力的分布可以被看作是源词的概率分布，告诉解码器在哪里寻找下一个词。然后，使用注意力分布来产生编码器隐藏状态的加权和，称为上下文向量$h _t$</p><p><img src="/2020/08/21/get-to-the-point-summarization-with-pointer-generator-networks/4.png"></p><p>上下文向量可以看作是从源内容的固定大小的表示,与译码器状态连接在一起,通过两个线性层提供，从而产生词汇分布$P _{vocab}$</p><p><img src="/2020/08/21/get-to-the-point-summarization-with-pointer-generator-networks/5.png"></p><p>$V , V′, b, b’$ 是可学习参数，$P _{vocab}$是该词汇表中所有单词的概率分布，它为我们提供了最终的分布，以便我们预测单词$w$</p><p><img src="/2020/08/21/get-to-the-point-summarization-with-pointer-generator-networks/6.png"></p><p>该时间步长的目标单词$w ^∗ _t$的负对数似然值</p><p><img src="/2020/08/21/get-to-the-point-summarization-with-pointer-generator-networks/7.png"></p><p>整个序列的损失是</p><p><img src="/2020/08/21/get-to-the-point-summarization-with-pointer-generator-networks/8.png"></p><h3 id="Pointer-generator-network"><a href="#Pointer-generator-network" class="headerlink" title="Pointer-generator network"></a>Pointer-generator network</h3><p>​    指针生成器网络是基线和指针网络的混合体, 允许通过指针复制单词，也允许从固定词汇表生成单词。</p><p>​    在指针生成器模型,如图３，注意分布和上下文向量$h ^∗ _t$按照Sequence-to-sequence attentional mode来计算，此外，生成概率$p _{gen} \in [0,1]$是从上下文向量$h ^∗ _t$、解码器状态$s _t$和解码器输入$x _t$计算出来的。</p><p><img src="/2020/08/21/get-to-the-point-summarization-with-pointer-generator-networks/9.png"></p><p>vectors$ w _{h∗}$, $w _s$, $w _x$ and scalar $b _{ptr}$ 是学习参数, $σ$是sigmoid, $p _{gen}$ 被用作通过从Pvocab抽样从词汇表中生成一个单词,还是通过从注意分布$a ^t$抽样从输入序列中复制一个单词。</p><p>扩展词汇表上的如下概率分布</p><p><img src="/2020/08/21/get-to-the-point-summarization-with-pointer-generator-networks/10.png"></p><p>如果$w$是词汇表以外的单词(OOV)，那么$P _{vocab(w)}$为零;类似地，如果w没有出现在源文档中，那么$\sum _{i:wi=w} a ^t _i$ 为０，生成OOV单词的能力是指针生成器模型的主要优势之一;基线等模型被限制在预先设置的词汇表中。</p><h3 id="Coverage-mechanism"><a href="#Coverage-mechanism" class="headerlink" title="Coverage mechanism"></a>Coverage mechanism</h3><p>重复是序列到序列模型的常见问题，在生成多句文本时尤其突, 采用Tu等人的覆盖模型来解决这个问题。在我们的覆盖模型中，我们保持叠加向量$c ^t$，前一个解码器时间步上的注意力分布的总和。</p><p><img src="/2020/08/21/get-to-the-point-summarization-with-pointer-generator-networks/11.png"></p><p>$c _t$是源文档词上的一个(未归一化的)分布，它表示这些词从注意机制得到的覆盖程度。$c _0$是一个零向量，第一个时间步骤中，没有包含任何源文档。</p><p>覆盖向量作为关注机制的额外输入，变化方程</p><p><img src="/2020/08/21/get-to-the-point-summarization-with-pointer-generator-networks/12.png"></p><p>$w _c$是与$v$相同长度的可学习参数,确保了注意机制当前的决策,容易避免重复关注相同的位置，从而避免产生重复的文本。</p><p>发现有必要额外定义附加损失，以惩罚重复处理相同地点。</p><p><img src="/2020/08/21/get-to-the-point-summarization-with-pointer-generator-networks/13.png"></p><p>注意覆盖率损失是有界的;特别是$covloss _t &lt;= \sum _{i}{a ^t _i } = 1$ 公式(12)不同于机器翻译中使用的覆盖损失。假设应该有一个大约一对一的翻译比率;因此，如果最终的覆盖率向量大于或小于1，就会受到惩罚。</p><p>损失函数更灵活,因为总结不应该要求统一的覆盖范围，我们只惩罚每个注意力分布和目前的覆盖范围之间的重叠——防止重复关注。最后，在主损失函数中加入由超参数参数模型模型重新加权的复盖损失，得到一个新的复合损失函数</p><p><img src="/2020/08/21/get-to-the-point-summarization-with-pointer-generator-networks/14.png"></p><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><ul><li>Neural abstractive summarization</li></ul><p>​         DUC-2004and Gigaword, two sentence-level summarizationdatasets.</p><p>​         theCNN/Daily Maildataset</p><p>​        在完整的数据集上，只有两个结果发表了</p><ul><li>Pointer-generator networks.</li></ul><p>　　　本文模型方法与Miao和Blun-som强迫注意句子压缩模型和Gu等人的CopyNet模型比较接近，但也有一些细微的差异</p><p>　　　（１）计算了一个显式概率$p _{gen}$ , Gu等人通过一个共享的$softmax$函数诱导</p><p>　　　（２）将注意力的循环分布作为复制分布，而Gu等人则将注意力的循环分布作为复制分布。使用两个单独的分布</p><p>　　　（３）当一个词在源文本中多次出现时，我们的概率质量来自于所有对应的注意分布部分，</p><p>​                       有效地计算一个明确的词能使我们同时提高或降低所有生成的词或所有复制的词的概率</p><p>​                       指针机制会复制一个单词，同时注意到它在源文本中多次出现</p><p>训练了它们的指针组件，使其只针对词汇表外的单词或命名的实体激活，而且它们没有混合来自复制分布和词汇分布的概率。</p><p>观察到混合模型可以让语言模型和复制机制一起工作，以执行抽象复制。</p><ul><li>Coverage</li></ul><p>　　　在该方法中，每个注意分布除以前一个注意分布的和，有效地抑制了重复注意。尝试过这种方法，但发现它破坏性太大，扭曲了注意力机制的信号，降低了表现。假设早期的干预方法（例如覆盖率）比事后的方法（例如暂时注意）更可取，因为它比完全忽略决策更能告知注意机制以帮助做出更好的决策。这一理论是由我们的ROUGE 评分的覆盖范围的巨大提升所支持的，相对于时间注意力对同一任务的刺激较小。</p><h2 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h2><p>CNN/Daily  Maildatase</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>model : hidden-size:256   embedding-dim: 128   字典大小 50K</p><p><img src="/2020/08/21/get-to-the-point-summarization-with-pointer-generator-networks/15.png"></p><p>表1：测试集上的ROUGE F1和METEOR得分。上半部分的模型和基准线是活跃的，下半部分的模型和基准线是可提取的。*标记在匿名数据集上进行了训练和评估，因此与在原始文本上的结果严格不具有可比性。官方ROUGE的报告，我们所有的ROUGE得分的95%置信区间最多为0.25。使用近似随机检验，发现从50k基线到指针生成器模型以及从指针生成器到指针生成器+覆盖率模型的METEOR改进在统计上都具有显着意义p&lt;0.01。</p><p> 指针和覆盖机制只向网络引入很少的附加参数 , 对于词汇表大小为50k的模型，基线模型有21,499,600个参数，指针生成器添加了1153个额外的参数($w _{h∗} , w _s,w _x , b _{ptr}$) , 覆盖率增加了512个额外的参数($w _c$)</p><p>预训练未使用为embedding, 使用Adagrad 进行训练，学习率为0.15，初始累加器值为0.1。</p><p>使用最大梯度范数为2的梯度剪裁，但不使用任何形式的正则化。</p><p>在验证集上使用损失来实现早期停止。</p><p>在训练期间和测试时，将文章截断为400个tokens，并且在训练时将长度限制为100个tokens，在测试时限制为120个tokens, batch_size ,batch为16</p><p>整体的训练时间缩短</p><p>训练不带损失函数的覆盖模型，希望注意机制可以自己学习不重复地注意相同的位置，但发现这是无效的，没有明显的重复减少。</p><h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><p><img src="/2020/08/21/get-to-the-point-summarization-with-pointer-generator-networks/16.png"></p><p>图４　覆盖消除了不必要的重复。来自我们的非覆盖模型的摘要包含许多重复的n-gram，而我们的覆盖模型产生的参考摘要的数字与之相似</p><h2 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h2><h3 id="Comparison-with-extractive-systems"><a href="#Comparison-with-extractive-systems" class="headerlink" title="Comparison with extractive systems"></a>Comparison with extractive systems</h3><ol><li><p>新闻文章的开头往往是最重要的信息;</p></li><li><p>任务的性质和ROUGE度量使提取方法和lead-3基线难以击败。抽象引入了更多的选项，进一步降低了匹配引用和的可能性</p></li><li><p>由于任务的主观性以及有效摘要的多样性，ROUGE似乎会奖励安全的策略，例如选择出现的内容或保留原始措辞。</p></li></ol><p><img src="/2020/08/21/get-to-the-point-summarization-with-pointer-generator-networks/17.png"></p><p>图６ 虽然最好的模型是抽象的，但它不会产生新奇的图形(例如没有出现在源文本中的n-gram)和引用摘要一样频繁。基线模型产生了更多新奇的图形，但其中许多是错误的．</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>​      在这项工作中，提出了一个覆盖的混合指针生成器架构，并展示了它减少了不准确性和重复。将模型应用到一个新的具有挑战性的长文本数据集上，并且显著优于SOAT结果。模型扩展了许多抽象能力，但是如何实现更高层次的抽象仍然是一个有待研究的问题。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 论文 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 指针 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NSCaching Simple and Efficient Negative Sampling for Knowledge Graph  Embedding</title>
      <link href="/2020/08/20/nscaching-simple-and-efficient-negative-sampling-for-knowledge-graph-embedding/"/>
      <url>/2020/08/20/nscaching-simple-and-efficient-negative-sampling-for-knowledge-graph-embedding/</url>
      
        <content type="html"><![CDATA[<h1 id="NSCaching-Simple-and-Efficient-Negative-Sampling-for-Knowledge-Graph-Embedding"><a href="#NSCaching-Simple-and-Efficient-Negative-Sampling-for-Knowledge-Graph-Embedding" class="headerlink" title="NSCaching Simple and Efficient Negative Sampling for Knowledge Graph Embedding"></a>NSCaching Simple and Efficient Negative Sampling for Knowledge Graph Embedding</h1><p><a href="https://arxiv.org/pdf/1812.06410.pdf">paper</a></p><p><a href="https://github.com/yzhangee/NSCaching">code github</a></p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>大概在2020-08-24看完完成</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 论文 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 知识图谱 </tag>
            
            <tag> 负采样 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Seq2Seq with Attention</title>
      <link href="/2020/08/16/seq2seq-with-attention/"/>
      <url>/2020/08/16/seq2seq-with-attention/</url>
      
        <content type="html"><![CDATA[<h1 id="Seq2Seq-with-Attention原理"><a href="#Seq2Seq-with-Attention原理" class="headerlink" title="Seq2Seq with Attention原理"></a>Seq2Seq with Attention原理</h1><ul><li>RNN结构的局限<ul><li>由长序列到定长向量转化而造成的信息损失的瓶颈</li><li>标准的Seq2Seq只关注当前的hidden state</li></ul></li></ul><h2 id="Seq2Seq-with-Attention"><a href="#Seq2Seq-with-Attention" class="headerlink" title="Seq2Seq with Attention"></a>Seq2Seq with Attention</h2><p>​     attention模型最早出现于cv领域，而首次用于解决nlp问题是在2014年[3]，seq2seq+attention 应用于机器翻译任务。以英译汉为例，当解码器对英文进行解码时，是一个词一个词生成的，而所生成的每个词对应的英文部分应该是不同，换句话说就是，解码器解码时不同step所分配的注意力是不同的。 再举一个例子，如看图说话（用一句话描述一幅图），所生成的词语应该对应图中的不同部分，即解码器在解码时，应该给图中“合适”的部位，分配更多的注意力（权重）。</p><p><img src="/2020/08/16/seq2seq-with-attention/1.jpg"></p><p>​       红圈标识的是编码器，其中h代表源文本的语义表示；紫圈标识的解码器，其中s代表目标文本的序列状态。c表示注意力向量，用来在解码时，控制源文本不同位置的attention分配。</p><p><img src="/2020/08/16/seq2seq-with-attention/2.jpeg"></p><ul><li>输入　$x = (x _1, x _2, x _3,…x _T)$</li><li>输出　$y = (y _1, y _2, y _3,…, y _T)$</li><li>$ h _t = RNN _{enc}(x _t, h _{t-1})$, Encoder方面接受的是每个单词Word Embedding, 和上一个时间点的hidden state, 输出的是这个时间点的hidden state．</li><li>$s _t = RNN _{enc}(y _{t-1}, s _{t-1})$ Decoder方面的接受目标句子单词Word Embedding，和上一个时间点的hidden state</li><li>$c _i = \sum _{j=1} ^{T _x}({\alpha} _{ij}h _j)$ context vector 是一个对于encoder 输出的hidden states 的一个加权平均</li><li>$\alpha _{ij} = \frac{exp(e _{ij})}{\sum _{k=1} ^{T _x}exp(e _{ij}) }$ 每一个encoder的hidden states的一个加权平均</li><li>$e _{ij} = score(s _{t-1}, h _j)$ 通过decoder的hidden states 加上encoder的hidden states 来计算一个分数，用于权重(4)</li><li>$ \widehat{s} = tanh(W _c[c _t; s _t])$,将context vector 和decoder 的hidden states串起来</li><li>$p(y _t|y&lt;t,x) = softmax(W _t \widehat{s _t})$ 计算最后的输出概率。</li><li>以上的步骤是luong在paper[4]里改良的decoder计算方式，paper[3]中在计算中加入了$c _t$）</li></ul><h3 id="Attention扩展"><a href="#Attention扩展" class="headerlink" title="Attention扩展"></a>Attention扩展</h3><p>​    luong在paper[4] 提出了一种attention改良方案，将attention划分为了两种形式：global， local.<br>global方式认为attention应该在所有源文本上进行，而local方式认为attention仅应该在部分源文本上进行。global理念与paper[3]相同，具体计算方式如下图所示：</p><p>$$score(h _t, \overline{h _s}) = \begin{cases}h _t ^T \overline{h _s}    \quad\quad\quad   dot\\ h _t ^TW _a\overline{h _s}      \quad\quad\quad general \\  v _a^Ttanh(W _a[h _t; \overline{h _s}]    \quad\quad\quad  concat \end{cases} $$</p><p>其中“concat” 与 paper[3] 中的计算方式相同。<br>第一种：dot</p><p><img src="/2020/08/16/seq2seq-with-attention/3.jpeg"></p><p>输入是encoder的所有hidden states H: 大小为 <strong>(hid dim, sequence length)</strong> 。decoder在一个时间点上的hidden state， s： 大小为 <strong>（hid dim, 1）</strong> 。</p><p>第一步：旋转H为（sequence length, hid dim) 与<strong>s</strong>做点乘得到一个 大小为(sequence length, 1)的分数。</p><p>第二步：<strong>对分数做softmax得到一个合为1的权重</strong>。</p><p>第三步：将H与第二步得到的权重做点乘得到一个大小为(hid dim, 1)的context vector。</p><p>第二种：general</p><p><img src="/2020/08/16/seq2seq-with-attention/4.jpeg"></p><p>输入是encoder的所有hidden states H: 大小为 <strong>(hid dim1, sequence length)</strong> 。decoder在一个时间点上的hidden state， s： 大小为 <strong>（hid dim2, 1）</strong>。此处两个hidden state的纬度并不一样。</p><p>第一步：旋转H为（sequence length, hid dim1) 与 <strong>Wa 大小为[ hid dim1, hid dim 2)]</strong> 做点乘， 再和s做点乘得到一个 大小为(sequence length, 1)的分数。</p><p>第二步：对分数做<strong>softmax</strong>得到一个合为1的权重。</p><p>第三步：将H与第二步得到的权重做点乘得到一个大小为(hid dim, 1)的context vector。</p><h3 id="需要注意的地方"><a href="#需要注意的地方" class="headerlink" title="需要注意的地方"></a>需要注意的地方</h3><ul><li>decoder 端的stst初始化：$s _0 = tanh(Wh _b ^1)$, 取encoder的反向RNN的初态的非线性，作为decoder的初态</li><li>teacher forcing模式与测试时（生成模式）不同，所以训练过程不能完全都用teacher forcing，teacher forcing 与 生成模式应按比例分配</li><li>beamsearch 只是在测试的时候用到</li><li>如果encoder 与 decoder 的序列都很长，显存装不下。可考虑对decoder端进行截断，分步优化（pytorch中 使用 state = state.detach()）</li><li>coding时，尽量别用for循环，会极大降低计算效率</li></ul><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>Attention与传统的Seq2Seq模型主要有以下两点不同：</p><ul><li><p>encoder提供了更多的数据给到decoder，<strong>encoder会把所有的节点的hidden state提供给decoder</strong>，而不仅仅只是encoder最后一个节点的hidden state 。</p></li><li><p>decoder并不是直接把所有encoder提供的hidden state作为输入，而是采取一种选择机制，把最符合当前位置的hidden state选出来，具体的步骤如下：</p><ul><li>确定哪一个hidden state与当前节点关系最为密切；</li></ul></li><li><p>计算每一个hidden state的分数值；</p><ul><li>对每个分数值做一个softmax的计算，这能让相关性高的hidden state的分数值更大，相关性低的hidden state的分数值更低。</li></ul></li></ul><p>计算细节汇总如下：</p><p>$p(y _t|y&lt;t,x) = softmax(W_s \widehat{s _t})$</p><p>$\widehat{s _t} =tanh(W _c[c _t; s _t])$</p><p>$s _t = f _d(y _{t-1}, s _{t-1})$</p><p>$c _t =\sum\alpha _{tj} h _j$</p><p>$\alpha _{ij} = softmax(e _{ij})$</p><p>$e _{ij} = a(s _{t-1}, h _j) = v _a ^Ttanh(W _{s _{t-1}} + Uh _j)$</p><p>$h _t = f _e(x _t, h _{t-1})$ 注：$f _e$ 可使用LSTM， GRu， Bi-LSTM 等</p><h2 id="Attentiom-缺点"><a href="#Attentiom-缺点" class="headerlink" title="Attentiom 缺点"></a>Attentiom 缺点</h2><p>m-&gt;原文长度</p><p>t-&gt;翻译长度</p><p>标准Seq2Seq　Ｏ(m+t)  time complexity</p><p>Seq2Seq +Attention Ｏ(mt) time complexity</p><p>[1] Sutskever I, Vinyals O, Le Q V. Sequence to sequence learning with neural networks[C] Advances in neural information processing systems. 2014.<br>[2] Cho K, Van Merriënboer B, Gulcehre C, et al. Learning phrase representations using RNN encoder-decoder for statistical machine translation[J]. arXiv, 2014.<br>[3] Bahdanau D, Cho K, Bengio Y. Neural machine translation by jointly learning to align and translate[J]. arXiv, 2014. &amp; ICLR, 2015.<br>[4] Luong M T, Pham H, Manning C D. Effective approaches to attention-based neural machine translation[J]. arXiv, 2015.</p><h4 id="参考网址："><a href="#参考网址：" class="headerlink" title="参考网址："></a>参考网址：</h4><p><a href="https://mp.weixin.qq.com/s/0k71fKKv2SRLv9M6BjDo4w">真正的完全图解Seq2Seq Attention模型</a>（非常好）<br><a href="https://blog.csdn.net/xy_free/article/details/80397426">seq2seq + attention 详解</a>（非常好）<br><a href="https://blog.csdn.net/u012526436/article/details/86293981">Attention机制详解</a>（有动图）<br><a href="https://blog.csdn.net/mystery_guest/article/details/82119527">【机器学习】【seq2seq模型与attention机制，Beam Search】</a><br><a href="https://www.jianshu.com/p/1c24eba3ba9c">【论文笔记】Effective Approaches to Attention-based Neural Machine Translation</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> nlp </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 序列到序列模型 </tag>
            
            <tag> Encoder Decoder </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Seq2Seq</title>
      <link href="/2020/08/15/seq2seq/"/>
      <url>/2020/08/15/seq2seq/</url>
      
        <content type="html"><![CDATA[<h1 id="Seq2Seq原理"><a href="#Seq2Seq原理" class="headerlink" title="Seq2Seq原理"></a>Seq2Seq原理</h1><h2 id="Seq2Seq模型简介"><a href="#Seq2Seq模型简介" class="headerlink" title="Seq2Seq模型简介"></a>Seq2Seq模型简介</h2><p>​      全称<strong>Sequence to sequence</strong>，由<strong>Encoder</strong>和<strong>Decoder</strong>两个部分组成，每部分都是一个RNNCell（RNN、LSTM、GRU等）结构。Encoder将一个序列编码为一个固定长度的语义向量，Decoder将该语义向量解码为另一个序列。</p><p>​      Seq2Seq模型是输出的长度不确定时采用的模型, 这种情况一般是在机器翻译的任务中出现，将一句中文翻译成英文，那么这句英文的长度有可能会比中文短，也有可能会比中文长，所以输出的长度就不确定了。</p><ul><li>特点：输入序列和输出序列的长度是可变的，输出序列长度可以不等于输入序列长度。</li><li>训练：对Encoder和Decoder进行联合训练，使给定输入序列的目标序列的条件概率最大化。</li><li>应用 ：seq2seq模型可以在给定输入序列的情况下生成目标序列，也可以对一对序列进行评分(以条件概率表示)。比如机器翻译、文本摘要生成、对话生成等。</li></ul><h2 id="框架一"><a href="#框架一" class="headerlink" title="框架一"></a>框架一</h2><p>该框架Paper<a href="https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf"> Sequence to Sequence Learning with Neural Networks  </a></p><p><img src="/2020/08/15/seq2seq/1.png"></p><p>Encoder输入序列A B C，生成语义向量$c$作为Decoder的初始隐藏状态，Decoder中初始时刻输入 $&lt; EOS &gt;$ 作为开始标志，直至输出 $&lt; EOS &gt;$ 结束预测。</p><p>和框架2不同的是，该框架Encoder输出的语义向量 $c$直接作为Decoder的初始隐藏状态，并不作用于之后的时刻。</p><p>这篇论文中使用LSTM作为Encoder和Decoder，为方便描述这里用RNN作为示范，公式为：</p><ul><li><p>Encoder</p><p>$h _t = tanh([W{h} _{t-1}, x _t])$</p><p>$o _t = softmax(Vh _t +c)$  </p><p>Encoder输出的语义向量：$c = h _T$</p></li></ul><blockquote><p>论文作者发现将输入序列反转后再输入Decoder中效果会好很多，以下是由此得出的结论。<br>We conclude that it is important to find a problem encoding that has the greatest number of short term dependencies, as they make the learning problem much simpler.</p></blockquote><ul><li><p>Decoder</p><p>$h _t = tanh(W[{h} _{t-1}, {y} _{t-1}] +b)$</p><p>$o _t = softmax(Vh _t +c)$</p><p>其中 $h _0 = c$</p></li></ul><h2 id="框架二"><a href="#框架二" class="headerlink" title="框架二"></a>框架二</h2><p>该框架Paper：<a href="https://arxiv.org/abs/1406.1078">Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation</a></p><p><img src="/2020/08/15/seq2seq/2.png"></p><p>​     这篇论文另一大贡献是提出了GRU，论文中Encoder和Decoder都是GRU。为了表达方便，这里我们假设Encoder和Decoder都为RNN，来看一下seq2seq的公式，注意： $c$ 与$c$不是一个参数。</p><ul><li><p>Encoder</p><p>$h _t = tanh(W([{h} _{t-1}, x _t]) + b)$</p><p>$o _t = softmax(Vh _t +c)$  </p><p>其中 $h _t$ 是隐藏状态， $o _t$ 是输出。</p><p>Encoder输出的语义向量：$c = tanh(Uh _T)$ </p><p>​         例如 假设有t个词，最终通过Encoder自定义函数 q 将各时刻的隐状态变换为向量c –&gt; $ c = q(h _0, h _1,…,h _t) $</p><p>其中 $U$ 为权重矩阵， $h _T$ 是Encoder最后的隐藏状态(记录了整个序列的信息)。</p></li><li><p>Decoder</p><p>$h _t = tanh(W[{h} _{t-1}, {y} _{t-1}] +b)$</p><p>$o _t = softmax(Vh _t +c)$</p><p>接收到Encoder来的语义向量$c$ ，首先输入一个开始信号 $y _0$ （比如为 $&lt; STRAT &gt;$ ，和一个初始化的隐藏状态 $h _0$，接下来就按照上面的公式一直传递下去。</p></li></ul><p>注意：语义向量 $c$ 作用于Decoder的每一时刻。</p><p>​        $h _1= tanh(W[h _0, y _0], +b) $</p><p>​        $o _1 = softmax(Vh _1, +c)$</p><p>​        $h _2= tanh(W[h _1, y _1], +b) $</p><p>​        $o _2 = softmax(Vh _2, +c)$</p><p>​        $…$</p><p>​        $h _T= tanh(W[{h} _{T-1}, {y} _{T-1}], +b) $</p><p>​        $o _T = softmax(Vh _T, +c)$</p><p>其中 $o _t$为每个时刻的输出，是一个向量，向量维度是词表长度，向量中的每个值是对应单词的概率。直到预测值 $ &lt; END &gt; $的概率最大时，结束预测。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://zhuanlan.zhihu.com/p/57155059">简说Seq2Seq原理及实现</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> nlp </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 序列到序列模型 </tag>
            
            <tag> Encoder Decoder </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>KBQA论文汇总</title>
      <link href="/2020/08/13/kbqs-lun-wen-hui-zong/"/>
      <url>/2020/08/13/kbqs-lun-wen-hui-zong/</url>
      
        <content type="html"><![CDATA[<p>A-Joint-Model-of-Entity-Linking-and-Predicate-Recognition-for-Knowledge-Base-Question-Answering</p><p>翻译链接　<a href="/2020/08/12/a-joint-model-of-entity-linking-and-predicate-recognition-for-knowledge-base-question-answering/" title="A Joint Model of Entity Linking and Predicate Recognition for Knowledge Base Question Answering">A Joint Model of Entity Linking and Predicate Recognition for Knowledge Base Question Answering</a></p><p>Part-of-Speech-Tagging-with-Bidirectional-Long-Short-Term-Memory-Recurrent-Neural-Network</p><p>翻译链接　<a href="/2020/08/07/part-of-speech-tagging-with-bidirectional-long-short-term-memory-recurrent-neural-network/" title="Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Recurrent Neural Network 阅读">Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Recurrent Neural Network 阅读</a></p><p>A-Chinese-Question-Answering-System-for-Single-Relation-Factoid-Questions</p><p>翻译链接　<a href="/2020/08/06/a-chinese-question-answering-system-for-single-relation-factoid-questions/" title="A Chinese Question Answering System for Single-Relation Factoid Questions 阅读">A Chinese Question Answering System for Single-Relation Factoid Questions 阅读</a></p><p>Open Domain Question Answering System Based on Knowledge Base</p><p>翻译链接　<a href="/2020/08/13/open-domain-question-answering-system-based-on-knowledge-base/" title="Open Domain Question Answering System Based on Knowledge Base">Open Domain Question Answering System Based on Knowledge Base</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>shell</title>
      <link href="/2020/08/12/shell/"/>
      <url>/2020/08/12/shell/</url>
      
        <content type="html"><![CDATA[<h1 id="shell"><a href="#shell" class="headerlink" title="shell"></a>shell</h1><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><pre class=" language-shell"><code class="language-shell">gnome-system-monitor # 查看ubantu内存watch -d -n 2 nvidia-smi # 定时刷新显卡内存使用情况 dpkg --list | grep -i jdk # 查看java版本</code></pre><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> shell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 脚本 </tag>
            
            <tag> lniux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A Joint Model of Entity Linking and Predicate Recognition for Knowledge Base Question Answering</title>
      <link href="/2020/08/12/a-joint-model-of-entity-linking-and-predicate-recognition-for-knowledge-base-question-answering/"/>
      <url>/2020/08/12/a-joint-model-of-entity-linking-and-predicate-recognition-for-knowledge-base-question-answering/</url>
      
        <content type="html"><![CDATA[<h1 id="A-Joint-Model-of-Entity-Linking-and-Predicate-Recognition-for-Knowledge-Base-Question-Answering"><a href="#A-Joint-Model-of-Entity-Linking-and-Predicate-Recognition-for-Knowledge-Base-Question-Answering" class="headerlink" title="A Joint Model of Entity Linking and Predicate Recognition for Knowledge Base Question Answering"></a>A Joint Model of Entity Linking and Predicate Recognition for Knowledge Base Question Answering</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>　　在本paper中，构建了一个自动找到正确答案的问答系统, 第一步是为一个问题确定知识库中所有的实体.利用预测分数对主题实体的所有排序后三元组合进行logistic建模，第二步使用联合训练实体链接和谓词识别模型对问题的三元组进行重新排序．最后，利用规则匹配从三元组中选择答案。方法在测试数据上的平均f1分为57.67%，获得CCKS 2018 COQA任务竞赛的第二名。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>　　本文首先是选择实体，然后选择one-hop 和 two-hop进行实体排序．然后使用语义匹配模型BiMPM[1]来训练一个用于实体链接和谓词识别的联合模型来对三元组进行重新排序。最后，基于规则匹配从三元组中选择答案。one-hop 和 two-hop三元组的问题占90.02%。因此，本文主要探讨这些问题。</p><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><p>　　KBQA是自然语言处理领域的一个重要课题。目前有两种主流方法:基于语义解析和基于检索。</p><p>　　语意解析首先解析问题的逻辑形式，该形式是一课语义树，组合的方式显式地表示问题的意义，根据知识库执行逻辑得到答案。这种方法的逻辑形式有助于理解问题的语义结构，也增加了这项任务的难度。Lai使用基于词嵌入的特征搜索最优的主语谓词对，并根据规则在NLPCC 2016 KBQA任务中获得第一名。Lai提出了一种新的基于深度cnn的方法来对由浅层特征生成的实体-谓词对进行重新排序。该方法在NLPCC 2017 KBQA任务竞赛中获得第一名。Hu提出了一种动态查询图匹配方法，从数据驱动的角度处理实体和关系的消歧任务。在我们的工作中，我们也使用了基于检索的方法。</p><h2 id="The-Proposed-System"><a href="#The-Proposed-System" class="headerlink" title="The Proposed System"></a>The Proposed System</h2><p><img src="/2020/08/12/a-joint-model-of-entity-linking-and-predicate-recognition-for-knowledge-base-question-answering/1.png"></p><p>  系统架构如图1所示。第一步是预处理，即对输入问题进行分词处理。然后根据预处理结果进行实体识别模块，然后对具有某些特征的三元组路径进行优先排序．</p><p>用BiMPM选择匹配的三元组路径。最后，从匹配的候选三元组路径中选择答案。</p><h2 id="Knowledge-Base-and-Dictionary"><a href="#Knowledge-Base-and-Dictionary" class="headerlink" title="Knowledge Base and Dictionary"></a>Knowledge Base and Dictionary</h2><p>​    知识库(KB)和实体提及字典 由CCKS提供, 此外，用于Hanlp构建了分割词典，以提高语分割和实体提及识别的结果。分割字典是由所有主题的KB，所有实体及其提到的字典。</p><h2 id="Topic-Entity-Recognition"><a href="#Topic-Entity-Recognition" class="headerlink" title="Topic Entity Recognition"></a>Topic Entity Recognition</h2><p>　　如果问题分割的词在字典中，则该词属于实体。从某些特征来看，被识别的提及实体被提及的概率是不同的。我们系统中使用的特性定义如下：</p><ul><li>有较长的字符串的实体更可能是主题实体，而不是较短的实体。</li><li>实体的TF值高词频(TF)值的实体比低词频(TF)值的实体成为主题实体的概率更低</li><li>实体与疑问词之间的距离靠近疑问词的问题中实体更可能是主题实体</li></ul><p>　　最后一步所识别的连接实体提到的实体并不是实体知识库，所以这一步的目的是确定实体提及的身份问题。实体的关系和属性信息有助于实体链接，因此首先提取实体 two-hop，根据所选择的候选实体提取率，我们使用以下三个特征对匹配的主题实体进行排序和选择．</p><ul><li>问题与候选实体的two-hop子图之间的重叠词越多，所提取的实体为主题实体的概率越大。</li><li>问题与候选实体的two-hop子图之间的相似度越大，所提取的实体为主题实体的概率越大。</li><li>问题和三元组路径的字符重叠特征是相似的。唯一的区别是该特性使用字符级而不是单词级。</li></ul><p>在计算和正则化所有特征后，利用线性加权方法对候选实体进行排序。分数方程定义如下式，其中$w _i$表示特征$i$的权重。</p><p>${Score} _{tpicentity} = w _1 * F _1 + w _2 * F _2  + w _3 * F _3 + w _4 * F _ 4 + w _5 * F _5 + w _6 * F _6$</p><h2 id="Predicate-Recognition"><a href="#Predicate-Recognition" class="headerlink" title="Predicate Recognition"></a>Predicate Recognition</h2><p>　　一个主题实体可以提取大约349.6条候选三元组路径。很难从如此多的候选三元组路径中选择最匹配的路径。缩小候选三元组路径是提高最终结果的重要步骤。在本模块中，我们首先提取关于三元组路径谓词的四个特性。然后利用logistic回归算法对具有以下四个特征和主题实体识别特征的候选三元组路径进行排序。最后，我们选择前10条三元组路径作为下一个语义匹配模块的候选路径。</p><ul><li><p>疑问句和谓语之间的单词重叠 </p><p>三元组路径的问题谓词与候选谓词之间共享的重叠词越多，候选谓词为真实谓词的概率越大。</p></li><li><p>疑问句与谓语之间的词嵌入相似性</p><p>问题与候选谓词之间的相似性越大，候选谓词真正是谓词的可能性越大。</p></li><li><p>问题和谓词之间的字符重叠</p><p>这个特性几乎与第一个相同。唯一的区别是这个特性使用char级而不是词水平</p></li></ul><ul><li><p>问题和谓词之间的字符嵌入相似性</p><p>这个特性几乎与第二句相同。唯一的区别是该特性使用字符级而不是单词级。</p></li></ul><h2 id="Semantic-Matching"><a href="#Semantic-Matching" class="headerlink" title="Semantic Matching"></a>Semantic Matching</h2><p>问题的形式化 </p><p>  这个模块的目标是从n个候选三元组路径{T P1, T P2，…Q是用户的问题。TPi是Q的候选三元组路径。在本文中，我们使用两两评分函数S(TPi,Q)对所有候选三元组路径进行评分和排序。在本文中，n为10。</p><p>BiMPM ＋Fea</p><p>  将单词嵌入和所有10个特性合并到BiMPM中，以选择最佳匹配的三元组路径。BiMPM+Fea包含5个内核层。</p><ul><li>单词表示层:该层的目标是用d维向量表示每个有问题的单词和三重路径。将嵌入到论文中的单词用Gensim进行预训练，d为100。</li><li>上下文表示层:此层的目的是将上下文信息合并到问题和三元组路径的每个时间步骤的表示中。本文使用BiLSTM对每个时间步长进行上下文嵌入编码。</li><li>匹配层:这是核心层，用于获取问题与三元组路径在时间步长上的相似性。此外，匹配是双向的，即问题与三元组路径相互匹配，并分别从各自的匹配信息中获得匹配信息。</li><li>聚合层:将匹配信息的问题和三元组路径聚合为定长。聚合层由BiLSTM组成，我们使用最终的隐藏状态来表示聚合的信息。</li><li>特征聚合层:该层将最后一层的定长张量与我们提取的10个特征连接起来。</li></ul><h2 id="Answers-Selection"><a href="#Answers-Selection" class="headerlink" title="Answers Selection"></a>Answers Selection</h2><p>在语义匹配模块中选择匹配的三元组路径。然后规则生成答案。图2显示了我们的规则示例。图中圆形节点或矩形节点仅表示实体或属性值，不影响规则选择答案。蓝色结点就是答案。</p><p><img src="/2020/08/12/a-joint-model-of-entity-linking-and-predicate-recognition-for-knowledge-base-question-answering/2.png"></p><p>One-hop 三元路径:　在这种情况下，答案是问题中没有出现的三元组路径的组件。</p><p>Two-hop 三元路径:　如果三元组路径中最右边的节点和最左边的节点都没有出现在问题中，那么中间的节点就是答案。如果最右边的节点或最左边的节点出现在问题中，那么另一个就是答案。</p><h2 id="Experiments-and-discussion"><a href="#Experiments-and-discussion" class="headerlink" title="Experiments and discussion"></a>Experiments and discussion</h2><p>我们通过使用CCKS数据来评估我们的方法。该数据集由CCKS 2018评估任务发布，包括知识库、知识实体提取文件和用于训练、验证和测试的问答对。知识库有4100万三倍。2018-Train集、2018-Val集、2018-Test集分别包含1283,400,400个样本。在训练过程中，为了获得负样本，对于每个问题，我们选择前10个错误的候选三元路径。为了减轻训练数据不平衡的影响，我们对正样本进行过采样。</p><h2 id="Topic-Entity-Recognition-Result"><a href="#Topic-Entity-Recognition-Result" class="headerlink" title="Topic Entity Recognition Result"></a>Topic Entity Recognition Result</h2><p><img src="/2020/08/12/a-joint-model-of-entity-linking-and-predicate-recognition-for-knowledge-base-question-answering/3.png"></p><p>表1显示了主题实体识别模块的系统性能。基本型baselineT E只使用F1, F2, F3, F4。第二个模型是baselineT E +Emb，它也使用了嵌入特性F5。最后一个是baselineT E +Emb+Char，也使用了嵌入特性F5和Char级特性F6。从表1可以明显看出，嵌入特征F5和char级特征F6都可以提高主题实体识．超参数$w _i$为[0.25,0.37，-0.32,0.67,0.71,0.58]。</p><h2 id="BiMPM-Re-ranking-Result"><a href="#BiMPM-Re-ranking-Result" class="headerlink" title="BiMPM Re-ranking Result"></a>BiMPM Re-ranking Result</h2><p><img src="/2020/08/12/a-joint-model-of-entity-linking-and-predicate-recognition-for-knowledge-base-question-answering/4.png"></p><p>表2显示了实验系统的性能, 基本模型BiMPM只使用预先训练好的字嵌入。第二个是BiMPM+Fea，它也使用了基于基线BiMPM提取的10个特征。最后一个是BiMPM+Fea+CV，它利用了基于BiMPM+Fea的10倍交叉验证。明显看出，提取的特征和交叉验证都可以提高f1得分.测试结果高出2.5%．原因是实体链接分数和预测识别分数对于三元组路径的预排序是有用的。</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>一种用于KBQA的实体连接和谓词识别的联合模型。系统在CCKS 2018 COQA任务中，f1得分为57.67%。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 论文 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kbqa </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>医疗知识问答</title>
      <link href="/2020/08/10/yi-liao-zhi-shi-wen-da/"/>
      <url>/2020/08/10/yi-liao-zhi-shi-wen-da/</url>
      
        <content type="html"><![CDATA[<h1 id="医疗知识问答"><a href="#医疗知识问答" class="headerlink" title="医疗知识问答"></a>医疗知识问答</h1><p>本内容参考了<a href="https://github.com/liuhuanyong/QASystemOnMedicalKG%EF%BC%8C%E4%B8%BB%E8%A6%81%E6%98%AF%E7%94%A8%E4%BA%8E%E5%AD%A6%E4%B9%A0,%E5%9C%A8%E8%BF%99%E9%87%8C%E6%84%9F%E8%B0%A2%E5%88%98%E7%84%95%E5%8B%87%E8%80%81%E5%B8%88%E7%9A%84%E4%BB%A3%E7%A0%81%E5%88%86%E4%BA%AB">https://github.com/liuhuanyong/QASystemOnMedicalKG，主要是用于学习,在这里感谢刘焕勇老师的代码分享</a></p><p>效果预览　pc端：<a href="http://106.13.47.214/full/#/knowledgeMap">http://106.13.47.214/full/#/knowledgeMap</a>　</p><p>　　　　　手机端APP下载（目前只生成安卓端，理论项目安卓，ios,小程序都将可以生成，可以跨多端）：<a href="https://pan.baidu.com/s/1hccJoHANdmn4kf251Clj_A">https://pan.baidu.com/s/1hccJoHANdmn4kf251Clj_A</a>  密码:rlv7</p><p>项目代码：</p><p>　　　　爬虫：(scrapy)<a href="https://gitee.com/daiyizheng/scrapy.git">https://gitee.com/daiyizheng/scrapy.git</a></p><p>　　　　网页前端（vue） <a href="https://gitee.com/daiyizheng/nlp-vue.git">https://gitee.com/daiyizheng/nlp-vue.git</a></p><p>​               后端（flask）<a href="https://gitee.com/daiyizheng/nlp-flask.git">https://gitee.com/daiyizheng/nlp-flask.git</a></p><p>　　　　手机app项目（uniapp h5跨多端开发）：<a href="https://gitee.com/daiyizheng/NLP-uniapp.git">https://gitee.com/daiyizheng/NLP-uniapp.git</a></p><p>​                模型（haggingface）<a href="https://gitee.com/daiyizheng/medicine-qa-on-knowledge-graph-with-ro-berta.git">https://gitee.com/daiyizheng/medicine-qa-on-knowledge-graph-with-ro-berta.git</a></p><h2 id="任务描述"><a href="#任务描述" class="headerlink" title="任务描述"></a>任务描述</h2><p>本项目是基于知识图谱的医疗问答系统，BERT+CRF做命名实体识别和BERT句子分类，最后实现torchserve的部署。（由于功能部署还是算半企业级，后期改用Rasa做医疗知识图谱问答）</p><h2 id="数据简介"><a href="#数据简介" class="headerlink" title="数据简介"></a>数据简介</h2><h3 id="原始数据网络爬取"><a href="#原始数据网络爬取" class="headerlink" title="原始数据网络爬取"></a>原始数据网络爬取</h3><p>scrapy 爬取<a href="http://www.xywy.com/%E7%BD%91%E7%AB%99%E5%8C%BB%E8%8D%AF%E7%9C%8B%E6%95%B0%E6%8D%AE%EF%BC%8C%E4%B8%8E%E5%8E%9F%E9%A1%B9%E7%9B%AE%E4%B8%8D%E5%90%8C%E7%9A%84%E6%98%AF%EF%BC%9A%E5%8E%9F%E9%A1%B9%E7%9B%AE%E4%BD%BF%E7%94%A8%E7%9A%84%E6%98%AFrequests%E5%BA%93%E6%9D%A5%E7%88%AC%E5%8F%96%E6%95%B0%E6%8D%AE%E7%9A%84%EF%BC%8C%E4%B8%AA%E4%BA%BA%E5%AD%A6%E4%B9%A0scrapy%E6%A1%86%E6%9E%B6%EF%BC%8C%E4%BD%BF%E7%94%A8%E8%AF%A5%E6%A1%86%E6%9E%B6%E7%88%AC%E5%8F%96%E6%95%B0%E6%8D%AE%EF%BC%88%E7%88%AC%E8%99%ABscrapy%E7%9F%A5%E8%AF%86%E5%8F%AF%E4%BB%A5%E5%8F%82%E8%80%83https://daiyizheng.github.io/2020/06/01/scrapy/%EF%BC%89">http://www.xywy.com/网站医药看数据，与原项目不同的是：原项目使用的是requests库来爬取数据的，个人学习scrapy框架，使用该框架爬取数据（爬虫scrapy知识可以参考https://daiyizheng.github.io/2020/06/01/scrapy/）</a></p><p><img src="/2020/08/10/yi-liao-zhi-shi-wen-da/8.png"></p><p>爬取的数据结构存放在mongdb中,可以通过插件Mongo Express查看</p><img src="/2020/08/10/yi-liao-zhi-shi-wen-da/10.png" style="zoom:80%;"><p>数据处理：</p><p>由于数据标注比较费时间，(标注方法可以使用标注精灵)</p><p>　　后来看到有人已经表过数据了，一直采用了他的数据</p><p>　　LSTM-CRF <a href="https://gitee.com/untilwty/chatbot-base-on-Knowledge-Graph/tree/master/data_ai">https://gitee.com/untilwty/chatbot-base-on-Knowledge-Graph/tree/master/data_ai</a></p><p>命名实体部分采用了BIOES标注</p><img src="/2020/08/10/yi-liao-zhi-shi-wen-da/11.png"><p>关系分类：</p><p>　　　　　<img src="/2020/08/10/yi-liao-zhi-shi-wen-da/12.png"></p><p>分类部分的数据做了数据增强（<strong>实体和关系倒叙</strong>）</p><h3 id="知识图谱结构"><a href="#知识图谱结构" class="headerlink" title="知识图谱结构"></a>知识图谱结构</h3><p>知识图谱实体类型</p><table><thead><tr><th align="left">实体类型</th><th align="center">中文含义</th><th align="center">实体数量</th><th align="left">举例</th></tr></thead><tbody><tr><td align="left">Check</td><td align="center">诊断检查项目</td><td align="center">3,353</td><td align="left">支气管造影;关节镜检查</td></tr><tr><td align="left">Department</td><td align="center">医疗科目</td><td align="center">54</td><td align="left">整形美容科;烧伤科</td></tr><tr><td align="left">Disease</td><td align="center">疾病</td><td align="center">8,807</td><td align="left">血栓闭塞性脉管炎;胸降主动脉动脉瘤</td></tr><tr><td align="left">Drug</td><td align="center">药品</td><td align="center">3,828</td><td align="left">京万红痔疮膏;布林佐胺滴眼液</td></tr><tr><td align="left">Food</td><td align="center">食物</td><td align="center">4,870</td><td align="left">番茄冲菜牛肉丸汤;竹笋炖羊肉</td></tr><tr><td align="left">Producer</td><td align="center">在售药品</td><td align="center">17,201</td><td align="left">通药制药青霉素V钾片;青阳醋酸地塞米松片</td></tr><tr><td align="left">Symptom</td><td align="center">疾病症状</td><td align="center">5,998</td><td align="left">乳腺组织肥厚;脑实质深部出血</td></tr><tr><td align="left">Total</td><td align="center">总计</td><td align="center">44,111</td><td align="left">约4.4万实体量级</td></tr></tbody></table><p>知识图谱实体关系类型</p><table><thead><tr><th align="left">实体关系类型</th><th align="center">中文含义</th><th align="center">关系数量</th><th align="left">举例</th></tr></thead><tbody><tr><td align="left">belongs_to</td><td align="center">属于</td><td align="center">8,844</td><td align="left">&lt;妇科,属于,妇产科&gt;</td></tr><tr><td align="left">common_drug</td><td align="center">疾病常用药品</td><td align="center">14,649</td><td align="left">&lt;阳强,常用,甲磺酸酚妥拉明分散片&gt;</td></tr><tr><td align="left">do_eat</td><td align="center">疾病宜吃食物</td><td align="center">22,238</td><td align="left">&lt;胸椎骨折,宜吃,黑鱼&gt;</td></tr><tr><td align="left">drugs_of</td><td align="center">药品在售药品</td><td align="center">17,315</td><td align="left">&lt;青霉素V钾片,在售,通药制药青霉素V钾片&gt;</td></tr><tr><td align="left">need_check</td><td align="center">疾病所需检查</td><td align="center">39,422</td><td align="left">&lt;单侧肺气肿,所需检查,支气管造影&gt;</td></tr><tr><td align="left">no_eat</td><td align="center">疾病忌吃食物</td><td align="center">22,247</td><td align="left">&lt;唇病,忌吃,杏仁&gt;</td></tr><tr><td align="left">recommand_drug</td><td align="center">疾病推荐药品</td><td align="center">59,467</td><td align="left">&lt;混合痔,推荐用药,京万红痔疮膏&gt;</td></tr><tr><td align="left">recommand_eat</td><td align="center">疾病推荐食谱</td><td align="center">40,221</td><td align="left">&lt;鞘膜积液,推荐食谱,番茄冲菜牛肉丸汤&gt;</td></tr><tr><td align="left">has_symptom</td><td align="center">疾病症状</td><td align="center">5,998</td><td align="left">&lt;早期乳腺癌,疾病症状,乳腺组织肥厚&gt;</td></tr><tr><td align="left">acompany_with</td><td align="center">疾病并发疾病</td><td align="center">12,029</td><td align="left">&lt;下肢交通静脉瓣膜关闭不全,并发疾病,血栓闭塞性脉管炎&gt;</td></tr><tr><td align="left">Total</td><td align="center">总计</td><td align="center">294,149</td><td align="left">约30万关系量级</td></tr></tbody></table><p>知识图谱属性类型</p><table><thead><tr><th align="left">属性类型</th><th align="center">中文含义</th><th align="center">举例</th></tr></thead><tbody><tr><td align="left">name</td><td align="center">疾病名称</td><td align="center">喘息样支气管炎</td></tr><tr><td align="left">desc</td><td align="center">疾病简介</td><td align="center">又称哮喘性支气管炎…</td></tr><tr><td align="left">cause</td><td align="center">疾病病因</td><td align="center">常见的有合胞病毒等…</td></tr><tr><td align="left">prevent</td><td align="center">预防措施</td><td align="center">注意家族与患儿自身过敏史…</td></tr><tr><td align="left">cure_lasttime</td><td align="center">治疗周期</td><td align="center">6-12个月</td></tr><tr><td align="left">cure_way</td><td align="center">治疗方式</td><td align="center">“药物治疗”,”支持性治疗”</td></tr><tr><td align="left">cured_prob</td><td align="center">治愈概率</td><td align="center">95%</td></tr><tr><td align="left">easy_get</td><td align="center">疾病易感人群</td><td align="center">无特定的人群</td></tr></tbody></table><p>实体关系的数据在<a href="https://github.com/liuhuanyong/QASystemOnMedicalKG/tree/master/dict">https://github.com/liuhuanyong/QASystemOnMedicalKG/tree/master/dict</a></p><p>可以通过脚本<a href="https://github.com/liuhuanyong/QASystemOnMedicalKG/blob/master/build_medicalgraph.py%E5%AF%BC%E5%85%A5neo4j(neo4j%E7%9A%84%E9%85%8D%E7%BD%AE%E5%8F%AF%E4%BB%A5%E5%8F%82%E8%80%83https://daiyizheng.github.io/2020/05/11/neo4j/)">https://github.com/liuhuanyong/QASystemOnMedicalKG/blob/master/build_medicalgraph.py导入neo4j(neo4j的配置可以参考https://daiyizheng.github.io/2020/05/11/neo4j/)</a></p><p>整体效果</p><p><img src="/2020/08/10/yi-liao-zhi-shi-wen-da/graph.svg"></p><h2 id="问答系统框架"><a href="#问答系统框架" class="headerlink" title="问答系统框架"></a>问答系统框架</h2><p><img src="/2020/08/10/yi-liao-zhi-shi-wen-da/13.png"></p><p>⼀个较完整的问答系统流程主要有以下三步：</p><p><strong>问题分析模块</strong>　</p><p>　　1. 实体提取<br>  　　2. 关系分类</p><p><strong>问题回答模块</strong></p><p>　根据实体关系进行答案检索</p><h2 id="技术方案流程"><a href="#技术方案流程" class="headerlink" title="技术方案流程"></a>技术方案流程</h2><ul><li>1 输入问句</li><li>2 通过实体识别模型检测问句中的实体，得到实体（BERT+CRF）</li><li>3 通过句子语义识别得到句子的类别（BERT），即关系．</li><li>4 通过实体和关系在neo4j中检索答案</li><li>5 输出答案</li></ul><h3 id="命名实体识别"><a href="#命名实体识别" class="headerlink" title="命名实体识别"></a>命名实体识别</h3><p><strong>模型</strong></p><pre class=" language-python"><code class="language-python"><span class="token keyword">from</span> typing <span class="token keyword">import</span> List<span class="token punctuation">,</span> Optional<span class="token keyword">from</span> transformers <span class="token keyword">import</span> BertForTokenClassification<span class="token punctuation">,</span> BertTokenizer<span class="token punctuation">,</span> BertConfig<span class="token punctuation">,</span>BertModel<span class="token keyword">from</span> CRF_Model <span class="token keyword">import</span> CRF<span class="token keyword">import</span> torch<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">import</span> os<span class="token keyword">import</span> codeMODEL_NAME <span class="token operator">=</span> <span class="token string">"bert-base-chinese-model.bin"</span>CONFIG_NAME <span class="token operator">=</span> <span class="token string">"bert-base-chinese-config.json"</span>VOB_NAME <span class="token operator">=</span> <span class="token string">"bert-base-chinese-bert-base-chinese-vocab.txt"</span><span class="token keyword">class</span> <span class="token class-name">BertCrf</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> config_name<span class="token punctuation">:</span> str<span class="token punctuation">,</span> model_name<span class="token punctuation">:</span> str <span class="token operator">=</span> None<span class="token punctuation">,</span> num_tags<span class="token punctuation">:</span> int <span class="token operator">=</span> <span class="token number">2</span><span class="token punctuation">,</span> batch_first<span class="token punctuation">:</span> bool <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> None<span class="token punctuation">:</span>        self<span class="token punctuation">.</span>batch_first <span class="token operator">=</span> batch_first        <span class="token keyword">if</span> <span class="token operator">not</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>exists<span class="token punctuation">(</span>config_name<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span>                <span class="token string">"未找到模型配置文件 '&amp;#123;&amp;#125;'"</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>config_name<span class="token punctuation">)</span>            <span class="token punctuation">)</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>config_name <span class="token operator">=</span> config_name        <span class="token keyword">if</span> model_name <span class="token keyword">is</span> <span class="token operator">not</span> None<span class="token punctuation">:</span>            <span class="token keyword">if</span> <span class="token operator">not</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>exists<span class="token punctuation">(</span>model_name<span class="token punctuation">)</span><span class="token punctuation">:</span>                <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span>                    <span class="token string">"未找到模型预训练参数文件 '&amp;#123;&amp;#125;'"</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>model_name<span class="token punctuation">)</span>                <span class="token punctuation">)</span>            <span class="token keyword">else</span><span class="token punctuation">:</span>                self<span class="token punctuation">.</span>model_name <span class="token operator">=</span> model_name        <span class="token keyword">else</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>model_name <span class="token operator">=</span> None        super<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>bert_config <span class="token operator">=</span> BertConfig<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>self<span class="token punctuation">.</span>config_name<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>lstm <span class="token operator">=</span> nn<span class="token punctuation">.</span>LSTM<span class="token punctuation">(</span>bidirectional<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> num_layers<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> input_size<span class="token operator">=</span>self<span class="token punctuation">.</span>bert_config<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> hidden_size<span class="token operator">=</span>self<span class="token punctuation">.</span>bert_config<span class="token punctuation">.</span>hidden_size<span class="token operator">//</span><span class="token number">2</span><span class="token punctuation">,</span> batch_first<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch_size, seq_len. num_directions * hidden_size]</span>        self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>bert_config<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> num_tags<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>bert_config<span class="token punctuation">.</span>num_labels <span class="token operator">=</span> num_tags        self<span class="token punctuation">.</span>model_kwargs <span class="token operator">=</span> <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;'config': self.bert_config&amp;#125;</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>model_name <span class="token keyword">is</span> <span class="token operator">not</span> None<span class="token punctuation">:</span>            self<span class="token punctuation">.</span>bertModel <span class="token operator">=</span> BertModel<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>self<span class="token punctuation">.</span>model_name<span class="token punctuation">,</span> <span class="token operator">**</span>self<span class="token punctuation">.</span>model_kwargs<span class="token punctuation">)</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>bertModel <span class="token operator">=</span> BertModel<span class="token punctuation">(</span>self<span class="token punctuation">.</span>bert_config<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>crf_model <span class="token operator">=</span> CRF<span class="token punctuation">(</span>num_tags<span class="token operator">=</span>num_tags<span class="token punctuation">,</span> batch_first<span class="token operator">=</span>batch_first<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_ids<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span>                tags<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor <span class="token operator">=</span> None<span class="token punctuation">,</span>                attention_mask<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>ByteTensor<span class="token punctuation">]</span> <span class="token operator">=</span> None<span class="token punctuation">,</span>                token_type_ids<span class="token operator">=</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span>                decode<span class="token punctuation">:</span> bool <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">,</span>                reduction<span class="token punctuation">:</span> str <span class="token operator">=</span> <span class="token string">"mean"</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> List<span class="token punctuation">:</span>        emissions <span class="token operator">=</span> self<span class="token punctuation">.</span>bertModel<span class="token punctuation">(</span>input_ids<span class="token operator">=</span>input_ids<span class="token punctuation">,</span> attention_mask<span class="token operator">=</span>attention_mask<span class="token punctuation">,</span> token_type_ids<span class="token operator">=</span>token_type_ids<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        emissions<span class="token punctuation">,</span> _ <span class="token operator">=</span> self<span class="token punctuation">.</span>lstm<span class="token punctuation">(</span>emissions<span class="token punctuation">)</span>        emissions <span class="token operator">=</span> self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>emissions<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 这里在seq_len的维度上去头，是去掉了[CLS]，去尾巴有两种情况</span>        <span class="token comment" spellcheck="true"># 1、是 &lt;pad> 2、[SEP]</span>        new_emissions <span class="token operator">=</span> emissions<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>  <span class="token comment" spellcheck="true">## [batch_size, seq_len-2, num_labels] 去掉cls 和 sep</span>        new_mask <span class="token operator">=</span> attention_mask<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">:</span><span class="token punctuation">]</span>  <span class="token comment" spellcheck="true">## [batch_size, seq_len-2] 去掉cls 和 sep</span>        <span class="token keyword">if</span> tags <span class="token keyword">is</span> None<span class="token punctuation">:</span>            loss <span class="token operator">=</span> None            <span class="token keyword">pass</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            new_tags <span class="token operator">=</span> tags<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>  <span class="token comment" spellcheck="true">#</span>            loss <span class="token operator">=</span> self<span class="token punctuation">.</span>crf_model<span class="token punctuation">(</span>emissions<span class="token operator">=</span>new_emissions<span class="token punctuation">,</span> tags<span class="token operator">=</span>new_tags<span class="token punctuation">,</span> mask<span class="token operator">=</span>new_mask<span class="token punctuation">,</span> reduction<span class="token operator">=</span>reduction<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># [1]</span>        <span class="token keyword">if</span> decode<span class="token punctuation">:</span>            tag_list <span class="token operator">=</span> self<span class="token punctuation">.</span>crf_model<span class="token punctuation">.</span>decode<span class="token punctuation">(</span>emissions<span class="token operator">=</span>new_emissions<span class="token punctuation">,</span> mask<span class="token operator">=</span>new_mask<span class="token punctuation">)</span>            <span class="token keyword">return</span> <span class="token punctuation">[</span>loss<span class="token punctuation">,</span> tag_list<span class="token punctuation">]</span>        <span class="token keyword">return</span> <span class="token punctuation">[</span>loss<span class="token punctuation">]</span>    <span class="token keyword">from</span> typing <span class="token keyword">import</span> List<span class="token punctuation">,</span> Optional<span class="token keyword">import</span> torch<span class="token keyword">import</span> code<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">class</span> <span class="token class-name">CRF</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> num_tags<span class="token punctuation">:</span> int <span class="token operator">=</span> <span class="token number">2</span><span class="token punctuation">,</span> batch_first<span class="token punctuation">:</span> bool <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> None<span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># if num_tags &lt;= 0:</span>        <span class="token comment" spellcheck="true">#     raise ValueError(f'invalid number of tags: &amp;#123;num_tags&amp;#125;')</span>        super<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>num_tags <span class="token operator">=</span> num_tags  <span class="token comment" spellcheck="true"># 3</span>        self<span class="token punctuation">.</span>batch_first <span class="token operator">=</span> batch_first  <span class="token comment" spellcheck="true"># true</span>        <span class="token comment" spellcheck="true"># start 到其他tag(不包含end)的得分</span>        self<span class="token punctuation">.</span>start_transitions <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>            torch<span class="token punctuation">.</span>empty<span class="token punctuation">(</span>num_tags<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># tensor([-0.0025, -0.0600, -0.0841], requires_grad=True)</span>        <span class="token comment" spellcheck="true"># 到其他tag(不包含start)到end的得分</span>        self<span class="token punctuation">.</span>end_transitions <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>            torch<span class="token punctuation">.</span>empty<span class="token punctuation">(</span>num_tags<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># tensor([-0.0673,  0.0638, -0.0416], requires_grad=True)</span>        <span class="token comment" spellcheck="true"># 从 _compute_normalizer 中 next_score = broadcast_score + self.transitions + broadcast_emissions 可以看出</span>        <span class="token comment" spellcheck="true"># transitions[i][j] 表示从第i个tag 到第j个 tag的分数</span>        self<span class="token punctuation">.</span>transitions <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>empty<span class="token punctuation">(</span>num_tags<span class="token punctuation">,</span> num_tags<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># [3,3] tensor([[1,1,1],[1,1,1],[1,1,1]])</span>        self<span class="token punctuation">.</span>reset_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true">#</span>    <span class="token keyword">def</span> <span class="token function">reset_parameters</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        init_range <span class="token operator">=</span> <span class="token number">0.1</span>        nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>uniform_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>start_transitions<span class="token punctuation">,</span> <span class="token operator">-</span>init_range<span class="token punctuation">,</span> init_range<span class="token punctuation">)</span>        nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>uniform_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>end_transitions<span class="token punctuation">,</span> <span class="token operator">-</span>init_range<span class="token punctuation">,</span> init_range<span class="token punctuation">)</span>        nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>uniform_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>transitions<span class="token punctuation">,</span> <span class="token operator">-</span>init_range<span class="token punctuation">,</span> init_range<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> emissions<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span>                tags<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor <span class="token operator">=</span> None<span class="token punctuation">,</span>                mask<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>ByteTensor<span class="token punctuation">]</span> <span class="token operator">=</span> None<span class="token punctuation">,</span>                reduction<span class="token punctuation">:</span> str <span class="token operator">=</span> <span class="token string">"mean"</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># emissions.shape = [12,62,3]=[batchsize, seq_len-2, num_tag]</span>        mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>mask<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>uint8<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># [12,62]</span>        self<span class="token punctuation">.</span>_validate<span class="token punctuation">(</span>emissions<span class="token punctuation">,</span> tags<span class="token operator">=</span>tags<span class="token punctuation">,</span> mask<span class="token operator">=</span>mask<span class="token punctuation">)</span>        reduction <span class="token operator">=</span> reduction<span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> mask <span class="token keyword">is</span> None<span class="token punctuation">:</span>            mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones_like<span class="token punctuation">(</span>tags<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>uint8<span class="token punctuation">)</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>batch_first<span class="token punctuation">:</span>            <span class="token comment" spellcheck="true"># emissions.shape (seq_len,batch_size,tag_num)</span>            emissions <span class="token operator">=</span> emissions<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># [62,12,3]</span>            tags <span class="token operator">=</span> tags<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># [62,12]</span>            mask <span class="token operator">=</span> mask<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># [62,12]</span>        <span class="token triple-quoted-string string">"""        CRF损失函数由两部分组成，真实路径的分数 和 所有路径的总分数。真实路径的分数应该是所有路径中分数最高的。        这些分数来自于CRF层，将这两类分数加和即可得到Si 和 路径分数eSi        """</span>        <span class="token comment" spellcheck="true"># numerator shape: (batch_size,) [12]</span>        numerator <span class="token operator">=</span> self<span class="token punctuation">.</span>_computer_score<span class="token punctuation">(</span>emissions<span class="token operator">=</span>emissions<span class="token punctuation">,</span> tags<span class="token operator">=</span>tags<span class="token punctuation">,</span> mask<span class="token operator">=</span>mask<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 计算s(X,y)</span>        <span class="token comment" spellcheck="true"># shape: (batch_size,) [12]</span>        denominator <span class="token operator">=</span> self<span class="token punctuation">.</span>_compute_normalizer<span class="token punctuation">(</span>emissions<span class="token operator">=</span>emissions<span class="token punctuation">,</span> mask<span class="token operator">=</span>mask<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 计算 log(Σe^(x,y'))</span>        <span class="token comment" spellcheck="true"># loss function</span>        <span class="token comment" spellcheck="true"># log(Σe^(S(X,y))) - S(X，y)</span>        llh <span class="token operator">=</span> denominator <span class="token operator">-</span> numerator  <span class="token comment" spellcheck="true"># [batch_size]12</span>        <span class="token keyword">if</span> reduction <span class="token operator">==</span> <span class="token string">'none'</span><span class="token punctuation">:</span>            <span class="token keyword">return</span> llh        <span class="token keyword">elif</span> reduction <span class="token operator">==</span> <span class="token string">'sum'</span><span class="token punctuation">:</span>            <span class="token keyword">return</span> llh<span class="token punctuation">.</span>sum<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">elif</span> reduction <span class="token operator">==</span> <span class="token string">'mean'</span><span class="token punctuation">:</span>            <span class="token keyword">return</span> llh<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">assert</span> reduction <span class="token operator">==</span> <span class="token string">'token_mean'</span>        <span class="token keyword">return</span> llh<span class="token punctuation">.</span>sum<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">/</span> mask<span class="token punctuation">.</span>float<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>sum<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">decode</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> emissions<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span>               mask<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>ByteTensor<span class="token punctuation">]</span> <span class="token operator">=</span> None<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> List<span class="token punctuation">[</span>List<span class="token punctuation">[</span>int<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>_validate<span class="token punctuation">(</span>emissions<span class="token operator">=</span>emissions<span class="token punctuation">,</span> mask<span class="token operator">=</span>mask<span class="token punctuation">)</span>        <span class="token keyword">if</span> mask <span class="token keyword">is</span> None<span class="token punctuation">:</span>            mask <span class="token operator">=</span> emissions<span class="token punctuation">.</span>new_ones<span class="token punctuation">(</span>emissions<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>uint8<span class="token punctuation">)</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>batch_first<span class="token punctuation">:</span>            emissions <span class="token operator">=</span> emissions<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># [62,12,3]</span>            mask <span class="token operator">=</span> mask<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># [62,12]</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>_viterbi_decode<span class="token punctuation">(</span>emissions<span class="token punctuation">,</span> mask<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 判断有效性</span>    <span class="token keyword">def</span> <span class="token function">_validate</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>                  emissions<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span>                  tags<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">]</span> <span class="token operator">=</span> None<span class="token punctuation">,</span>                  mask<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>ByteTensor<span class="token punctuation">]</span> <span class="token operator">=</span> None<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> None<span class="token punctuation">:</span>        <span class="token keyword">if</span> tags <span class="token keyword">is</span> <span class="token operator">not</span> None<span class="token punctuation">:</span>            no_empty_seq <span class="token operator">=</span> <span class="token operator">not</span> self<span class="token punctuation">.</span>batch_first <span class="token operator">and</span> mask<span class="token punctuation">[</span>                <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>all<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># false tensor.all()功能: 如果张量tensor中所有元素都是True, 才返回True; 否则返回False tensor.any()功能: 如果张量tensor中存在一个元素为True, 那么返回True; 只有所有元素都是False时才返回False</span>            no_empty_seq_bf <span class="token operator">=</span> self<span class="token punctuation">.</span>batch_first <span class="token operator">and</span> mask<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>all<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 计算公式s(X,y)</span>    <span class="token keyword">def</span> <span class="token function">_computer_score</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>                        emissions<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span>                        tags<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">,</span>                        mask<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>ByteTensor<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># batch second</span>        <span class="token keyword">assert</span> emissions<span class="token punctuation">.</span>dim<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">3</span> <span class="token operator">and</span> tags<span class="token punctuation">.</span>dim<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">2</span>        <span class="token keyword">assert</span> emissions<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">==</span> tags<span class="token punctuation">.</span>shape        <span class="token keyword">assert</span> emissions<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token operator">==</span> self<span class="token punctuation">.</span>num_tags        <span class="token keyword">assert</span> mask<span class="token punctuation">.</span>shape <span class="token operator">==</span> tags<span class="token punctuation">.</span>shape        <span class="token keyword">assert</span> mask<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>all<span class="token punctuation">(</span><span class="token punctuation">)</span>        tags<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 62        12</span>        seq_length<span class="token punctuation">,</span> batch_size <span class="token operator">=</span> tags<span class="token punctuation">.</span>shape        mask <span class="token operator">=</span> mask<span class="token punctuation">.</span>float<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># self.start_transitions  start 到其他tag(不包含end)的得分</span>        score <span class="token operator">=</span> self<span class="token punctuation">.</span>start_transitions<span class="token punctuation">[</span>tags<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span>  <span class="token comment" spellcheck="true"># tag[0].shape = [62] 每一句的第一个单词，start到其它tag的得分，随机给一个值</span>        <span class="token comment" spellcheck="true"># code.interact(local = locals())</span>        score <span class="token operator">+=</span> emissions<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>batch_size<span class="token punctuation">)</span><span class="token punctuation">,</span> tags<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span>  <span class="token comment" spellcheck="true"># 计算所有句子中第一个单词的发射的得分</span>        <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> seq_length<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># [1,2,...,seq_length-1]</span>            <span class="token comment" spellcheck="true"># if mask[i].sum() == 0:</span>            <span class="token comment" spellcheck="true">#     break</span>            <span class="token comment" spellcheck="true"># transitions[i][j] 表示从第i个tag 到第j个tag的分数</span>            score <span class="token operator">+=</span> self<span class="token punctuation">.</span>transitions<span class="token punctuation">[</span>tags<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> tags<span class="token punctuation">[</span>i <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">*</span> mask<span class="token punctuation">[</span>i<span class="token punctuation">]</span>  <span class="token comment" spellcheck="true"># Aij</span>            score <span class="token operator">+=</span> emissions<span class="token punctuation">[</span>i<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>batch_size<span class="token punctuation">)</span><span class="token punctuation">,</span> tags<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">*</span> mask<span class="token punctuation">[</span>i<span class="token punctuation">]</span>  <span class="token comment" spellcheck="true"># P&amp;#123;i,y_j&amp;#125;</span>        <span class="token comment" spellcheck="true"># 这里是为了获取每一个样本最后一个词的tag。</span>        <span class="token comment" spellcheck="true"># shape: (batch_size,)   每一个batch 的真实长度</span>        <span class="token comment" spellcheck="true"># .long 变成整型 .sum(dim=0) 计算每个句子中一共有多少个字</span>        seq_ends <span class="token operator">=</span> mask<span class="token punctuation">.</span>long<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>sum<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">1</span>        <span class="token comment" spellcheck="true"># 每个样本最后一个词的tag</span>        last_tags <span class="token operator">=</span> tags<span class="token punctuation">[</span>seq_ends<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>batch_size<span class="token punctuation">)</span><span class="token punctuation">]</span>        <span class="token comment" spellcheck="true"># shape: (batch_size,) 每一个样本到最后一个词的得分加上之前的score</span>        score <span class="token operator">+=</span> self<span class="token punctuation">.</span>end_transitions<span class="token punctuation">[</span>last_tags<span class="token punctuation">]</span>        <span class="token keyword">return</span> score    <span class="token comment" spellcheck="true"># 计算 log(Σe^(S(X,y)))</span>    <span class="token keyword">def</span> <span class="token function">_compute_normalizer</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> emissions<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> mask<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>ByteTensor<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># emissions: (seq_length, batch_size, num_tags) [62,32,3]</span>        <span class="token comment" spellcheck="true"># mask: (seq_length, batch_size)</span>        <span class="token keyword">assert</span> emissions<span class="token punctuation">.</span>dim<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">3</span> <span class="token operator">and</span> mask<span class="token punctuation">.</span>dim<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">2</span>        <span class="token keyword">assert</span> emissions<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">==</span> mask<span class="token punctuation">.</span>shape        <span class="token keyword">assert</span> emissions<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token operator">==</span> self<span class="token punctuation">.</span>num_tags        <span class="token keyword">assert</span> mask<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>all<span class="token punctuation">(</span><span class="token punctuation">)</span>        mask <span class="token operator">=</span> mask<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>        seq_length <span class="token operator">=</span> emissions<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>        score <span class="token operator">=</span> self<span class="token punctuation">.</span>start_transitions <span class="token operator">+</span> emissions<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>  <span class="token comment" spellcheck="true"># [3] + [12,3]  每个句子的第一个单词，对应的3中不同tag的得分</span>        <span class="token comment" spellcheck="true"># code.interact(local = locals())</span>        <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> seq_length<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token comment" spellcheck="true"># shape : (batch_size,num_tag,1) [12,3,1]</span>            <span class="token comment" spellcheck="true"># &lt;start> -> 别的tag的得分</span>            broadcast_score <span class="token operator">=</span> score<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 初始化得分</span>            <span class="token comment" spellcheck="true"># shape: (batch_size,1,num_tags)</span>            <span class="token comment" spellcheck="true"># 每个句子的第i个单词的发射分数 [12,1,3]</span>            broadcast_emissions <span class="token operator">=</span> emissions<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>            <span class="token comment" spellcheck="true"># 广播得分 + transitions[i][j] 表示从第j个tag 到第 i 个 tag的分数+ 每句第i个单词的发射得分  [12,3,3]</span>            next_score <span class="token operator">=</span> broadcast_score <span class="token operator">+</span> self<span class="token punctuation">.</span>transitions <span class="token operator">+</span> broadcast_emissions            next_score <span class="token operator">=</span> torch<span class="token punctuation">.</span>logsumexp<span class="token punctuation">(</span>next_score<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># [12,3]</span>            <span class="token comment" spellcheck="true"># 依次取出每句话的i个单词的mask, 有的话，有得分，没有的话，就原来的值 [12,3]</span>            score <span class="token operator">=</span> torch<span class="token punctuation">.</span>where<span class="token punctuation">(</span>mask<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> next_score<span class="token punctuation">,</span> score<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># shape (batch_size,num_tags)</span>        <span class="token comment" spellcheck="true"># 到其他tag(不包含start)到end的得分</span>        score <span class="token operator">+=</span> self<span class="token punctuation">.</span>end_transitions        <span class="token comment" spellcheck="true"># shape: (batch_size)</span>        <span class="token keyword">return</span> torch<span class="token punctuation">.</span>logsumexp<span class="token punctuation">(</span>score<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">_viterbi_decode</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> emissions<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">,</span>                        mask<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>ByteTensor<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> List<span class="token punctuation">[</span>List<span class="token punctuation">[</span>int<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># emissions: (seq_length, batch_size, num_tags)</span>        <span class="token comment" spellcheck="true"># mask: (seq_length, batch_size)</span>        <span class="token keyword">assert</span> emissions<span class="token punctuation">.</span>dim<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">3</span> <span class="token operator">and</span> mask<span class="token punctuation">.</span>dim<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">2</span>        <span class="token keyword">assert</span> emissions<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">==</span> mask<span class="token punctuation">.</span>shape        <span class="token keyword">assert</span> emissions<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token operator">==</span> self<span class="token punctuation">.</span>num_tags        mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>mask<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>uint8<span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">assert</span> mask<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>all<span class="token punctuation">(</span><span class="token punctuation">)</span>        seq_length<span class="token punctuation">,</span> batch_size <span class="token operator">=</span> mask<span class="token punctuation">.</span>shape        <span class="token comment" spellcheck="true"># self.start_transitions  start 到其他tag(不包含end)的得分</span>        <span class="token comment" spellcheck="true"># &lt;start>->其他tag的发射得分 + 每句话的第一个字的tag的发射得分</span>        <span class="token comment" spellcheck="true"># emissions.shape = [62,12,3]  start_transitions.shape = [3]</span>        score <span class="token operator">=</span> self<span class="token punctuation">.</span>start_transitions <span class="token operator">+</span> emissions<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>  <span class="token comment" spellcheck="true"># 广播</span>        history <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> seq_length<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token comment" spellcheck="true"># score.shape = [32,3] -> [32,3,1]</span>            <span class="token comment" spellcheck="true"># 起始得分，发射得分, 扩展维度，公式中的expand previous</span>            broadcast_score <span class="token operator">=</span> score<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>            <span class="token comment" spellcheck="true"># emissions.shape = [62,32,3] 然后emissions[i]是每句话中的第i个单词的发射得分</span>            <span class="token comment" spellcheck="true"># emissions[i].shape = [32,3].unsqueeze(1) = [32,1,3]</span>            broadcast_emission <span class="token operator">=</span> emissions<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 扩展维度</span>            <span class="token comment" spellcheck="true">#            初始得分         + 发射得分         + 之前得分+现在得分</span>            <span class="token comment" spellcheck="true"># [32,3,3]=  [32,3,1]        + [3,3,3]          + [32,1,3]</span>            <span class="token comment" spellcheck="true">#            初始得分           公式中的t，转移得分  发射得分，单词wi->tagj的发射概率</span>            next_score <span class="token operator">=</span> broadcast_score <span class="token operator">+</span> self<span class="token punctuation">.</span>transitions <span class="token operator">+</span> broadcast_emission            <span class="token comment" spellcheck="true"># 每句话中每个单词对应的tag的得分最大值，tag[i]->tag[j]最大得分</span>            <span class="token comment" spellcheck="true"># next_score.shape = [32,3] =indices.shape</span>            <span class="token comment" spellcheck="true"># 这个时刻中的最大值被保留下来</span>            next_score<span class="token punctuation">,</span> indices <span class="token operator">=</span> next_score<span class="token punctuation">.</span>max<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>            <span class="token comment" spellcheck="true"># 不计算padding部分的得分</span>            score <span class="token operator">=</span> torch<span class="token punctuation">.</span>where<span class="token punctuation">(</span>mask<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> next_score<span class="token punctuation">,</span> score<span class="token punctuation">)</span>            history<span class="token punctuation">.</span>append<span class="token punctuation">(</span>indices<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 遍历完一句话，还得加上最后 &lt;end> tag  [32,3]</span>        score <span class="token operator">+=</span> self<span class="token punctuation">.</span>end_transitions        <span class="token comment" spellcheck="true"># 计算到最后一个单词的下标</span>        seq_ends <span class="token operator">=</span> mask<span class="token punctuation">.</span>long<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>sum<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">1</span>  <span class="token comment" spellcheck="true"># [32]</span>        best_tags_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        jj <span class="token operator">=</span> <span class="token number">0</span>        <span class="token keyword">for</span> idx <span class="token keyword">in</span> range<span class="token punctuation">(</span>batch_size<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># 32</span>            <span class="token comment" spellcheck="true"># score.shape = [32,3] 每句话中找最好的tag</span>            _<span class="token punctuation">,</span> best_last_tag <span class="token operator">=</span> score<span class="token punctuation">[</span>idx<span class="token punctuation">]</span><span class="token punctuation">.</span>max<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 然后找最好的tag</span>            best_tags <span class="token operator">=</span> <span class="token punctuation">[</span>best_last_tag<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span>            <span class="token comment" spellcheck="true"># history[:seq_ends[idx]].shape  (seq_ends[idx])</span>            <span class="token comment" spellcheck="true"># history 的长度是一个句子的长度 61</span>            <span class="token comment" spellcheck="true"># history[i].shape = [32,3]</span>            <span class="token comment" spellcheck="true"># history[:seq_ends[idx] 取句子长度，seq_ends之后是padding部分</span>            <span class="token comment" spellcheck="true"># reversed(history[:seq_ends[idx]]) 将句子反过来</span>            <span class="token comment" spellcheck="true"># hist第一个取到的是最后一个字</span>            <span class="token keyword">for</span> hist <span class="token keyword">in</span> reversed<span class="token punctuation">(</span>history<span class="token punctuation">[</span><span class="token punctuation">:</span>seq_ends<span class="token punctuation">[</span>idx<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># 画图</span>                <span class="token comment" spellcheck="true"># hist.shape = [32,3]</span>                <span class="token comment" spellcheck="true"># code.interact(local = locals())</span>                best_last_tag <span class="token operator">=</span> hist<span class="token punctuation">[</span>idx<span class="token punctuation">]</span><span class="token punctuation">[</span>best_tags<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span>                best_tags<span class="token punctuation">.</span>append<span class="token punctuation">(</span>best_last_tag<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>            best_tags<span class="token punctuation">.</span>reverse<span class="token punctuation">(</span><span class="token punctuation">)</span>            best_tags_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>best_tags<span class="token punctuation">)</span>        <span class="token keyword">return</span> best_tags_list</code></pre><p><strong>训练参数</strong></p><pre class=" language-shell"><code class="language-shell">  --data_dir  ./corpus/ner_data  --vob_file /media/daiyizheng/SSD/bert-pretrainmodel/roberta/chinese_roberta_wwm_ext_pytorch/vocab.txt  --model_config /media/daiyizheng/SSD/bert-pretrainmodel/roberta/chinese_roberta_wwm_ext_pytorch/config.json  --output ./output  --max_seq_length 41  --do_train  --train_batch_size 32  --eval_batch_size 32  --gradient_accumulation_steps 1  --num_train_epochs 15  --pre_train_model  /media/daiyizheng/SSD/bert-pretrainmodel/roberta/chinese_roberta_wwm_ext_pytorch/pytorch_model.bin</code></pre><p><strong>整体训练结果</strong></p><pre class=" language-shell"><code class="language-shell">  O precision = 0.999725 recall = 0.999983  f1 = 0.999854 support = 181571  E-dis precision = 0.995097 recall = 0.995396  f1 = 0.995247 support = 19983  B-sym precision = 0.988231 recall = 0.985985  f1 = 0.987107 support = 7920  B-che precision = 0.996992 recall = 0.995495  f1 = 0.996243 support = 1998  E-dru precision = 0.994522 recall = 0.998500  f1 = 0.996507 support = 2000  I-che precision = 0.998802 recall = 0.997699  f1 = 0.998250 support = 10864  E-che precision = 0.998494 recall = 0.995495  f1 = 0.996992 support = 1998  E-sym precision = 0.985266 recall = 0.987879  f1 = 0.986571 support = 7920  I-dis precision = 0.994998 recall = 0.998148  f1 = 0.996571 support = 89089  S precision = 0.000000 recall = 0.000000  f1 = 0.000000 support = 22  I-sym precision = 0.993844 recall = 0.980469  f1 = 0.987111 support = 26675  B-dis precision = 0.994605 recall = 0.996347  f1 = 0.995475 support = 19983  I-dru precision = 0.996810 recall = 0.998462  f1 = 0.997635 support = 8451  B-dru precision = 0.994519 recall = 0.998000  f1 = 0.996257 support = 2000  attention AVG:precision = 0.994377 recall = 0.994253  f1 = 0.994308   all AVG:precision = 0.923708 recall = 0.923418  f1 = 0.923559 </code></pre><p>对于加权后的f1-score达到了0.99以上，基本识别出来了，对于单体S标注f1为０，可能原因数量太少(22),未能识别出来</p><h3 id="意图分类"><a href="#意图分类" class="headerlink" title="意图分类"></a>意图分类</h3><p><strong>模型</strong></p><p>采用haggingface自带的BertForSequenceClassification，一个多分类模型，数据格式上面已经介绍了，</p><p><strong>参数</strong></p><pre class=" language-shell"><code class="language-shell">  --data_dir  ./corpus/classify_data  --vob_file /media/daiyizheng/SSD/bert-pretrainmodel/roberta/chinese_roberta_wwm_ext_pytorch/vocab.txt  --model_config /media/daiyizheng/SSD/bert-pretrainmodel/roberta/chinese_roberta_wwm_ext_pytorch/config.json  --output ./output  --max_seq_length 60  --do_train  --train_batch_size 32  --eval_batch_size 32  --gradient_accumulation_steps 1  --num_train_epochs 15  --pre_train_model  /media/daiyizheng/SSD/bert-pretrainmodel/roberta/chinese_roberta_wwm_ext_pytorch/pytorch_model.bin</code></pre><p><strong>整体结果</strong></p><p>忘了截图：整体分类准确率大概是0.72相对比较低</p><p>例如：</p><pre class=" language-latex"><code class="language-latex">0    有什么用，苏孜阿甫片drug    41    眶内海绵状血管瘤disease有啥表现    02    怎么才可以预防结肠血管扩张症disease    63    有啥表现，雅司病disease    0</code></pre><p>大致原因：</p><p>　　　　１．训练数据来源于自制的问句生成器，质量不够高</p><p>　　　　２．实体后加了实体的类别，可能在语义上有影响</p><h3 id="模型部署"><a href="#模型部署" class="headerlink" title="模型部署"></a>模型部署</h3><p>　　前期使用了flask框架来加载模型，发现对后端的性能影响很大，后续采用了torchserve来部署，减少模型对后端的依赖，对项目的可复用性有很大的帮助</p><p>torchserve的功能参考<a href="https://daiyizheng.github.io/2020/09/17/torchserve-bu-shu-zhu-yi-shi-xiang/">https://daiyizheng.github.io/2020/09/17/torchserve-bu-shu-zhu-yi-shi-xiang/</a></p><p><strong>命名实体部署</strong></p><p>文件</p><pre class=" language-text"><code class="language-text">├── best_ner.bin　#自己训练最好的模型├── chatbotBERT.mar　#打包的文件├── chatbot_medical_qa_ner_handler.py　#函数入口以及模型加载函数├── config.json　#预训练配置├── config.properties　#torchserve配置文件├── online_test.py　#接口测试├── pytorch_model.bin #预训练模型├── vocab.txt　#预训练词库</code></pre><p>handler代码</p><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">TransformerClassifierHandler</span><span class="token punctuation">(</span>BaseHandler<span class="token punctuation">,</span> ABC<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>TransformerClassifierHandler<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        logger<span class="token punctuation">.</span>debug<span class="token punctuation">(</span><span class="token string">'__init__ the handler'</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>initialized <span class="token operator">=</span> <span class="token boolean">False</span>    <span class="token keyword">def</span> <span class="token function">initialize</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> ctx<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># 初始化handler, 读入模型</span>        logger<span class="token punctuation">.</span>debug<span class="token punctuation">(</span><span class="token string">'Initializating the handler'</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>manifest <span class="token operator">=</span> ctx<span class="token punctuation">.</span>manifest        self<span class="token punctuation">.</span>max_length <span class="token operator">=</span> <span class="token number">41</span>        properties <span class="token operator">=</span> ctx<span class="token punctuation">.</span>system_properties        logger<span class="token punctuation">.</span>debug<span class="token punctuation">(</span>str<span class="token punctuation">(</span>properties<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        model_dir <span class="token operator">=</span> properties<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">"model_dir"</span><span class="token punctuation">)</span>        logger<span class="token punctuation">.</span>debug<span class="token punctuation">(</span><span class="token string">'Loading model from &amp;#123;0&amp;#125;'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>model_dir<span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>tokenizer <span class="token operator">=</span> BertTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>model_dir<span class="token punctuation">,</span> <span class="token string">"vocab.txt"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># self.device = torch.device("cuda:" + str(properties.get("gpu_id")) if torch.cuda.is_available() else "cpu")</span>        self<span class="token punctuation">.</span>device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cpu"</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># w我这边云服务器没有GPU</span>        self<span class="token punctuation">.</span>labels <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'O'</span><span class="token punctuation">,</span> <span class="token string">'E-dis'</span><span class="token punctuation">,</span> <span class="token string">'B-sym'</span><span class="token punctuation">,</span> <span class="token string">'B-che'</span><span class="token punctuation">,</span> <span class="token string">'E-dru'</span><span class="token punctuation">,</span> <span class="token string">'I-che'</span><span class="token punctuation">,</span> <span class="token string">'E-che'</span><span class="token punctuation">,</span> <span class="token string">'E-sym'</span><span class="token punctuation">,</span> <span class="token string">'I-dis'</span><span class="token punctuation">,</span> <span class="token string">'S'</span><span class="token punctuation">,</span> <span class="token string">'I-sym'</span><span class="token punctuation">,</span> <span class="token string">'B-dis'</span><span class="token punctuation">,</span> <span class="token string">'I-dru'</span><span class="token punctuation">,</span> <span class="token string">'B-dru'</span><span class="token punctuation">]</span>        model <span class="token operator">=</span> BertCrf<span class="token punctuation">(</span>config_name<span class="token operator">=</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>model_dir<span class="token punctuation">,</span><span class="token string">'config.json'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> num_tags<span class="token operator">=</span>len<span class="token punctuation">(</span>self<span class="token punctuation">.</span>labels<span class="token punctuation">)</span><span class="token punctuation">,</span> batch_first<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>        model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>model_dir<span class="token punctuation">,</span><span class="token string">'best_ner.bin'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>model <span class="token operator">=</span> model<span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>        logger<span class="token punctuation">.</span>debug<span class="token punctuation">(</span><span class="token string">'Transformer model from path &amp;#123;0&amp;#125; loaded successfully'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>model_dir<span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>initialized <span class="token operator">=</span> <span class="token boolean">True</span>    <span class="token keyword">def</span> <span class="token function">preprocess</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> text<span class="token punctuation">)</span><span class="token punctuation">:</span>        logger<span class="token punctuation">.</span>debug<span class="token punctuation">(</span><span class="token string">'Preprocessing in the handler'</span><span class="token punctuation">)</span>        lines <span class="token operator">=</span> <span class="token punctuation">[</span>line<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> line <span class="token keyword">in</span> text<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">"\n"</span><span class="token punctuation">)</span><span class="token punctuation">]</span>        dataset <span class="token operator">=</span> self<span class="token punctuation">.</span>input_dataset<span class="token punctuation">(</span>lines<span class="token punctuation">)</span>        <span class="token keyword">return</span> <span class="token punctuation">(</span>lines<span class="token punctuation">,</span> dataset<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">inference</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> inputs<span class="token punctuation">)</span><span class="token punctuation">:</span>        lines<span class="token punctuation">,</span> dataset <span class="token operator">=</span> inputs        sampler <span class="token operator">=</span> SequentialSampler<span class="token punctuation">(</span>dataset<span class="token punctuation">)</span>        dataloader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>dataset<span class="token punctuation">,</span> sampler<span class="token operator">=</span>sampler<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">)</span>        pred_list <span class="token operator">=</span> self<span class="token punctuation">.</span>ner_predict<span class="token punctuation">(</span>dataloader<span class="token punctuation">)</span>        <span class="token keyword">return</span> pred_list    <span class="token keyword">def</span> <span class="token function">ner_predict</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dataloader<span class="token punctuation">)</span><span class="token punctuation">:</span>        logger<span class="token punctuation">.</span>debug<span class="token punctuation">(</span><span class="token string">'Preprocessing in the model prediction'</span><span class="token punctuation">)</span>        pred_token_label <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        <span class="token keyword">for</span> batch <span class="token keyword">in</span> tqdm<span class="token punctuation">(</span>dataloader<span class="token punctuation">,</span> desc<span class="token operator">=</span><span class="token string">"predict"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>model<span class="token punctuation">.</span>eval<span class="token punctuation">(</span><span class="token punctuation">)</span>            batch <span class="token operator">=</span> tuple<span class="token punctuation">(</span>t<span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span> <span class="token keyword">for</span> t <span class="token keyword">in</span> batch<span class="token punctuation">)</span>            <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                inputs <span class="token operator">=</span> <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;'input_ids': batch[0],</span>                          <span class="token string">'attention_mask'</span><span class="token punctuation">:</span> batch<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                          <span class="token string">'token_type_ids'</span><span class="token punctuation">:</span> batch<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                          <span class="token string">'decode'</span><span class="token punctuation">:</span> <span class="token boolean">True</span><span class="token punctuation">,</span>                          <span class="token string">'reduction'</span><span class="token punctuation">:</span> <span class="token string">'none'</span>                          <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#125;</span>                outputs <span class="token operator">=</span> self<span class="token punctuation">.</span>model<span class="token punctuation">(</span><span class="token operator">**</span>inputs<span class="token punctuation">)</span>                pred_token_label <span class="token operator">=</span> pred_token_label <span class="token operator">+</span> outputs<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>        <span class="token keyword">return</span> pred_token_label    <span class="token keyword">def</span> <span class="token function">input_dataset</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> lines<span class="token punctuation">)</span><span class="token punctuation">:</span>        logger<span class="token punctuation">.</span>debug<span class="token punctuation">(</span><span class="token string">'input_dataset in the handler'</span><span class="token punctuation">)</span>        all_input_ids <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        all_attention_mask <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        all_token_type_ids <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        <span class="token keyword">for</span> text <span class="token keyword">in</span> lines<span class="token punctuation">:</span>            inputs <span class="token operator">=</span> self<span class="token punctuation">.</span>tokenizer<span class="token punctuation">.</span>encode_plus<span class="token punctuation">(</span>text<span class="token operator">=</span><span class="token string">" "</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>list<span class="token punctuation">(</span>text<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> add_special_tokens<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> max_length<span class="token operator">=</span>self<span class="token punctuation">.</span>max_length<span class="token punctuation">,</span> truncation_strategy<span class="token operator">=</span><span class="token string">'longest_first'</span><span class="token punctuation">,</span> truncation<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>            input_ids<span class="token punctuation">,</span> token_type_ids <span class="token operator">=</span> inputs<span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> inputs<span class="token punctuation">[</span><span class="token string">"token_type_ids"</span><span class="token punctuation">]</span>            attention_mask <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">*</span> len<span class="token punctuation">(</span>input_ids<span class="token punctuation">)</span>            padding_length <span class="token operator">=</span> self<span class="token punctuation">.</span>max_length <span class="token operator">-</span> len<span class="token punctuation">(</span>input_ids<span class="token punctuation">)</span>            input_ids <span class="token operator">=</span> input_ids <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">*</span> padding_length<span class="token punctuation">)</span>            attention_mask <span class="token operator">=</span> attention_mask <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">*</span> padding_length<span class="token punctuation">)</span>            token_type_ids <span class="token operator">=</span> token_type_ids <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">*</span> padding_length<span class="token punctuation">)</span>            <span class="token keyword">assert</span> len<span class="token punctuation">(</span>input_ids<span class="token punctuation">)</span> <span class="token operator">==</span> self<span class="token punctuation">.</span>max_length<span class="token punctuation">,</span> <span class="token string">"Error with input length &amp;#123;&amp;#125; vs &amp;#123;&amp;#125;"</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>len<span class="token punctuation">(</span>input_ids<span class="token punctuation">)</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>max_length<span class="token punctuation">)</span>            <span class="token keyword">assert</span> len<span class="token punctuation">(</span>attention_mask<span class="token punctuation">)</span> <span class="token operator">==</span> self<span class="token punctuation">.</span>max_length<span class="token punctuation">,</span> <span class="token string">"Error with input length &amp;#123;&amp;#125; vs &amp;#123;&amp;#125;"</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>len<span class="token punctuation">(</span>attention_mask<span class="token punctuation">)</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>max_length<span class="token punctuation">)</span>            <span class="token keyword">assert</span> len<span class="token punctuation">(</span>token_type_ids<span class="token punctuation">)</span> <span class="token operator">==</span> self<span class="token punctuation">.</span>max_length<span class="token punctuation">,</span> <span class="token string">"Error with input length &amp;#123;&amp;#125; vs &amp;#123;&amp;#125;"</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>len<span class="token punctuation">(</span>token_type_ids<span class="token punctuation">)</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>max_length<span class="token punctuation">)</span>            all_input_ids<span class="token punctuation">.</span>append<span class="token punctuation">(</span>input_ids<span class="token punctuation">)</span>            all_attention_mask<span class="token punctuation">.</span>append<span class="token punctuation">(</span>attention_mask<span class="token punctuation">)</span>            all_token_type_ids<span class="token punctuation">.</span>append<span class="token punctuation">(</span>token_type_ids<span class="token punctuation">)</span>        all_input_ids <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>all_input_ids<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>long<span class="token punctuation">)</span>        all_attention_mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>all_attention_mask<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>long<span class="token punctuation">)</span>        all_token_type_ids <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>all_token_type_ids<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>long<span class="token punctuation">)</span>        dataset <span class="token operator">=</span> TensorDataset<span class="token punctuation">(</span>all_input_ids<span class="token punctuation">,</span> all_attention_mask<span class="token punctuation">,</span> all_token_type_ids<span class="token punctuation">)</span>        <span class="token keyword">return</span> dataset    <span class="token keyword">def</span> <span class="token function">postprocess</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> inference_output<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># TODO: Add any needed post-processing of the model predictions here</span>        logger<span class="token punctuation">.</span>debug<span class="token punctuation">(</span><span class="token string">'Preprocessing in the postprocess'</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> <span class="token punctuation">[</span><span class="token punctuation">[</span>self<span class="token punctuation">.</span>labels<span class="token punctuation">[</span>idx<span class="token punctuation">]</span> <span class="token keyword">for</span> idx <span class="token keyword">in</span> idx_list<span class="token punctuation">]</span> <span class="token keyword">for</span> idx_list <span class="token keyword">in</span> inference_output<span class="token punctuation">]</span></code></pre><p>脚本</p><pre class=" language-shell"><code class="language-shell">#打包torch-model-archiver \    --model-name chatbotBERT \    --version 1.0 --serialized-file ./pytorch_model.bin \    --extra-files "./config.json,./best_ner.bin,./config.json,./vocab.txt" \    --handler "chatbot_medical_qa_ner_handler.py"    #启动torchserve --start --ncs --model-store model-store --models chatbotBERT.mar --ts-config ./model-store/config.properties</code></pre><p>分类部署</p><pre class=" language-text"><code class="language-text">├── best_classify.bin# 训练最好的模型├── best_classify.pt　#torchscript模型├── chatbot_medical_qa_classify_handler.py　#torchserve入口函数├── Download_Transformer_models.py　#model文件转torchscript模型├── online_test.py　## 接口测试├── setup_config.json　## bert自定义配置文件　这边未用到├── vocab.txt ## bert词典</code></pre><pre class=" language-shell"><code class="language-shell">#打包torch-model-archiver \--model-name chatbotMedicalQAClassify \--version 1.0 \--serialized-file ./best_classify.pt \--handler ./chatbot_medical_qa_classify_handler.py \--extra-files "./setup_config.json,./vocab.txt"#b部署torchserve --start --ncs \--model-store model-store \--models chatbotMedicalQAClassify.mar \--ts-config ./model-store/config.properties</code></pre><blockquote><p>需要注意的是NER 部署是加载原模型，而classifty部署用的是torchscript模型，推荐使用torchscript．</p></blockquote><h4 id="回答生成"><a href="#回答生成" class="headerlink" title="回答生成"></a>回答生成</h4><p>知识图谱三元组&lt;实体，关系，实体&gt;或者是&lt;实体，属性，属性值&gt;</p><p>前面抽取的“医疗命名实体”就三元组的第一个元素——实体</p><p>前面进行的“用户意图识别”则是三元组中的第二个元素——关系/属性</p><p>得到三元组的这两个元素就可以用cypher语言在neo4j图数据库中进行查找对应的实体或属性值，然后构建回答返回给用户</p><pre class=" language-shell"><code class="language-shell">def sql_transfer(self, question_type, entities):        if not entities:            return []        # 查询语句        sql = []        # 查询疾病的原因        if question_type == 'disease_cause':            sql = ["MATCH (m:Disease) where m.name = '&#123;0&#125;' return m.name, m.cause".format(i) for i in entities]        # 查询疾病的防御措施        elif question_type == 'disease_prevent':            sql = ["MATCH (m:Disease) where m.name = '&#123;0&#125;' return m.name, m.prevent".format(i) for i in entities]        # 查询疾病的持续时间        elif question_type == 'disease_lasttime':            sql = ["MATCH (m:Disease) where m.name = '&#123;0&#125;' return m.name, m.cure_lasttime".format(i) for i in entities]        # 查询疾病的治愈概率        elif question_type == 'disease_cureprob':            sql = ["MATCH (m:Disease) where m.name = '&#123;0&#125;' return m.name, m.cured_prob".format(i) for i in entities]      '''      ...      '''        return sql</code></pre><h2 id="效果展示"><a href="#效果展示" class="headerlink" title="效果展示"></a>效果展示</h2><p><strong>PC</strong></p><p><img src="/2020/08/10/yi-liao-zhi-shi-wen-da/14.png"></p><p><strong>APP</strong></p><img src="/2020/08/10/yi-liao-zhi-shi-wen-da/15.jpg" style="zoom:67%;"><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>目前来说整体的流程还是比较完整，但对于在工业界的运用来说还只是个玩具．</li><li>模型有提高的部分，特别是数据的标注处理以及句子质量（比较费时间）</li></ul><p>参考：</p><p><a href="https://blog.csdn.net/weixin_46133588/article/details/104700425">https://blog.csdn.net/weixin_46133588/article/details/104700425</a></p><p><a href="https://github.com/liuhuanyong/QASystemOnMedicalKG">https://github.com/liuhuanyong/QASystemOnMedicalKG</a></p><p>　　</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> nlp </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 问答 </tag>
            
            <tag> 图数据库 </tag>
            
            <tag> 医疗 </tag>
            
            <tag> 规则 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>git</title>
      <link href="/2020/08/08/git/"/>
      <url>/2020/08/08/git/</url>
      
        <content type="html"><![CDATA[<h1 id="git"><a href="#git" class="headerlink" title="git"></a>git</h1><h2 id="更换git远程仓库地址"><a href="#更换git远程仓库地址" class="headerlink" title="更换git远程仓库地址"></a>更换git远程仓库地址</h2><p>方法一 ： 通过命令直接修改远程仓库地址</p><pre class=" language-shell"><code class="language-shell">git remote 查看所有远程仓库git remote xxx 查看指定远程仓库地址git remote set-url origin 你新的远程仓库地址123</code></pre><p>方法二： 先删除在添加你的远程仓库</p><pre class=" language-shell"><code class="language-shell">git remote rm origingit remote add origin 你的新远程仓库地址12</code></pre><p>方法三： 直接修改你本地的.git文件</p><pre class=" language-shell"><code class="language-shell">这里需要注意的问题是需要进入你的项目目录中例如：你的项目名为test，那么你就进入test文件夹。**.git文件是隐藏文件你需要显示隐藏文件才能看见**</code></pre><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> git </tag>
            
            <tag> github </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>知识图谱问答KBQA数据集介绍</title>
      <link href="/2020/08/08/zhi-shi-tu-pu-wen-da-kbqa-shu-ju-ji-jie-shao/"/>
      <url>/2020/08/08/zhi-shi-tu-pu-wen-da-kbqa-shu-ju-ji-jie-shao/</url>
      
        <content type="html"><![CDATA[<p>转载 <a href="https://blog.csdn.net/qq_37974244/article/details/107020421">https://blog.csdn.net/qq_37974244/article/details/107020421</a></p><h1 id="一、WebQuestions"><a href="#一、WebQuestions" class="headerlink" title="一、WebQuestions"></a>一、WebQuestions</h1><p>提出该数据集的论文：<a href="https://www.aclweb.org/anthology/D13-1160/">Semantic Parsing on Freebase from Question-Answer Pairs</a></p><p>数据集地址：<a href="https://worksheets.codalab.org/worksheets/0xba659fe363cb46e7a505c5b6a774dc8a">https://worksheets.codalab.org/worksheets/0xba659fe363cb46e7a505c5b6a774dc8a</a></p><p>WebQuestions数据集（2013年提出）是由斯坦福大学研究人员通过Google Suggest API构建得到的，数据集本身共包含5810条（问题，答案）对，其中简单问题占比在84%，复杂的多跳和推理问题相对较少。根据提出者的最初数据划分方式，WebQuestions被分为训练集和测试集两个集合，其中训练集包含3778条数据，测试集包含2032条数据。</p><h1 id="二、SimpleQuestions"><a href="#二、SimpleQuestions" class="headerlink" title="二、SimpleQuestions"></a>二、SimpleQuestions</h1><p>提出该数据集的论文：<a href="https://arxiv.org/pdf/1506.02075.pdf">Large-scale Simple Question Answering with Memory Networks</a></p><p>数据集地址：<a href="https://research.fb.com/downloads/babi/">https://research.fb.com/downloads/babi/</a></p><p>SimpleQuestions数据集（2015年提出）是一个针对简单问题而构建的数据集，它采用人工标注的方法根据知识库中的事实生成对应的问句，并且以<strong>Freebase</strong>作为答案来源。该数据集共包含108,442条数据（包含关系标注），其中训练集为75910条（70%），验证集为10845条（10%），测试集为21687条（20%）。</p><h1 id="三、ComplexQuestions"><a href="#三、ComplexQuestions" class="headerlink" title="三、ComplexQuestions"></a>三、ComplexQuestions</h1><p>提出该数据集的论文：<a href="https://www.aclweb.org/anthology/C16-1236/">Constraint-Based Question Answering with Knowledge Graph</a></p><p>作者本人的数据集地址：<a href="https://github.com/JunweiBao/MulCQA/tree/ComplexQuestions">https://github.com/JunweiBao/MulCQA/tree/ComplexQuestions</a></p><p>ComplexQuestions数据集（2016年提出）是一个专门针对复杂问题而构建的数据集，在构建该数据集过程中，作者从一个实际使用的搜索引擎（具体哪个暂未知）中筛选并得到了878条可用的问答对。除了这878条数据，作者还从WebQuestions等数据集上额外选出了1222条数据，由此共得到了2100条复杂问题对。总体来说，该数据集共包含2100条问答对，其中训练集个数为1300条，测试集个数为800条。</p><h1 id="四、GraphQuestions"><a href="#四、GraphQuestions" class="headerlink" title="四、GraphQuestions"></a>四、GraphQuestions</h1><p>提出该数据集的论文：<a href="https://www.aclweb.org/anthology/D16-1054/">On Generating Characteristic-rich Question Sets for QA Evaluation</a></p><p>数据集地址：<a href="https://github.com/ysu1989/GraphQuestions">https://github.com/ysu1989/GraphQuestions</a></p><p>这是一个比较难的数据集（2016年提出），涉及较多比较复杂的逻辑，以<strong>Freebase</strong>作为知识库。该数据集在构建时先设计问题涉及的模式（即知识库中的一个子图），然后让人根据图改写成自然语言题目。举个例子：“the nine eleven were carried out with the involvement of what terrorist organizations?”</p><h1 id="五、30M-Factoid-Questions"><a href="#五、30M-Factoid-Questions" class="headerlink" title="五、30M Factoid Questions"></a>五、30M Factoid Questions</h1><p>提出该数据集的论文：<a href="https://arxiv.org/pdf/1603.06807.pdf">Generating Factoid QuestionsWith Recurrent Neural Networks: The 30M Factoid Question-Answer Corpus</a></p><p>数据集地址：<a href="https://academictorrents.com/details/973fb709bdb9db6066213bbc5529482a190098ce">https://academictorrents.com/details/973fb709bdb9db6066213bbc5529482a190098ce</a></p><p>该数据集是由模型自动构建的，包含30M的问答对。按照论文中的说法，问句质量和人类构建的质量相当，很有使用价值。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 数据集 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 问答数据 </tag>
            
            <tag> 数据相关论文 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Bert BiLSTM CRF pytorch</title>
      <link href="/2020/08/07/bert-bilstm-crf-pytorch/"/>
      <url>/2020/08/07/bert-bilstm-crf-pytorch/</url>
      
        <content type="html"><![CDATA[<h1 id="医疗命名实体识别"><a href="#医疗命名实体识别" class="headerlink" title="医疗命名实体识别"></a>医疗命名实体识别</h1><h2 id="任务描述"><a href="#任务描述" class="headerlink" title="任务描述"></a>任务描述</h2><p>​       实体识别作为信息抽取的一个重要子任务，近些年已经取得了阶段性成果。对于医学领域的自然语言文献，例如医学教材、医学百科、临床病例、医学期刊、入院记录、检验报告等，这些文本中蕴含大量医学专业知识和医学术语。将实体识别技术与医学专业领域结合，利用机器读取医学文本，可以显著提高临床科研的效率和质量，并且可服务于下游子任务。要想让机器“读懂”医学数据，核心在于让计算机在大量医学文本中准确的提取出关键信息，这就涉及到了命名实体识别、关系抽取等自然语言处理技术。医学领域中非结构化的文本，都是由中文自然语言句子或句子集合组成。实体抽取是从非结构化医学文本中找出医学实体，如疾病、症状的过程。</p><h2 id="数据简介"><a href="#数据简介" class="headerlink" title="数据简介"></a>数据简介</h2><blockquote><p>原始数据</p></blockquote><img src="/2020/08/07/bert-bilstm-crf-pytorch/3.png" style="zoom:80%;"><p>标记</p><blockquote><p>BIOES数据格式</p><p>标签列表如下：</p><ul><li>B，即Begin，表示开始</li><li>I，即Intermediate，表示中间</li><li>E，即End，表示结尾</li><li>S，即Single，表示单个字符</li><li>O，即Other，表示其他，用于标记无关字符</li></ul></blockquote><table><thead><tr><th>分类实体</th><th>说明</th></tr></thead><tbody><tr><td>PAT</td><td><strong>临床表现</strong></td></tr><tr><td>OPS</td><td><strong>手术</strong></td></tr><tr><td>DSE</td><td><strong>疾病和诊断</strong></td></tr><tr><td>DRG</td><td><strong>药物</strong></td></tr><tr><td>INF</td><td><strong>部位</strong></td></tr><tr><td>LAB</td><td><strong>生化指标</strong></td></tr><tr><td>O</td><td><strong>其他</strong></td></tr></tbody></table><p>{‘I-PAT’, ‘B-OPS’, ‘I-DSE’, ‘I-OPS’, ‘B-DSE’, ‘E-DSE’, ‘B-DRG’, ‘E-OPS’, ‘I-INF’, ‘S-PAT’, ‘I-DRG’, ‘I-LAB’, ‘E-LAB’, ‘O’, ‘B-PAT’, ‘B-LAB’, ‘E-DRG’, ‘S-LAB’, ‘E-INF’, ‘S-DSE’, ‘B-INF’, ‘E-PAT’}</p><h5 id="数据集描述"><a href="#数据集描述" class="headerlink" title="数据集描述"></a>数据集描述</h5><p>本次评测的训练数据有：<br><strong>1)</strong> 1000条标注数据<br><strong>2)</strong> 1000条非标注数据。<br><strong>3)</strong> 7个类别</p><h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ol><li>通过train数据中的标记实体信息, 抽取出test数据集中的数据, 记录ac, loss, 保存最佳的模型</li></ol><h2 id="项目结构"><a href="#项目结构" class="headerlink" title="项目结构"></a>项目结构</h2><p>├── checkpoints　＃＃　模型保存文件<br>├── config.py　＃＃　配置文件<br>├── crf_predict.py　＃＃预测<br>├── crf.py ## ＣＲＦ模型<br>├── data ## 源数据<br>├── LICENSE<br>├── main.py  # 路口文件<br>├── model.py ## 模型<br>├── processed  ## 预处理数据<br>├── README.md<br>└── utils.py　＃＃　工具</p><h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><ul><li><p>价格原来BIOES的数据转成BIO结构</p><img src="/2020/08/07/bert-bilstm-crf-pytorch/1.png" style="zoom:80%;"></li></ul><h2 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h2><pre class=" language-python"><code class="language-python">   parser <span class="token operator">=</span> argparse<span class="token punctuation">.</span>ArgumentParser<span class="token punctuation">(</span><span class="token punctuation">)</span>    parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">"--batch_size"</span><span class="token punctuation">,</span> type<span class="token operator">=</span>int<span class="token punctuation">,</span> default<span class="token operator">=</span><span class="token number">128</span><span class="token punctuation">)</span>     parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">"--lr"</span><span class="token punctuation">,</span> type<span class="token operator">=</span>float<span class="token punctuation">,</span> default<span class="token operator">=</span><span class="token number">0.0001</span><span class="token punctuation">)</span>    parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">"--n_epochs"</span><span class="token punctuation">,</span> type<span class="token operator">=</span>int<span class="token punctuation">,</span> default<span class="token operator">=</span><span class="token number">30</span><span class="token punctuation">)</span>    parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">"--finetuning"</span><span class="token punctuation">,</span> dest<span class="token operator">=</span><span class="token string">"finetuning"</span><span class="token punctuation">,</span> action<span class="token operator">=</span><span class="token string">"store_true"</span><span class="token punctuation">)</span>    parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">"--top_rnns"</span><span class="token punctuation">,</span> dest<span class="token operator">=</span><span class="token string">"top_rnns"</span><span class="token punctuation">,</span> action<span class="token operator">=</span><span class="token string">"store_true"</span><span class="token punctuation">)</span>    parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">"--logdir"</span><span class="token punctuation">,</span> type<span class="token operator">=</span>str<span class="token punctuation">,</span> default<span class="token operator">=</span><span class="token string">"checkpoints/01"</span><span class="token punctuation">)</span>    parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">"--trainset"</span><span class="token punctuation">,</span> type<span class="token operator">=</span>str<span class="token punctuation">,</span> default<span class="token operator">=</span><span class="token string">"processed/processed_training_bio.txt"</span><span class="token punctuation">)</span>    parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">"--validset"</span><span class="token punctuation">,</span> type<span class="token operator">=</span>str<span class="token punctuation">,</span> default<span class="token operator">=</span><span class="token string">"processed/processed_dev_bio.txt"</span><span class="token punctuation">)</span>    hp <span class="token operator">=</span> parser<span class="token punctuation">.</span>parse_args<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><blockquote><p>BILSTM(bidirectional=True, num_layers=2, input_size=768, hidden_size=768//2, batch_first=True) ## 双向，两层　</p><p>BERT () # 中文bert预处理模型12层768hidden_size</p><p>CRF</p></blockquote><ul><li>关于CRF 原理，请参照<a href="/2020/07/25/crf/" title="CRF">CRF</a></li></ul><h2 id="DATASET"><a href="#DATASET" class="headerlink" title="DATASET"></a>DATASET</h2><ul><li><p>最大长度　256</p></li><li><p>CLS 开头　‘[SEP]’　结尾</p><p>sentence</p></li></ul><p>‘[CLS]’, ‘，’, ‘患’, ‘者’, ‘3’, ‘月’, ‘前’, ‘因’, ‘“’, ‘直’, ‘肠’, ‘癌’, ‘”’, ‘于’, ‘在’, ‘我’, ‘院’, ‘于’, ‘全’, ‘麻’, ‘上’, ‘行’, ‘直’, ‘肠’, ‘癌’, ‘根’, ‘治’, ‘术’, ‘（’, ‘D’, ‘I’, ‘X’, ‘O’, ‘N’, ‘术’, ‘）’, ‘，’, ‘手’, ‘术’, ‘过’, ‘程’, ‘顺’, ‘利’, ‘，’, ‘术’, ‘后’, ‘给’, ‘予’, ‘抗’, ‘感’, ‘染’, ‘及’, ‘营’, ‘养’, ‘支’, ‘持’, ‘治’, ‘疗’, ‘，’, ‘患’, ‘者’, ‘恢’, ‘复’, ‘好’, ‘，’, ‘切’, ‘口’, ‘愈’, ‘合’, ‘良’, ‘好’, ‘[SEP]’</p><p>tags</p><p>‘[CLS]’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘B-DSE’, ‘I-DSE’, ‘I-DSE’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘B-OPS’, ‘I-OPS’, ‘I-OPS’, ‘I-OPS’, ‘I-OPS’, ‘I-OPS’, ‘I-OPS’, ‘I-OPS’, ‘I-OPS’, ‘I-OPS’, ‘I-OPS’, ‘I-OPS’, ‘I-OPS’, ‘I-OPS’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘O’, ‘[SEP]’</p><h2 id="定义主模型"><a href="#定义主模型" class="headerlink" title="定义主模型"></a>定义主模型</h2><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Bert_BiLSTM_CRF</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> tag_to_ix<span class="token punctuation">,</span> hidden_dim<span class="token operator">=</span><span class="token number">768</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>Bert_BiLSTM_CRF<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>tag_to_ix <span class="token operator">=</span> tag_to_ix        self<span class="token punctuation">.</span>tagset_size <span class="token operator">=</span> len<span class="token punctuation">(</span>tag_to_ix<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># self.hidden = self.init_hidden()</span>        self<span class="token punctuation">.</span>lstm <span class="token operator">=</span> nn<span class="token punctuation">.</span>LSTM<span class="token punctuation">(</span>bidirectional<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> num_layers<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> input_size<span class="token operator">=</span><span class="token number">768</span><span class="token punctuation">,</span> hidden_size<span class="token operator">=</span>hidden_dim<span class="token operator">//</span><span class="token number">2</span><span class="token punctuation">,</span> batch_first<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>transitions <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>self<span class="token punctuation">.</span>tagset_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>tagset_size<span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>hidden_dim <span class="token operator">=</span> hidden_dim        self<span class="token punctuation">.</span>start_label_id <span class="token operator">=</span> self<span class="token punctuation">.</span>tag_to_ix<span class="token punctuation">[</span><span class="token string">'[CLS]'</span><span class="token punctuation">]</span>        self<span class="token punctuation">.</span>end_label_id <span class="token operator">=</span> self<span class="token punctuation">.</span>tag_to_ix<span class="token punctuation">[</span><span class="token string">'[SEP]'</span><span class="token punctuation">]</span>        self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">,</span> self<span class="token punctuation">.</span>tagset_size<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>bert <span class="token operator">=</span> BertModel<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">'/home/daiyizheng/.cache/torch/transformers/bert-pretrainmodel/bert/bert-base-chinese'</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># self.bert.eval()  # 知用来取bert embedding</span>        self<span class="token punctuation">.</span>transitions<span class="token punctuation">.</span>data<span class="token punctuation">[</span>self<span class="token punctuation">.</span>start_label_id<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">10000</span>        self<span class="token punctuation">.</span>transitions<span class="token punctuation">.</span>data<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>end_label_id<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">10000</span>        self<span class="token punctuation">.</span>device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'cuda'</span><span class="token punctuation">)</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'cpu'</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># self.transitions.to(self.device)</span>    <span class="token keyword">def</span> <span class="token function">init_hidden</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""        初始化LSTM隐层        :return:        """</span>        <span class="token keyword">return</span> <span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_dim <span class="token operator">//</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_dim <span class="token operator">//</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">_forward_alg</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> feats<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">'''        this also called alpha-recursion or forward recursion, to calculate log_prob of all barX         '''</span>        <span class="token comment" spellcheck="true"># T = self.max_seq_length</span>        T <span class="token operator">=</span> feats<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>          batch_size <span class="token operator">=</span> feats<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        <span class="token comment" spellcheck="true"># alpha_recursion,forward, alpha(zt)=p(zt,bar_x_1:t)</span>        log_alpha <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>tagset_size<span class="token punctuation">)</span><span class="token punctuation">.</span>fill_<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">10000</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true">#[batch_size, 1, 16]</span>        <span class="token comment" spellcheck="true"># normal_alpha_0 : alpha[0]=Ot[0]*self.PIs</span>        <span class="token comment" spellcheck="true"># self.start_label has all of the score. it is log,0 is p=1</span>        log_alpha<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>start_label_id<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>        <span class="token comment" spellcheck="true"># feats: sentances -> word embedding -> lstm -> MLP -> feats</span>        <span class="token comment" spellcheck="true"># feats is the probability of emission, feat.shape=(1,tag_size)</span>        <span class="token keyword">for</span> t <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> T<span class="token punctuation">)</span><span class="token punctuation">:</span>            log_alpha <span class="token operator">=</span> <span class="token punctuation">(</span>log_sum_exp_batch<span class="token punctuation">(</span>self<span class="token punctuation">.</span>transitions <span class="token operator">+</span> log_alpha<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">+</span> feats<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> t<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># log_prob of all barX</span>        log_prob_all_barX <span class="token operator">=</span> log_sum_exp_batch<span class="token punctuation">(</span>log_alpha<span class="token punctuation">)</span>        <span class="token keyword">return</span> log_prob_all_barX    <span class="token keyword">def</span> <span class="token function">_score_sentence</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> feats<span class="token punctuation">,</span> label_ids<span class="token punctuation">)</span><span class="token punctuation">:</span>        T <span class="token operator">=</span> feats<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>        batch_size <span class="token operator">=</span> feats<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        batch_transitions <span class="token operator">=</span> self<span class="token punctuation">.</span>transitions<span class="token punctuation">.</span>expand<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span>self<span class="token punctuation">.</span>tagset_size<span class="token punctuation">,</span>self<span class="token punctuation">.</span>tagset_size<span class="token punctuation">)</span>        batch_transitions <span class="token operator">=</span> batch_transitions<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>        score <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>feats<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># the 0th node is start_label->start_word,the probability of them=1. so t begin with 1.</span>        <span class="token keyword">for</span> t <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> T<span class="token punctuation">)</span><span class="token punctuation">:</span>            score <span class="token operator">=</span> score <span class="token operator">+</span> batch_transitions<span class="token punctuation">.</span>gather<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>label_ids<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> t<span class="token punctuation">]</span><span class="token operator">*</span>self<span class="token punctuation">.</span>tagset_size<span class="token operator">+</span>label_ids<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> t<span class="token number">-1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">+</span> feats<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> t<span class="token punctuation">]</span><span class="token punctuation">.</span>gather<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> label_ids<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> t<span class="token punctuation">]</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> score    <span class="token keyword">def</span> <span class="token function">_bert_enc</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""        x: [batchsize, sent_len]        enc: [batch_size, sent_len, 768]        """</span>        <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            encoded_layer<span class="token punctuation">,</span> _  <span class="token operator">=</span> self<span class="token punctuation">.</span>bert<span class="token punctuation">(</span>x<span class="token punctuation">)</span>            enc <span class="token operator">=</span> encoded_layer<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>        <span class="token keyword">return</span> enc    <span class="token keyword">def</span> <span class="token function">_viterbi_decode</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> feats<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">'''        Max-Product Algorithm or viterbi algorithm, argmax(p(z_0:t|x_0:t))        '''</span>        <span class="token comment" spellcheck="true"># T = self.max_seq_length</span>        T <span class="token operator">=</span> feats<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>        batch_size <span class="token operator">=</span> feats<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        <span class="token comment" spellcheck="true"># batch_transitions=self.transitions.expand(batch_size,self.tagset_size,self.tagset_size)</span>        log_delta <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>tagset_size<span class="token punctuation">)</span><span class="token punctuation">.</span>fill_<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">10000</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>        log_delta<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>start_label_id<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token comment" spellcheck="true"># psi is for the vaule of the last latent that make P(this_latent) maximum.</span>        psi <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> T<span class="token punctuation">,</span> self<span class="token punctuation">.</span>tagset_size<span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>long<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># psi[0]=0000 useless</span>        <span class="token keyword">for</span> t <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> T<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token comment" spellcheck="true"># delta[t][k]=max_z1:t-1( p(x1,x2,...,xt,z1,z2,...,zt-1,zt=k|theta) )</span>            <span class="token comment" spellcheck="true"># delta[t] is the max prob of the path from  z_t-1 to z_t[k]</span>            log_delta<span class="token punctuation">,</span> psi<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> t<span class="token punctuation">]</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>max<span class="token punctuation">(</span>self<span class="token punctuation">.</span>transitions <span class="token operator">+</span> log_delta<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>            <span class="token comment" spellcheck="true"># psi[t][k]=argmax_z1:t-1( p(x1,x2,...,xt,z1,z2,...,zt-1,zt=k|theta) )</span>            <span class="token comment" spellcheck="true"># psi[t][k] is the path choosed from z_t-1 to z_t[k],the value is the z_state(is k) index of z_t-1</span>            log_delta <span class="token operator">=</span> <span class="token punctuation">(</span>log_delta <span class="token operator">+</span> feats<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> t<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># trace back</span>        path <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> T<span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>long<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># max p(z1:t,all_x|theta)</span>        max_logLL_allz_allx<span class="token punctuation">,</span> path<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>max<span class="token punctuation">(</span>log_delta<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        <span class="token keyword">for</span> t <span class="token keyword">in</span> range<span class="token punctuation">(</span>T<span class="token number">-2</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token comment" spellcheck="true"># choose the state of z_t according the state choosed of z_t+1.</span>            path<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> t<span class="token punctuation">]</span> <span class="token operator">=</span> psi<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> t<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>gather<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span>path<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> t<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> max_logLL_allz_allx<span class="token punctuation">,</span> path    <span class="token keyword">def</span> <span class="token function">neg_log_likelihood</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> sentence<span class="token punctuation">,</span> tags<span class="token punctuation">)</span><span class="token punctuation">:</span>        feats <span class="token operator">=</span> self<span class="token punctuation">.</span>_get_lstm_features<span class="token punctuation">(</span>sentence<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true">#[batch_size, max_len, 16]</span>        forward_score <span class="token operator">=</span> self<span class="token punctuation">.</span>_forward_alg<span class="token punctuation">(</span>feats<span class="token punctuation">)</span>        gold_score <span class="token operator">=</span> self<span class="token punctuation">.</span>_score_sentence<span class="token punctuation">(</span>feats<span class="token punctuation">,</span> tags<span class="token punctuation">)</span>        <span class="token keyword">return</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>forward_score <span class="token operator">-</span> gold_score<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">_get_lstm_features</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> sentence<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""sentence is the ids"""</span>        <span class="token comment" spellcheck="true"># self.hidden = self.init_hidden()</span>        embeds <span class="token operator">=</span> self<span class="token punctuation">.</span>_bert_enc<span class="token punctuation">(</span>sentence<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># [8, 75, 768]</span>        <span class="token comment" spellcheck="true"># 过lstm</span>        enc<span class="token punctuation">,</span> _ <span class="token operator">=</span> self<span class="token punctuation">.</span>lstm<span class="token punctuation">(</span>embeds<span class="token punctuation">)</span>        lstm_feats <span class="token operator">=</span> self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>enc<span class="token punctuation">)</span>        <span class="token keyword">return</span> lstm_feats  <span class="token comment" spellcheck="true"># [8, 75, 16]</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> sentence<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># dont confuse this with _forward_alg above.</span>        <span class="token comment" spellcheck="true"># Get the emission scores from the BiLSTM</span>        lstm_feats <span class="token operator">=</span> self<span class="token punctuation">.</span>_get_lstm_features<span class="token punctuation">(</span>sentence<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># [8, 180,768]</span>        <span class="token comment" spellcheck="true"># Find the best path, given the features.</span>        score<span class="token punctuation">,</span> tag_seq <span class="token operator">=</span> self<span class="token punctuation">.</span>_viterbi_decode<span class="token punctuation">(</span>lstm_feats<span class="token punctuation">)</span>        <span class="token keyword">return</span> score<span class="token punctuation">,</span> tag_seq</code></pre><h2 id="训练结果"><a href="#训练结果" class="headerlink" title="训练结果"></a>训练结果</h2><blockquote><p>num_proposed:46616<br>num_correct:38213<br>num_gold:46865<br>precision=0.82<br>recall=0.82<br>f1=0.82</p></blockquote><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> nlp </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LSTM </tag>
            
            <tag> CRF </tag>
            
            <tag> Bert </tag>
            
            <tag> 命名实体识别 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Recurrent Neural Network 阅读</title>
      <link href="/2020/08/07/part-of-speech-tagging-with-bidirectional-long-short-term-memory-recurrent-neural-network/"/>
      <url>/2020/08/07/part-of-speech-tagging-with-bidirectional-long-short-term-memory-recurrent-neural-network/</url>
      
        <content type="html"><![CDATA[<h1 id="Part-of-Speech-Tagging-with-Bidirectional-Long-Short-Term-Memory-Recurrent-Neural-Network"><a href="#Part-of-Speech-Tagging-with-Bidirectional-Long-Short-Term-Memory-Recurrent-Neural-Network" class="headerlink" title="Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Recurrent Neural Network"></a>Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Recurrent Neural Network</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>​    <img src="/2020/08/07/part-of-speech-tagging-with-bidirectional-long-short-term-memory-recurrent-neural-network/1.png" style="zoom:50%;align= center"></p><p>​      LSTM对标签数据的任务非常有效,而且字向量是一个强大的表示方法.本论文使用带词嵌入的BLSTM-RNN进行词性标注,在Penn Treebank WSJ 测试集上的准确率达到97.4%</p><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><img src="/2020/08/07/part-of-speech-tagging-with-bidirectional-long-short-term-memory-recurrent-neural-network/3.png" style="zoom:50%;align= center"><p>本文的贡献</p><ul><li>BLSTM在词性标注的有效性方法</li><li>训练词嵌入方法的提出</li><li>证明了不使用特征也可以获得标记准确性</li></ul><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><img src="/2020/08/07/part-of-speech-tagging-with-bidirectional-long-short-term-memory-recurrent-neural-network/2.png" style="zoom:50%;align= center"><p>如图, 给我一个句子w1, w2，…y1, y2，…利用BLSTM RNN预测每个单词的标签概率分布。</p><ul><li><p>$w _i$ 表示当前词one-hot 表示 </p></li><li><p>$V$ 是词表</p></li><li><p>$f({w} _{i})$ 三维向量,表示判断$w _i$是全小写、全大写还是以大写字母开头</p></li><li><p>整体输入的词向量信息 $I _i = {W} _{1}{w} _{i} + {W} _{2}f({w} _{i})$</p></li><li><p>$W _1$被实现查表过程(使用其他预训练词向量)，参考该表中存储的$wi$嵌入词返回${W} _{1} {w} _{i}$</p></li><li><p>词嵌入初始化为均匀分布的随机值，范围从-0.1到0.1.</p></li><li><p>输出层是softmax层，其维度为标记类型的数量。</p></li><li><p>使用反向传播和梯度下降算法进行训练 ${\prod} _{ i\in 1,…n }{ { P } _{ i } ({y} _{i} | {w} _{1}, {w} _{2} … {w} _{n}) }$</p></li><li><p>输入词$w_i$中最可能的标签${y} _{i}$可以选择为${y} ^{‘} _{i} = arg _{ i\in 1,…n } { P } _{ i } ({y} _{i} | {w} _{1}, {w} _{2} … {w} _{n}) $</p></li></ul><h2 id="词向量"><a href="#词向量" class="headerlink" title="词向量"></a>词向量</h2><p>​      <img src="/2020/08/07/part-of-speech-tagging-with-bidirectional-long-short-term-memory-recurrent-neural-network/4.png" style="zoom:50%;align= center"></p><p>​      我们提出了一种新的训练无标记数据嵌入词的方法, 由随机选择的单词代替的正常句子。对于那些被替换的单词，它们的标签是0(不正确)，对于那些没有被替换的单词，它们的标签是1(正确).使二值分类训练</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>BLSTM RNN系统是利用CURRENT实现的，CURRENT是一种采用GPU加速的RNN机器学习库。</p><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><img src="/2020/08/07/part-of-speech-tagging-with-bidirectional-long-short-term-memory-recurrent-neural-network/6.png" style="zoom:50%;align= center"><p>使用Penn Treebank标记器1进行标记。一个单词中出现的所有连续字都替换为符号“#”。例如，单词“Tel192”和“Tel6”都被转移到同一个单词“Tel#”上。</p><h3 id="Hidden-Layer-Size"><a href="#Hidden-Layer-Size" class="headerlink" title="Hidden Layer Size"></a>Hidden Layer Size</h3><img src="/2020/08/07/part-of-speech-tagging-with-bidirectional-long-short-term-memory-recurrent-neural-network/7.png" style="zoom:50%;align= center"><p>如图 BLSTM RNN中隐藏层的不同大小与测试集的精确度所示,在精度、模型尺寸和运行时间上保持良好的平衡,100作为Hidden Layer Size</p><h3 id="POS-Tagging-Accuracies"><a href="#POS-Tagging-Accuracies" class="headerlink" title="POS Tagging Accuracies"></a>POS Tagging Accuracies</h3><img src="/2020/08/07/part-of-speech-tagging-with-bidirectional-long-short-term-memory-recurrent-neural-network/8.png" style="zoom:50%;align= center"><p>四个典型的系统作为基线系统 </p><ul><li><p>Stanford tagger </p></li><li><p>Moore</p></li><li><p>Shen et al</p></li><li><p>Collobert et al</p></li></ul><p><strong>BLSTM-RNN</strong></p><p>​    在这个实验中使用的词汇是所有出现在WSJ Penn Treebank训练集的词汇，与北美新闻语料库中最常见的10万个词汇合并，再加上一个“UNK”符号，用来替换所有没有词汇的词汇。</p><p><strong>BLSTM-RNN+WE</strong> </p><p>​     为了构建训练词嵌入的语料库，我们将北美新闻语料库中正常句子中约20%的词替换为随机选取的词。然后训练BLSTM RNN判断哪个词被替换了.</p><p>​    表2展示了使用单词嵌入训练北美新闻语料库的前1000万个单词(WE(10m))、前1亿个单词(WE(100m))和所有5.3亿个单词(WE(all))的结果。WE(100m)和WE(all)显著提高了性能。这表明使用更大的未标记数据集可以进一步提高结果。</p><img src="/2020/08/07/part-of-speech-tagging-with-bidirectional-long-short-term-memory-recurrent-neural-network/10.png" style="zoom:50%;align= center"><h3 id="Different-Word-Embeddings"><a href="#Different-Word-Embeddings" class="headerlink" title="Different Word Embeddings"></a>Different Word Embeddings</h3><p>​    在这个实验中，评估了六种已经发布的经过训练的单词嵌入。涉及词的基本信息嵌入和结果在表3列出RCV1代表语料集。OOV(词汇)列表示的单词在词汇外。单词嵌入的用法与实验的BLSTM-RNN+中相同，只是这里的输入层大小等于外部单词嵌入的维数。</p><p>   全词嵌入带来更高的精度。然而，尽管训练的语料库较大，OOV率较低，但它们都不能通过增强BLSTM RNN标记来获得高的准确率。实验表明，词性标注方法中，BLSTM RNN训练的词嵌入是必不可少的。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>​     BLSTM RNN用于词性标注和训练词嵌入。结合在大量未标记数据上训练的单词嵌入，这种方法在不使用丰富的特征的情况下，在华尔街日报测试集上获得了最好的准确性。带单词嵌入的BLSTM RNN有望成为标签任务的有效解决方案.</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 论文 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LSTM </tag>
            
            <tag> 词性标注 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Open Domain Question Answering System Based on Knowledge Base</title>
      <link href="/2020/08/07/open-domain-question-answering-system-based-on-knowledge-base/"/>
      <url>/2020/08/07/open-domain-question-answering-system-based-on-knowledge-base/</url>
      
        <content type="html"><![CDATA[<h1 id="Open-Domain-Question-Answering-System-Based-on-Knowledge-Base"><a href="#Open-Domain-Question-Answering-System-Based-on-Knowledge-Base" class="headerlink" title="Open Domain Question Answering System Based on Knowledge Base"></a>Open Domain Question Answering System Based on Knowledge Base</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>　　<img src="/2020/08/07/open-domain-question-answering-system-based-on-knowledge-base/1.png"></p><p>　　针对NLP&amp;CC 2016中基于知识库的开放域问题回答任务，论文提出了SEP算法，自动从一个简单问句中提取主语谓语词对，将其翻译成知识库查询。提出了一种基于词向量相似度和谓词注意度的候选谓词评分方法。在NLP&amp;CC 2016(KBQA子任务)竞赛中获得第一名，测试数据f1分82.47%。此外，还进行了一系列的实验和全面的误差分析，以显示新数据集的性质和缺陷。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>​         开放域问题回答(QA)是一个重要而又具有挑战性的问题，在很大程度上仍未解决。近年来，出版了大量的英文QA著作。但我们所知,KBQA(基于知识库的问答)数据集2016年NLP&amp;CC评估任务是中国第一个大规模KBQA数据集。在本文中,我们在中国数据集集中精力回答单一关系似是而非的问题。一个SPE算法KB querie翻译一个中文的问题。该算法在逻辑上解决了多个关联问题，该问题可以表示为一个主题实体，并由谓词链连接。但受限于数据集，我们没有进行相关实验。</p><p>　　候选谓词评价是该算法的重要组成部分，提出了一种基于词向量相似度和谓词注意的新方法。在某种程度上，这种方法看起来像一个带有注意机制的神经网络，但它是如此浅显，以至于除了单词向量之外不需要训练任何参数。因此，它更简洁、高效、可解释性强，可以更灵活地与先验知识结合，但丧失了深度神经网络较强的表征能力等优点。还有尝试处理由spider、问题分类和训练数据引起的知识库错误，以提高性能。我们的方法测试数据f1得分为82.47%，在评估任务中获得第一名。</p><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><p>​      KBQA的大多数方法都是基于一些解析方法，如语义解析、依赖解析和CCG(组合类别语法)，将问题映射到它的语义表示(例如一阶逻辑形式)。但是这些工作很少使用KBs来帮助进行解析，而且它们受到解析方法准确性的限制，在处理中文时尤其严重。为了避免这些缺点，一些方法借助知识库从问题中提取知识库查询，如最近的WebQuestions研究。</p><h2 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h2><img src="/2020/08/07/open-domain-question-answering-system-based-on-knowledge-base/2.png" style="zoom:80%;">   <img src="/2020/08/07/open-domain-question-answering-system-based-on-knowledge-base/3.png" style="zoom:80%;"><img src="/2020/08/07/open-domain-question-answering-system-based-on-knowledge-base/4.png" style="zoom:80%;"><p>​       使用一个简单的主题实体链接方法来提取可能的KB实体。根据词向量相似度和谓词注意度对这些实体后的所有谓词进行评估。利用实体长度和谓词得分的线性组合对主谓对进行排序，利用训练集中从问题-答案对中提取的答案模式和处理备选问题的先验规则进行排序。</p><h2 id="Topic-Entity-Linking"><a href="#Topic-Entity-Linking" class="headerlink" title="Topic Entity Linking"></a>Topic Entity Linking</h2><p>​      <img src="/2020/08/07/open-domain-question-answering-system-based-on-knowledge-base/5.png" style="zoom:80%;"></p><p>​       问题的主题实体是相应KB查询的核心实体。在我们的系统中，所有以KB表示的实体(它们是问题的子字符串，不被其他实体重叠)都被视为潜在的主题实体。还有一个由高频实体组成的停止词列表，以减少噪音实体(如”is||是”，＂什么||what＂)和提高效率。在排序阶段，考虑实体的长度作为一个特征。</p><p>​      例如，问题的潜在主题实体 “做铁板鱼要用到什么调料？|| What sauce will be used when you make iron fish (Chinese cuisine)?”  是　＂铁板鱼|| iron fish＂, “到||to” 和　“调料|| sauce”. “做||make”, “要||will”, “用||use”, “哪 and些”（”哪些”　mean “What”）  被忽略，因为它们是高频实体。</p><h2 id="Predicate-Scoring"><a href="#Predicate-Scoring" class="headerlink" title="Predicate Scoring"></a>Predicate Scoring</h2><p>​      主题实体链接之后，有一些潜在的实体，并且每个实体都有一些KB中的谓词。一个好的谓词可以完美地处理问题其余部分的语义。则候选谓词的分数为:</p><img src="/2020/08/07/open-domain-question-answering-system-based-on-knowledge-base/6.png" style="zoom:80%;"><p>​       　 其中$wp _i$是谓语的第i个词，$wq _j$是问题的第j个词。$lp _i$是$wp _i$的长度。$Sim$为两个词的语义相似度，这里采用词向量的余弦相似度。此分数将用于排名阶段。</p><p>​      　语义相似性被认为是表征谓语词对有问题词的关注程度。如果p_i与qj1的相似性大于pi与qj2的相似性，则qj1涉及的pi语义较多。由于在评价每个谓词词时只考虑一个问题词，因此注意类似于加权对齐过程。最关心的疑问词构成了整个谓语的注意。以谓词词的长度作为权重，对词的相似性进行加权平均，衡量谓词是否适合于此问题。</p><p>​     　全细分模式的性能优于常规模式，但仍明显低于我们的细分模式。此外，助词和标点符号在这里被忽略，因为它们没有意义。</p><p>　　分词性能更好有两个原因。首先，在汉语中，如果一个词覆盖另一个词，它们通常有相似的语义，特别是在口语中。其次，词与词之间往往存在一种“类”的关系．因此，该分词器可以处理口语中灵活多变的词汇。</p><p>　　使用词向量计算语义相似度在处理疑问句时存在困难。     “s什么时候||when”与“日期||date”，”在哪 里||where” and “地点||place”之间的余弦相似度远低于预期。因此，我们使用一些手写的模式来指出问题类型，并直接在分词结果中加入一些符号词。由于语句非常灵活，只处理了3种类型的问题(表1)，占所有问题的10%左右。这只是将谓词评分与一些先前的规则组合在一起的一个小尝试，有很大的改进空间。</p><img src="/2020/08/07/open-domain-question-answering-system-based-on-knowledge-base/7.png" style="zoom:80%;"><p>谓词注意的实例如图2所示，余弦相似度和映射关系如表2所示。</p><img src="/2020/08/07/open-domain-question-answering-system-based-on-knowledge-base/9.png" style="zoom:80%;"><h2 id="Answer-Pattern"><a href="#Answer-Pattern" class="headerlink" title="Answer Pattern"></a>Answer Pattern</h2><img src="/2020/08/07/open-domain-question-answering-system-based-on-knowledge-base/10.png" style="zoom:80%;"><p>​         答案模式是从标记的问题-答案对中提取的规则，可以揭示问题陈述与对应正确答案的知识库结构之间的联系。对于每个问题-答案对，候选三元组是KB中的SPO(主题-谓词-对象)三元组，其主题是问题的子字符串，而对象与答案相同。使用几个简单的规则来过滤高可信度三元组(主要基于匹配的长度)。还有一些情况是，没有候选三元组，为了确保最佳三元组的高精度，忽略这些问题-答案对。</p><p>​         答案模式提取示例如表3所示。</p><img src="/2020/08/07/open-domain-question-answering-system-based-on-knowledge-base/11.png" style="zoom:80%;"><h2 id="Ranking"><a href="#Ranking" class="headerlink" title="Ranking"></a>Ranking</h2><p>​       在排序阶段，利用实体长度、谓词得分和答案模式出现次数等特征的线性组合来选择黄金答案。</p><p>​       可以在这里添加一些规则来处理不适合此体系结构的问题。例如，可选问题几乎不涉及谓词的意义，只有有限的对象可供选择。</p><h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><p>数据集由NLP&amp;CC2016评估任务发布，包括一个知识库和用于训练和测试的问题答案对.</p><h2 id="Experiment-Settings"><a href="#Experiment-Settings" class="headerlink" title="Experiment Settings"></a>Experiment Settings</h2><p>​       在主题实体链接过程中，在训练题和测试题中出现超过150次的实体被视为停止词。在24,479个问题中，有52,916个不同的实体，只有496个高频率实体，不足总数的1%。</p><img src="/2020/08/07/open-domain-question-answering-system-based-on-knowledge-base/13.png" style="zoom:80%;"><p>​     使用word2vec软件生成单词向量。采用CBOW模型。窗口大小为5，向量维数为300，对频繁词的降采样阈值为20。以百度百科中的句子作为训练数据，生成155,837个词向量.</p><img src="/2020/08/07/open-domain-question-answering-system-based-on-knowledge-base/12.png" style="zoom:80%;"><p>设r为谓词分数与实体长度之间的权重比率。测试r (fixed w_el = 1, w_po = 100)对训练集的影响，结果如图3所示。当r在10到16之间时，性能是最佳，因此我们设置r = 10，这意味着w_ps = 10。</p><h2 id="Benchmark-Systems"><a href="#Benchmark-Systems" class="headerlink" title="Benchmark Systems"></a>Benchmark Systems</h2><p>三个基准测试系统</p><ul><li>KBQA任务的正式基准系统。所以我们就叫它NLPCC</li><li>“模式匹配”，它只使用从训练数据中提取的答案模式来回答问题。保留核心问题提取，提高命中率。这个基测试的精度系统可以被视为在训练数据标记为答案的性能,这也是一种上界的这个数据集。</li><li>发现最长的主谓对问题和相应的对象被认为是答案,这是下界的准确性所有SPE的算法。</li></ul><h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><img src="/2020/08/07/open-domain-question-answering-system-based-on-knowledge-base/14.png" style="zoom:80%;"><p>系统与一些基准测试系统进行比较 如表4所示。</p><p>​        那么准确率@ n等于准确率@ n乘以回答率。“PatternMatch+NaiveSearch”是这两种系统的结合，直接将答案分数相加。</p><p>​        我们的系统的精确度@1接近于模式匹配和NaiveSearch系统的精确度@1，这两个系统是基于强规则的系统，精度高，召回率低，这意味着我们的系统在这个数据集上接近于理论上最好的系统。</p><p>​    我们还测试了系统核心的性能以及规则部分的影响，如表5所示。由于所有系统都打印有序答案而不是并行答案，所以最佳策略是选择第一个答案，因此F1平均分等于accuracy@1。因为所有这些测试的“回答”率都是100%，所以我们在这里用accuracy@N代替precision@N。</p><img src="/2020/08/07/open-domain-question-answering-system-based-on-knowledge-base/15.png" style="zoom:80%;"><p>​         在表5中，“Tr”表示使用从训练数据中提取的答案模式。QCore是指使用问题的核心部分而不是完整的问题。“qclassification”是指在谓词评分中加入问题分类方法。“AQ”是指备选问题的处理后规则。“Full”等于“Core + Tr + QCore + qclassified + AQ”。“核心”系统是一个无监督的系统使用基本的SPE算法。</p><p>​         从表5可以看出，添加的方法使核心系统的性能有所提高。这一影响之所以不显著，有两个原因。首先，除了“QCore”之外，所有添加的方法都只涉及很少的问题，而QCore只是一种数据清理方法。其次，这些规则改善了在训练数据进行交叉验证时发现的一些不足，但问题在测试数据中的分布略有不同，使得效果较弱。详情见表6。</p><img src="/2020/08/07/open-domain-question-answering-system-based-on-knowledge-base/16.png" style="zoom:80%;"><p>​     NLP&amp;CC2016评估任务的结果如表7所示(21次提交的前5名结果)。我们的系统在所有团队中表现最好。与表5相比，即使是核心系统在本次评估任务中也优于第二组。</p><img src="/2020/08/07/open-domain-question-answering-system-based-on-knowledge-base/17.png" style="zoom:80%;"><h2 id="Error-Analysis"><a href="#Error-Analysis" class="headerlink" title="Error Analysis        "></a>Error Analysis        <img src="/2020/08/07/open-domain-question-answering-system-based-on-knowledge-base/18.png" style="zoom:80%;"></h2><p>　　近一半的错误实际上是由于标签问题或问题设计，这不是真正的错误。其中涉及主体不明(31%，如“兴龙镇的邮政编码是多少”，中国兴龙镇很多)、知识库矛盾(5%)、意图不明(2%)或答案标注错误(5%)。30%的错误是由于不成功的实体链接或谓词评分，这是最重要的改进空间。有27%的错误是由于问题的答案不是问题主体的对象引起的。</p><h2 id="Dataset-Analysis"><a href="#Dataset-Analysis" class="headerlink" title="Dataset Analysis"></a>Dataset Analysis</h2><p>​       有些歧义是由具有相同名称的实体造成的，问题中没有线索可以帮助区分它们。其中4773题(19.50%)，训练集3189题(21.83%)，测试集1584题(16.05%)。我们的系统对这些问题的原始准确率只有62.43%，训练集的62.40%，测试集的62.50%，如果通过寻找正确的主谓词对来判断这些问题的准确率，我们的系统在测试集中对这些问题的准确率为82.07%。因此，通过这一改变，系统性能将达到85.61%。</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>​     提出了一个基于SPE算法的KBQA系统，该系统可以回答简单关系中文问题。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 论文 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 问答 </tag>
            
            <tag> LSTM </tag>
            
            <tag> 词性标注 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A Chinese Question Answering System for Single-Relation Factoid Questions 阅读</title>
      <link href="/2020/08/06/a-chinese-question-answering-system-for-single-relation-factoid-questions/"/>
      <url>/2020/08/06/a-chinese-question-answering-system-for-single-relation-factoid-questions/</url>
      
        <content type="html"><![CDATA[<h1 id="A-Chinese-Question-Answering-System-for-Single-Relation-Factoid-Questions"><a href="#A-Chinese-Question-Answering-System-for-Single-Relation-Factoid-Questions" class="headerlink" title="A Chinese Question Answering System for Single-Relation Factoid Questions"></a>A Chinese Question Answering System for Single-Relation Factoid Questions</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract."></a>Abstract.</h2><p>​     针对NLPCC 2017中基于知识库的开放领域问题回答的任务，构建了一个能够自动查找单关系问题承诺实体和谓词的问题回答系统。在基于特征的实体链接组件和基于单词向量的候选谓词生成组件之后，使用深度卷积新网络对实体-谓词对重新排序，并使用所有中间分数来选择最终的预测答案。我们的方法在测试数据上的f1得分为47.23%，在NLPCC 2017共享任务5(KBQA子任务)竞赛中获得第一名。此外，还有一系列的实验，可以帮助其他开发人员了解我们系统的每个部分的贡献。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>​     开放领域的问题回答是一个重要而又具有挑战性的问题，它在很大程度上仍未得到解决。近年来，随着DBPedia[12]、Freebase[13]等大型知识库的发展，很多研究都集中在从知识库中为开放域问题生成精确可靠的答案。本文介绍了一个中文单关系factoid问题的回答系统，该系统是NLPCC KBQA评估任务的主要组成部分。提出了一种新的基于深度网络神经网络的实体-谓词对重排序方法。我们的系统在测试数据上的f1得分为47.23%，在评价任务中获得第一名.</p><p>​    在本文的其余部分中，我们首先在第2节中回顾相关工作，在第3节中，我们将详细介绍我们的方法的架构。第4节将讨论实验设置、结果和实现技巧。第五部分对全文进行了总结，并对未来的研究进行了展望。</p><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><p>​       开放域问题回答是自然语言处理领域中一个长期存在的问题，被称为ai完全问题。通过知识库回答开放领域问题可以生成更精确和可靠的答案。许多传统的KBQA技术都是基于信息检索[7][8]和语义解析[9][10][11]。最近，一些研究利用表示学习来确定实体提及与知识库实体[1]、问题模式、知识库谓词[1]或知识库子图[2]之间的相似性。他们证明了神经网络方法可以更好地处理高级语义相似度。在处理复杂的自然语言任务，如回答问题时，将神经网络与传统的浅层特征相结合是有益的。按照他们的想法，我们也将传统的浅层特征与CNNs特征结合在我们的系统中。</p><p>​    NLPCC组织中国KBQA评估任务已有三年。Ye的系统[18]在NLPCC 2015中文KBQA任务中取得了最好的性能，该系统将主题谓词提取算法与web知识检索相结合。Lai[19]在NLPCC 2016 KBQA任务中使用基于词向量的特征搜索最优的主语谓词对，取得了最好的性能。Yang[20]结合了基于特征的实体链接、基于朴素贝叶斯的答案选择和基于CNNs的重新排序，获得了2016年的第二名。我们的系统主要受到他们的作品[19][20]的启发，但是我们实现了一种新颖的CNN架构，并适当的结合了他们系统的优点。我们还改进了[19]中基于词向量的谓词选择算法，我们的实体链接方法与[20]略有不同。此外，我们采用了一种精巧的生成对抗方法，如负采样方法来处理CNN训练中的数据不平衡。</p><p>​      深度卷积神经网络在计算机视觉领域中崭露头角。最近，一些工作尝试在NLP任务中使用深层架构，如文本分类[5]和机器翻译[6]。他们遵循了VGG[14]和ResNet[15]的设计，使用窄滤波器和剩余连接来减少参数，使深层架构更易于训练。我们还尝试在系统中实现深度cnn，但遵循GoogLeNet[16]架构，使用带有剩余连接的多视角过滤器。</p><h2 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h2><p>​      我们的系统架构如图1所示。受前人[19]的启发，采用了一些手写的规则来纠正知识库中意外的特殊符号以及从问题中提取核心表达式等蜘蛛错误。然后，使用基于特征的方法来选择承诺的实体提及，然后使用基于谓词的无监督词向量评分方法。候选实体-谓词对生成后，利用深度CNNs模型对候选实体-谓词对重新排序。所有的中间分数被用来选择最终的预测答案。</p><p>​     用于预处理NLPCC数据集的规则几乎与之前的工作相同(参见[19]中的附录)。但是在处理知识库时，删除规则会被忽略。如果一个问题的核心表达是一个实体，我们会添加单词“introduction”，这样我们的系统就会尝试对这个实体进行介绍。无论如何，7631个问题中只有26个问题是受这个引导技巧影响的。</p><img src="/2020/08/06/a-chinese-question-answering-system-for-single-relation-factoid-questions/1.png" style="zoom:70%;"><h2 id="Entity-Linking"><a href="#Entity-Linking" class="headerlink" title="Entity Linking"></a>Entity Linking</h2><p>​       一个KB实体“李娜(跳水运动员)”是由实体名称“李娜”和实体解释“跳水运动员”(有时缺席)组成的。问题的主题实体是相应知识库查询的核心实体，实体提及是问题的子串，它包含主题实体。当且仅当提及与主题实体相同，或只是名称，或通信在提供的文件“nlpcc-iccpol 2016.kbqa.kb.mention2id”中提及时，实体提及包含一个主题实体。受[20]的启发，我们训练了一种基于特性的GBDT(gradient boost decision tree)来从所有可能的问题子串中选择提到的实体。</p><p>​     为了培养监督实体链接模型，黄金提到标签是先决条件。黄金提示必须包含一个与黄金答案相同的对象的KB实体。为了保证黄金标注的精度，采用了提及间的覆盖范围、提及长度和位置等多条规则，每个问题最多有一个黄金标注。统计结果见表1。人工检查，大多数被排除的候选提到是有缺陷的。</p><p>​    实体链接模型所采用的所有特性如下所示，类似于之前[20]的透水工作。但没有考虑词性信息，大部分特征都有多个角度。由于我们提到的是子字符串，而不是连续单词，而是汉字，因此使用FMM(前向最大匹配)查找下一个单词，使用RMM(反向最大匹配)查找最后一个单词。GBDT模型是针对gloden提到的基于这些特性的问题进行训练的。设置和结果在第4节中显示。</p><img src="/2020/08/06/a-chinese-question-answering-system-for-single-relation-factoid-questions/2.png" style="zoom:70%;"><ul><li>位置和长度。提字头、中间和尾的绝对和相对位置。提及的绝对长度和相对长度。提到的是不是一个单一的汉字。</li><li>DF得分。所有问题中提到的字符串的IDF得分。根据wikipedia 1，我们使用4种方法计算IDF得分。</li><li>其他特性。提及中是否有中文，提及是否等同于实体名称，提及是否被其他提及所覆盖。</li></ul><h2 id="Candidate-Predicates-Generation"><a href="#Candidate-Predicates-Generation" class="headerlink" title="Candidate Predicates Generation"></a>Candidate Predicates Generation</h2><p>​     我们使用与[19]相同的方法来评估问题模式的语义是否能覆盖谓词(参见eq 1)，但删除了大部分技巧，如问题分类和高频实体过滤。我们使用一种变体(见eq 2)来评价谓词的语义是否能覆盖问题模式，其中ave q是所有问题中单词的平均向量，用于匹配停止词。本节的分词方法与[19]相同。因此，所有疑问句和谓语中可能出现的词都会被考虑在内。关于该基于词向量的评价方法的详细说明以及对所选择的分词方法的讨论可以在[19]的3.2节中找到。</p><img src="/2020/08/06/a-chinese-question-answering-system-for-single-relation-factoid-questions/3.png" style="zoom:67%;"><img src="/2020/08/06/a-chinese-question-answering-system-for-single-relation-factoid-questions/4.png" style="zoom:100%;"><p>​     为了在重新排序过程中限制候选实体-谓词对的数量，我们使用这些特征的线性组合(参见eq 3)来过滤不太可能的候选，类似于之前的工作[19]。其中l men表示所提到的长度，l pre表示所提到的长度。如果一个实体涉及多个具有相同谓词的KB实体，则只考虑第一个实体的谓词(按KB文件中的外观排序)，因此不会生成重复的实体-谓词对。</p><img src="/2020/08/06/a-chinese-question-answering-system-for-single-relation-factoid-questions/5.png" style="zoom:100%;"><h2 id="Deep-CNNs-Architecture"><a href="#Deep-CNNs-Architecture" class="headerlink" title="Deep CNNs Architecture"></a>Deep CNNs Architecture</h2><p>​     采用深度卷积神经网络对候选实体谓词对进行重新排序。提交版本中使用的深度CNNs模型的详细架构如图2所示。该模型评估了预命题模式与问题模式的相似性，即未提及实体的问题。使用预先训练好的词向量来表示输入，然后是几个卷积块(图2中的2个卷积块)来生成高级特征。然后，在最大池化层之后，采用element-wise乘法来组合来自问题和谓词的特性。最后，使用带dropout的多层感知器来评估最终的相似度。卷积层的参数在谓词和问题的处理之间共享。</p><p>​     受GoogLeNet[16]的启发，在每个卷积块中有多个滤波器宽度(在图2中，256个宽度为1,512个宽度为2的滤波器，256个宽度为3的滤波器)，在ResNet[15]之后，相邻块之间存在残差连接。受更深的模型和计算能力带来的池改进的限制，提交版本只有2块.</p><h2 id="Ranking"><a href="#Ranking" class="headerlink" title="Ranking"></a>Ranking</h2><p>​     采用所有中间分值的线性组合来生成候选答案的最终排名。自高精度实体连接(3.1节)和单一功能的良好的性能由深cnn(3.3节)或基于词向量的方法(3.2节),结合方程是非常粗略的不精细调整(见eq 4),男性,f,和S cnn实体提到分数,entity-predicate对基于词向量方法的评估,评估分别由cnn和谓词。</p><img src="/2020/08/06/a-chinese-question-answering-system-for-single-relation-factoid-questions/6.png" style="zoom:100%;"><h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><p>​     数据集由NLPCC 2017评估任务发布，包括一个知识库和用于训练和测试的问答对。知识中大约有4300万个SPO对，其中涉及约600万个主题、60万个谓词和1600万个对象。2017-训练集包含14,609对2016-训练试题-答案和9,870对2016-测试试题-答案。2017年测试集包含7631对问答。答案由人标记，大多数问题可以由KB对象回答。</p><h3 id="Settings"><a href="#Settings" class="headerlink" title="Settings"></a>Settings</h3><p>​      我们系统中的所有单词向量与之前的工作[19]中的单词向量列表相同，该工作使用了Tomas Mikolov 1生成的word2vector工具和在Baidubaike语库上训练的CBOW[17]模型。分词使用的词表由词向量表中的所有词组成。</p><p>​    GBDT实体链接模型中使用的参数为:max depth=8, eta=0.1, objective=reg:logistic, nrounds=100。在训练CNN模型时，batch size为64,loss function为binary crossentropy, optimizer为adadelta[23]。提交版本已经训练了21个epoch，但是相同设置下最好的f1得分出现在7个epoch完成时，达到47.35%。CNN模型由keras实现。</p><p>​    在实体链接过程中，只有排名前三且得分高于排名前三者意愿0.01倍的提及，这就是我们的提及过滤规则。只有前20个候选的实体-谓词对将被用于cnn。</p><p>   针对网络神经网络在训练时段性能不稳定的问题，提出了一种集成学习方法。S cnn是8个cnn输出的平均值。其中4个具有与图2相同的架构，其他的与图2相似，但在每个卷积块中都有384个宽度为1的过滤器和640个宽度为2的过滤器。所有的CNN模型在初始化时都有不同的种子.</p><p>   虽然在训练CNN模型之前，在生成候选谓词时，大部分的负实体-谓词对已经被过滤掉了，但是正样本和负样本的数量仍然是不平衡的。因此采用了动态负采样的方法。负的实体谓词对P ep i的可能性如eq 5所示，其中排名ep i是该实体谓词对在最后一次迭代结束时得分的问题的排名。它就像一个简单的生成对抗机制，其中生成模型是判别模型的最后一次迭代。</p><img src="/2020/08/06/a-chinese-question-answering-system-for-single-relation-factoid-questions/7.png" style="zoom:100%;"><h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><p>​      <img src="/2020/08/06/a-chinese-question-answering-system-for-single-relation-factoid-questions/8.png" style="zoom:100%;"></p><p>​      我们的实体链接模型的结果如表2所示。我们使用5倍交叉验证对2016年和2017年的训练数据集以及每个测试数据集与相应的训练数据进行测试。Rec_filter是提及过滤器规则的召回。与之前[20]的工作相比，在2016年的训练数据上，我们的模型的准确率(98.75%)略低于他们的he f1-score(99.04%)。但他们只标注了14033个问题，而我们标注了14306，数据中的每个问题都只有一个黄金提及。因此，哪一种模式更好还不是很明显。</p><p>​    候选谓词生成一些详细信息如表3所示，包括问题数量、每个问题提到的候选词数量和每个问题候选知识库三元组的数量。由于2017年测试数据中排名前一的实体链接的准确率降低，实体过滤器在每个问题中自动持有更多的实体提及。</p><img src="/2020/08/06/a-chinese-question-answering-system-for-single-relation-factoid-questions/9.png" style="zoom:100%;"><p>​     <img src="/2020/08/06/a-chinese-question-answering-system-for-single-relation-factoid-questions/10.png" style="zoom:100%;"></p><p>​          另外，基于词向量的方法在不同设置下对2016测试集的结果如表4所示。Baseline是NLPCC 2016 KBQA任务[19]中最好的系统。但是为了与我们的方法进行公正的比较，对于相同的实体-谓词对，只应答一个对象，因此top-n精度(n&gt;1)将低于报告的精度。baseline -rules是一个没有问题分类和基于模式训练等技巧的基线系统，这是我们系统的实际基线。我们认为这些规则应该被CNNs自动总结出来。整个系统采用了实体链接过滤器和基于反向词向量的相似度查询。从表4可以看出，实体链接和反向相似度都可以提高性能，候选实体的限制可以大幅度提升pre@20，这是CNN重新排序的一个重要指标。</p><p>​    CNN对我们不同深度的CNN模型的2017测试集的结果重新排序如表5所示。+s f &amp;s men代表ensemble CNNs和previous features的组合。每个块有1024个过滤器，如图2所示。似乎越深入越能带来不稳定的改善。由于计算能力的限制，我们不能很好地调整滤波器数量和块结构等参数。因此，深层架构仍有很大的潜力。我们提交架构中最好的pre@1是47.35%，它包含8个2块模型。在2017年测试集上，基于单词向量的特征s f可以达到43.10%，将渗透特征与cnn结合得到明显的提高。</p><img src="/2020/08/06/a-chinese-question-answering-system-for-single-relation-factoid-questions/11.png" style="zoom:100%;"><p>​      提交体系结构的详细结果如表6所示。CNN模型训练了2016年数据集的17个epoch和2017年数据集的7个epoch。根据[19]，如果通过找到正确的实体-谓词对来判断性能，基线系统在2016测试集上的准确率高达85.61%，而我们的系统在2016测试集上的准确率为89.65%。</p><img src="/2020/08/06/a-chinese-question-answering-system-for-single-relation-factoid-questions/12.png" style="zoom:100%;"><p>​      NLPCC2017评估任务提交结果如表7所示(14次提交的前5名结果)。我们的系统在所有团队中取得了最好的成绩。</p><h2 id="Upper-Bound-Analysis"><a href="#Upper-Bound-Analysis" class="headerlink" title="Upper Bound Analysis"></a>Upper Bound Analysis</h2><p>​      测试数据集不是KB对象，其中14%的主题实体提到的是KB实体的别名，没有在文件“nlpcc-iccpol 2016.kbqa.kb中提及。mention2id”，使我们的实体喜欢方法与他们成为无效的。因此，别名链接也非常重要，除了给定的知识库之外，还需要更多的信息。</p><img src="/2020/08/06/a-chinese-question-answering-system-for-single-relation-factoid-questions/13.png" style="zoom:100%;"><h3 id="Further-Experiments"><a href="#Further-Experiments" class="headerlink" title="Further Experiments"></a>Further Experiments</h3><p>​    我们也在qald数据集2的一个子集上做了实验。由于qald中的训练数据对于深度CNN模型来说不够大，所以我们使用了仅限s f的设置，这是一种无监督的方法。从qord -6 task 1的训练集(包含350个问题的RDF数据上的多语言问题回答)中选出78个英语单关系factoid问题，系统能正确回答其中的82%(64/78)，说明语言不是系统的限制。</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>​     本文提出了一个复杂的KBQA系统，该系统由基于特征的实体喜好、基于词向量的候选谓词生成和基于深度cnn的重排序方法组成，能够回答简单关系的中文问题。针对CNNs输入的不平衡，我们提出了一种生成的对抗方法，如负采样方法。我们的系统在NLPCC 2017共享任务5 (KBQA子任务)竞赛中获得第一名。文中还演示了详细的实验结果，这有助于其他开发人员理解我们的组件所做的贡献。在将来，我们将扩展我们的系统以回答多关系问题，并尝试组合来自知识库三元组对象的信息。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 论文 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KBQA </tag>
            
            <tag> 阅读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HMM - 无监督</title>
      <link href="/2020/08/03/hmm/"/>
      <url>/2020/08/03/hmm/</url>
      
        <content type="html"><![CDATA[<h1 id="隐马尔科夫"><a href="#隐马尔科夫" class="headerlink" title="隐马尔科夫"></a>隐马尔科夫</h1><h2 id="两个假设"><a href="#两个假设" class="headerlink" title="两个假设"></a>两个假设</h2><ul><li>齐次性假设，（状态与前一个状态有关）</li><li>观测独立性假设，输出仅与当前状态有关</li></ul><h2 id="三个基本问题"><a href="#三个基本问题" class="headerlink" title="三个基本问题"></a>三个基本问题</h2><ul><li><p>评估问题：已知模型参数$ λ= (A, B, π)$，计算某个观测序列发生的概率，即求P(O|λ)，即<strong>概率计算问题</strong>；</p><p><strong>前向-后向算法：</strong>给定模型$λ=(A,B,π)$和观测序列$Q={q_1,q_2,…,q_T}$，计算模型λ下观测到序列Q出现的概率$P(Q|λ)$。</p></li><li><p>学习问题：如何调整模型参数 $λ=(π, A, B)$，使得$P(O|λ)$最大？</p><p><strong>Baum-Welch算法：</strong>已知观测序列$Q={q_1,q_2,…,q_T}$，估计模型$λ=(A,B,π)$的参数，使得在该模型下观测序列$P(Q|λ)$最大。</p></li><li><p>解码问题：给出观测序列O和模型μ，怎样选择一个隐藏状态序列S$s_1,s_2,…{s}_{t+1}$，能最好的解释观测序列O；</p><p><strong>Viterbi算法：</strong>给定模型$λ=(A,B,π)$和观测序列$Q={q_1,q_2,…,q_T}$，求使观测序列条件概率$P(I|Q，λ)$最大的状态序列I。</p></li></ul><h3 id="HMM的组成"><a href="#HMM的组成" class="headerlink" title="ＨＭＭ的组成"></a>ＨＭＭ的组成</h3><p>五元组：$\lambda = (N, M, A, B, pi)$</p><p>简记：$\lambda = ( A, B, pi)$</p><p>N：状态数目</p><p>Ｍ：可能的观测数目</p><p>Ａ：与时间无关的状态转移概率矩阵</p><p>Ｂ：给定状态下，观测值概率分布</p><p>pi：初始状态空间的概率分布</p><ul><li><p>前向算法</p><p>思想：高效递归向前变量　，以求最终结果</p><p>前向变量　$\alpha_t(i) = P(O_1, O_2,O_3….O_t, q_t=S_i|\lambda), 1&lt;= t &lt;= T$</p><p>初始状态：$\alpha _1(i) = pi _i b _i(O _1)$</p><p>递归：${\alpha}_{t+1}(j) = [\sum _{i=1}^{N}{\alpha} _{i}*{a} _{ij}] * {b} _{j}({O} _{t+1})$</p><p>结束：$P(O|\lambda) = \sum _{i=1}^{N}{\alpha} _{T}(i)$</p></li><li><p>后向算法</p><p>思想：与向前算法类似</p><p>后向变量：${\beta} _{t}(i) = P({O} _{1}{O} _{2}…., {q} _{t}= {S} _{i} | \lambda)$</p><p>初始状态　$\beta _T(i) = 1$</p><p>递归   ${\beta} _{i} = {\sum} _{i=j}^{N}{a} _{ij} * {b} _{j}({O} _{t+1}) * {\beta} _{t+1}(j)$</p><p>结束  $P(O|\lambda)=\pi_ib_i(o_1)\beta_1(i)$</p></li><li><p>viterbi算法</p><p>目标: 给定一个观测序列和HMM模型,如何有效选择”最优”状态序列,以”最好的解释”观察序列</p><p>思想:利用动态规划求解, 复杂度$O{N}^{2}T$</p><p>Viterbi变量  ${\delta} _{t}(i) = {\max} _{q1 …qt-1}  P({q} _{1},  {q} _{2} ….., {q} _{t} = {S} _{t}, {O} _{1} ….. {O} _{t} | \lambda)$</p><p>递归  ${\delta} _{t+1}(i) = [{\max} _{j}{\delta} _{t}(j) * {\alpha} _{ij} ] * {b} _{i}({O} _{t+1})$</p><p>路径变量  $\varphi _t(i)$ 记录概率最大的路径上当前状态的前一个状态</p></li></ul><h2 id="HMM三个算法的作用："><a href="#HMM三个算法的作用：" class="headerlink" title="HMM三个算法的作用："></a>HMM三个算法的作用：</h2><ul><li><p>forward算法：（评估）给定一HMM模型，计算一观察序列<em>O1O2…OLEN</em>出现的概率。</p></li><li><p>viterbi算法：（解码）给定一HMM模型，计算一观察序列<em>O1O2…OLEN</em>对应的最可能的隐藏序列<em>H1H2…HLEN</em>及该隐藏序列出现的概率。</p></li><li><p>forward-backward算法：（学习）给定一观察序列<em>O1O2…OLEN，</em>求解能够拟合这个序列的HMM模型。</p></li></ul><h2 id="HMM三个算法之间的关系："><a href="#HMM三个算法之间的关系：" class="headerlink" title="HMM三个算法之间的关系："></a>HMM三个算法之间的关系：</h2><ul><li><p>forward算法中的forward变量就是forward-backforward算法中的forward变量，而backward变量与forward变量是类似的；</p></li><li><p>forward-backward算法是为了通过类似最大似然估计的方法找到局部最优的模型参数，在迭代过程中forward变量和backward变量起了很大作用；</p></li><li><p>viterbi算法和forward算法很相似，只是forward算法迭代过程需要的是sum，viterbi算法迭代过程需要的是max，而且viterbi算法除了输出概率，还要用逆推过程求解路径；</p></li><li><p>当用forward-backward算法求解出模型参数之后，用户给出一个观察序列，用viterbi算法就能求出最可能的隐藏序列以及概率了。 </p></li></ul><h2 id="HMM实现库"><a href="#HMM实现库" class="headerlink" title="HMM实现库"></a>HMM实现库</h2><p>hmmlearn实现了三种HMM模型类，按照观测状态是连续状态还是离散状态，可以分为两类：</p><ul><li>连续观测状态：GaussianHMM和GMMHMM；</li><li>离散观测状态：Multinomial</li></ul><p>github：<a href="https://github.com/hmmlearn/hmmlearn">https://github.com/hmmlearn/hmmlearn</a></p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">class</span> <span class="token class-name">HMM</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> Ann<span class="token punctuation">,</span> Bnm<span class="token punctuation">,</span> pi1n<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>A <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>Ann<span class="token punctuation">)</span>     <span class="token comment" spellcheck="true"># 状态转移概率 NxN</span>        self<span class="token punctuation">.</span>B <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>Bnm<span class="token punctuation">)</span>     <span class="token comment" spellcheck="true"># 观测概率矩阵 NxM</span>        self<span class="token punctuation">.</span>pi <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>pi1n<span class="token punctuation">)</span>   <span class="token comment" spellcheck="true"># 初始状态概率向量，1xN</span>        self<span class="token punctuation">.</span>N <span class="token operator">=</span> self<span class="token punctuation">.</span>A<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>  <span class="token comment" spellcheck="true">#第一维长度，N种状态</span>        self<span class="token punctuation">.</span>M <span class="token operator">=</span> self<span class="token punctuation">.</span>B<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>  <span class="token comment" spellcheck="true">#第二维长度，M种观测结果</span>    <span class="token keyword">def</span> <span class="token function">printhmm</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"=================================================="</span><span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"HMM content: N ="</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>N<span class="token punctuation">,</span> <span class="token string">",M ="</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>M<span class="token punctuation">)</span>        <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>self<span class="token punctuation">.</span>N<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">if</span> i <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>                <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"hmm.A "</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>A<span class="token punctuation">[</span>i<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token string">" hmm.B "</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>B<span class="token punctuation">[</span>i<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span>            <span class="token keyword">else</span><span class="token punctuation">:</span>                <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"      "</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>A<span class="token punctuation">[</span>i<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token string">"       "</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>B<span class="token punctuation">[</span>i<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"hmm.pi"</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>pi<span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"=================================================="</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 函数名称：Forward *功能：前向算法估计参数 *参数:phmm:指向HMM的指针</span>    <span class="token comment" spellcheck="true"># T:观察值序列的长度 O:观察值序列</span>    <span class="token comment" spellcheck="true"># alpha:运算中用到的临时数组 pprob:返回值,所要求的概率</span>    <span class="token keyword">def</span> <span class="token function">Forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> T<span class="token punctuation">,</span> O<span class="token punctuation">,</span> alpha<span class="token punctuation">,</span> pprob<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true">#     1. Initialization 初始化</span>        <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>self<span class="token punctuation">.</span>N<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token comment" spellcheck="true"># 初始状态为i且满足O(0)的概率，a|t=0（i）</span>            alpha<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> i<span class="token punctuation">]</span> <span class="token operator">=</span> self<span class="token punctuation">.</span>pi<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>B<span class="token punctuation">[</span>i<span class="token punctuation">,</span> O<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span>        <span class="token comment" spellcheck="true">#     2. Induction 递归</span>        <span class="token keyword">for</span> t <span class="token keyword">in</span> range<span class="token punctuation">(</span>T <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">for</span> j <span class="token keyword">in</span> range<span class="token punctuation">(</span>self<span class="token punctuation">.</span>N<span class="token punctuation">)</span><span class="token punctuation">:</span>                sum <span class="token operator">=</span> <span class="token number">0.0</span>   <span class="token comment" spellcheck="true"># 中间量</span>                <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>self<span class="token punctuation">.</span>N<span class="token punctuation">)</span><span class="token punctuation">:</span>                    sum <span class="token operator">+=</span> alpha<span class="token punctuation">[</span>t<span class="token punctuation">,</span> i<span class="token punctuation">]</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>A<span class="token punctuation">[</span>i<span class="token punctuation">,</span> j<span class="token punctuation">]</span>                alpha<span class="token punctuation">[</span>t <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> j<span class="token punctuation">]</span> <span class="token operator">=</span> sum <span class="token operator">*</span> self<span class="token punctuation">.</span>B<span class="token punctuation">[</span>j<span class="token punctuation">,</span> O<span class="token punctuation">[</span>t <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span>        <span class="token comment" spellcheck="true">#     3. Termination 终止</span>        sum <span class="token operator">=</span> <span class="token number">0.0</span>        <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>self<span class="token punctuation">.</span>N<span class="token punctuation">)</span><span class="token punctuation">:</span>            sum <span class="token operator">+=</span> alpha<span class="token punctuation">[</span>T <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">,</span> i<span class="token punctuation">]</span>    <span class="token comment" spellcheck="true">#   T-1时候的终止态</span>        pprob<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">*=</span> sum   <span class="token comment" spellcheck="true"># sum是所有可能终止状态的加权和</span>    <span class="token comment" spellcheck="true"># 带修正的前向算法</span>    <span class="token comment" spellcheck="true"># scale[t] 走到第t步骤时，符合前向所有状态的概率，就是t时刻的N种状态的加权和</span>    <span class="token keyword">def</span> <span class="token function">ForwardWithScale</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> T<span class="token punctuation">,</span> O<span class="token punctuation">,</span> alpha<span class="token punctuation">,</span> scale<span class="token punctuation">,</span> pprob<span class="token punctuation">)</span><span class="token punctuation">:</span>        scale<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0.0</span>        <span class="token comment" spellcheck="true">#     1. Initialization</span>        <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>self<span class="token punctuation">.</span>N<span class="token punctuation">)</span><span class="token punctuation">:</span>            alpha<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> i<span class="token punctuation">]</span> <span class="token operator">=</span> self<span class="token punctuation">.</span>pi<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>B<span class="token punctuation">[</span>i<span class="token punctuation">,</span> O<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span>            scale<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">+=</span> alpha<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> i<span class="token punctuation">]</span>        <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>self<span class="token punctuation">.</span>N<span class="token punctuation">)</span><span class="token punctuation">:</span>            alpha<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> i<span class="token punctuation">]</span> <span class="token operator">/=</span> scale<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        <span class="token comment" spellcheck="true">#     2. Induction</span>        <span class="token keyword">for</span> t <span class="token keyword">in</span> range<span class="token punctuation">(</span>T <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            scale<span class="token punctuation">[</span>t <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0.0</span>            <span class="token keyword">for</span> j <span class="token keyword">in</span> range<span class="token punctuation">(</span>self<span class="token punctuation">.</span>N<span class="token punctuation">)</span><span class="token punctuation">:</span>                sum <span class="token operator">=</span> <span class="token number">0.0</span>                <span class="token comment" spellcheck="true"># alpha[t + 1, j] 使用了alpha[t, i]，是一个不断迭代的过程                </span>                <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>self<span class="token punctuation">.</span>N<span class="token punctuation">)</span><span class="token punctuation">:</span>                    sum <span class="token operator">+=</span> alpha<span class="token punctuation">[</span>t<span class="token punctuation">,</span> i<span class="token punctuation">]</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>A<span class="token punctuation">[</span>i<span class="token punctuation">,</span> j<span class="token punctuation">]</span>                alpha<span class="token punctuation">[</span>t <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> j<span class="token punctuation">]</span> <span class="token operator">=</span> sum <span class="token operator">*</span> self<span class="token punctuation">.</span>B<span class="token punctuation">[</span>j<span class="token punctuation">,</span> O<span class="token punctuation">[</span>t <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span>                <span class="token comment" spellcheck="true"># scale[t] t时刻 满足前向所有o（t）的概率</span>                scale<span class="token punctuation">[</span>t <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">+=</span> alpha<span class="token punctuation">[</span>t <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> j<span class="token punctuation">]</span>            <span class="token keyword">for</span> j <span class="token keyword">in</span> range<span class="token punctuation">(</span>self<span class="token punctuation">.</span>N<span class="token punctuation">)</span><span class="token punctuation">:</span>                <span class="token comment" spellcheck="true"># alpha[t, j]是t时刻，此状态为j的概率可能性</span>                alpha<span class="token punctuation">[</span>t <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> j<span class="token punctuation">]</span> <span class="token operator">/=</span> scale<span class="token punctuation">[</span>t <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span>        <span class="token comment" spellcheck="true">#     3. Termination</span>        <span class="token comment" spellcheck="true"># 所有t时间的信息熵？？？？？？   为了求取极大似然函数做准备？</span>        <span class="token keyword">for</span> t <span class="token keyword">in</span> range<span class="token punctuation">(</span>T<span class="token punctuation">)</span><span class="token punctuation">:</span>            pprob<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">+=</span> np<span class="token punctuation">.</span>log<span class="token punctuation">(</span>scale<span class="token punctuation">[</span>t<span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true">#np.log以e为底</span>    <span class="token comment" spellcheck="true"># 函数名称：Backward * 功能:后向算法估计参数 * 参数:phmm:指向HMM的指针</span>    <span class="token comment" spellcheck="true"># T:观察值序列的长度 O:观察值序列</span>    <span class="token comment" spellcheck="true"># beta:运算中用到的临时数组 pprob:返回值，所要求的概率</span>    <span class="token keyword">def</span> <span class="token function">Backword</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> T<span class="token punctuation">,</span> O<span class="token punctuation">,</span> beta<span class="token punctuation">,</span> pprob<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true">#     1. Intialization</span>        <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>self<span class="token punctuation">.</span>N<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token comment" spellcheck="true"># 最后一个为i时，P（o(t=T)| iT=q(i))=1</span>            beta<span class="token punctuation">[</span>T <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">,</span> i<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1.0</span>        <span class="token comment" spellcheck="true">#     2. Induction</span>        <span class="token keyword">for</span> t <span class="token keyword">in</span> range<span class="token punctuation">(</span>T <span class="token operator">-</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>   <span class="token comment" spellcheck="true"># 从最后一个往前数到第0个（start，stop，step）</span>            <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>self<span class="token punctuation">.</span>N<span class="token punctuation">)</span><span class="token punctuation">:</span>                sum <span class="token operator">=</span> <span class="token number">0.0</span>                <span class="token keyword">for</span> j <span class="token keyword">in</span> range<span class="token punctuation">(</span>self<span class="token punctuation">.</span>N<span class="token punctuation">)</span><span class="token punctuation">:</span>                    sum <span class="token operator">+=</span> self<span class="token punctuation">.</span>A<span class="token punctuation">[</span>i<span class="token punctuation">,</span> j<span class="token punctuation">]</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>B<span class="token punctuation">[</span>j<span class="token punctuation">,</span> O<span class="token punctuation">[</span>t <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">*</span> beta<span class="token punctuation">[</span>t <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> j<span class="token punctuation">]</span>                beta<span class="token punctuation">[</span>t<span class="token punctuation">,</span> i<span class="token punctuation">]</span> <span class="token operator">=</span> sum        <span class="token comment" spellcheck="true">#     3. Termination</span>        pprob<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0.0</span>        <span class="token comment" spellcheck="true"># 所有满足序列的状态和，pprob[0]是所有的初始状态</span>        <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>self<span class="token punctuation">.</span>N<span class="token punctuation">)</span><span class="token punctuation">:</span>            pprob<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">+=</span> self<span class="token punctuation">.</span>pi<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>B<span class="token punctuation">[</span>i<span class="token punctuation">,</span> O<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">*</span> beta<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> i<span class="token punctuation">]</span>    <span class="token comment" spellcheck="true"># 带修正的后向算法</span>    <span class="token keyword">def</span> <span class="token function">BackwardWithScale</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> T<span class="token punctuation">,</span> O<span class="token punctuation">,</span> beta<span class="token punctuation">,</span> scale<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true">#     1. Intialization</span>        <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>self<span class="token punctuation">.</span>N<span class="token punctuation">)</span><span class="token punctuation">:</span>            beta<span class="token punctuation">[</span>T <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">,</span> i<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1.0</span>        <span class="token comment" spellcheck="true">#     2. Induction</span>        <span class="token keyword">for</span> t <span class="token keyword">in</span> range<span class="token punctuation">(</span>T <span class="token operator">-</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>self<span class="token punctuation">.</span>N<span class="token punctuation">)</span><span class="token punctuation">:</span>                sum <span class="token operator">=</span> <span class="token number">0.0</span>                <span class="token keyword">for</span> j <span class="token keyword">in</span> range<span class="token punctuation">(</span>self<span class="token punctuation">.</span>N<span class="token punctuation">)</span><span class="token punctuation">:</span>                    sum <span class="token operator">+=</span> self<span class="token punctuation">.</span>A<span class="token punctuation">[</span>i<span class="token punctuation">,</span> j<span class="token punctuation">]</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>B<span class="token punctuation">[</span>j<span class="token punctuation">,</span> O<span class="token punctuation">[</span>t <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">*</span> beta<span class="token punctuation">[</span>t <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> j<span class="token punctuation">]</span>                <span class="token comment" spellcheck="true"># 最终目的是求得每种状态占t时刻的概率大小，最终态无所谓的？</span>                beta<span class="token punctuation">[</span>t<span class="token punctuation">,</span> i<span class="token punctuation">]</span> <span class="token operator">=</span> sum <span class="token operator">/</span> scale<span class="token punctuation">[</span>t <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span>    <span class="token comment" spellcheck="true"># Viterbi算法</span>    <span class="token comment" spellcheck="true"># 输入：A，B，pi,O 输出P(O|lambda)最大时Poptimal的路径I</span>    <span class="token keyword">def</span> <span class="token function">viterbi</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> O<span class="token punctuation">)</span><span class="token punctuation">:</span>        T <span class="token operator">=</span> len<span class="token punctuation">(</span>O<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 初始化</span>        delta <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>T<span class="token punctuation">,</span> self<span class="token punctuation">.</span>N<span class="token punctuation">)</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>float<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># delta是针对t时刻，t+1时刻的最优路径的概率值</span>        phi <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>T<span class="token punctuation">,</span> self<span class="token punctuation">.</span>N<span class="token punctuation">)</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>float<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 记录</span>        I <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>T<span class="token punctuation">,</span> np<span class="token punctuation">.</span>int<span class="token punctuation">)</span>        <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>self<span class="token punctuation">.</span>N<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token comment" spellcheck="true"># 0时刻 初态*变成Bi（O1）时刻的概率</span>            delta<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> i<span class="token punctuation">]</span> <span class="token operator">=</span> self<span class="token punctuation">.</span>pi<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>B<span class="token punctuation">[</span>i<span class="token punctuation">,</span> O<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span>            phi<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> i<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>        <span class="token comment" spellcheck="true"># 递推</span>        <span class="token keyword">for</span> t <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> T<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">for</span> j <span class="token keyword">in</span> range<span class="token punctuation">(</span>self<span class="token punctuation">.</span>N<span class="token punctuation">)</span><span class="token punctuation">:</span>                delta<span class="token punctuation">[</span>t<span class="token punctuation">,</span> j<span class="token punctuation">]</span> <span class="token operator">=</span> self<span class="token punctuation">.</span>B<span class="token punctuation">[</span>j<span class="token punctuation">,</span> O<span class="token punctuation">[</span>t<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">*</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span>delta<span class="token punctuation">[</span>t <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">,</span> i<span class="token punctuation">]</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>A<span class="token punctuation">[</span>i<span class="token punctuation">,</span> j<span class="token punctuation">]</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>self<span class="token punctuation">.</span>N<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>max<span class="token punctuation">(</span><span class="token punctuation">)</span>                phi<span class="token punctuation">[</span>t<span class="token punctuation">,</span> j<span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span>delta<span class="token punctuation">[</span>t <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">,</span> i<span class="token punctuation">]</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>A<span class="token punctuation">[</span>i<span class="token punctuation">,</span> j<span class="token punctuation">]</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>self<span class="token punctuation">.</span>N<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>argmax<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 终结</span>        prob <span class="token operator">=</span> delta<span class="token punctuation">[</span>T <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>max<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># T时刻的最大值对应的状态 是 最优路径</span>        I<span class="token punctuation">[</span>T <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> delta<span class="token punctuation">[</span>T <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>argmax<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 取得最大值时 T时刻的变量</span>        <span class="token comment" spellcheck="true"># 状态序列求取，从最大值往前倒推</span>        <span class="token keyword">for</span> t <span class="token keyword">in</span> range<span class="token punctuation">(</span>T <span class="token operator">-</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            I<span class="token punctuation">[</span>t<span class="token punctuation">]</span> <span class="token operator">=</span> phi<span class="token punctuation">[</span>t <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> I<span class="token punctuation">[</span>t <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span>        <span class="token keyword">return</span> I<span class="token punctuation">,</span> prob  <span class="token comment" spellcheck="true"># prob是最终时刻的状态</span>    <span class="token comment" spellcheck="true"># 计算gamma : 满足观测序列，在t时刻处于状态i的概率  前向概率*后向概率 TxN矩阵</span>    <span class="token comment" spellcheck="true"># 返回值 gamma[T, N] 二维列表</span>    <span class="token keyword">def</span> <span class="token function">ComputeGamma</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> T<span class="token punctuation">,</span> alpha<span class="token punctuation">,</span> beta<span class="token punctuation">,</span> gamma<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">for</span> t <span class="token keyword">in</span> range<span class="token punctuation">(</span>T<span class="token punctuation">)</span><span class="token punctuation">:</span>            denominator <span class="token operator">=</span> <span class="token number">0.0</span> <span class="token comment" spellcheck="true"># 分母</span>            <span class="token keyword">for</span> j <span class="token keyword">in</span> range<span class="token punctuation">(</span>self<span class="token punctuation">.</span>N<span class="token punctuation">)</span><span class="token punctuation">:</span>                gamma<span class="token punctuation">[</span>t<span class="token punctuation">,</span> j<span class="token punctuation">]</span> <span class="token operator">=</span> alpha<span class="token punctuation">[</span>t<span class="token punctuation">,</span> j<span class="token punctuation">]</span> <span class="token operator">*</span> beta<span class="token punctuation">[</span>t<span class="token punctuation">,</span> j<span class="token punctuation">]</span>                denominator <span class="token operator">+=</span> gamma<span class="token punctuation">[</span>t<span class="token punctuation">,</span> j<span class="token punctuation">]</span>            <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>self<span class="token punctuation">.</span>N<span class="token punctuation">)</span><span class="token punctuation">:</span>                gamma<span class="token punctuation">[</span>t<span class="token punctuation">,</span> i<span class="token punctuation">]</span> <span class="token operator">=</span> gamma<span class="token punctuation">[</span>t<span class="token punctuation">,</span> i<span class="token punctuation">]</span> <span class="token operator">/</span> denominator  <span class="token comment" spellcheck="true"># 除以总概率</span>    <span class="token comment" spellcheck="true"># 计算sai(i,j) 为给定训练序列O和模型lambda时：</span>    <span class="token comment" spellcheck="true"># t时刻 i状态，t+1时刻 j状态 在两个时刻N*N种可能性下的概率</span>    <span class="token comment" spellcheck="true"># 返回值 xi[T, N, N]</span>    <span class="token keyword">def</span> <span class="token function">ComputeXi</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> T<span class="token punctuation">,</span> O<span class="token punctuation">,</span> alpha<span class="token punctuation">,</span> beta<span class="token punctuation">,</span> gamma<span class="token punctuation">,</span> xi<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">for</span> t <span class="token keyword">in</span> range<span class="token punctuation">(</span>T <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            sum <span class="token operator">=</span> <span class="token number">0.0</span>            <span class="token comment" spellcheck="true"># t时刻时候，把xi序列填满</span>            <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>self<span class="token punctuation">.</span>N<span class="token punctuation">)</span><span class="token punctuation">:</span>                <span class="token keyword">for</span> j <span class="token keyword">in</span> range<span class="token punctuation">(</span>self<span class="token punctuation">.</span>N<span class="token punctuation">)</span><span class="token punctuation">:</span>                    <span class="token comment" spellcheck="true"># 前向x后向x i状态转移到j状态的概率aij x 下一个显示成O(t+1)的概率bj(o(t+1))</span>                    <span class="token comment" spellcheck="true"># t+1时刻是未知的，所以直接用beta[t + 1, j]</span>                    xi<span class="token punctuation">[</span>t<span class="token punctuation">,</span> i<span class="token punctuation">,</span> j<span class="token punctuation">]</span> <span class="token operator">=</span> alpha<span class="token punctuation">[</span>t<span class="token punctuation">,</span> i<span class="token punctuation">]</span> <span class="token operator">*</span> beta<span class="token punctuation">[</span>t <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> j<span class="token punctuation">]</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>A<span class="token punctuation">[</span>i<span class="token punctuation">,</span> j<span class="token punctuation">]</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>B<span class="token punctuation">[</span>j<span class="token punctuation">,</span> O<span class="token punctuation">[</span>t <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span>                    sum <span class="token operator">+=</span> xi<span class="token punctuation">[</span>t<span class="token punctuation">,</span> i<span class="token punctuation">,</span> j<span class="token punctuation">]</span>            <span class="token comment" spellcheck="true"># 归一化</span>            <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>self<span class="token punctuation">.</span>N<span class="token punctuation">)</span><span class="token punctuation">:</span>                <span class="token keyword">for</span> j <span class="token keyword">in</span> range<span class="token punctuation">(</span>self<span class="token punctuation">.</span>N<span class="token punctuation">)</span><span class="token punctuation">:</span>                    xi<span class="token punctuation">[</span>t<span class="token punctuation">,</span> i<span class="token punctuation">,</span> j<span class="token punctuation">]</span> <span class="token operator">/=</span> sum    <span class="token comment" spellcheck="true"># Baum-Welch算法</span>    <span class="token comment" spellcheck="true"># 输入 L个观察序列O，初始模型：HMM=&amp;#123;A,B,pi,N,M&amp;#125;</span>    <span class="token keyword">def</span> <span class="token function">BaumWelch</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> L<span class="token punctuation">,</span> T<span class="token punctuation">,</span> O<span class="token punctuation">,</span> alpha<span class="token punctuation">,</span> beta<span class="token punctuation">,</span> gamma<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true">#  初试参数</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Baum-Welch"</span><span class="token punctuation">)</span>        DELTA <span class="token operator">=</span> <span class="token number">0.01</span>        round <span class="token operator">=</span> <span class="token number">0</span>        flag <span class="token operator">=</span> <span class="token number">1</span>        probf <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0.0</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># 只有 probf[0]的怪胎</span>        delta <span class="token operator">=</span> <span class="token number">0.0</span>        deltaprev <span class="token operator">=</span> <span class="token number">0.0</span>        probprev <span class="token operator">=</span> <span class="token number">0.0</span>        ratio <span class="token operator">=</span> <span class="token number">0.0</span>        deltaprev <span class="token operator">=</span> <span class="token number">10e</span><span class="token operator">-</span><span class="token number">70</span>        <span class="token comment" spellcheck="true">#  初始化</span>        xi <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>T<span class="token punctuation">,</span> self<span class="token punctuation">.</span>N<span class="token punctuation">,</span> self<span class="token punctuation">.</span>N<span class="token punctuation">)</span><span class="token punctuation">)</span>           <span class="token comment" spellcheck="true"># 9x2x2</span>        pi <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>T<span class="token punctuation">)</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>float<span class="token punctuation">)</span>                 <span class="token comment" spellcheck="true"># 1x9</span>        denominatorA <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>N<span class="token punctuation">)</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>float<span class="token punctuation">)</span>         <span class="token comment" spellcheck="true"># 1x2 分母</span>        denominatorB <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>N<span class="token punctuation">)</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>float<span class="token punctuation">)</span>         <span class="token comment" spellcheck="true"># 1x2</span>        numeratorA <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>N<span class="token punctuation">,</span> self<span class="token punctuation">.</span>N<span class="token punctuation">)</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>float<span class="token punctuation">)</span>      <span class="token comment" spellcheck="true"># 2x2  分子</span>        numeratorB <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>N<span class="token punctuation">,</span> self<span class="token punctuation">.</span>M<span class="token punctuation">)</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>float<span class="token punctuation">)</span>      <span class="token comment" spellcheck="true"># 2x2  NxM</span>        scale <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>T<span class="token punctuation">)</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>float<span class="token punctuation">)</span>                <span class="token comment" spellcheck="true"># 1x9</span>        <span class="token keyword">while</span> <span class="token boolean">True</span><span class="token punctuation">:</span>            probf<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>            <span class="token comment" spellcheck="true"># E - step</span>            <span class="token comment" spellcheck="true"># L个长度为T的观测序列，由于齐次性，忽略时间t参数对 lamda(A, B, pi)的影响</span>            <span class="token keyword">for</span> l <span class="token keyword">in</span> range<span class="token punctuation">(</span>L<span class="token punctuation">)</span><span class="token punctuation">:</span>                <span class="token comment" spellcheck="true"># self.ForwardWithScale(T, O[l], alpha, scale, probf)</span>                self<span class="token punctuation">.</span>Forward<span class="token punctuation">(</span>T<span class="token punctuation">,</span> O<span class="token punctuation">[</span>l<span class="token punctuation">]</span><span class="token punctuation">,</span> alpha<span class="token punctuation">,</span> probf<span class="token punctuation">)</span>                self<span class="token punctuation">.</span>Backword<span class="token punctuation">(</span>T<span class="token punctuation">,</span> O<span class="token punctuation">[</span>l<span class="token punctuation">]</span><span class="token punctuation">,</span> beta<span class="token punctuation">,</span> probf<span class="token punctuation">)</span>                <span class="token comment" spellcheck="true"># self.BackwardWithScale(T, O[l], beta, scale)</span>                self<span class="token punctuation">.</span>ComputeGamma<span class="token punctuation">(</span>T<span class="token punctuation">,</span> alpha<span class="token punctuation">,</span> beta<span class="token punctuation">,</span> gamma<span class="token punctuation">)</span>                self<span class="token punctuation">.</span>ComputeXi<span class="token punctuation">(</span>T<span class="token punctuation">,</span> O<span class="token punctuation">[</span>l<span class="token punctuation">]</span><span class="token punctuation">,</span> alpha<span class="token punctuation">,</span> beta<span class="token punctuation">,</span> gamma<span class="token punctuation">,</span> xi<span class="token punctuation">)</span>                <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>self<span class="token punctuation">.</span>N<span class="token punctuation">)</span><span class="token punctuation">:</span>                    pi<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">+=</span> gamma<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> i<span class="token punctuation">]</span>   <span class="token comment" spellcheck="true"># pi初始值+0时刻处于i的概率，用L个初态做修正</span>                    <span class="token keyword">for</span> t <span class="token keyword">in</span> range<span class="token punctuation">(</span>T <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                        <span class="token comment" spellcheck="true"># A和B有区别吗 ？？？？</span>                        denominatorA<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">+=</span> gamma<span class="token punctuation">[</span>t<span class="token punctuation">,</span> i<span class="token punctuation">]</span>   <span class="token comment" spellcheck="true"># 1x2 满足观测序列，所有状态为i的概率</span>                        denominatorB<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">+=</span> gamma<span class="token punctuation">[</span>t<span class="token punctuation">,</span> i<span class="token punctuation">]</span>   <span class="token comment" spellcheck="true"># 1x2</span>                    denominatorB<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">+=</span> gamma<span class="token punctuation">[</span>T <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">,</span> i<span class="token punctuation">]</span>   <span class="token comment" spellcheck="true"># B加上了最后一个的状态</span>                    <span class="token keyword">for</span> j <span class="token keyword">in</span> range<span class="token punctuation">(</span>self<span class="token punctuation">.</span>N<span class="token punctuation">)</span><span class="token punctuation">:</span>                        <span class="token keyword">for</span> t <span class="token keyword">in</span> range<span class="token punctuation">(</span>T <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                            numeratorA<span class="token punctuation">[</span>i<span class="token punctuation">,</span> j<span class="token punctuation">]</span> <span class="token operator">+=</span> xi<span class="token punctuation">[</span>t<span class="token punctuation">,</span> i<span class="token punctuation">,</span> j<span class="token punctuation">]</span>  <span class="token comment" spellcheck="true"># 满足观测序列，从i到j的概率</span>                    <span class="token keyword">for</span> k <span class="token keyword">in</span> range<span class="token punctuation">(</span>self<span class="token punctuation">.</span>M<span class="token punctuation">)</span><span class="token punctuation">:</span>                        <span class="token keyword">for</span> t <span class="token keyword">in</span> range<span class="token punctuation">(</span>T<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># 所有序列中，满足的个数</span>                            <span class="token keyword">if</span> O<span class="token punctuation">[</span>l<span class="token punctuation">]</span><span class="token punctuation">[</span>t<span class="token punctuation">]</span> <span class="token operator">==</span> k<span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># 观测序列t中 我们正在考察的那个</span>                                numeratorB<span class="token punctuation">[</span>i<span class="token punctuation">,</span> k<span class="token punctuation">]</span> <span class="token operator">+=</span> gamma<span class="token punctuation">[</span>t<span class="token punctuation">,</span> i<span class="token punctuation">]</span>  <span class="token comment" spellcheck="true"># 由于i状态 导致 观测为k 的概率！</span>            <span class="token comment" spellcheck="true"># M - step</span>            <span class="token comment" spellcheck="true"># 重估 pi 状态转移矩阵A 和 观察概率矩阵B</span>            <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>self<span class="token punctuation">.</span>N<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment" spellcheck="true">## 0.001 / self.N 平滑</span>                self<span class="token punctuation">.</span>pi<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0.001</span> <span class="token operator">/</span> self<span class="token punctuation">.</span>N <span class="token operator">+</span> <span class="token number">0.999</span> <span class="token operator">*</span> pi<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">/</span> L  <span class="token comment" spellcheck="true"># pi[i]在E-step算了 L次</span>                <span class="token keyword">for</span> j <span class="token keyword">in</span> range<span class="token punctuation">(</span>self<span class="token punctuation">.</span>N<span class="token punctuation">)</span><span class="token punctuation">:</span>                    <span class="token comment" spellcheck="true"># 这里不用管，因为numeratorA[i, j] 和 denominatorA[i] 都算了 L次</span>                    self<span class="token punctuation">.</span>A<span class="token punctuation">[</span>i<span class="token punctuation">,</span> j<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0.001</span> <span class="token operator">/</span> self<span class="token punctuation">.</span>N <span class="token operator">+</span> <span class="token number">0.999</span> <span class="token operator">*</span> numeratorA<span class="token punctuation">[</span>i<span class="token punctuation">,</span> j<span class="token punctuation">]</span> <span class="token operator">/</span> denominatorA<span class="token punctuation">[</span>i<span class="token punctuation">]</span>                    numeratorA<span class="token punctuation">[</span>i<span class="token punctuation">,</span> j<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0.0</span>  <span class="token comment" spellcheck="true"># 为了下一轮的迭代</span>                <span class="token keyword">for</span> k <span class="token keyword">in</span> range<span class="token punctuation">(</span>self<span class="token punctuation">.</span>M<span class="token punctuation">)</span><span class="token punctuation">:</span>                    self<span class="token punctuation">.</span>B<span class="token punctuation">[</span>i<span class="token punctuation">,</span> k<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0.001</span> <span class="token operator">/</span> self<span class="token punctuation">.</span>M <span class="token operator">+</span> <span class="token number">0.999</span> <span class="token operator">*</span> numeratorB<span class="token punctuation">[</span>i<span class="token punctuation">,</span> k<span class="token punctuation">]</span> <span class="token operator">/</span> denominatorB<span class="token punctuation">[</span>i<span class="token punctuation">]</span>                    numeratorB<span class="token punctuation">[</span>i<span class="token punctuation">,</span> k<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0.0</span>                <span class="token comment" spellcheck="true"># 为了下一轮的迭代的初始化</span>                pi<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> denominatorA<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> denominatorB<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0.0</span>            <span class="token comment" spellcheck="true"># Flag决定此次迭代，直接下一次迭代</span>            <span class="token comment" spellcheck="true"># 没有更新参数？ A[i, j]，B[i, k]，pi[i]在上面已经更新完了</span>            <span class="token comment" spellcheck="true"># 没有把更新的参数保存下来</span>            <span class="token keyword">if</span> flag <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>      <span class="token comment" spellcheck="true"># 没有更新参数？</span>                flag <span class="token operator">=</span> <span class="token number">0</span>                probprev <span class="token operator">=</span> probf<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>                ratio <span class="token operator">=</span> <span class="token number">1</span>                <span class="token keyword">continue</span>            delta <span class="token operator">=</span> probf<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">-</span> probprev  <span class="token comment" spellcheck="true"># 更新的大小</span>            ratio <span class="token operator">=</span> delta <span class="token operator">/</span> deltaprev   <span class="token comment" spellcheck="true"># 和上一次相比更新的速率</span>            probprev <span class="token operator">=</span> probf<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>    <span class="token comment" spellcheck="true"># 上一次参数存储一轮</span>            deltaprev <span class="token operator">=</span> delta      <span class="token comment" spellcheck="true"># 上一次更新大小存储下来吗。。。。</span>            round <span class="token operator">+=</span> <span class="token number">1</span>            <span class="token comment" spellcheck="true"># ratio 更新率小于阈值 0.01，结束EM迭代</span>            <span class="token keyword">if</span> ratio <span class="token operator">&lt;=</span> DELTA<span class="token punctuation">:</span>                <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Num iteration： "</span><span class="token punctuation">,</span> round<span class="token punctuation">)</span>                <span class="token keyword">break</span><span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">"__main__"</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Python my HMM"</span><span class="token punctuation">)</span>    A <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.8125</span><span class="token punctuation">,</span> <span class="token number">0.1875</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token number">0.8</span><span class="token punctuation">]</span><span class="token punctuation">]</span>    B <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.875</span><span class="token punctuation">,</span> <span class="token number">0.125</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0.25</span><span class="token punctuation">,</span> <span class="token number">0.75</span><span class="token punctuation">]</span><span class="token punctuation">]</span>    pi <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">]</span>    hmm <span class="token operator">=</span> HMM<span class="token punctuation">(</span>A<span class="token punctuation">,</span> B<span class="token punctuation">,</span> pi<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># N=2，2个状态； M=2，2种观测结果</span>    O <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span>    L <span class="token operator">=</span> len<span class="token punctuation">(</span>O<span class="token punctuation">)</span>     <span class="token comment" spellcheck="true"># L=3</span>    T <span class="token operator">=</span> len<span class="token punctuation">(</span>O<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># T=9，时间长度序列为9？</span>    alpha <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>T<span class="token punctuation">,</span> hmm<span class="token punctuation">.</span>N<span class="token punctuation">)</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>float<span class="token punctuation">)</span>   <span class="token comment" spellcheck="true"># 9x2</span>    beta <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>T<span class="token punctuation">,</span> hmm<span class="token punctuation">.</span>N<span class="token punctuation">)</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>float<span class="token punctuation">)</span>    gamma <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>T<span class="token punctuation">,</span> hmm<span class="token punctuation">.</span>N<span class="token punctuation">)</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>float<span class="token punctuation">)</span>    hmm<span class="token punctuation">.</span>BaumWelch<span class="token punctuation">(</span>L<span class="token punctuation">,</span> T<span class="token punctuation">,</span> O<span class="token punctuation">,</span> alpha<span class="token punctuation">,</span> beta<span class="token punctuation">,</span> gamma<span class="token punctuation">)</span>    a <span class="token operator">=</span> hmm<span class="token punctuation">.</span>viterbi<span class="token punctuation">(</span>O<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    hmm<span class="token punctuation">.</span>printhmm<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><h2 id="参考说明"><a href="#参考说明" class="headerlink" title="参考说明"></a>参考说明</h2><p><a href="https://www.bilibili.com/video/BV1MW41167Rf?from=search&amp;seid=13221277013126441324">白板视频</a></p><p><a href="https://blog.csdn.net/YZXnuaa/article/details/79525488">参考1</a></p><p><a href="https://wenku.baidu.com/view/2f0d944769eae009581bec04.html">参考2</a></p><p><a href="https://www.cnblogs.com/liuerdou/p/11332428.html">参考3</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> nlp </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 命名实体 </tag>
            
            <tag> 序列标注 </tag>
            
            <tag> 马尔科夫 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HMM - 有监督</title>
      <link href="/2020/08/03/hmm-you-jian-du/"/>
      <url>/2020/08/03/hmm-you-jian-du/</url>
      
        <content type="html"><![CDATA[<h1 id="隐马尔科夫"><a href="#隐马尔科夫" class="headerlink" title="隐马尔科夫"></a>隐马尔科夫</h1><p>未完,待续………..</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">from</span> utils <span class="token keyword">import</span> <span class="token operator">*</span><span class="token keyword">from</span> tqdm <span class="token keyword">import</span> tqdm<span class="token keyword">class</span> <span class="token class-name">HMM_NER</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> char2idx_path<span class="token punctuation">,</span> tag2idx_path<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># 载入一些字典</span>        <span class="token comment" spellcheck="true"># char2idx: 字 转换为 token</span>        self<span class="token punctuation">.</span>char2idx <span class="token operator">=</span> load_dict<span class="token punctuation">(</span>char2idx_path<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># tag2idx: 标签转换为 token</span>        self<span class="token punctuation">.</span>tag2idx <span class="token operator">=</span> load_dict<span class="token punctuation">(</span>tag2idx_path<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># idx2tag: token转换为标签</span>        self<span class="token punctuation">.</span>idx2tag <span class="token operator">=</span> <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;v: k for k, v in self.tag2idx.items()&amp;#125;</span>        <span class="token comment" spellcheck="true"># 初始化隐状态数量(实体标签数)和观测数量(字数)</span>        self<span class="token punctuation">.</span>tag_size <span class="token operator">=</span> len<span class="token punctuation">(</span>self<span class="token punctuation">.</span>tag2idx<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>vocab_size <span class="token operator">=</span> max<span class="token punctuation">(</span><span class="token punctuation">[</span>v <span class="token keyword">for</span> _<span class="token punctuation">,</span> v <span class="token keyword">in</span> self<span class="token punctuation">.</span>char2idx<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1</span>        <span class="token comment" spellcheck="true"># 初始化A, B, pi为全0</span>        self<span class="token punctuation">.</span>transition <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">[</span>self<span class="token punctuation">.</span>tag_size<span class="token punctuation">,</span>                                    self<span class="token punctuation">.</span>tag_size<span class="token punctuation">]</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>emission <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">[</span>self<span class="token punctuation">.</span>tag_size<span class="token punctuation">,</span>                                  self<span class="token punctuation">.</span>vocab_size<span class="token punctuation">]</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>pi <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>self<span class="token punctuation">.</span>tag_size<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 偏置, 用来防止log(0)或乘0的情况</span>        self<span class="token punctuation">.</span>epsilon <span class="token operator">=</span> <span class="token number">1e</span><span class="token operator">-</span><span class="token number">8</span>    <span class="token keyword">def</span> <span class="token function">fit</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> train_dic_path<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""        fit用来训练HMM模型        :param train_dic_path: 训练数据目录        """</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"initialize training..."</span><span class="token punctuation">)</span>        train_dic <span class="token operator">=</span> load_data<span class="token punctuation">(</span>train_dic_path<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 估计转移概率矩阵, 发射概率矩阵和初始概率矩阵的参数</span>        self<span class="token punctuation">.</span>estimate_transition_and_initial_probs<span class="token punctuation">(</span>train_dic<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>estimate_emission_probs<span class="token punctuation">(</span>train_dic<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># take the logarithm</span>        <span class="token comment" spellcheck="true"># 取log防止计算结果下溢</span>        self<span class="token punctuation">.</span>pi <span class="token operator">=</span> np<span class="token punctuation">.</span>log<span class="token punctuation">(</span>self<span class="token punctuation">.</span>pi<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>transition <span class="token operator">=</span> np<span class="token punctuation">.</span>log<span class="token punctuation">(</span>self<span class="token punctuation">.</span>transition<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>emission <span class="token operator">=</span> np<span class="token punctuation">.</span>log<span class="token punctuation">(</span>self<span class="token punctuation">.</span>emission<span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"DONE!"</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">estimate_emission_probs</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> train_dic<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""        发射矩阵参数的估计        estimate p( Observation | Hidden_state )        :param train_dic:        :return:        """</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"estimating emission probabilities..."</span><span class="token punctuation">)</span>        <span class="token keyword">for</span> dic <span class="token keyword">in</span> tqdm<span class="token punctuation">(</span>train_dic<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">for</span> char<span class="token punctuation">,</span> tag <span class="token keyword">in</span> zip<span class="token punctuation">(</span>dic<span class="token punctuation">[</span><span class="token string">"text"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dic<span class="token punctuation">[</span><span class="token string">"label"</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                self<span class="token punctuation">.</span>emission<span class="token punctuation">[</span>self<span class="token punctuation">.</span>tag2idx<span class="token punctuation">[</span>tag<span class="token punctuation">]</span><span class="token punctuation">,</span>                              self<span class="token punctuation">.</span>char2idx<span class="token punctuation">[</span>char<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">+=</span> <span class="token number">1</span>        self<span class="token punctuation">.</span>emission<span class="token punctuation">[</span>self<span class="token punctuation">.</span>emission <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> self<span class="token punctuation">.</span>epsilon        self<span class="token punctuation">.</span>emission <span class="token operator">/=</span> np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>self<span class="token punctuation">.</span>emission<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> keepdims<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">estimate_transition_and_initial_probs</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> train_dic<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""        转移矩阵和初始概率的参数估计, 也就是bigram二元模型        estimate p( Y_t+1 | Y_t )        :param train_dic:        :return:        """</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"estimating transition and initial probabilities..."</span><span class="token punctuation">)</span>        <span class="token keyword">for</span> dic <span class="token keyword">in</span> tqdm<span class="token punctuation">(</span>train_dic<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">for</span> i<span class="token punctuation">,</span> tag <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>dic<span class="token punctuation">[</span><span class="token string">"label"</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                <span class="token keyword">if</span> i <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>                    self<span class="token punctuation">.</span>pi<span class="token punctuation">[</span>self<span class="token punctuation">.</span>tag2idx<span class="token punctuation">[</span>tag<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">+=</span> <span class="token number">1</span>                curr_tag <span class="token operator">=</span> self<span class="token punctuation">.</span>tag2idx<span class="token punctuation">[</span>tag<span class="token punctuation">]</span>                next_tag <span class="token operator">=</span> self<span class="token punctuation">.</span>tag2idx<span class="token punctuation">[</span>dic<span class="token punctuation">[</span><span class="token string">"label"</span><span class="token punctuation">]</span><span class="token punctuation">[</span>i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span>                self<span class="token punctuation">.</span>transition<span class="token punctuation">[</span>curr_tag<span class="token punctuation">,</span> next_tag<span class="token punctuation">]</span> <span class="token operator">+=</span> <span class="token number">1</span>        self<span class="token punctuation">.</span>transition<span class="token punctuation">[</span>self<span class="token punctuation">.</span>transition <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> self<span class="token punctuation">.</span>epsilon        self<span class="token punctuation">.</span>transition <span class="token operator">/=</span> np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>self<span class="token punctuation">.</span>transition<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> keepdims<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>pi<span class="token punctuation">[</span>self<span class="token punctuation">.</span>pi <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> self<span class="token punctuation">.</span>epsilon        self<span class="token punctuation">.</span>pi <span class="token operator">/=</span> np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>self<span class="token punctuation">.</span>pi<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">get_p_Obs_State</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> char<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># 计算p( observation | state)</span>        <span class="token comment" spellcheck="true"># 如果当前字属于未知, 则讲p( observation | state)设为均匀分布</span>        char_token <span class="token operator">=</span> self<span class="token punctuation">.</span>char2idx<span class="token punctuation">.</span>get<span class="token punctuation">(</span>char<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> char_token <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>            <span class="token keyword">return</span> np<span class="token punctuation">.</span>log<span class="token punctuation">(</span>np<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>self<span class="token punctuation">.</span>tag_size<span class="token punctuation">)</span><span class="token operator">/</span>self<span class="token punctuation">.</span>tag_size<span class="token punctuation">)</span>        <span class="token keyword">return</span> np<span class="token punctuation">.</span>ravel<span class="token punctuation">(</span>self<span class="token punctuation">.</span>emission<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> char_token<span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">predict</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> text<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># 预测并打印出预测结果</span>        <span class="token comment" spellcheck="true"># 维特比算法解码</span>        <span class="token keyword">if</span> len<span class="token punctuation">(</span>text<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>            <span class="token keyword">raise</span> NotImplementedError<span class="token punctuation">(</span><span class="token string">"输入文本为空!"</span><span class="token punctuation">)</span>        best_tag_id <span class="token operator">=</span> self<span class="token punctuation">.</span>viterbi_decode<span class="token punctuation">(</span>text<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>print_func<span class="token punctuation">(</span>text<span class="token punctuation">,</span> best_tag_id<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">print_func</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> text<span class="token punctuation">,</span> best_tags_id<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># 用来打印预测结果</span>        <span class="token keyword">for</span> char<span class="token punctuation">,</span> tag_id <span class="token keyword">in</span> zip<span class="token punctuation">(</span>text<span class="token punctuation">,</span> best_tags_id<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">print</span><span class="token punctuation">(</span>char<span class="token operator">+</span><span class="token string">"_"</span><span class="token operator">+</span>self<span class="token punctuation">.</span>idx2tag<span class="token punctuation">[</span>tag_id<span class="token punctuation">]</span><span class="token operator">+</span><span class="token string">"|"</span><span class="token punctuation">,</span> end<span class="token operator">=</span><span class="token string">""</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">viterbi_decode</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> text<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""        维特比解码, 详见视频教程或文字版教程        :param text: 一段文本string        :return: 最可能的隐状态路径        """</span>        <span class="token comment" spellcheck="true"># 得到序列长度</span>        seq_len <span class="token operator">=</span> len<span class="token punctuation">(</span>text<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 初始化T1和T2表格</span>        T1_table <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">[</span>seq_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>tag_size<span class="token punctuation">]</span><span class="token punctuation">)</span>        T2_table <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">[</span>seq_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>tag_size<span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 得到第1时刻的发射概率</span>        start_p_Obs_State <span class="token operator">=</span> self<span class="token punctuation">.</span>get_p_Obs_State<span class="token punctuation">(</span>text<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 计算第一步初始概率, 填入表中</span>        T1_table<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> self<span class="token punctuation">.</span>pi <span class="token operator">+</span> start_p_Obs_State        T2_table<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>nan        <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> seq_len<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token comment" spellcheck="true"># 维特比算法在每一时刻计算落到每一个隐状态的最大概率和路径</span>            <span class="token comment" spellcheck="true"># 并把他们暂存起来</span>            <span class="token comment" spellcheck="true"># 这里用到了矩阵化计算方法, 详见视频教程</span>            p_Obs_State <span class="token operator">=</span> self<span class="token punctuation">.</span>get_p_Obs_State<span class="token punctuation">(</span>text<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span>            p_Obs_State <span class="token operator">=</span> np<span class="token punctuation">.</span>expand_dims<span class="token punctuation">(</span>p_Obs_State<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>            prev_score <span class="token operator">=</span> np<span class="token punctuation">.</span>expand_dims<span class="token punctuation">(</span>T1_table<span class="token punctuation">[</span>i<span class="token number">-1</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>            <span class="token comment" spellcheck="true"># 广播算法, 发射概率和转移概率广播 + 转移概率</span>            curr_score <span class="token operator">=</span> prev_score <span class="token operator">+</span> self<span class="token punctuation">.</span>transition <span class="token operator">+</span> p_Obs_State            <span class="token comment" spellcheck="true"># 存入T1 T2中</span>            T1_table<span class="token punctuation">[</span>i<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>max<span class="token punctuation">(</span>curr_score<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>            T2_table<span class="token punctuation">[</span>i<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>curr_score<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 回溯</span>        best_tag_id <span class="token operator">=</span> int<span class="token punctuation">(</span>np<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>T1_table<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        best_tags <span class="token operator">=</span> <span class="token punctuation">[</span>best_tag_id<span class="token punctuation">,</span> <span class="token punctuation">]</span>        <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>seq_len<span class="token number">-1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            best_tag_id <span class="token operator">=</span> int<span class="token punctuation">(</span>T2_table<span class="token punctuation">[</span>i<span class="token punctuation">,</span> best_tag_id<span class="token punctuation">]</span><span class="token punctuation">)</span>            best_tags<span class="token punctuation">.</span>append<span class="token punctuation">(</span>best_tag_id<span class="token punctuation">)</span>        <span class="token keyword">return</span> list<span class="token punctuation">(</span>reversed<span class="token punctuation">(</span>best_tags<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>    model <span class="token operator">=</span> HMM_NER<span class="token punctuation">(</span>char2idx_path<span class="token operator">=</span><span class="token string">"./dicts/char2idx.json"</span><span class="token punctuation">,</span>                    tag2idx_path<span class="token operator">=</span><span class="token string">"./dicts/tag2idx.json"</span><span class="token punctuation">)</span>    model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span><span class="token string">"./corpus/train_data.txt"</span><span class="token punctuation">)</span>    model<span class="token punctuation">.</span>predict<span class="token punctuation">(</span><span class="token string">"我在中国吃美国的面包"</span><span class="token punctuation">)</span></code></pre><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> nlp </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 命名实体 </tag>
            
            <tag> 序列标注 </tag>
            
            <tag> 马尔科夫 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>FastText</title>
      <link href="/2020/07/26/fasttext/"/>
      <url>/2020/07/26/fasttext/</url>
      
        <content type="html"><![CDATA[<h1 id="Fasttext"><a href="#Fasttext" class="headerlink" title="Fasttext"></a>Fasttext</h1><p>Paper:<a href="https://arxiv.org/pdf/1607.01759.pdf">Bag of Tricks for Efficient Text Classification(2016)</a></p><p>code:<a href="https://gitee.com/daiyizheng/nlp/blob/master/1-3FastText/FastText-torch.py">Torch</a></p><h2 id="Fasttext-特点"><a href="#Fasttext-特点" class="headerlink" title="Fasttext 特点"></a>Fasttext 特点</h2><ol><li>模型简单，只有一层的隐层以及输出层，因此训练速度非常快</li><li>不需要训练词向量，Fasttext自己会训练</li><li>两个优化：Hierarchical Softmax、N-gram</li></ol><h2 id="Fasttext模型架构"><a href="#Fasttext模型架构" class="headerlink" title="Fasttext模型架构"></a>Fasttext模型架构</h2><p>　　fastText模型架构和word2vec中的CBOW很相似， 不同之处是fastText预测标签而CBOW预测的是中间词，即模型架构类似但是模型的任务不同。</p><h3 id="CBOW架构"><a href="#CBOW架构" class="headerlink" title="CBOW架构"></a>CBOW架构</h3><p><img src="/2020/07/26/fasttext/2.png" alt="CBOW"></p><p>　　word2vec将上下文关系转化为多分类任务，进而训练逻辑回归模型，这里的类别数量|V|词库大小。</p><p>通常的文本数据中，词库少则数万，多则百万，在训练中直接训练多分类逻辑回归并不现实。</p><p>word2vec中提供了两种针对大规模多分类问题的优化手段， negative sampling 和hierarchical softmax。</p><p>在优化中，negative sampling 只更新少量负面类，从而减轻了计算量。</p><p>hierarchical softmax 将词库表示成前缀树，从树根到叶子的路径可以表示为一系列二分类器，一次多分类计算的复杂度从|V|降低到了树的高度。整体的复杂度从。</p><h3 id="Fasttext架构"><a href="#Fasttext架构" class="headerlink" title="Fasttext架构"></a>Fasttext架构</h3><p><img src="/2020/07/26/fasttext/3.png" alt="Fasttext"></p><p>　　　fastText模型架构:其中$x _1,x _2,…,x _{N−1},x _N$表示一个文本中的n-gram向量，每个特征是词向量的平均值。这和前文中提到的cbow相似，cbow用上下文去预测中心词，而此处用全部的n-gram去预测指定类别。</p><ul><li>[Math Processing Error] 一个句子的特征，初始值为随机生成(也可以采用预训练的词向量)</li><li>hidden:$X _i$的平均值 <em>x</em></li><li>output: 样本标签</li></ul><h4 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h4><p><img src="/2020/07/26/fasttext/4.png"></p><p>N:样本个数</p><p>$y _n$:第n个样本对应的类别</p><p>f:损失函数softmaxt</p><p>$x _n$:第n个样本的归一化特征</p><p>A：权重矩阵（构建词，embedding）</p><p>B：权重举证（隐层到输出层）</p><p>词向量初始化</p><p>一个句子的embedding为[$iw _1,iw _2,….iw _n,ow _1,ow _2,…ow _s$]</p><p> $iw _i$:语料中出现的词，排在数组的前面</p><p>$ow _i$:n-gram或n-char特征</p><p>初始化为随机数, 如果提供预训练的词向量，对应的词采用预训练的词向量</p><h4 id="层次softmax"><a href="#层次softmax" class="headerlink" title="层次softmax"></a>层次softmax</h4><p><img src="/2020/07/26/fasttext/5.png"></p><p>​    当语料类别较多时，使用hierarchical Softmax(hs)减轻计算量 hs利用Huffman 树实现，词(生成词向量)或label(分类问题)作为叶子节点 根据词或label的count构建Huffman 树，则叶子到root一定存在一条路径利用逻辑回归二分类计算loss</p><h3 id="n-gram和n-char"><a href="#n-gram和n-char" class="headerlink" title="n-gram和n-char"></a>n-gram和n-char</h3><p>Fasttext方法不同与word2vec方法，引入了两类特征并进行embedding。其中n-gram颗粒度是词与词之间，n-char是单个词之间。两类特征的存储均通过计算hash值的方法实现。 </p><p>n-gram</p><p>示例: who am I? n-gram设置为2</p><p>n-gram特征有，who, who am, am, am I, I<br>n-char<br>示例: where, n=3, 设置起止符&lt;, &gt;<br>n-char特征有，&lt;wh, whe, her, ere, er&gt;</p><h2 id="FastText词向量与word2vec对比"><a href="#FastText词向量与word2vec对比" class="headerlink" title="FastText词向量与word2vec对比"></a>FastText词向量与word2vec对比</h2><ul><li>模型的输出层 <ul><li>word2vec的输出层，对应下一个单词的概率最大值；fasttext的输出层是分类的label</li></ul></li><li>模型的输入层：<ul><li>word2vec的输入层，是 context window 内的word；而fasttext 对应的整个sentence的内容，包括word，也包括 n-gram的内容；</li></ul></li></ul><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> fasttext<span class="token comment" spellcheck="true">## 1 is positive, 0 is negative</span>f <span class="token operator">=</span> open<span class="token punctuation">(</span><span class="token string">'train.txt'</span><span class="token punctuation">,</span> <span class="token string">'w'</span><span class="token punctuation">)</span>f<span class="token punctuation">.</span>write<span class="token punctuation">(</span><span class="token string">'__label__1 i love you\n'</span><span class="token punctuation">)</span>f<span class="token punctuation">.</span>write<span class="token punctuation">(</span><span class="token string">'__label__1 he loves me\n'</span><span class="token punctuation">)</span>f<span class="token punctuation">.</span>write<span class="token punctuation">(</span><span class="token string">'__label__1 she likes baseball\n'</span><span class="token punctuation">)</span>f<span class="token punctuation">.</span>write<span class="token punctuation">(</span><span class="token string">'__label__0 i hate you\n'</span><span class="token punctuation">)</span>f<span class="token punctuation">.</span>write<span class="token punctuation">(</span><span class="token string">'__label__0 sorry for that\n'</span><span class="token punctuation">)</span>f<span class="token punctuation">.</span>write<span class="token punctuation">(</span><span class="token string">'__label__0 this is awful'</span><span class="token punctuation">)</span>f<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>f <span class="token operator">=</span> open<span class="token punctuation">(</span><span class="token string">'test.txt'</span><span class="token punctuation">,</span> <span class="token string">'w'</span><span class="token punctuation">)</span>f<span class="token punctuation">.</span>write<span class="token punctuation">(</span><span class="token string">'sorry hate you'</span><span class="token punctuation">)</span>f<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">## 训练</span>trainDataFile <span class="token operator">=</span> <span class="token string">'train.txt'</span>classifier <span class="token operator">=</span> fasttext<span class="token punctuation">.</span>train_supervised<span class="token punctuation">(</span>    input<span class="token operator">=</span>trainDataFile<span class="token punctuation">,</span> <span class="token comment" spellcheck="true">## 文件输入</span>    label_prefix<span class="token operator">=</span><span class="token string">'__label__'</span><span class="token punctuation">,</span>　<span class="token comment" spellcheck="true">## label前缀</span>    dim<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token comment" spellcheck="true"># 词向量维度</span>    epoch<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>　<span class="token comment" spellcheck="true">## epoch次数</span>    lr<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>　<span class="token comment" spellcheck="true"># 学习率</span>    lr_update_rate<span class="token operator">=</span><span class="token number">50</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true">#　多少步更新学习率</span>    min_count<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true"># 最少的单词出现次数</span>    loss<span class="token operator">=</span><span class="token string">'softmax'</span><span class="token punctuation">,</span>　<span class="token comment" spellcheck="true">## loss function &amp;#123;ns, hs, softmax&amp;#125; [softmax] </span>    word_ngrams<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>　<span class="token comment" spellcheck="true"># n-gram 窗口大小</span>    bucket<span class="token operator">=</span><span class="token number">1000000</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">## 叶子数</span><span class="token comment" spellcheck="true">## 保存模型</span>classifier<span class="token punctuation">.</span>save_model<span class="token punctuation">(</span><span class="token string">"Model.bin"</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">## 测试集测试</span>testDataFile <span class="token operator">=</span> <span class="token string">'test.txt'</span>classifier <span class="token operator">=</span> fasttext<span class="token punctuation">.</span>load_model<span class="token punctuation">(</span><span class="token string">'Model.bin'</span><span class="token punctuation">)</span>result <span class="token operator">=</span> classifier<span class="token punctuation">.</span>test<span class="token punctuation">(</span>testDataFile<span class="token punctuation">)</span><span class="token keyword">print</span> <span class="token punctuation">(</span><span class="token string">'测试集上数据量'</span><span class="token punctuation">,</span> result<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span> <span class="token punctuation">(</span><span class="token string">'测试集上准确率'</span><span class="token punctuation">,</span> result<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span> <span class="token punctuation">(</span><span class="token string">'测试集上召回率'</span><span class="token punctuation">,</span> result<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>res <span class="token operator">=</span> classifier<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>text<span class="token operator">=</span>testDataFile<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>res<span class="token punctuation">)</span></code></pre><h2 id="Fasttext-参数"><a href="#Fasttext-参数" class="headerlink" title="Fasttext 参数"></a>Fasttext 参数</h2><pre class=" language-shell"><code class="language-shell">The following arguments are mandatory:  -input              training file path  -output             output file pathThe following arguments are optional:  -verbose            verbosity level [2]The following arguments for the dictionary are optional:  -minCount           minimal number of word occurrences [1]  -minCountLabel      minimal number of label occurrences [0]  -wordNgrams         max length of word ngram [1]  -bucket             number of buckets [2000000]  -minn               min length of char ngram [0]  -maxn               max length of char ngram [0]  -t                  sampling threshold [0.0001]  -label              labels prefix [__label__]The following arguments for training are optional:  -lr                 learning rate [0.1]  -lrUpdateRate       change the rate of updates for the learning rate [100]  -dim                size of word vectors [100]  -ws                 size of the context window [5]  -epoch              number of epochs [5]  -neg                number of negatives sampled [5]  -loss               loss function &#123;ns, hs, softmax&#125; [softmax]  -thread             number of threads [12]  -pretrainedVectors  pretrained word vectors for supervised learning []  -saveOutput         whether output params should be saved [0]The following arguments for quantization are optional:  -cutoff             number of words and ngrams to retain [0]  -retrain            finetune embeddings if a cutoff is applied [0]  -qnorm              quantizing the norm separately [0]  -qout               quantizing the classifier [0]  -dsub               size of each sub-vector [2]</code></pre><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> nlp </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 文本分类 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CRF</title>
      <link href="/2020/07/25/crf/"/>
      <url>/2020/07/25/crf/</url>
      
        <content type="html"><![CDATA[<h1 id="CRF"><a href="#CRF" class="headerlink" title="CRF"></a>CRF</h1><h2 id="CRF-能做什么"><a href="#CRF-能做什么" class="headerlink" title="CRF 能做什么"></a>CRF 能做什么</h2><p>​     CRF是这样的一个模型。我们这里只讨论Linear Chain CRF模型，不涉及更general的CRF模型。大部分时候同学们能够接触到的CRF模型都是Linear Chain CRF。是一个序列化标注算法（sequence labeling algorithm），接收一个输入序列如 $X = (x _1, x _2,…. x _n)$ 并且输出目标序列$Y = (y _1, y _2,…. y _n)$ 。</p><h2 id="CRF与其他"><a href="#CRF与其他" class="headerlink" title="CRF与其他"></a>CRF与其他</h2><p>隐马尔可夫模型（Hidden Markov Model，HMM），最大熵马尔可夫模型（Maximum Entropy Markov Model，MEMM）以及条件随机场（Conditional Random Field，CRF）是序列标注中最常用也是最基本的三个模型。</p><p>HMM首先出现，MEMM其次，CRF最后。三个算法主要思想如下：</p><p>1）HMM模型是对转移概率和表现概率直接建模，统计共现概率，HMM就是典型的概率有向图，其就是概率有向图的计算概率方式。</p><p>2）MEMM模型是对转移概率和表现概率建立联合概率，统计时统计的是条件概率，但MEMM容易陷入局部最优，是因为MEMM只在局部做归一化。</p><p>3）CRF模型中，统计了全局概率，在 做归一化时，考虑了数据在全局的分布，而不是仅仅在局部归一化，这样就解决了MEMM中的标记偏置（label bias）的问题。</p><p><strong>本质上，CRF有以下三个优点：</strong></p><p>1）与HMM比较，CRF没有HMM那样严格的独立性假设条件，因而可以容纳任意的上下文信息。特征设计灵活（与ME一样） </p><p>2）与与MEMM比较，由于CRF计算全局最优输出节点的条件概率，它还克服了最大熵马尔可夫模型标记偏置（Label-bias）的缺点。 ­­</p><p>3）CRF是在给定需要标记的观察序列的<strong>条件下</strong>，计算整个标记序列的联合概率分布，而不是在给定当前状态条件下，定义下一个状态的状态分布。凡事都有两面，正由于这些优点，CRF需要训练的参数更多，与MEMM和HMM相比，它存在训练代价大、复杂度高的缺点。 </p><h2 id="适用场景"><a href="#适用场景" class="headerlink" title="适用场景"></a>适用场景</h2><p>​      目前序列标注的使用方法: HMM、CRF、LSTM/Bi-LSTM、LSTM/Bi-LSTM+CRF</p><p>​      采用Bi-LSTM+CRF就是结合了Bi-LSTM的特征表达能力与CRF的无向图生成模型的优点，成为经典就是必然。其典型架构如下图：</p>   <img src="/2020/07/25/crf/1.jpg" style="zoom:80%;"><p> 注：在Bi-LSTM+CRF架构中，CRF最终的计算基于状态转移概率矩阵和发射概率矩阵(均指非归一化概率)。而Bi-LSTM的输出就充当了上述发射概率矩阵的角色。</p><h2 id="原理解析"><a href="#原理解析" class="headerlink" title="原理解析"></a>原理解析</h2><p>​       直接利用LSTM进行序列标注。但是这样的做法有一个问题：<strong>每个时刻的输出没有考虑上一时刻的输出。</strong>以“我 喜欢 跑步”为例，LSTM输出“喜欢”的标签是“动词”，而“跑步”的标签可能也是“动词”。但是实际上，“名词”标签更为合适，因为“跑步”这里是一项运动。也就是“动词”+“名词”这个规则并没有被LSTM模型捕捉到。也就是说这样使用LSTM<strong>无法对标签转移关系进行建模</strong>。<strong>CRF有两类特征函数，一类是针对观测序列与状态的对应关系（如“我”一般是“名词”），一类是针对状态间关系（如“动词”后一般跟“名词”）。在LSTM+CRF模型中，前一类特征函数的输出由LSTM的输出替代，后一类特征函数就变成了标签转移矩阵</strong>。</p><h2 id="标签的score和损失函数的定义"><a href="#标签的score和损失函数的定义" class="headerlink" title="标签的score和损失函数的定义"></a>标签的score和损失函数的定义</h2><p>Bi-LSTM 最后输出一般是［batch_size,seq_len, target_size］，target_size相当于是每个词映射到tag的发射概率值，设Bi-LSTM的输出矩阵为${P} _{ij}$代表词$w _i$映射到$tag _j$的非归一化概率。对于CRF来说，我们假定存在一个转移矩阵${A} _{ij}$，代表$tag _i$转移到$tag _j$的转移概率。 对于输入序列 <strong>X</strong> 对应的输出tag序列 y，定义分数为</p><p>$s(X, y) = {\sum} ^{n} _{i=0}{A} _{ {y} _{i+1} , y _i} + {\sum} ^{n} _{i=0}{P} _{i, y _i}$</p><p>利用Softmax函数，我们为每一个正确的tag序列y定义一个概率值（$\widetilde { Y } $代表所有的tag序列，包括不可能出现的）</p><p>$P(y|X) = \frac { {e} ^{s(X, y)} }{ {\sum} _{\widetilde { Y } \in {Y} _{X}}{ {e} ^{s(X, \widetilde { Y })} } } $</p><p>因而在训练中，我们只需要最大化似然概率p(y|X)即可，这里我们利用对数似然</p><p>$log(p(y|X)) = log(\frac { {e} ^{s(X, y)} }{ {\sum} _{\widetilde { Y } \in {Y} _{X}}{ {e} ^{s(X, \widetilde { Y })} } }) = S(X, y) - log({ {\sum} _{\widetilde { Y } \in {Y} _{X}}{ {e} ^{s(X, \widetilde { Y })} } })$</p><p>所以我们将损失函数定义为-log(p(y|X))，就可以利用梯度下降法来进行网络的学习了。 <strong>loss function:</strong></p><p>$Loss = log({ {\sum} _{\widetilde { Y } \in {Y} _{X}}{ {e} ^{s(X, \widetilde { Y })} } })　- S(X, y)$</p><p>在对损失函数进行计算的时候，S(X,y)的计算很简单，而</p><p>$log({ {\sum} _{\widetilde { Y } \in {Y} _{X}}{ {e} ^{s(X, \widetilde { Y })}}})$</p><p>（下面记作logsumexp）的计算稍微复杂一些，因为需要计算每一条可能路径的分数。这里用一种简便的方法，对于到词${w} _{i+1}$的路径，可以先把到词$w _i$的logsumexp计算出来，因为</p><p>$log({\sum} _{e}^{log({\sum}{e} ^{x} )+ y})  = log({\sum} {\sum} {e} ^{x+y})$</p><p>因此先计算每一步的路径分数和直接计算全局分数相同，但这样可以大大减少计算的时间。</p><p>我们进一步简化</p><p>$Z(X, y) = {e} ^{s(X, \widetilde { Y })}$</p><blockquote><p>其中的logsumexp是各种科学运算库（numpy, pytorch）中都会提供的基本运算。</p><p>$log{\sum} _{i} {e} ^{Z(X, y)} = max _i {Z _i} + log ({\sum} _{i}{e (Z _i - max _i (Z _i))})$</p></blockquote><h2 id="Pytorch-官方代码解析"><a href="#Pytorch-官方代码解析" class="headerlink" title="Pytorch 官方代码解析"></a><a href="https://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html?highlight=crf">Pytorch 官方代码解析</a></h2><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">import</span> torch<span class="token punctuation">.</span>autograd <span class="token keyword">as</span> autograd<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optimtorch<span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">argmax</span><span class="token punctuation">(</span>vec<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># return the argmax as a python int</span>    _<span class="token punctuation">,</span> idx <span class="token operator">=</span> torch<span class="token punctuation">.</span>max<span class="token punctuation">(</span>vec<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> idx<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">prepare_sequence</span><span class="token punctuation">(</span>seq<span class="token punctuation">,</span> to_ix<span class="token punctuation">)</span><span class="token punctuation">:</span>    idxs <span class="token operator">=</span> <span class="token punctuation">[</span>to_ix<span class="token punctuation">[</span>w<span class="token punctuation">]</span> <span class="token keyword">for</span> w <span class="token keyword">in</span> seq<span class="token punctuation">]</span>    <span class="token keyword">return</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>idxs<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>long<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># TODO 以一种数值稳定性算法求log_sum_exp: 先标准化，取出行中最大值进行broadcast，让每个元素减去最大值后求exp然后再求和</span><span class="token keyword">def</span> <span class="token function">log_sum_exp</span><span class="token punctuation">(</span>vec<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment" spellcheck="true">## z(x) -> log(∑_i exp(z_i)) =  max_i(z_i) + log(∑_i)exp(z_i - max_i(z_i))</span>    max_score <span class="token operator">=</span> vec<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> argmax<span class="token punctuation">(</span>vec<span class="token punctuation">)</span><span class="token punctuation">]</span>    max_score_broadcast <span class="token operator">=</span> max_score<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>expand<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> vec<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> max_score <span class="token operator">+</span> torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>vec <span class="token operator">-</span> max_score_broadcast<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">class</span> <span class="token class-name">BiLSTM_CRF</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> vocab_size<span class="token punctuation">,</span> tag_to_ix<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>BiLSTM_CRF<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>embedding_dim <span class="token operator">=</span> embedding_dim        self<span class="token punctuation">.</span>hidden_dim <span class="token operator">=</span> hidden_dim        self<span class="token punctuation">.</span>vocab_size <span class="token operator">=</span> vocab_size        self<span class="token punctuation">.</span>tag_to_ix <span class="token operator">=</span> tag_to_ix        self<span class="token punctuation">.</span>tagset_size <span class="token operator">=</span> len<span class="token punctuation">(</span>tag_to_ix<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># tag_size : 3 + 2</span>        self<span class="token punctuation">.</span>word_embeds <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>lstm <span class="token operator">=</span> nn<span class="token punctuation">.</span>LSTM<span class="token punctuation">(</span>embedding_dim<span class="token punctuation">,</span> hidden_dim <span class="token operator">//</span> <span class="token number">2</span><span class="token punctuation">,</span> num_layers<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bidirectional<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># TODO 把LSTM 输出映射到状态空间（tag space）</span>        self<span class="token punctuation">.</span>hidden2tag <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">,</span> self<span class="token punctuation">.</span>tagset_size<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># TODO 状态转移矩阵参数:T(i,j)表示从状态 j 转移到状态 i 的概率，这样第i行就是所有状态转移到状态i的概率</span>        self<span class="token punctuation">.</span>transitions <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>self<span class="token punctuation">.</span>tagset_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>tagset_size<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># TODO 根据上面矩阵元素定义：限制不能从其他状态转移到起始状态，不能从结束状态转移到其他任何状态</span>        self<span class="token punctuation">.</span>transitions<span class="token punctuation">.</span>data<span class="token punctuation">[</span>tag_to_ix<span class="token punctuation">[</span>START_TAG<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">10000</span>        self<span class="token punctuation">.</span>transitions<span class="token punctuation">.</span>data<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> tag_to_ix<span class="token punctuation">[</span>STOP_TAG<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">10000</span>        self<span class="token punctuation">.</span>hidden <span class="token operator">=</span> self<span class="token punctuation">.</span>init_hidden<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> sentence<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># dont confuse this with _forward_alg above.</span>        <span class="token comment" spellcheck="true"># Get the emission scores from the BiLSTM</span>        lstm_feats <span class="token operator">=</span> self<span class="token punctuation">.</span>_get_lstm_features<span class="token punctuation">(</span>sentence<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Find the best path, given the features.</span>        score<span class="token punctuation">,</span> tag_seq <span class="token operator">=</span> self<span class="token punctuation">.</span>_viterbi_decode<span class="token punctuation">(</span>lstm_feats<span class="token punctuation">)</span>        <span class="token keyword">return</span> score<span class="token punctuation">,</span> tag_seq    <span class="token keyword">def</span> <span class="token function">init_hidden</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_dim <span class="token operator">//</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_dim <span class="token operator">//</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token triple-quoted-string string">'''        _forward_alg求出的是损失函数的log-sum-exp这一项，另一项比较简单        因为计算误差score需要计算每一条可能路径的分数。这里用一种简便的方法，对于到词w_(i+1)的路径，        可以先把到词w_i的log-sum-exp计算出来，然后累加，类似递推的思想    '''</span>    <span class="token keyword">def</span> <span class="token function">_forward_alg</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> feats<span class="token punctuation">)</span><span class="token punctuation">:</span>        init_alphas <span class="token operator">=</span> torch<span class="token punctuation">.</span>full<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>tagset_size<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">10000</span><span class="token punctuation">.</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># TODO 起始状态score定义为0</span>        init_alphas<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span>self<span class="token punctuation">.</span>tag_to_ix<span class="token punctuation">[</span>START_TAG<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">.</span>        forward_var <span class="token operator">=</span> init_alphas        <span class="token comment" spellcheck="true"># TODO 依次遍历句子中的所有词</span>        <span class="token comment" spellcheck="true"># feats : (seq_len,tag_size) LSTM映射到tag space的结果</span>        <span class="token keyword">for</span> feat <span class="token keyword">in</span> feats<span class="token punctuation">:</span>            <span class="token comment" spellcheck="true">## TODO 句子中没有cls sep或者start end</span>            alphas_t <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>  <span class="token comment" spellcheck="true"># 当前时间步的forward tensor</span>            <span class="token keyword">for</span> next_tag <span class="token keyword">in</span> range<span class="token punctuation">(</span>self<span class="token punctuation">.</span>tagset_size<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># TODO 遍历当前时间步(word)的所有可能状态</span>                <span class="token comment" spellcheck="true"># TODO 广播当前状态值为tag_size大小，用于和转移到当前时间步的那些状态操作，即 transiton[next_tag]</span>                emit_score <span class="token operator">=</span> feat<span class="token punctuation">[</span>next_tag<span class="token punctuation">]</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>expand<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>tagset_size<span class="token punctuation">)</span>                <span class="token comment" spellcheck="true"># TODO 其他状态转移到next_tag（当前状态）的概率</span>                trans_score <span class="token operator">=</span> self<span class="token punctuation">.</span>transitions<span class="token punctuation">[</span>next_tag<span class="token punctuation">]</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>                <span class="token comment" spellcheck="true"># TODO next_tag_var[i] 是 计算log-sum-exp 之前 状态 i -> 到状态 next_tag 值</span>                next_tag_var <span class="token operator">=</span> forward_var <span class="token operator">+</span> trans_score <span class="token operator">+</span> emit_score                alphas_t<span class="token punctuation">.</span>append<span class="token punctuation">(</span>log_sum_exp<span class="token punctuation">(</span>next_tag_var<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># view(1) 把scalar变为[scalar]</span>            <span class="token comment" spellcheck="true"># TODO 此时 forward_var:size (1，tag_size) 作为下一个词word_(i+1)的初始值</span>            forward_var <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>alphas_t<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># TODO 最优加上所有状态转移到结束状态的值</span>        terminal_var <span class="token operator">=</span> forward_var <span class="token operator">+</span> self<span class="token punctuation">.</span>transitions<span class="token punctuation">[</span>self<span class="token punctuation">.</span>tag_to_ix<span class="token punctuation">[</span>STOP_TAG<span class="token punctuation">]</span><span class="token punctuation">]</span>        <span class="token comment" spellcheck="true"># TODO 求出最终log-sum-exp值</span>        alpha <span class="token operator">=</span> log_sum_exp<span class="token punctuation">(</span>terminal_var<span class="token punctuation">)</span>        <span class="token keyword">return</span> alpha    <span class="token keyword">def</span> <span class="token function">_get_lstm_features</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> sentence<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>hidden <span class="token operator">=</span> self<span class="token punctuation">.</span>init_hidden<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># self.word_embeds(sentence).shape : (seq_len,voca_size)</span>        embeds <span class="token operator">=</span> self<span class="token punctuation">.</span>word_embeds<span class="token punctuation">(</span>sentence<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>len<span class="token punctuation">(</span>sentence<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># seq_len x batch_size x voca_size</span>        lstm_out<span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden <span class="token operator">=</span> self<span class="token punctuation">.</span>lstm<span class="token punctuation">(</span>embeds<span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden<span class="token punctuation">)</span>        lstm_out <span class="token operator">=</span> lstm_out<span class="token punctuation">.</span>view<span class="token punctuation">(</span>len<span class="token punctuation">(</span>sentence<span class="token punctuation">)</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_dim<span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#[seq_len, batch_size, hiidden_size]</span>        lstm_feats <span class="token operator">=</span> self<span class="token punctuation">.</span>hidden2tag<span class="token punctuation">(</span>lstm_out<span class="token punctuation">)</span>        <span class="token keyword">return</span> lstm_feats    <span class="token triple-quoted-string string">'''        这里求出损失函数另一项    '''</span>    <span class="token keyword">def</span> <span class="token function">_score_sentence</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> feats<span class="token punctuation">,</span> tags<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># Gives the score of a provided tag sequence</span>        score <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># TODO 加上起始状态</span>        tags <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>self<span class="token punctuation">.</span>tag_to_ix<span class="token punctuation">[</span>START_TAG<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>long<span class="token punctuation">)</span><span class="token punctuation">,</span> tags<span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># TODO 根据损失函数定义 transitions[tags[i + 1], tags[i]] 表示从 状态i到状态(i+1)的概率</span>        <span class="token comment" spellcheck="true"># TODO feat[tag[i]] 表示 word[i] 到 tag[i] 的发射概率，这里用 tag[i+1] 因为前面填充了起始状态，</span>        <span class="token keyword">for</span> i<span class="token punctuation">,</span> feat <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>feats<span class="token punctuation">)</span><span class="token punctuation">:</span>            score <span class="token operator">=</span> score <span class="token operator">+</span> self<span class="token punctuation">.</span>transitions<span class="token punctuation">[</span>tags<span class="token punctuation">[</span>i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> tags<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">+</span> feat<span class="token punctuation">[</span>tags<span class="token punctuation">[</span>i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span>        score <span class="token operator">=</span> score <span class="token operator">+</span> self<span class="token punctuation">.</span>transitions<span class="token punctuation">[</span>self<span class="token punctuation">.</span>tag_to_ix<span class="token punctuation">[</span>STOP_TAG<span class="token punctuation">]</span><span class="token punctuation">,</span> tags<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span>        <span class="token keyword">return</span> score    <span class="token keyword">def</span> <span class="token function">_viterbi_decode</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> feats<span class="token punctuation">)</span><span class="token punctuation">:</span>        backpointers <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        <span class="token comment" spellcheck="true"># Initialieze the viterbi variables in log spac</span>        init_vvars <span class="token operator">=</span> torch<span class="token punctuation">.</span>full<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>tagset_size<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">10000</span><span class="token punctuation">.</span><span class="token punctuation">)</span>        init_vvars<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span>self<span class="token punctuation">.</span>tag_to_ix<span class="token punctuation">[</span>START_TAG<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>        <span class="token comment" spellcheck="true"># forward_var at step i holds the viterbi variables for step i-1</span>        forward_var <span class="token operator">=</span> init_vvars        <span class="token keyword">for</span> feat <span class="token keyword">in</span> feats<span class="token punctuation">:</span>            bptrs_t <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>  <span class="token comment" spellcheck="true"># holds the backpointers for this step</span>            viterbivars_t <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>  <span class="token comment" spellcheck="true"># holds the viterbi variables for this step</span>            <span class="token keyword">for</span> next_tag <span class="token keyword">in</span> range<span class="token punctuation">(</span>self<span class="token punctuation">.</span>tagset_size<span class="token punctuation">)</span><span class="token punctuation">:</span>                <span class="token comment" spellcheck="true"># next_tag_var[i] holds the viterbi variable for tag i at the</span>                <span class="token comment" spellcheck="true"># previous step, plus the score of transitioning</span>                <span class="token comment" spellcheck="true"># from tag i to next_tag.</span>                <span class="token comment" spellcheck="true"># We don't include the emission scores here because the max</span>                <span class="token comment" spellcheck="true"># does not depend on them (we add them in below)</span>                next_tag_var <span class="token operator">=</span> forward_var <span class="token operator">+</span> self<span class="token punctuation">.</span>transitions<span class="token punctuation">[</span>next_tag<span class="token punctuation">]</span>                best_tag_id <span class="token operator">=</span> argmax<span class="token punctuation">(</span>next_tag_var<span class="token punctuation">)</span>                bptrs_t<span class="token punctuation">.</span>append<span class="token punctuation">(</span>best_tag_id<span class="token punctuation">)</span>                viterbivars_t<span class="token punctuation">.</span>append<span class="token punctuation">(</span>next_tag_var<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span>best_tag_id<span class="token punctuation">]</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>            <span class="token comment" spellcheck="true"># Now add in the emission scores, and assign forward_var to the set</span>            <span class="token comment" spellcheck="true"># of viterbi variables we just computed</span>            forward_var <span class="token operator">=</span> <span class="token punctuation">(</span>torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>viterbivars_t<span class="token punctuation">)</span> <span class="token operator">+</span> feat<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>            backpointers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>bptrs_t<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Transition to STOP_TAG</span>        terminal_var <span class="token operator">=</span> forward_var <span class="token operator">+</span> self<span class="token punctuation">.</span>transitions<span class="token punctuation">[</span>self<span class="token punctuation">.</span>tag_to_ix<span class="token punctuation">[</span>STOP_TAG<span class="token punctuation">]</span><span class="token punctuation">]</span>        best_tag_id <span class="token operator">=</span> argmax<span class="token punctuation">(</span>terminal_var<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># TODO 到达STOP_TAG最大的分数</span>        path_score <span class="token operator">=</span> terminal_var<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span>best_tag_id<span class="token punctuation">]</span>        <span class="token comment" spellcheck="true"># Follow the back pointers to decode the best path.</span>        best_path <span class="token operator">=</span> <span class="token punctuation">[</span>best_tag_id<span class="token punctuation">]</span>        <span class="token keyword">for</span> bptrs_t <span class="token keyword">in</span> reversed<span class="token punctuation">(</span>backpointers<span class="token punctuation">)</span><span class="token punctuation">:</span>            best_tag_id <span class="token operator">=</span> bptrs_t<span class="token punctuation">[</span>best_tag_id<span class="token punctuation">]</span>            best_path<span class="token punctuation">.</span>append<span class="token punctuation">(</span>best_tag_id<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Pop off the start tag (we dont want to return that to the caller)</span>        start <span class="token operator">=</span> best_path<span class="token punctuation">.</span>pop<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">assert</span> start <span class="token operator">==</span> self<span class="token punctuation">.</span>tag_to_ix<span class="token punctuation">[</span>START_TAG<span class="token punctuation">]</span>  <span class="token comment" spellcheck="true"># Sanity check</span>        best_path<span class="token punctuation">.</span>reverse<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> path_score<span class="token punctuation">,</span> best_path    <span class="token keyword">def</span> <span class="token function">neg_log_likelihood</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> sentence<span class="token punctuation">,</span> tags<span class="token punctuation">)</span><span class="token punctuation">:</span>        feats <span class="token operator">=</span> self<span class="token punctuation">.</span>_get_lstm_features<span class="token punctuation">(</span>sentence<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># size : (seq_len,tag_size)</span>        forward_score <span class="token operator">=</span> self<span class="token punctuation">.</span>_forward_alg<span class="token punctuation">(</span>feats<span class="token punctuation">)</span>        gold_score <span class="token operator">=</span> self<span class="token punctuation">.</span>_score_sentence<span class="token punctuation">(</span>feats<span class="token punctuation">,</span> tags<span class="token punctuation">)</span>        <span class="token keyword">return</span> forward_score <span class="token operator">-</span> gold_scoreSTART_TAG <span class="token operator">=</span> <span class="token string">"&lt;START>"</span>STOP_TAG <span class="token operator">=</span> <span class="token string">"&lt;STOP>"</span>EMBEDDING_DIM <span class="token operator">=</span> <span class="token number">5</span>HIDDEN_DIM <span class="token operator">=</span> <span class="token number">4</span><span class="token comment" spellcheck="true"># Make up some training data</span>training_data <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">(</span>    <span class="token string">"the wall street journal reported today that apple corporation made money"</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token string">"B I I I O O O B I O O"</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>    <span class="token string">"georgia tech is a university in georgia"</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token string">"B I O O O O B"</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span>word_to_ix <span class="token operator">=</span> <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;&amp;#125;</span><span class="token keyword">for</span> sentence<span class="token punctuation">,</span> tags <span class="token keyword">in</span> training_data<span class="token punctuation">:</span>    <span class="token keyword">for</span> word <span class="token keyword">in</span> sentence<span class="token punctuation">:</span>        <span class="token keyword">if</span> word <span class="token operator">not</span> <span class="token keyword">in</span> word_to_ix<span class="token punctuation">:</span>            word_to_ix<span class="token punctuation">[</span>word<span class="token punctuation">]</span> <span class="token operator">=</span> len<span class="token punctuation">(</span>word_to_ix<span class="token punctuation">)</span>tag_to_ix <span class="token operator">=</span> <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;"B": 0, "I": 1, "O": 2, START_TAG: 3, STOP_TAG: 4&amp;#125;</span>ix_to_tag <span class="token operator">=</span> <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;&amp;#125;</span><span class="token keyword">for</span> k<span class="token punctuation">,</span> v <span class="token keyword">in</span> tag_to_ix<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    ix_to_tag<span class="token punctuation">[</span>v<span class="token punctuation">]</span> <span class="token operator">=</span> kmodel <span class="token operator">=</span> BiLSTM_CRF<span class="token punctuation">(</span>len<span class="token punctuation">(</span>word_to_ix<span class="token punctuation">)</span><span class="token punctuation">,</span> tag_to_ix<span class="token punctuation">,</span> EMBEDDING_DIM<span class="token punctuation">,</span> HIDDEN_DIM<span class="token punctuation">)</span>optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">,</span> weight_decay<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># # Check predictions before training</span><span class="token comment" spellcheck="true"># with torch.no_grad():</span><span class="token comment" spellcheck="true">#     precheck_sent = prepare_sequence(training_data[0][0], word_to_ix)</span><span class="token comment" spellcheck="true">#     precheck_tags = torch.tensor([tag_to_ix[t] for t in training_data[0][1]], dtype=torch.long)</span><span class="token comment" spellcheck="true">#     # print(model(precheck_sent))</span><span class="token comment" spellcheck="true"># Make sure prepare_sequence from earlier in the LSTM section is loaded</span><span class="token keyword">for</span> epoch <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">300</span><span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># again, normally you would NOT do 300 epochs, it is toy data</span>    <span class="token keyword">for</span> sentence<span class="token punctuation">,</span> tags <span class="token keyword">in</span> training_data<span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># Step 1. Remember that Pytorch accumulates gradients.</span>        <span class="token comment" spellcheck="true"># We need to clear them out before each instance</span>        model<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Step 2. Get our inputs ready for the network, that is,</span>        <span class="token comment" spellcheck="true"># turn them into Tensors of word indices.</span>        sentence_in <span class="token operator">=</span> prepare_sequence<span class="token punctuation">(</span>sentence<span class="token punctuation">,</span> word_to_ix<span class="token punctuation">)</span>        targets <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>tag_to_ix<span class="token punctuation">[</span>t<span class="token punctuation">]</span> <span class="token keyword">for</span> t <span class="token keyword">in</span> tags<span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>long<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Step 3. Run our forward pass.</span>        <span class="token comment" spellcheck="true"># TODO 这里区别于一般的训练模型，一般模型是从forward的输出用于计算误差</span>        <span class="token comment" spellcheck="true"># TODO 而这里因为加上了CRF模型，在neg_log_likelihood定义了CRF误差用于梯度更新</span>        loss <span class="token operator">=</span> model<span class="token punctuation">.</span>neg_log_likelihood<span class="token punctuation">(</span>sentence_in<span class="token punctuation">,</span> targets<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Step 4. Compute the loss, gradients, and update the parameters by</span>        <span class="token comment" spellcheck="true"># calling optimizer.step()</span>        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><h2 id="viterbi-解码"><a href="#viterbi-解码" class="headerlink" title="viterbi 解码"></a>viterbi 解码</h2><p>具体请参考　<a href="/2020/07/08/viterbi-suan-fa/" title="viterbi算法">viterbi算法</a></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://zhuanlan.zhihu.com/p/59845590">参考１</a></p><p><a href="https://www.seoxiehui.cn/article-215601-1.html">参考公式</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CRF </tag>
            
            <tag> 序列 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>知识问答KBQA</title>
      <link href="/2020/07/20/zhi-shi-wen-da-kbqa/"/>
      <url>/2020/07/20/zhi-shi-wen-da-kbqa/</url>
      
        <content type="html"><![CDATA[<h1 id="知识问答KBQA"><a href="#知识问答KBQA" class="headerlink" title="知识问答KBQA"></a>知识问答KBQA</h1><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>​    基于知识图谱的自动问答（Question Answering over Knowledge Base, 即 KBQA）问题的大概形式是，预先给定一个知识库（比如Freebase），知识库中包含着大量的先验知识数据，然后利用这些知识资源自动回答自然语言形态的问题（比如“肉夹馍是江苏的美食吗”，“虵今年多大了”等人民群众喜闻乐见的问题）。</p><h2 id="什么是知识库"><a href="#什么是知识库" class="headerlink" title="什么是知识库"></a>什么是知识库</h2><p>以知识为主要单位，实体为主要载体。一般来说，知识（或者事实）主要以三元组形式呈现：&lt;头实体，关系，尾实体&gt;，其中实体即人、地点、或特定概念等万物。举例来说，&lt;虵，改变了，中国&gt; 就是一条简单的三元组示例，其中头尾皆为知识库中固有的实体单元。</p><h2 id="方法框架"><a href="#方法框架" class="headerlink" title="方法框架"></a>方法框架</h2><p><img src="/2020/07/20/zhi-shi-wen-da-kbqa/1.png"></p><p>对着图说：假设要回答“Where was Leslie Cheung born”这个问题，主要分四步：</p><ol><li><strong>实体识别（Named Entity Recognition）</strong>，即把问题中的主要实体的名字从问题中抽出来，这样才知道应该去知识库中搜取哪个实体的信息来解决问题，即图中把“Leslie Cheung”这个人名抽出来；</li><li><strong>实体链接（Entity Linking）</strong>，把抽取出来的实体名和知识库中具体的实体对应起来，做这一步是因为，由于同名实体的存在，名字不是实体的唯一标识，实体独一无二的编号（id）才是，找到了实体名没卵用，必须要对应到知识库中具体的实体id，才能在知识库中把具体实体找到，获取相关信息。即图中将“Leslie Cheung”映射到“m.sdjk1s”这个 id 上（Freebase 的实体 id 是这个格式的）。这一步会存在一些问题，比如直接搜“姓名”叫“Leslie Cheung”的实体是搜不到的，因为“Leslie Cheung”其实是某个实体的“外文名”，他的“姓名”叫“张国荣”，以及有时候还会有多个叫“Leslie Cheung”的人。具体解决方式后面再说。</li><li><strong>关系预测（Relation Prediction），</strong>根据原问句中除去实体名以外的其他词语预测出应该从知识库中哪个关系去解答这个问题，<strong>是整个问题中最主要的一步</strong>。即图中从“Where was <e> born”预测出“people.person.place_of_birth”（Freebase 的关系名格式，翻译过来就是“出生地”）这个关系应该连接着问题的主要实体“Leslie Cheung”与这个问题的答案。</e></li><li><strong>找到了实体与关系</strong>，直接在知识库中把对应的三元组检索出来，即 “&lt;m.sdjk1s,<br>people.person.place_of_birth, m.s1kjds&gt;”，那么这条三元组的尾实体，即“m.s1kjds”就是问题的答案，查询其名字，就是“Hong Kong”。</li></ol><blockquote><p>KBQA 的解决方法有两个方向</p><ol><li>通过逻辑表达式直接生成 SPARQL（数据库查询语言，类似 SQL 那种）查数据库</li><li>就是按上面说的框架那四步，也是按照这个框架来做</li></ol></blockquote><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> nlp </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 三元组 </tag>
            
            <tag> 问答 </tag>
            
            <tag> 知识图谱 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>BPE分词算法</title>
      <link href="/2020/07/18/bpe-fen-ci-suan-fa/"/>
      <url>/2020/07/18/bpe-fen-ci-suan-fa/</url>
      
        <content type="html"><![CDATA[<h1 id="BPE-WordPiece"><a href="#BPE-WordPiece" class="headerlink" title="BPE(WordPiece)"></a>BPE(WordPiece)</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>2018年最火的论文要属google的BERT，不过今天我们不介绍BERT的模型，而是要介绍BERT中的一个小模块WordPiece。</p><h2 id="WordPiece原理"><a href="#WordPiece原理" class="headerlink" title="WordPiece原理"></a>WordPiece原理</h2><p>现在基本性能好一些的NLP模型，例如OpenAI GPT，google的BERT，在数据预处理的时候都会有WordPiece的过程。WordPiece字面理解是把word拆成piece一片一片，其实就是这个意思。</p><p>WordPiece的一种主要的实现方式叫做BPE（Byte-Pair Encoding）双字节编码。</p><p>BPE的过程可以理解为把一个单词再拆分，使得我们的此表会变得精简，并且寓意更加清晰。</p><p>比如”loved”,”loving”,”loves”这三个单词。其实本身的语义都是“爱”的意思，但是如果我们以单词为单位，那它们就算不一样的词，在英语中不同后缀的词非常的多，就会使得词表变的很大，训练速度变慢，训练的效果也不是太好。</p><p>BPE算法通过训练，能够把上面的3个单词拆分成”lov”,”ed”,”ing”,”es”几部分，这样可以把词的本身的意思和时态分开，有效的减少了词表的数量。</p><h2 id="BPE算法"><a href="#BPE算法" class="headerlink" title="BPE算法"></a>BPE算法</h2><p>BPE的大概训练过程：首先将词分成一个一个的字符，然后在词的范围内统计字符对出现的次数，每次将次数最多的字符对保存起来，直到循环次数结束。</p><p>我们模拟一下BPE算法。</p><p>我们原始词表如下：</p><p>{‘l o w e r ‘: 2, ‘n e w e s t ‘: 6, ‘w i d e s t ‘: 3, ‘l o w ‘: 5}</p><p>其中的key是词表的单词拆分层字母，再加代表结尾，value代表词出现的频率。</p><p>下面我们每一步在整张词表中找出频率最高相邻序列，并把它合并，依次循环。</p><pre><code>原始词表 &amp;#123;'l o w e r &lt;/w&gt;': 2, 'n e w e s t &lt;/w&gt;': 6, 'w i d e s t &lt;/w&gt;': 3, 'l o w &lt;/w&gt;': 5&amp;#125;出现最频繁的序列 ('s', 't') 9合并最频繁的序列后的词表 &amp;#123;'n e w e st &lt;/w&gt;': 6, 'l o w e r &lt;/w&gt;': 2, 'w i d e st &lt;/w&gt;': 3, 'l o w &lt;/w&gt;': 5&amp;#125;出现最频繁的序列 ('e', 'st') 9合并最频繁的序列后的词表 &amp;#123;'l o w e r &lt;/w&gt;': 2, 'l o w &lt;/w&gt;': 5, 'w i d est &lt;/w&gt;': 3, 'n e w est &lt;/w&gt;': 6&amp;#125;出现最频繁的序列 ('est', '&lt;/w&gt;') 9合并最频繁的序列后的词表 &amp;#123;'w i d est&lt;/w&gt;': 3, 'l o w e r &lt;/w&gt;': 2, 'n e w est&lt;/w&gt;': 6, 'l o w &lt;/w&gt;': 5&amp;#125;出现最频繁的序列 ('l', 'o') 7合并最频繁的序列后的词表 &amp;#123;'w i d est&lt;/w&gt;': 3, 'lo w e r &lt;/w&gt;': 2, 'n e w est&lt;/w&gt;': 6, 'lo w &lt;/w&gt;': 5&amp;#125;出现最频繁的序列 ('lo', 'w') 7合并最频繁的序列后的词表 &amp;#123;'w i d est&lt;/w&gt;': 3, 'low e r &lt;/w&gt;': 2, 'n e w est&lt;/w&gt;': 6, 'low &lt;/w&gt;': 5&amp;#125;出现最频繁的序列 ('n', 'e') 6合并最频繁的序列后的词表 &amp;#123;'w i d est&lt;/w&gt;': 3, 'low e r &lt;/w&gt;': 2, 'ne w est&lt;/w&gt;': 6, 'low &lt;/w&gt;': 5&amp;#125;出现最频繁的序列 ('w', 'est&lt;/w&gt;') 6合并最频繁的序列后的词表 &amp;#123;'w i d est&lt;/w&gt;': 3, 'low e r &lt;/w&gt;': 2, 'ne west&lt;/w&gt;': 6, 'low &lt;/w&gt;': 5&amp;#125;出现最频繁的序列 ('ne', 'west&lt;/w&gt;') 6合并最频繁的序列后的词表 &amp;#123;'w i d est&lt;/w&gt;': 3, 'low e r &lt;/w&gt;': 2, 'newest&lt;/w&gt;': 6, 'low &lt;/w&gt;': 5&amp;#125;出现最频繁的序列 ('low', '&lt;/w&gt;') 5合并最频繁的序列后的词表 &amp;#123;'w i d est&lt;/w&gt;': 3, 'low e r &lt;/w&gt;': 2, 'newest&lt;/w&gt;': 6, 'low&lt;/w&gt;': 5&amp;#125;出现最频繁的序列 ('i', 'd') 3合并最频繁的序列后的词表 &amp;#123;'w id est&lt;/w&gt;': 3, 'newest&lt;/w&gt;': 6, 'low&lt;/w&gt;': 5, 'low e r &lt;/w&gt;': 2&amp;#125;</code></pre><p>这样我们通过BPE得到了更加合适的词表了，这个词表可能会出现一些不是单词的组合，但是这个本身是有意义的一种形式，加速NLP的学习，提升不同词之间的语义的区分度。</p><ol><li><p>单词按照char分词， 单词结尾替换为某字符（\w or -）</p></li><li><p>构造vocab：将相连的char组成pair，频次为word的cnt</p></li><li><p>选取频次最高的max_pair，将vocab中的max_pair合并。 （频次最高的可能有多个pair，每次选择一个合并）<br> 参考代码：</p></li></ol><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> re<span class="token keyword">def</span> <span class="token function">process_raw_words</span><span class="token punctuation">(</span>words<span class="token punctuation">,</span> endtag<span class="token operator">=</span><span class="token string">'-'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">'''把单词分割成最小的符号，并且加上结尾符号'''</span>    vocabs <span class="token operator">=</span> <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;&amp;#125;</span>    <span class="token keyword">for</span> word<span class="token punctuation">,</span> count <span class="token keyword">in</span> words<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># 加上空格</span>        word <span class="token operator">=</span> re<span class="token punctuation">.</span>sub<span class="token punctuation">(</span>r<span class="token string">'([a-zA-Z])'</span><span class="token punctuation">,</span> r<span class="token string">' \1'</span><span class="token punctuation">,</span> word<span class="token punctuation">)</span>        word <span class="token operator">+=</span> <span class="token string">' '</span> <span class="token operator">+</span> endtag        vocabs<span class="token punctuation">[</span>word<span class="token punctuation">]</span> <span class="token operator">=</span> count    <span class="token keyword">return</span> vocabs<span class="token keyword">def</span> <span class="token function">get_symbol_pairs</span><span class="token punctuation">(</span>vocabs<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">''' 获得词汇中所有的字符pair，连续长度为2，并统计出现次数    Args:        vocabs: 单词dict，(word, count)单词的出现次数。单词已经分割为最小的字符    Returns:        pairs: ((符号1, 符号2), count)    '''</span>    <span class="token comment" spellcheck="true"># pairs = collections.defaultdict(int)</span>    pairs <span class="token operator">=</span> dict<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">for</span> word<span class="token punctuation">,</span> freq <span class="token keyword">in</span> vocabs<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># 单词里的符号</span>        symbols <span class="token operator">=</span> word<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>symbols<span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            p <span class="token operator">=</span> <span class="token punctuation">(</span>symbols<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> symbols<span class="token punctuation">[</span>i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>            pairs<span class="token punctuation">[</span>p<span class="token punctuation">]</span> <span class="token operator">=</span> pairs<span class="token punctuation">.</span>get<span class="token punctuation">(</span>p<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token operator">+</span> freq    <span class="token keyword">return</span> pairs<span class="token keyword">def</span> <span class="token function">merge_symbols</span><span class="token punctuation">(</span>symbol_pair<span class="token punctuation">,</span> vocabs<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">'''把vocabs中的所有单词中的'a b'字符串用'ab'替换    Args:        symbol_pair: (a, b) 两个符号        vocabs: 用subword(symbol)表示的单词，(word, count)。其中word使用subword空格分割    Returns:        vocabs_new: 替换'a b'为'ab'的新词汇表    '''</span>    vocabs_new <span class="token operator">=</span> <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;&amp;#125;</span>    raw <span class="token operator">=</span> <span class="token string">' '</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>symbol_pair<span class="token punctuation">)</span>    merged <span class="token operator">=</span> <span class="token string">''</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>symbol_pair<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 非字母和数字字符做转义</span>    bigram <span class="token operator">=</span> re<span class="token punctuation">.</span>escape<span class="token punctuation">(</span>raw<span class="token punctuation">)</span>    p <span class="token operator">=</span> re<span class="token punctuation">.</span>compile<span class="token punctuation">(</span>r<span class="token string">'(?&lt;!\S)'</span> <span class="token operator">+</span> bigram <span class="token operator">+</span> r<span class="token string">'(?!\S)'</span><span class="token punctuation">)</span>    <span class="token keyword">for</span> word<span class="token punctuation">,</span> count <span class="token keyword">in</span> vocabs<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        word_new <span class="token operator">=</span> p<span class="token punctuation">.</span>sub<span class="token punctuation">(</span>merged<span class="token punctuation">,</span> word<span class="token punctuation">)</span>        vocabs_new<span class="token punctuation">[</span>word_new<span class="token punctuation">]</span> <span class="token operator">=</span> count    <span class="token keyword">return</span> vocabs_newraw_words <span class="token operator">=</span> <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;"low": 5, "lower": 2, "newest": 6, "widest": 3&amp;#125;</span>vocabs <span class="token operator">=</span> process_raw_words<span class="token punctuation">(</span>raw_words<span class="token punctuation">)</span>num_merges <span class="token operator">=</span> <span class="token number">10</span><span class="token keyword">print</span><span class="token punctuation">(</span>vocabs<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># &lt;class 'dict'>: &amp;#123;' l o w -': 5, ' l o w e r -': 2, ' n e w e s t -': 6, ' w i d e s t -': 3&amp;#125;</span><span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>num_merges<span class="token punctuation">)</span><span class="token punctuation">:</span>    pairs <span class="token operator">=</span> get_symbol_pairs<span class="token punctuation">(</span>vocabs<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 选择出现频率最高的pair</span>    symbol_pair <span class="token operator">=</span> max<span class="token punctuation">(</span>pairs<span class="token punctuation">,</span> key<span class="token operator">=</span>pairs<span class="token punctuation">.</span>get<span class="token punctuation">)</span>    vocabs <span class="token operator">=</span> merge_symbols<span class="token punctuation">(</span>symbol_pair<span class="token punctuation">,</span> vocabs<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>vocabs<span class="token punctuation">)</span></code></pre><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> nlp </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
            <tag> 分词 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>mongodb docker 部署-非集群</title>
      <link href="/2020/07/08/mongodb-docker-bu-shu-fei-ji-qun/"/>
      <url>/2020/07/08/mongodb-docker-bu-shu-fei-ji-qun/</url>
      
        <content type="html"><![CDATA[<h1 id="Docker-部署-MongoDB"><a href="#Docker-部署-MongoDB" class="headerlink" title="Docker 部署 MongoDB"></a>Docker 部署 MongoDB</h1><h3 id="1-拉取-docker-镜像"><a href="#1-拉取-docker-镜像" class="headerlink" title="1. 拉取 docker 镜像"></a>1. 拉取 docker 镜像</h3><pre><code>docker pull mongo:3.4</code></pre><h3 id="2-运行"><a href="#2-运行" class="headerlink" title="2. 运行"></a>2. 运行</h3><pre><code>docker run -d --name mongodb --volume /root/docker/mongo/data:/data/db -p 27017:27017 mongo:3.4 --auth</code></pre><h3 id="3-进入-mongo"><a href="#3-进入-mongo" class="headerlink" title="3. 进入 mongo"></a>3. 进入 mongo</h3><pre><code>docker exec -it mongodb mongo</code></pre><h3 id="4-创建数据库帐号"><a href="#4-创建数据库帐号" class="headerlink" title="4. 创建数据库帐号"></a>4. 创建数据库帐号</h3><pre><code>use admin;db.createUser(&amp;#123; user: 'root', pwd: '123456', roles: [ &amp;#123; role: "root", db: "admin" &amp;#125; ] &amp;#125;);</code></pre><h3 id="5-安装-mongo-express-可视化工具"><a href="#5-安装-mongo-express-可视化工具" class="headerlink" title="5. 安装 mongo-express 可视化工具"></a>5. 安装 mongo-express 可视化工具</h3><pre><code>docker pull mongo-expressdocker run -d --name mongo-express -p 8081:8081 --link mongodb:mongo --env ME_CONFIG_MONGODB_ADMINUSERNAME='root' --env ME_CONFIG_MONGODB_ADMINPASSWORD='123456' mongo-express</code></pre><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 数据库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 非集群 </tag>
            
            <tag> mongo </tag>
            
            <tag> docker 部署 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>viterbi算法</title>
      <link href="/2020/07/08/viterbi-suan-fa/"/>
      <url>/2020/07/08/viterbi-suan-fa/</url>
      
        <content type="html"><![CDATA[<p>这一片博客通俗易懂，我这边直接拿过来了</p><p>转载至　<a href="https://www.zhihu.com/question/20136144">https://www.zhihu.com/question/20136144</a></p><h1 id="维特比算法"><a href="#维特比算法" class="headerlink" title="维特比算法"></a>维特比算法</h1><p><img src="/2020/07/08/viterbi-suan-fa/1.jpg"></p><p>​     viterbi维特比算法解决的是篱笆型的图的最短路径问题，图的节点按列组织，每列的节点数量可以不一样，每一列的节点只能和相邻列的节点相连，不能跨列相连，节点之间有着不同的距离，距离的值就不在图上一一标注出来了，大家自行脑补</p><p>答案：viterbi (维特比)算法。</p><p>过程非常简单：</p><p>为了找出S到E之间的最短路径，我们先从S开始从左到右一列一列地来看。</p><p>首先起点是S，从S到A列的路径有三种可能：S-A1、S-A2、S-A3，如下图：</p><p><img src="/2020/07/08/viterbi-suan-fa/2.jpg"></p><p>​    我们不能武断地说S-A1、S-A2、S-A3中的哪一段必定是全局最短路径中的一部分，目前为止任何一段都有可能是全局最短路径的备选项。</p><p>   我们继续往右看，到了B列。按B列的B1、B2、B3逐个分析。</p><p>先看B1：</p><p><img src="/2020/07/08/viterbi-suan-fa/3.jpg"></p><p>如上图，经过B1的所有路径只有3条：</p><p>S-A1-B1</p><p>S-A2-B1</p><p>S-A3-B1</p><p>以上这三条路径，各节点距离加起来对比一下，我们就可以知道其中哪一条是最短的。假设S-A3-B1是最短的，那么我们就知道了经过B1的所有路径当中S-A3-B1是最短的，其它两条路径路径S-A1-B1和S-A2-B1都比S-A3-B1长，绝对不是目标答案，可以大胆地删掉了。删掉了不可能是答案的路径，就是viterbi算法（维特比算法）的重点，因为后面我们再也不用考虑这些被删掉的路径了。现在经过B1的所有路径只剩一条路径了，如下图：</p><p><img src="/2020/07/08/viterbi-suan-fa/4.jpg"></p><p>接下来，我们继续看B2：</p><p><img src="/2020/07/08/viterbi-suan-fa/5.jpg"></p><p>同理，如上图，经过B2的路径有3条：</p><p>S-A1-B2</p><p>S-A2-B2</p><p>S-A3-B2</p><p>这三条路径中，各节点距离加起来对比一下，我们肯定也可以知道其中哪一条是最短的，假设S-A1-B2是最短的，那么我们就知道了经过B2的所有路径当中S-A1-B2是最短的，其它两条路径路径S-A2-B2和S-A3-B1也可以删掉了。经过B2所有路径只剩一条，如下图：</p><p><img src="/2020/07/08/viterbi-suan-fa/6.jpg"></p><p>接下来我们继续看B3：</p><p><img src="/2020/07/08/viterbi-suan-fa/7.jpg"></p><p>同理，如上图，经过B3的路径也有3条：</p><p>S-A1-B3</p><p>S-A2-B3</p><p>S-A3-B3</p><p>这三条路径中我们也肯定可以算出其中哪一条是最短的，假设S-A2-B3是最短的，那么我们就知道了经过B3的所有路径当中S-A2-B3是最短的，其它两条路径路径S-A1-B3和S-A3-B3也可以删掉了。经过B3的所有路径只剩一条，如下图：</p><p><img src="/2020/07/08/viterbi-suan-fa/8.jpg"></p><p>现在对于B列的所有节点我们都过了一遍，B列的每个节点我们都删除了一些不可能是答案的路径，看看我们剩下哪些备选的最短路径，如下图：</p><p><img src="/2020/07/08/viterbi-suan-fa/9.jpg"></p><p>上图是我们删掉了其它不可能是最短路径的情况，留下了三个有可能是最短的路径：S-A3-B1、S-A1-B2、S-A2-B3。现在我们将这三条备选的路径放在一起汇总到下图：</p><p><img src="/2020/07/08/viterbi-suan-fa/12.jpg"></p><p>S-A3-B1、S-A1-B2、S-A2-B3都有可能是全局的最短路径的备选路径，我们还没有足够的信息判断哪一条一定是全局最短路径的子路径。</p><p>   如果我们你认为没毛病就继续往下看C列，如果不理解，回头再看一遍，前面的步骤决定你是否能看懂viterbi算法（维特比算法）。</p><p>​    接下来讲到C列了，类似上面说的B列，我们从C1、C2、C3一个个节点分析。</p><p>经过C1节点的路径有：</p><p>S-A3-B1-C1、S-A1-B2-C1、S-A2-B3-C1</p><p><img src="/2020/07/08/viterbi-suan-fa/13.jpg"></p><p>和B列的做法一样，从这三条路径中找到最短的那条（假定是S-A3-B1-C1），其它两条路径同样道理可以删掉了。那么经过C1的所有路径只剩一条，如下图：</p><p><img src="/2020/07/08/viterbi-suan-fa/14.jpg"></p><p>同理，我们可以找到经过C2和C3节点的最短路径，汇总一下：</p><p><img src="/2020/07/08/viterbi-suan-fa/15.jpg"></p><p>到达C列时最终也只剩3条备选的最短路径，我们仍然没有足够信息断定哪条才是全局最短。</p><p>最后，我们继续看E节点，才能得出最后的结论。</p><p>到E的路径也只有3种可能性：</p><p><img src="/2020/07/08/viterbi-suan-fa/16.jpg"></p><p>E点已经是终点了，我们稍微对比一下这三条路径的总长度就能知道哪条是最短路径了。</p><p><img src="/2020/07/08/viterbi-suan-fa/17.jpg"></p><p>在效率方面相对于粗暴地遍历所有路径，viterbi 维特比算法到达每一列的时候都会删除不符合最短路径要求的路径，大大降低时间复杂度。</p><h2 id="公式推导"><a href="#公式推导" class="headerlink" title="公式推导"></a>公式推导</h2><p>我们首先定义一个变量</p><p><img src="/2020/07/08/viterbi-suan-fa/1.png"></p><p>这个$v _t(j)$表示的是我们看到了前 $t$ 个 观测值，通过了$t-1$个能够取得最大概率的隐状态，且第t个状态值为j的概率值。</p><p>与forward算法类似，$v _{t-1}$与$v _t$之间也存在着某种递推关系。</p><p><img src="/2020/07/08/viterbi-suan-fa/2.png"></p><p>其中 $a _{ij}$ 表示从隐状态i 到 隐状态 $j$的转换概率, $b _j(o_t)$表示从隐状态 $j$ 生成观测值 $o _t$ 的概率。</p><p>$max _ {j=1} ^N v _T(j) $</p><p>初始化</p><p><img src="/2020/07/08/viterbi-suan-fa/3"></p><p>bt: backtrack,一张二维表格，记录了当前位置的上一个隐状态的值。</p><p>递推公式</p><p><img src="/2020/07/08/viterbi-suan-fa/4.png"></p><p>最终结果</p><p><img src="/2020/07/08/viterbi-suan-fa/5.png"></p><h2 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h2><p>​       很多人都用隐马尔科夫模型来回答viterbi算法，其实viterbi算法只是解决隐马第三个问题（求观察序列的最可能的标注序列）的一种实现方式。这个问题可以用于viterbi算法实现，也可以用其他方式实现（如穷举法）；而viterbi算法可以用于解决隐马第三问题，也可以用于解决其他问题。所以千万不要把viterbi算法和隐马尔科夫模型等价了。</p><p>​       viterbi算法其实就是多步骤每步多选择模型的最优选择问题，其在每一步的所有选择都保存了前续所有步骤到当前步骤当前选择的最小总代价（或者最大价值）以及当前代价的情况下前继步骤的选择。依次计算完所有步骤后，通过回溯的方法找到最优选择路径。符合这个模型的都可以用viterbi算法解决，隐马模型的第三问题刚好符合这个模型，所以才采用了viterbi算法。</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">run_viterbi</span><span class="token punctuation">(</span>emission_scores<span class="token punctuation">,</span> trans_scores<span class="token punctuation">,</span> start_scores<span class="token punctuation">,</span> end_scores<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""Run the Viterbi algorithm.    N - number of tokens (length of sentence)    L - number of labels    As an input, you are given:    - Emission scores, as an NxL array    - Transition scores (Yp -> Yc), as an LxL array    - Start transition scores (S -> Y), as an Lx1 array    - End transition scores (Y -> E), as an Lx1 array    You have to return a tuple (s,y), where:    - s is the score of the best sequence    - y is the size N array/seq of integers representing the best sequence.    """</span>    L <span class="token operator">=</span> start_scores<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>    <span class="token keyword">assert</span> end_scores<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">==</span> L    <span class="token keyword">assert</span> trans_scores<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">==</span> L    <span class="token keyword">assert</span> trans_scores<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">==</span> L    <span class="token keyword">assert</span> emission_scores<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">==</span> L    N <span class="token operator">=</span> emission_scores<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>    y <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token comment" spellcheck="true">## start</span>    scores <span class="token operator">=</span> <span class="token number">0.0</span> <span class="token operator">+</span> start_scores<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span>L<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># L x 1 </span>    prevs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>N<span class="token number">-1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        emit_scores <span class="token operator">=</span> emission_scores<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span>L<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># L x 1　</span>        scores <span class="token operator">=</span> scores <span class="token operator">+</span> emit_scores <span class="token operator">+</span> trans_scores <span class="token comment" spellcheck="true"># L x L t-1状态的全部分量＋当前t时刻的发射状态＋从t-１转移到t状态的值</span>        prevs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>scores<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        scores <span class="token operator">=</span> scores<span class="token punctuation">.</span>max<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span>L<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># L x 1 </span>    scores <span class="token operator">=</span> <span class="token punctuation">(</span>scores<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span>L<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">+</span> end_scores<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span>L<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">+</span> emission_scores<span class="token punctuation">[</span>N<span class="token number">-1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span>L<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span>L<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># L ＃　最后的scores </span>    y <span class="token operator">=</span> <span class="token punctuation">[</span>scores<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true">## 求最后位置的最大值索引</span>    score <span class="token operator">=</span> scores<span class="token punctuation">.</span>max<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">## 求最后位置的最大值</span>    <span class="token keyword">for</span> indices <span class="token keyword">in</span> prevs<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">:</span><span class="token comment" spellcheck="true">##倒叙从最后一个回溯最优的路径</span>        last_index <span class="token operator">=</span> y<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        new_index <span class="token operator">=</span> indices<span class="token punctuation">[</span>last_index<span class="token punctuation">]</span>        y <span class="token operator">=</span> <span class="token punctuation">[</span>new_index<span class="token punctuation">]</span> <span class="token operator">+</span> y    <span class="token keyword">return</span> <span class="token punctuation">[</span>score<span class="token punctuation">,</span> y<span class="token punctuation">]</span></code></pre><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">run_viterbi_test</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""A simple tester for Viterbi algorithm.    This function generates a bunch of random emission and transition scores,    and computes the best sequence by performing a brute force search over all    possible sequences and scoring them. It then runs Viterbi code to see what    is the score and sequence returned by it.    Compares both the best sequence and its score to make sure Viterbi is correct.    """</span>    <span class="token keyword">from</span> viterbi <span class="token keyword">import</span> run_viterbi    <span class="token keyword">from</span> numpy <span class="token keyword">import</span> random    <span class="token keyword">import</span> numpy <span class="token keyword">as</span> np    <span class="token keyword">from</span> itertools <span class="token keyword">import</span> product    maxN <span class="token operator">=</span> <span class="token number">7</span> <span class="token comment" spellcheck="true"># maximum length of a sentence (min is 1)</span>    maxL <span class="token operator">=</span> <span class="token number">4</span> <span class="token comment" spellcheck="true"># maximum number of labels (min is 2)</span>    num_tests <span class="token operator">=</span> <span class="token number">1000</span> <span class="token comment" spellcheck="true"># number of sentences to generate</span>    random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>    tolerance <span class="token operator">=</span> <span class="token number">1e</span><span class="token operator">-</span><span class="token number">5</span> <span class="token comment" spellcheck="true"># how close do the scores have to be?</span>    emission_var <span class="token operator">=</span> <span class="token number">1.0</span> <span class="token comment" spellcheck="true"># variance of the gaussian generating emission scores</span>    trans_var <span class="token operator">=</span> <span class="token number">1.0</span> <span class="token comment" spellcheck="true"># variance of the gaussian generating transition scores</span>    passed_y <span class="token operator">=</span> <span class="token number">0</span> <span class="token comment" spellcheck="true"># how many times the correct sequence was predicted</span>    passed_s <span class="token operator">=</span> <span class="token number">0</span> <span class="token comment" spellcheck="true"># how many times the correct score was returned</span>    <span class="token keyword">for</span> t <span class="token keyword">in</span> range<span class="token punctuation">(</span>num_tests<span class="token punctuation">)</span><span class="token punctuation">:</span>        N <span class="token operator">=</span> random<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> maxN<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span>        L <span class="token operator">=</span> random<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> maxL<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Generate the scores</span>        emission_scores <span class="token operator">=</span> random<span class="token punctuation">.</span>normal<span class="token punctuation">(</span><span class="token number">0.0</span><span class="token punctuation">,</span> emission_var<span class="token punctuation">,</span> <span class="token punctuation">(</span>N<span class="token punctuation">,</span>L<span class="token punctuation">)</span><span class="token punctuation">)</span>        trans_scores <span class="token operator">=</span> random<span class="token punctuation">.</span>normal<span class="token punctuation">(</span><span class="token number">0.0</span><span class="token punctuation">,</span> trans_var<span class="token punctuation">,</span> <span class="token punctuation">(</span>L<span class="token punctuation">,</span>L<span class="token punctuation">)</span><span class="token punctuation">)</span>        start_scores <span class="token operator">=</span> random<span class="token punctuation">.</span>normal<span class="token punctuation">(</span><span class="token number">0.0</span><span class="token punctuation">,</span> trans_var<span class="token punctuation">,</span> L<span class="token punctuation">)</span>        end_scores <span class="token operator">=</span> random<span class="token punctuation">.</span>normal<span class="token punctuation">(</span><span class="token number">0.0</span><span class="token punctuation">,</span> trans_var<span class="token punctuation">,</span> L<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># run viterbi</span>        <span class="token punctuation">(</span>viterbi_s<span class="token punctuation">,</span>viterbi_y<span class="token punctuation">)</span> <span class="token operator">=</span> run_viterbi<span class="token punctuation">(</span>emission_scores<span class="token punctuation">,</span> trans_scores<span class="token punctuation">,</span> start_scores<span class="token punctuation">,</span> end_scores<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># print "Viterbi", viterbi_s, viterbi_y</span>        <span class="token comment" spellcheck="true"># compute the best sequence and score</span>        best_y <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        best_s <span class="token operator">=</span> <span class="token operator">-</span>np<span class="token punctuation">.</span>inf        <span class="token keyword">for</span> y <span class="token keyword">in</span> product<span class="token punctuation">(</span>range<span class="token punctuation">(</span>L<span class="token punctuation">)</span><span class="token punctuation">,</span> repeat<span class="token operator">=</span>N<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment" spellcheck="true"># all possible ys</span>            <span class="token comment" spellcheck="true"># compute its score</span>            score <span class="token operator">=</span> <span class="token number">0.0</span>            score <span class="token operator">+=</span> start_scores<span class="token punctuation">[</span>y<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span>            <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>N<span class="token number">-1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                score <span class="token operator">+=</span> trans_scores<span class="token punctuation">[</span>y<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> y<span class="token punctuation">[</span>i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span>                score <span class="token operator">+=</span> emission_scores<span class="token punctuation">[</span>i<span class="token punctuation">,</span>y<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">]</span>            score <span class="token operator">+=</span> emission_scores<span class="token punctuation">[</span>N<span class="token number">-1</span><span class="token punctuation">,</span>y<span class="token punctuation">[</span>N<span class="token number">-1</span><span class="token punctuation">]</span><span class="token punctuation">]</span>            score <span class="token operator">+=</span> end_scores<span class="token punctuation">[</span>y<span class="token punctuation">[</span>N<span class="token number">-1</span><span class="token punctuation">]</span><span class="token punctuation">]</span>            <span class="token comment" spellcheck="true"># update the best</span>            <span class="token keyword">if</span> score <span class="token operator">></span> best_s<span class="token punctuation">:</span>                best_s <span class="token operator">=</span> score                best_y <span class="token operator">=</span> list<span class="token punctuation">(</span>y<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># print "Brute", best_s, best_y</span>        <span class="token comment" spellcheck="true"># mismatch if any label prediction doesn't match</span>        match_y <span class="token operator">=</span> <span class="token boolean">True</span>        <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>best_y<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">if</span> viterbi_y<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">!=</span> best_y<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">:</span>                match_y <span class="token operator">=</span> <span class="token boolean">False</span>        <span class="token keyword">if</span> match_y<span class="token punctuation">:</span> passed_y <span class="token operator">+=</span> <span class="token number">1</span>        <span class="token comment" spellcheck="true"># the scores should also be very close</span>        <span class="token keyword">if</span> abs<span class="token punctuation">(</span>viterbi_s<span class="token operator">-</span>best_s<span class="token punctuation">)</span> <span class="token operator">&lt;</span> tolerance<span class="token punctuation">:</span>            passed_s <span class="token operator">+=</span> <span class="token number">1</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Passed(y)"</span><span class="token punctuation">,</span> passed_y<span class="token operator">*</span><span class="token number">100.0</span><span class="token operator">/</span>num_tests<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Passed(s)"</span><span class="token punctuation">,</span> passed_s<span class="token operator">*</span><span class="token number">100.0</span><span class="token operator">/</span>num_tests<span class="token punctuation">)</span>    <span class="token keyword">assert</span> passed_y <span class="token operator">==</span> num_tests    <span class="token keyword">assert</span> passed_s <span class="token operator">==</span> num_tests<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">"__main__"</span><span class="token punctuation">:</span>    run_viterbi_test<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> nlp </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
            <tag> 路径规划 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NNLM</title>
      <link href="/2020/07/06/nnlm/"/>
      <url>/2020/07/06/nnlm/</url>
      
        <content type="html"><![CDATA[<h1 id="NNLM-原理"><a href="#NNLM-原理" class="headerlink" title="NNLM 原理"></a>NNLM 原理</h1><p>Paper: <a href="http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf">A Neural Probabilistic Language Model(2003)</a></p><p>本文算是训练语言模型的经典之作，Bengio 将神经网络引入语言模型的训练中，并得到了词向量这个副产物。词向量对后面深度学习在自然语言处理方面有很大的贡献，也是获取词的语义特征的有效方法</p><p>其主要架构为三层神经网络，如下图所示</p><p><img src="/2020/07/06/nnlm/1.webp"></p><p>现在的任务是输入 wt−n+1,…,wt−1 这前 n-1 个单词，然后预测出下一个单词 wt</p><p>数学符号说明：</p><ul><li>C(i)：单词 w 对应的词向量，其中 i 为词 w 在整个词汇表中的索引</li><li>C：词向量，大小为 |V|×m 的矩阵</li><li>|V|：词汇表的大小，即预料库中去重后的单词个数</li><li>m：词向量的维度，一般是 50 到 200</li><li>H：隐藏层的 weight</li><li>d：隐藏层的 bias</li><li>U：输出层的 weight</li><li>b：输出层的 bias</li><li>W：输入层到输出层的 weight</li><li>h：隐藏层神经元个数</li></ul><p>计算流程：</p><ol><li>首先将输入的 n-1 个单词索引转为词向量，然后将这 n-1 个词向量进行 concat，形成一个 (n-1)*w 的向量，用 X 表示</li><li>将 X 送入隐藏层进行计算，hiddenout=tanh(d+X∗H)</li><li>输出层共有 |V| 个节点，每个节点 yi 表示预测下一个单词 i 的概率，y 的计算公式为 y=b+X∗W+${ hidden } _{ out }$∗U</li></ol><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true">#!/usr/bin/env python3</span><span class="token comment" spellcheck="true"># encoding: utf-8</span><span class="token triple-quoted-string string">'''@author: daiyizheng: (C) Copyright 2017-2019, Personal exclusive right.@contact: 387942239@qq.com@software: tool@application:@file: NNLM-torch.py@time: 2020-07-06 下午11:30@desc:'''</span><span class="token comment" spellcheck="true">#!/usr/bin/env python</span><span class="token comment" spellcheck="true"># -*- coding: utf-8 -*-</span><span class="token comment" spellcheck="true"># @Time : 2020-07-06 09:36</span><span class="token comment" spellcheck="true"># @Author : daiyizheng</span><span class="token comment" spellcheck="true"># @Site :</span><span class="token comment" spellcheck="true"># @File : NNLM.py</span><span class="token comment" spellcheck="true"># @Software: PyCharm</span><span class="token keyword">import</span> torch<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optim<span class="token keyword">import</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">as</span> Data<span class="token triple-quoted-string string">"""定义类型"""</span>dtype <span class="token operator">=</span> torch<span class="token punctuation">.</span>FloatTensor<span class="token triple-quoted-string string">"""定义词汇表"""</span>sentences <span class="token operator">=</span> <span class="token punctuation">[</span> <span class="token string">"i like dog"</span><span class="token punctuation">,</span> <span class="token string">"i love coffee"</span><span class="token punctuation">,</span> <span class="token string">"i hate milk"</span><span class="token punctuation">]</span>word_list <span class="token operator">=</span> <span class="token string">" "</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>sentences<span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># ['i', 'like', 'dog', 'dog', 'i', 'love', 'coffee', 'i', 'hate', 'milk']</span>word_list <span class="token operator">=</span> list<span class="token punctuation">(</span>set<span class="token punctuation">(</span>word_list<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># ['i', 'like', 'dog', 'love', 'coffee', 'hate', 'milk']</span>word_dict <span class="token operator">=</span> <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;w: i for i, w in enumerate(word_list)&amp;#125; # &amp;#123;'i':0, 'like':1, 'dog':2, 'love':3, 'coffee':4, 'hate':5, 'milk':6&amp;#125;</span>number_dict <span class="token operator">=</span> <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;i: w for i, w in enumerate(word_list)&amp;#125; # &amp;#123;0:'i', 1:'like', 2:'dog', 3:'love', 4:'coffee', 5:'hate', 6:'milk'&amp;#125;</span>n_class <span class="token operator">=</span> len<span class="token punctuation">(</span>word_dict<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># number of Vocabulary, just like |V|, in this task n_class=7</span><span class="token comment" spellcheck="true"># NNLM(Neural Network Language Model) Parameter</span>n_step <span class="token operator">=</span> len<span class="token punctuation">(</span>sentences<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">-</span><span class="token number">1</span> <span class="token comment" spellcheck="true"># n-1 in paper, look back n_step words and predict next word. In this task n_step=2</span>n_hidden <span class="token operator">=</span> <span class="token number">2</span> <span class="token comment" spellcheck="true"># h in paper</span>m <span class="token operator">=</span> <span class="token number">2</span> <span class="token comment" spellcheck="true"># m in paper, word embedding dim</span><span class="token keyword">def</span> <span class="token function">make_batch</span><span class="token punctuation">(</span>sentences<span class="token punctuation">)</span><span class="token punctuation">:</span>    input_batch <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    target_batch <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> sen <span class="token keyword">in</span> sentences<span class="token punctuation">:</span>        word <span class="token operator">=</span> sen<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span>        input <span class="token operator">=</span> <span class="token punctuation">[</span>word_dict<span class="token punctuation">[</span>n<span class="token punctuation">]</span> <span class="token keyword">for</span> n <span class="token keyword">in</span> word<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># [0, 1], [0, 3], [0, 5]</span>        target <span class="token operator">=</span> word_dict<span class="token punctuation">[</span>word<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># 2, 4, 6</span>        input_batch<span class="token punctuation">.</span>append<span class="token punctuation">(</span>input<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [[0, 1], [0, 3], [0, 5]]</span>        target_batch<span class="token punctuation">.</span>append<span class="token punctuation">(</span>target<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [2, 4, 6]</span>    <span class="token keyword">return</span> input_batch<span class="token punctuation">,</span> target_batch<span class="token triple-quoted-string string">"""预处理数据"""</span>input_data<span class="token punctuation">,</span> target_data <span class="token operator">=</span> make_batch<span class="token punctuation">(</span>sentences<span class="token punctuation">)</span><span class="token triple-quoted-string string">"""数据转为longtensor"""</span>input_data<span class="token punctuation">,</span> target_data <span class="token operator">=</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span>input_data<span class="token punctuation">)</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span>target_data<span class="token punctuation">)</span><span class="token triple-quoted-string string">"""数据批处理"""</span>dataset <span class="token operator">=</span> Data<span class="token punctuation">.</span>TensorDataset<span class="token punctuation">(</span>input_data<span class="token punctuation">,</span> target_data<span class="token punctuation">)</span>loader <span class="token operator">=</span> Data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>dataset<span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token boolean">True</span> <span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># dataset batch_size, shuffle</span><span class="token keyword">class</span> <span class="token class-name">NNLM</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>NNLM<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>C <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>n_class<span class="token punctuation">,</span> m<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>H <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>n_step <span class="token operator">*</span> m<span class="token punctuation">,</span> n_hidden<span class="token punctuation">)</span><span class="token punctuation">.</span>type<span class="token punctuation">(</span>dtype<span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>W <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>n_step <span class="token operator">*</span> m<span class="token punctuation">,</span> n_class<span class="token punctuation">)</span><span class="token punctuation">.</span>type<span class="token punctuation">(</span>dtype<span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>d <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>n_hidden<span class="token punctuation">)</span><span class="token punctuation">.</span>type<span class="token punctuation">(</span>dtype<span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>U <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>n_hidden<span class="token punctuation">,</span> n_class<span class="token punctuation">)</span><span class="token punctuation">.</span>type<span class="token punctuation">(</span>dtype<span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>b <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>n_class<span class="token punctuation">)</span><span class="token punctuation">.</span>type<span class="token punctuation">(</span>dtype<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">'''        X: [batch_size, n_step]        '''</span>        X <span class="token operator">=</span> self<span class="token punctuation">.</span>C<span class="token punctuation">(</span>X<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch_size, n_step] => [batch_size, n_step, m]</span>        X <span class="token operator">=</span> X<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> n_step <span class="token operator">*</span> m<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch_size, n_step * m]</span>        hidden_out <span class="token operator">=</span> torch<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span>self<span class="token punctuation">.</span>d <span class="token operator">+</span> torch<span class="token punctuation">.</span>mm<span class="token punctuation">(</span>X<span class="token punctuation">,</span> self<span class="token punctuation">.</span>H<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch_size, n_hidden]</span>        output <span class="token operator">=</span> self<span class="token punctuation">.</span>b <span class="token operator">+</span> torch<span class="token punctuation">.</span>mm<span class="token punctuation">(</span>X<span class="token punctuation">,</span> self<span class="token punctuation">.</span>W<span class="token punctuation">)</span> <span class="token operator">+</span> torch<span class="token punctuation">.</span>mm<span class="token punctuation">(</span>hidden_out<span class="token punctuation">,</span> self<span class="token punctuation">.</span>U<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [batch_size, n_class]</span>        <span class="token keyword">return</span> outputmodel <span class="token operator">=</span> NNLM<span class="token punctuation">(</span><span class="token punctuation">)</span>criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># Training</span><span class="token keyword">for</span> epoch <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">5000</span><span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token keyword">for</span> batch_x<span class="token punctuation">,</span> batch_y <span class="token keyword">in</span> loader<span class="token punctuation">:</span>    optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>    output <span class="token operator">=</span> model<span class="token punctuation">(</span>batch_x<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># output : [batch_size, n_class], batch_y : [batch_size] (LongTensor, not one-hot)</span>    loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>output<span class="token punctuation">,</span> batch_y<span class="token punctuation">)</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">%</span><span class="token number">1000</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Epoch:'</span><span class="token punctuation">,</span> <span class="token string">'%04d'</span> <span class="token operator">%</span> <span class="token punctuation">(</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'cost ='</span><span class="token punctuation">,</span> <span class="token string">'&amp;#123;:.6f&amp;#125;'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>loss<span class="token punctuation">)</span><span class="token punctuation">)</span>    loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>    optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># Predict</span>predict <span class="token operator">=</span> model<span class="token punctuation">(</span>input_data<span class="token punctuation">)</span><span class="token punctuation">.</span>data<span class="token punctuation">.</span>max<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> keepdim <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token comment" spellcheck="true"># Test</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token punctuation">[</span>sen<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span>n_step<span class="token punctuation">]</span> <span class="token keyword">for</span> sen <span class="token keyword">in</span> sentences<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token string">'->'</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>number_dict<span class="token punctuation">[</span>n<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token keyword">for</span> n <span class="token keyword">in</span> predict<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> nlp </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语言模型 </tag>
            
            <tag> 词向量 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Word2vec</title>
      <link href="/2020/07/05/word2vec/"/>
      <url>/2020/07/05/word2vec/</url>
      
        <content type="html"><![CDATA[<h1 id="Word2vec"><a href="#Word2vec" class="headerlink" title="Word2vec"></a>Word2vec</h1><p>Word2vec是谷歌团队在2013年开源推出的一个专门用于获取词向量的工具包，其核心算法是对NNLM运算量最大的那部分进行了效率上的改进，让我们来一探究竟。</p><h2 id="Word2vec原理解析"><a href="#Word2vec原理解析" class="headerlink" title="Word2vec原理解析"></a>Word2vec原理解析</h2><p>word2vec有两种模型–CBOW和Skip-gram，这两个模型目的都是通过训练语言模型任务，得到词向量。CBOW的语言模型任务是给定上下文预测当前词，Skip-gram的语言模型任务是根据当前词预测其上下文。</p><p>Word2vec还提供了两种框架–Hierarchical Softmax和Negative-Sampling，所以word2vec有四种实现方式，下面将对其原理进行介绍。</p><h2 id="CBOW"><a href="#CBOW" class="headerlink" title="CBOW"></a>CBOW</h2><h3 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h3><p>CBOW的核心思想就是利用给定上下文，预测当前词，其结构表示为</p><p><img src="/2020/07/05/word2vec/1.png">CBOW结构</p><p>我们知道基于神经网络的语言模型的目标函数通常是对数似然函数，这里CBOW的目标函数应该就是</p><p><img src="/2020/07/05/word2vec/2.png"></p><p>其中是单词的上下文，是词典。所以我们关注的重点应该就是的构造上，NNLM和Word2vec的本质区别也是在于该条件概率函数的构造方式不同。</p><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><h3 id="基于Hierarchical-Softmax"><a href="#基于Hierarchical-Softmax" class="headerlink" title="基于Hierarchical Softmax"></a>基于Hierarchical Softmax</h3><p>CBOW模型的训练任务是在给定上下文，预测当前词。基于Hierarchical Softmax的CBOW结构相当于在NNLM上的改进，如下图所示</p><p><img src="/2020/07/05/word2vec/3.png">      </p><p>​                                                    基于Hierarchical Softmax的CBOW模型</p><p>CBOW模型一共是有三层，分别是输入层（input layer），投影层（projection layer）和输出层（output layer）。</p><p>输入层：输入是当前词的上下文，共2c个词向量</p><p>投影层：投影层将2c个词向量进行累加，得到${ X }<em>{ w }$，即${ X }</em>{ w }=\sum { <em>{ i=1 }^{ 2c }{ v({ Countext(w) }</em>{ i }) } } $，这里不包含当前词的向量表示</p><p>输出层：输出是一个用语料中出现的词为叶子结点构建的Huffman树，结点的权值是各词在语料中出现的次数。图中的Huffman树共N个结点，对应语料中的N个词，非叶子结点N-1个，即图中标黄的。</p><p>基于Hierarchical Softmax的CBOW模型和之前介绍的NNLM的主要区别：</p><ul><li>从输入层到投影层，CBOW采用的是求和，而NNLM是拼接</li><li>CBOW没有隐藏层</li><li>输出层，基于Hierarchical Softmax的CBOW是树形结构，NNLM是线性结构</li></ul><p>之前说过，NNLM的主要运算量就是在于隐藏层和输出层的softmax的计算量，基于Hierarchical Softmax的方法通过对这些关键点进行了针对性的优化，去掉了隐藏层，更改了输出结构，大大提升了运算效率。下面介绍基于Hierarchical Softmax的CBOW是如何利用树结构训练得到词向量的。</p><p>对于huffman的某个叶子结点，假设该结点对应词典中的词w，定义以下符号</p><ul><li>${ p } ^{ w }$：从根结点出发到达对应的叶子结点的路径</li><li>${ l } ^{ w }$：路径${ p }^{ w }$中包含的结点个数</li><li>${ p}  <em>{1} ^{ w },{ p } _{ 2 } ^{ w },{ p } _{ 3 } ^{ w }…{ p } _{ { l } ^{ w } } ^{ w }$：路径${ p }^{ w }$中的${l}^{ w }$个结点，其中${ p } _{ 1 }^{ w }$代表根结点，${ p }</em>{ { l }^{ w } }^{ w }$代表词对应的结点</li><li>${ d }<em>{ 2 }^{ w },{ d }</em>{ 2 }^{ w },{ d }<em>{ 2 }^{ w }…{ d }</em>{ 2 }^{ w }\in { 0,1} $：词w的Huffman编码，${ l }^{ w }-1$由位编码构成，即根结点不对应编码</li><li>${ \theta  }<em>{ 1 }^{ w },{ { \theta  } }</em>{ 1 }^{ w },{ { \theta  } }<em>{ 1 }^{ w }…{ { \theta  } }</em>{ 1 }^{ w }\in { R }^{ m }$：${ p }^{ w }$路径中非叶子结点对应的向量，${ \theta  }_{ j }^{ w }$代表路径${ p }^{ w }$中第$j$个非叶子结点对应的向量</li></ul><p>下面通过一个示例说明运算原理，如下图所示，考虑当前词$w$=“足球”，那么图中的红色边串起来的5个结点就构成路径${ p }^{ w }$，其长度${ l }^{ w }=5$是路径${ p }<em>{ 1 }^{ w },{ p }</em>{ 2 }^{ w },{ p }<em>{ 3 }^{ w },{ p }</em>{ 4 }^{ w },{ p }<em>{ 5 }^{ w }$的5个结点，其中对应的${ p }</em>{ 1 }^{ w }$是根结点，分别是1，0，0，1，即当前词“足球”的Huffman编码为1001，${ \theta  }<em>{ 1 }^{ w },{ \theta  }</em>{ 2 }^{ w },{ \theta  }<em>{ 3 }^{ w },{ \theta  }</em>{ 4 }^{ w }$分别是路径上4个非叶子结点对应的向量。</p><p><img src="/2020/07/05/word2vec/4.png">   </p><p>​                                                           CBOW原理示例</p><p>所以，讲了这么多，我们的重点$p(w|Countext(w))$究竟是怎么构建的呢？</p><p>我们知道词库里的每一个词都可以被表示为一个Huffman编码，如果把Huffman树每一个结点都看成一次二分类，那么这个语言模型的任务就变成输入上下文，使得当前词的Huffman编码对应的路径的概率最大。这里约定将一个结点进行二分类时，分到左边为负类，分到右边为正类（主要为了和源码对应），即</p><p><img src="/2020/07/05/word2vec/5.png"></p><p>由前面介绍的逻辑回归可以知道，结点分为正类的概率是</p><p><img src="/2020/07/05/word2vec/6.png"></p><p>所以分为负类的概率就是$1-\theta ({ X }<em>{ w }^{ T }\Theta )$。这里的${ X }</em>{ w }$就是当前词$w$的上下文通过求和得到的，$\Theta$就是每一个结点的${ \Theta }_{ i }^{ w }$，是待训练参数。</p><p>举个栗子，还是前面说的当前词是“足球”，从根结点出发到达这个词一共需要经过4次二分类</p><ul><li>第一次：$P({ d }<em>{ 2 }^{ w }|{ X }</em>{ w },{ \Theta  }<em>{ 1 }^{ w })=1-\sigma ({ X }</em>{ w }^{ T }{ \Theta  }_{ 1 }^{ w })$</li><li>第二次：$P({ d }<em>{ 3 }^{ w }|{ X }</em>{ w },{ \Theta  }<em>{ 2 }^{ w })=\sigma ({ X }</em>{ w }^{ T }{ \Theta  }_{ 2 }^{ w })$</li><li>第三次：$P({ d }<em>{ 4 }^{ w }|{ X }</em>{ w },{ \Theta  }<em>{ 3 }^{ w })=\sigma ({ X }</em>{ w }^{ T }{ \Theta  }_{ 3 }^{ w })$</li><li>第四次：$P({ d }<em>{ 5 }^{ w }|{ X }</em>{ w },{ \Theta  }<em>{ 4 }^{ w })=1-\sigma ({ X }</em>{ w }^{ T }{ \Theta  }_{ 4 }^{ w })$</li></ul><p>所以对于当前词“足球”，我们的条件概率函数为</p><p><img src="/2020/07/05/word2vec/7.png"></p><p>由上面的小例子，我们可以扩展出一般的条件概率公式</p><p><img src="/2020/07/05/word2vec/8.png"></p><p>其中</p><p><img src="/2020/07/05/word2vec/9.png"></p><p>写成整体表达式</p><p><img src="/2020/07/05/word2vec/10.png"></p><p>所以我们要优化的目标函数就是</p><p><img src="/2020/07/05/word2vec/11.png"></p><p>令</p><p><img src="/2020/07/05/word2vec/12.png"></p><p>使用随机梯度上升法进行目标函数优化，下面推导一下梯度的计算</p><p><img src="/2020/07/05/word2vec/13.png"></p><p>所以，更新参数时</p><p><img src="/2020/07/05/word2vec/14.png"></p><p>其中，$\eta $是学习率。</p><p>同理可以计算得到</p><p><img src="/2020/07/05/word2vec/15.png"></p><p>我们最终想优化的向量是词表中的每一个词向量，而是当前词上下文的累加，那么该如何更新每个词的向量表示呢？word2vec中的做法很简单直接，就是将${ X }_{ w }$的梯度变化等效于每个词的梯度变化，即</p><p><img src="/2020/07/05/word2vec/16.png"></p><p>至此,基于Hierarchical Softmax的CBOW模型的原理就介绍完了，通过不断训练就可以得到我们想要的词向量表。</p><h3 id="基于Negative-Sampling"><a href="#基于Negative-Sampling" class="headerlink" title="基于Negative-Sampling"></a>基于Negative-Sampling</h3><p>基于Negative-Sampling（负采样，NEG）的方法的目的是进一步提升训练速度，同时改善词向量的质量。NEG不再使用相对复杂的huffman树，而是使用简单的随机负采样，大幅度提升性能。</p><p>CBOW是利用上下文预测当前词，那么将当前词替换为其他词时，就构成了一个负样本。</p><p>书面一点，对于给定的$Context(w)$，词$w$就是一个正样本，其他词就是负样本，假设我们通过负采样方法选取好一个关于的负样本子集$NEG(w)$。得到样本集后，我们的目标是最大化</p><p><img src="/2020/07/05/word2vec/17.png"></p><p>其中</p><p><img src="/2020/07/05/word2vec/18.png"></p><p>所以</p><p><img src="/2020/07/05/word2vec/19.png"></p><p>这里，${ X}<em>{ w }^{ T }$还是代表上下文$Context(w)$的词向量之和，${\Theta}^{u}$表示词对应的一个待训练辅助向量。直观上看，这就是最大化${\sigma({X}</em>{w}^{T}{\Theta}^{w})}$的同时，最小化${\sigma({X}_{w}^{T}{\Theta}^{u})}$，其中$u\in{NEG(w)}$，也就是最大化正样本的概率的同时最小化负样本的概率。</p><p>对于给定的语料库C，最终的目标函数是</p><p><img src="/2020/07/05/word2vec/20.png"></p><p>为了推导梯度更新的过程，令</p><p><img src="/2020/07/05/word2vec/21.png"></p><p>所以</p><p><img src="/2020/07/05/word2vec/22.png"></p><p>同理</p><p><img src="/2020/07/05/word2vec/23.png"></p><h2 id="Skip-gram"><a href="#Skip-gram" class="headerlink" title="Skip-gram"></a>Skip-gram</h2><h3 id="结构-1"><a href="#结构-1" class="headerlink" title="结构"></a>结构</h3><p>Skip-gram的核心思想就是利用给定的当前词${w}<em>{t}$，预测上下文${w}</em>{t-2},{w}<em>{t-1},{w}</em>{t+1},{w}_{t+2}$，其结构表示为</p><p><img src="/2020/07/05/word2vec/s1.png"></p><p>​                                                  Skip-gram结构</p><p>我们知道基于神经网络的语言模型的目标函数通常是对数似然函数，这里Skip-gram的目标函数应该就是</p><p><img src="/2020/07/05/word2vec/s2.png"></p><p>其中$Context(w)$是单词$w$的上下文，$C$是词典。所以我们关注的重点应该就是$P(Context(w|w))$的构造上。</p><h3 id="原理-1"><a href="#原理-1" class="headerlink" title="原理"></a>原理</h3><h4 id="基于Hierarchical-Softmax-1"><a href="#基于Hierarchical-Softmax-1" class="headerlink" title="基于Hierarchical Softmax"></a>基于Hierarchical Softmax</h4><p>前面说了，Skip-gram的语言模型任务是根据当前词预测其上下文，原理上和CBOW很相似，基于Hierarchical Softmax的Skip-gram的结构如下图所示</p><p><img src="/2020/07/05/word2vec/s3.png">   </p><p>​                                               基于Hierarchical Softmax的Skip-gram模型</p><p>输入层：中心词$w$的词向量$v(w)$</p><p>投影层：恒等投影，只是为了和CBOW模型进行对称类比</p><p>输出层：和CBOW一样，是一颗huffman树</p><p>Skip-gram的重点是条件概率函数$P(Context(w)|w)$的构造，SKip-gram中对于该条件概率函数的定义为</p><p><img src="/2020/07/05/word2vec/s4.png"></p><p>所以，Skip-gram的目标函数为</p><p><img src="/2020/07/05/word2vec/s5.png"></p><p>令</p><p><img src="/2020/07/05/word2vec/s6.png"></p><p>则</p><p><img src="/2020/07/05/word2vec/s7.png"></p><p>所以</p><p><img src="/2020/07/05/word2vec/s8.png"></p><p>同理</p><p><img src="/2020/07/05/word2vec/s9.png"></p><h3 id="基于-Negative-Sampling"><a href="#基于-Negative-Sampling" class="headerlink" title="基于 Negative-Sampling"></a>基于 Negative-Sampling</h3><p>基于负采样的skip-gram模型基本与前面基于负采样的CBOW推导的类似，目标函数为</p><p><img src="/2020/07/05/word2vec/s10.png"></p><p>也就是当给定当前词时，希望其上下文$Context(w)$出现的概率最大，其中</p><p><img src="/2020/07/05/word2vec/s11.png"></p><p>这里$g(u)$代表的就是上下文$Context(w)$中某个词$u$出现的概率，$NEG(u)$表示处理词$u$时生成的负样本子集，$v(w)$是输入的当前词，$\Theta^{z}$是待训练的辅助向量。</p><p>所以</p><p><img src="/2020/07/05/word2vec/s12.png"></p><p>可以看出来，这里需要对上下文$Context(w)$中的每一个词做一次负采样。实际上，作者在源码实现过程中并没有这么做，而是针对$w$进行了$|Context(w)|$次负采样，这样依赖，我们的目标函数变成</p><p><img src="/2020/07/05/word2vec/s13.png"></p><p>其中</p><p><img src="/2020/07/05/word2vec/s14.png"></p><p>这里${NEG(w)}$表示处理词$\tilde { w } $时生成的负采样子集。</p><p>最终的目标函数变成</p><p><img src="/2020/07/05/word2vec/s15.png"></p><p>考虑梯度更新，令</p><p><img src="/2020/07/05/word2vec/s16.png"></p><p>所以</p><p><img src="/2020/07/05/word2vec/s17.png"></p><p>同理</p><p><img src="/2020/07/05/word2vec/s18.png"></p><h2 id="Word2vec不可忽视的问题"><a href="#Word2vec不可忽视的问题" class="headerlink" title="Word2vec不可忽视的问题"></a>Word2vec不可忽视的问题</h2><p>​     Word2vec算法虽然在前些年取得了良好的效果，但是也有不足之处，其中最不可忽视的问题就是<strong>一词多义</strong>的问题。Word2vec中最后对每一个词生成一个固定的向量表示，那么对于相同单词有多个含义时，就不能很好的解决。比如bank，既有银行的意思，也有岸边的意思，当算法的结果只有一个固定向量对其进行表征时，必然导致这个词的表征不够准确。</p><h2 id="PyTorch-代码实现"><a href="#PyTorch-代码实现" class="headerlink" title="PyTorch 代码实现"></a>PyTorch 代码实现</h2><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optim<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt<span class="token keyword">import</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">as</span> Datadtype <span class="token operator">=</span> torch<span class="token punctuation">.</span>FloatTensordevice <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cuda"</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">"cpu"</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">"""文本预处理"""</span>sentences <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"jack like dog"</span><span class="token punctuation">,</span> <span class="token string">"jack like cat"</span><span class="token punctuation">,</span> <span class="token string">"jack like animal"</span><span class="token punctuation">,</span>  <span class="token string">"dog cat animal"</span><span class="token punctuation">,</span> <span class="token string">"banana apple cat dog like"</span><span class="token punctuation">,</span> <span class="token string">"dog fish milk like"</span><span class="token punctuation">,</span>  <span class="token string">"dog cat animal like"</span><span class="token punctuation">,</span> <span class="token string">"jack like apple"</span><span class="token punctuation">,</span> <span class="token string">"apple like"</span><span class="token punctuation">,</span> <span class="token string">"jack like banana"</span><span class="token punctuation">,</span>  <span class="token string">"apple banana jack movie book music like"</span><span class="token punctuation">,</span> <span class="token string">"cat dog hate"</span><span class="token punctuation">,</span> <span class="token string">"cat dog like"</span><span class="token punctuation">]</span>word_sequence <span class="token operator">=</span> <span class="token string">" "</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>sentences<span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># ['jack', 'like', 'dog', 'jack', 'like', 'cat', 'animal',...]</span>vocab <span class="token operator">=</span> list<span class="token punctuation">(</span>set<span class="token punctuation">(</span>word_sequence<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># build words vocabulary</span>word2idx <span class="token operator">=</span> <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;w: i for i, w in enumerate(vocab)&amp;#125; # &amp;#123;'jack':0, 'like':1,...&amp;#125;</span><span class="token triple-quoted-string string">"""模型相关参数"""</span><span class="token comment" spellcheck="true"># Word2Vec Parameters</span>batch_size <span class="token operator">=</span> <span class="token number">8</span>embedding_size <span class="token operator">=</span> <span class="token number">2</span>  <span class="token comment" spellcheck="true"># 2 dim vector represent one word</span>C <span class="token operator">=</span> <span class="token number">2</span> <span class="token comment" spellcheck="true"># window size</span>voc_size <span class="token operator">=</span> len<span class="token punctuation">(</span>vocab<span class="token punctuation">)</span><span class="token triple-quoted-string string">"""数据预处理"""</span><span class="token comment" spellcheck="true"># 1.</span>skip_grams <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token keyword">for</span> idx <span class="token keyword">in</span> range<span class="token punctuation">(</span>C<span class="token punctuation">,</span> len<span class="token punctuation">(</span>word_sequence<span class="token punctuation">)</span> <span class="token operator">-</span> C<span class="token punctuation">)</span><span class="token punctuation">:</span>  center <span class="token operator">=</span> word2idx<span class="token punctuation">[</span>word_sequence<span class="token punctuation">[</span>idx<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># center word</span>  context_idx <span class="token operator">=</span> list<span class="token punctuation">(</span>range<span class="token punctuation">(</span>idx <span class="token operator">-</span> C<span class="token punctuation">,</span> idx<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">+</span> list<span class="token punctuation">(</span>range<span class="token punctuation">(</span>idx <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> idx <span class="token operator">+</span> C <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># context word idx</span>  context <span class="token operator">=</span> <span class="token punctuation">[</span>word2idx<span class="token punctuation">[</span>word_sequence<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> context_idx<span class="token punctuation">]</span>  <span class="token keyword">for</span> w <span class="token keyword">in</span> context<span class="token punctuation">:</span>    skip_grams<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">[</span>center<span class="token punctuation">,</span> w<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 2.</span><span class="token keyword">def</span> <span class="token function">make_data</span><span class="token punctuation">(</span>skip_grams<span class="token punctuation">)</span><span class="token punctuation">:</span>  input_data <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>  output_data <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>  <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>skip_grams<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    input_data<span class="token punctuation">.</span>append<span class="token punctuation">(</span>np<span class="token punctuation">.</span>eye<span class="token punctuation">(</span>voc_size<span class="token punctuation">)</span><span class="token punctuation">[</span>skip_grams<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    output_data<span class="token punctuation">.</span>append<span class="token punctuation">(</span>skip_grams<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token keyword">return</span> input_data<span class="token punctuation">,</span> output_data<span class="token comment" spellcheck="true"># 3.</span>input_data<span class="token punctuation">,</span> output_data <span class="token operator">=</span> make_data<span class="token punctuation">(</span>skip_grams<span class="token punctuation">)</span>input_data<span class="token punctuation">,</span> output_data <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span>input_data<span class="token punctuation">)</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span>output_data<span class="token punctuation">)</span>dataset <span class="token operator">=</span> Data<span class="token punctuation">.</span>TensorDataset<span class="token punctuation">(</span>input_data<span class="token punctuation">,</span> output_data<span class="token punctuation">)</span>loader <span class="token operator">=</span> Data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>dataset<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span> <span class="token boolean">True</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">"""构建模型"""</span><span class="token comment" spellcheck="true"># Model</span><span class="token keyword">class</span> <span class="token class-name">Word2Vec</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>    super<span class="token punctuation">(</span>Word2Vec<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># W and V is not Traspose relationship</span>    self<span class="token punctuation">.</span>W <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>voc_size<span class="token punctuation">,</span> embedding_size<span class="token punctuation">)</span><span class="token punctuation">.</span>type<span class="token punctuation">(</span>dtype<span class="token punctuation">)</span><span class="token punctuation">)</span>    self<span class="token punctuation">.</span>V <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>embedding_size<span class="token punctuation">,</span> voc_size<span class="token punctuation">)</span><span class="token punctuation">.</span>type<span class="token punctuation">(</span>dtype<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># X : [batch_size, voc_size] one-hot</span>    <span class="token comment" spellcheck="true"># torch.mm only for 2 dim matrix, but torch.matmul can use to any dim</span>    hidden_layer <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>X<span class="token punctuation">,</span> self<span class="token punctuation">.</span>W<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># hidden_layer : [batch_size, embedding_size]</span>    output_layer <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>hidden_layer<span class="token punctuation">,</span> self<span class="token punctuation">.</span>V<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># output_layer : [batch_size, voc_size]</span>    <span class="token keyword">return</span> output_layermodel <span class="token operator">=</span> Word2Vec<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">"""训练"""</span><span class="token comment" spellcheck="true"># Training</span><span class="token keyword">for</span> epoch <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">2000</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">for</span> i<span class="token punctuation">,</span> <span class="token punctuation">(</span>batch_x<span class="token punctuation">,</span> batch_y<span class="token punctuation">)</span> <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>loader<span class="token punctuation">)</span><span class="token punctuation">:</span>        batch_x <span class="token operator">=</span> batch_x<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>        batch_y <span class="token operator">=</span> batch_y<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>        pred <span class="token operator">=</span> model<span class="token punctuation">(</span>batch_x<span class="token punctuation">)</span>        loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>pred<span class="token punctuation">,</span> batch_y<span class="token punctuation">)</span>        <span class="token keyword">if</span> <span class="token punctuation">(</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">%</span> <span class="token number">1000</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>            <span class="token keyword">print</span><span class="token punctuation">(</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> i<span class="token punctuation">,</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">for</span> i<span class="token punctuation">,</span> label <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>vocab<span class="token punctuation">)</span><span class="token punctuation">:</span>  W<span class="token punctuation">,</span> WT <span class="token operator">=</span> model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span>  x<span class="token punctuation">,</span>y <span class="token operator">=</span> float<span class="token punctuation">(</span>W<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> float<span class="token punctuation">(</span>W<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  plt<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span>  plt<span class="token punctuation">.</span>annotate<span class="token punctuation">(</span>label<span class="token punctuation">,</span> xy<span class="token operator">=</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">,</span> xytext<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> textcoords<span class="token operator">=</span><span class="token string">'offset points'</span><span class="token punctuation">,</span> ha<span class="token operator">=</span><span class="token string">'right'</span><span class="token punctuation">,</span> va<span class="token operator">=</span><span class="token string">'bottom'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> nlp </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 语言模型 </tag>
            
            <tag> 词向量 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RNN(smapleRNN LSTM  GRU)</title>
      <link href="/2020/06/06/rnn/"/>
      <url>/2020/06/06/rnn/</url>
      
        <content type="html"><![CDATA[<h1 id="RNN-smapleRNN-LSTM-GRU"><a href="#RNN-smapleRNN-LSTM-GRU" class="headerlink" title="RNN(smapleRNN LSTM  GRU)"></a>RNN(smapleRNN LSTM  GRU)</h1><h3 id="短期记忆"><a href="#短期记忆" class="headerlink" title="短期记忆"></a><strong>短期记忆</strong></h3><p>RNN受到短期记忆的影响。如果序列很长，他们将很难将信息从较早的时间步传送到后面的时间步。因此，如果你尝试处理一段文本进行预测，RNN可能会遗漏开头的重要信息。</p><p>在反向传播期间，RNN存在梯度消失的问题（梯度用于更新神经网络权重的值）。梯消失度问题是当梯度反向传播随着时间的推梯度逐渐收缩。如果梯度值变得非常小，则不会产生太多的学习。</p><p><img src="/2020/06/06/rnn/lstm_gru-1.jpg"></p><p>梯度更新规则</p><p>因此，在递归神经网络中，获得小梯度更新的层会停止学习。那些通常是较早的层。因为这些层不再学习，RNN会忘记它在较长序列中看到的内容，因此只有短期记忆。</p><h2 id="LSTM和GRU解决方案"><a href="#LSTM和GRU解决方案" class="headerlink" title="LSTM和GRU解决方案"></a><strong>LSTM和GRU解决方案</strong></h2><p>LSTM和GRU是作为短期记忆的解决方案而创建的。它们具有称为门（gate）的内部机制，它可以调节信息流。</p><p><img src="/2020/06/06/rnn/lstm-gru-2.jpg"></p><p>这些门可以了解序列中哪些数据重要以进行保留或丢弃。这样，它可以将相关信息传递到长序列中进行预测。现有的基于RNN的几乎所有技术结果都是通过LSTM和GRU这两个网络实现的。LSTM和GRU进行语音识别，语音合成和文本生成，甚至可以使用它们为视频生成字幕。</p><p>将通过直观的解释和插图来进行解释，尽量避免使用数学。</p><h2 id="直觉"><a href="#直觉" class="headerlink" title="直觉"></a>直觉</h2><p>让我们从一个思想实验开始。假设你在网上查看评论来决定你是否要买Life麦片。你会首先阅读评论，确定某人认为麦片好不好</p><p><img src="/2020/06/06/rnn/lstm-gru-3.jpg"></p><p>当你阅读评论时，你的大脑下意识地只会记住重要的关键词。你会选择“amazing”和“perfectly balanced breakfast”这样的词汇。你不太关心“this”，“give”，“all”，“should”等字样。如果朋友第二天问你评论说什么，你一般不会一字不漏地记住它。你可能还记得主要观点，比如“will definitely be buying again”。如果你是这样，那么其他的单词就会从记忆中逐渐消失。</p><p><img src="/2020/06/06/rnn/lstm-gru-4.gif"></p><p>这就是LSTM或GRU的作用。它可以学习只保留相关信息来进行预测，忘记不相关的数据。在这种情况下，你记得的单词让你判断麦片是好的。</p><h2 id="RNN的回顾"><a href="#RNN的回顾" class="headerlink" title="RNN的回顾"></a>RNN的回顾</h2><p>为了理解LSTM或GRU如何实现这一点，让我们回顾一下RNN。RNN的工作原理是：第一个词被转换成机器可读的向量。然后RNN逐个处理向量序列。</p><p><img src="/2020/06/06/rnn/lstm-gru-2.gif"></p><p>逐个处理序列</p><p>处理时，它将先前的隐藏状态传递给序列的下一步。隐藏状态充当神经网络的记忆。它保存着网络以前见过的数据信息。</p><p><img src="/2020/06/06/rnn/lstm-gru-5.gif"></p><p>将隐藏状态传递给下一个时间步</p><p>让我们观察RNN的一个单元格，看看如何计算隐藏状态。首先，将输入和先前隐藏状态组合成一个向量。这个向量现在含有当前输入和先前输入的信息。向量经过tanh激活，输出新的隐藏状态，或网络的记忆。</p><p><img src="/2020/06/06/rnn/lstm-gru-6.gif"></p><h2 id="TANH激活"><a href="#TANH激活" class="headerlink" title="TANH激活"></a>TANH激活</h2><p>tanh激活用于帮助调节流经网络的值。tanh函数将值压缩在-1和1之间。</p><p><img src="/2020/06/06/rnn/lstm-gru-7.webp"></p><p>Tanh将值压缩到-1和1之间</p><p>当向量流经神经网络时，由于各种数学运算，它经历了许多变换。假设一个值连续乘以3。你可以看到某些值如何爆炸增长的，导致其他值看起来微不足道。</p><p><img src="/2020/06/06/rnn/lstm-gru-8.gif"></p><p>没有tanh的矢量变换</p><p>tanh函数确保值在-1和1之间，从而调节神经网络的输出。你可以看到上面的相同值通过tanh函数保持界限之间。</p><p><img src="/2020/06/06/rnn/lstm-gru-9.webp"></p><p>使用tanh的矢量变换</p><p>这是一个RNN。它内部的操作很少，但在适当的情况下（如短序列）工作得很好。RNN使用的计算资源比它的进化变体LSTM和GRU要少得多。</p><h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><p>LSTM具有与RNN类似的控制流。它在前向传播时处理传递信息的数据。区别在于LSTM单元内的操作。</p><p><img src="/2020/06/06/rnn/lstm-gru-10.jpg"></p><p>LSTM单元及其操作</p><p>这些操作用于允许LSTM保留或忘记信息。这些操作可能会有点难，所以我们将逐步介绍这些它们。</p><h2 id="核心概念"><a href="#核心概念" class="headerlink" title="核心概念"></a>核心概念</h2><p>LSTM的核心概念是单元状态（cell state），它是多种不同的门。单元状态充当传输的高速公路，在序列链中传递相关信息。你可以将其视为网络的记忆。理论上，单元状态可以在序列的整个处理过程中携带相关信息。因此，即使来自较早时间步的信息也可用于较晚时间步，从而减少短期记忆的影响。随着单元状态继续进行，信息通过门被添加或移除到单元状态。门是不同的神经网络，用来决定哪些信息可以允许进入单元状态。在训练中，门可以知道哪些信息是需要保存或忘记的。</p><h2 id="SIGMOID"><a href="#SIGMOID" class="headerlink" title="SIGMOID"></a>SIGMOID</h2><p>“门”包括sigmoid激活。它类似于tanh激活，但不是在-1和1之间压缩值，而是在0和1之间取值。这有助于更新或忘记数据，因为任何数字乘以0都是0，使值消失或者说被“遗忘”。任何数字乘以1都是相同的值，因此值保持相同”。网络可以了解哪些数据不重要可以被遗忘，或者哪些数据需要保存。</p><p><img src="/2020/06/06/rnn/lstm-gru-11.gif"></p><p>Sigmoid将值压缩至0和1之间</p><p>让我们深入了解不同的大门在做什么，不是吗？因此，我们有三个不同的门来调节LSTM单元中的信息流。忘记门，输入门和输出门。</p><h2 id="遗忘门"><a href="#遗忘门" class="headerlink" title="遗忘门"></a>遗忘门</h2><p>首先，我们介绍遗忘门（forget gate）。此门决定应丢弃或保留哪些信息。来自先前隐藏状态和来自当前输入的信息通过sigmoid函数传递。值介于0和1之间。越接近0越容易遗忘，越接近1则意味着要保留。</p><p><img src="/2020/06/06/rnn/lstm-gru-12.webp"></p><p>遗忘门操作</p><h2 id="输入门"><a href="#输入门" class="headerlink" title="输入门"></a>输入门</h2><p>要更新单元状态，我们需要输入门。首先，我们将先前的隐藏状态和当前输入传递给sigmoid函数。这决定了通过将值转换为0到1来更新哪些值。0表示不重要，1表示重要。你还将隐藏状态和当前输入传递给tanh函数，将它们压缩到-1和1之间以帮助调节网络。然后将tanh输出与sigmoid输出相乘。sigmoid输出将决定哪些信息很重要，需要tanh输出保存。</p><p><img src="/2020/06/06/rnn/lstm-gru-13.webp"></p><p>输入门操作</p><h2 id="单元状态"><a href="#单元状态" class="headerlink" title="单元状态"></a>单元状态</h2><p>现在我们有足够的信息来计算单元状态。首先，单元状态逐点乘以遗忘向量。如果它乘以接近0的值，则有可能在单元状态中丢弃值。然后我们从输入门获取输出并进行逐点加法，将单元状态更新为神经网络发现相关的新值。这就得到了新的单元状态。</p><p><img src="/2020/06/06/rnn/lstm-gru-14.webp"></p><p>计算细胞状态</p><h2 id="输出门"><a href="#输出门" class="headerlink" title="输出门"></a>输出门</h2><p>最后我们有输出门。输出门决定下一个隐藏状态是什么。请记住，隐藏状态包含有关先前输入的信息。隐藏状态也用于预测。首先，我们将先前的隐藏状态和当前输入传递给sigmoid函数。然后我们将新的单元状态传递给tanh函数。将tanh输出与sigmoid输出相乘，以决定隐藏状态应携带的信息。它的输出是隐藏状态。然后将新的单元状态和新的隐藏状态传递到下一个时间步。</p><p><img src="/2020/06/06/rnn/lstm-gru-15.gif"></p><p>输出门操作</p><p>回顾一下，遗忘门决定了哪些内容与前面的时间步相关。输入门决定了从当前时间步添加哪些信息。输出门决定下一个隐藏状态应该是什么。</p><h2 id="代码演示"><a href="#代码演示" class="headerlink" title="代码演示"></a>代码演示</h2><p>通过查看代码有些人可以更好的理解，以下是一个使用python伪代码的例子。</p><p><img src="/2020/06/06/rnn/lstm-gru-16.jpg"></p><p>python伪代码</p><p>就是这些！LSTM网络的控制流程是几个张量操作和一个for循环。你可以使用隐藏状态进行预测。结合所有这些机制，LSTM能够选择在序列处理期间需要记住或忘记哪些信息。</p><h2 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h2><p>所以现在我们知道LSTM是如何工作的，让我们简单地看一下GRU。GRU是新一代RNN，与LSTM非常相似。GRU不使用单元状态，而是使用隐藏状态来传输信息。它也只有两个门，一个重置门和一个更新门（reset gate and update gate）。</p><p><img src="/2020/06/06/rnn/lstm-gru-17.jpg"></p><p>GRU单元和它的门</p><h2 id="更新门"><a href="#更新门" class="headerlink" title="更新门"></a>更新门</h2><p>更新门的作用类似于LSTM的遗忘和输入门。它决定要丢弃哪些信息和要添加哪些新信息。</p><h2 id="重置门"><a href="#重置门" class="headerlink" title="重置门"></a>重置门</h2><p>重置门是另一个用来决定要忘记多少过去的信息的门。</p><p>这就是GRU。GRU的张量操作较少；因此，他们的训练速度要比LSTM快一些。但还说不清哪个更好。研究人员和工程师通常都会尝试，以确定哪一个更适合他们的用例。</p><h2 id="lstm的变种gru公式"><a href="#lstm的变种gru公式" class="headerlink" title="lstm的变种gru公式"></a>lstm的变种gru公式</h2><p>现在来讲述lstm的变种：循环门单元（Gated Recurrent Unit，GRU），由 <a href="https://link.zhihu.com/?target=http://arxiv.org/pdf/1406.1078v3.pdf">Cho, et al. (2014)</a>提出。它组合了遗忘门和输入门到一个单独的“更新门”中。它也合并了cell state和hidden state，并且做了一些其他的改变。结果模型比标准LSTM模型更简单，并且正越来越受欢迎，下面详细说明：</p><p><img src="/2020/06/06/rnn/lstm-gru-18.jpg"></p><p>首先介绍GRU的两个门，分别是reset gate $ r_{ t }$和<code>update gate</code>  $z_{ t }$，计算方法和LSTM中门的计算方法一致：</p><p><img src="/2020/06/06/rnn/lstm-gru-19.jpg"></p><p>然后是计算候选隐藏层（candidate hidden layer） $ \overset \sim{h_t}  $，这个候选隐藏层 和LSTM中的$\overset { \sim  }{ c } _{ t }$是类似，可以看成是当前时刻的新信息，其中  $ r_t $用来控制需要 保留多少之前的记忆，比如如果  $ r_{ t }$为0，那么 $\overset { \sim  }{ h }  _{ t }$ 只包含当前词的信息：</p><p><img src="/2020/06/06/rnn/lstm-gru-20.jpg"></p><p>最后  $z_{ t }$ 控制需要从前一时刻的隐藏层  ${ h }_{ t-1 }$中遗忘多少信息，需要加入多少当前 时刻的隐藏层信息   $\overset { \sim  }{ h }  _{ t }$ ，最后得到  $\overset { \sim  }{ h }  _{ t }$  ，直接得到最后输出的隐藏层信息， 需要注意这里与LSTM的区别是GRU中没有output gate：</p><p><img src="/2020/06/06/rnn/lstm-gru-21.jpg"></p><blockquote><p>一般来说那些具有短距离依赖的单元reset gate比较活跃（如果  $ r_{ t }$ 为1，而   $z_{ t }$ 为0 那么相当于变成了一个标准的RNN，能处理短距离依赖），具有长距离依赖的单元<code>update gate</code>比较活跃。</p></blockquote><p>​    </p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> nlp </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 循环神经网络 </tag>
            
            <tag> 序列模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>scrapy</title>
      <link href="/2020/06/01/scrapy/"/>
      <url>/2020/06/01/scrapy/</url>
      
        <content type="html"><![CDATA[<h1 id="scrapy"><a href="#scrapy" class="headerlink" title="scrapy"></a>scrapy</h1><h3 id="scrapy官方文档"><a href="#scrapy官方文档" class="headerlink" title="scrapy官方文档"></a>scrapy官方文档</h3><blockquote><p><a href="https://scrapy-chs.readthedocs.io/zh_CN/0.24/index.html">https://scrapy-chs.readthedocs.io/zh_CN/0.24/index.html</a></p></blockquote><h2 id="scrapy的概念"><a href="#scrapy的概念" class="headerlink" title="scrapy的概念"></a>scrapy的概念</h2><p><strong>Scrapy是一个Python编写的开源网络爬虫框架。它是一个被设计用于爬取网络数据、提取结构性数据的框架。</strong></p><blockquote><p>Scrapy 使用了Twisted[‘twɪstɪd]异步网络框架，可以加快我们的下载速度。</p></blockquote><blockquote><p>Scrapy文档地址：<a href="http://scrapy-chs.readthedocs.io/zh_CN/1.0/intro/overview.html">http://scrapy-chs.readthedocs.io/zh_CN/1.0/intro/overview.html</a></p></blockquote><h3 id="scrapy框架的作用"><a href="#scrapy框架的作用" class="headerlink" title="scrapy框架的作用"></a>scrapy框架的作用</h3><blockquote><p>少量的代码，就能够快速的抓取</p></blockquote><h3 id="scrapy的工作流程"><a href="#scrapy的工作流程" class="headerlink" title="scrapy的工作流程"></a>scrapy的工作流程</h3><h4 id="回顾之前的爬虫流程"><a href="#回顾之前的爬虫流程" class="headerlink" title="回顾之前的爬虫流程"></a>回顾之前的爬虫流程</h4><p><img src="/2020/06/01/scrapy/1.3.1.%E7%88%AC%E8%99%AB%E6%B5%81%E7%A8%8B-1.png"></p><h4 id="上面的流程可以改写为"><a href="#上面的流程可以改写为" class="headerlink" title="上面的流程可以改写为"></a>上面的流程可以改写为</h4><p><img src="/2020/06/01/scrapy/1.3.2.%E7%88%AC%E8%99%AB%E6%B5%81%E7%A8%8B-2.png"></p><h4 id="3-3-scrapy的流程"><a href="#3-3-scrapy的流程" class="headerlink" title="3.3 scrapy的流程"></a>3.3 scrapy的流程</h4><p><img src="/2020/06/01/scrapy/1.3.3.scrapy%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B.png"></p><h5 id="其流程可以描述如下："><a href="#其流程可以描述如下：" class="headerlink" title="其流程可以描述如下："></a>其流程可以描述如下：</h5><ol><li>爬虫中起始的url构造成request对象–&gt;爬虫中间件–&gt;引擎–&gt;调度器</li><li>调度器把request–&gt;引擎–&gt;下载中间件—&gt;下载器</li><li>下载器发送请求，获取response响应—-&gt;下载中间件—-&gt;引擎—&gt;爬虫中间件—&gt;爬虫</li><li>爬虫提取url地址，组装成request对象—-&gt;爬虫中间件—&gt;引擎—&gt;调度器，重复步骤2</li><li>爬虫提取数据—&gt;引擎—&gt;管道处理和保存数据</li></ol><h5 id="注意："><a href="#注意：" class="headerlink" title="注意："></a>注意：</h5><ul><li>图中中文是为了方便理解后加上去的</li><li>图中绿色线条的表示数据的传递</li><li>注意图中中间件的位置，决定了其作用</li><li>注意其中引擎的位置，所有的模块之前相互独立，只和引擎进行交互</li></ul><h4 id="scrapy的三个内置对象"><a href="#scrapy的三个内置对象" class="headerlink" title="scrapy的三个内置对象"></a>scrapy的三个内置对象</h4><ul><li>request请求对象：由url method post_data headers等构成</li><li>response响应对象：由url body status headers等构成</li><li>item数据对象：本质是个字典</li></ul><h4 id="scrapy中每个模块的具体作用"><a href="#scrapy中每个模块的具体作用" class="headerlink" title="scrapy中每个模块的具体作用"></a>scrapy中每个模块的具体作用</h4><h5 id="注意：-1"><a href="#注意：-1" class="headerlink" title="注意："></a><img src="/2020/06/01/scrapy/1.3.4%E7%BB%84%E4%BB%B6%E4%BD%9C%E7%94%A8.png">注意：</h5><ul><li>爬虫中间件和下载中间件只是运行逻辑的位置不同，作用是重复的：如替换UA等</li></ul><h2 id="scrapy的入门使用"><a href="#scrapy的入门使用" class="headerlink" title="scrapy的入门使用"></a>scrapy的入门使用</h2><h3 id="安装scrapy"><a href="#安装scrapy" class="headerlink" title="安装scrapy"></a>安装scrapy<br></h3><p>命令:<br><br>    sudo apt-get install scrapy<br><br>或者：<br><br>    pip/pip3 install scrapy</p><h3 id="2-scrapy项目开发流程"><a href="#2-scrapy项目开发流程" class="headerlink" title="2 scrapy项目开发流程"></a>2 scrapy项目开发流程</h3><ol><li>创建项目:<br><br>    scrapy startproject mySpider</li><li>生成一个爬虫:<br><br>    scrapy genspider xywx xywx.com</li><li>提取数据:<br><br>    根据网站结构在spider中实现数据采集相关内容</li><li>保存数据:<br><br>    使用pipeline进行数据后续处理和保存</li></ol><h3 id="3-创建项目"><a href="#3-创建项目" class="headerlink" title="3. 创建项目"></a>3. 创建项目</h3><blockquote><p>通过命令将scrapy项目的的文件生成出来，后续步骤都是在项目文件中进行相关操作</p><p>创建scrapy项目的命令：<br><br>    scrapy startproject &lt;项目名字&gt;<br><br>示例：<br><br>    scrapy startproject myspider</p></blockquote><p>生成的目录和文件结果如下：<img src="/2020/06/01/scrapy/2.1.scrapy入门使用-1.png" width="60%"> </p><h3 id="4-创建爬虫"><a href="#4-创建爬虫" class="headerlink" title="4. 创建爬虫"></a>4. 创建爬虫</h3><blockquote><p>通过命令创建出爬虫文件，爬虫文件为主要的代码作业文件，通常一个网站的爬取动作都会在爬虫文件中进行编写。</p></blockquote><p>命令：<br><br>    <strong>在项目路径下执行</strong>:<br><br>    scrapy genspider &lt;爬虫名字&gt; &lt;允许爬取的域名&gt;<br></p><p><strong>爬虫名字</strong>: 作为爬虫运行时的参数<br><br><strong>允许爬取的域名</strong>: 为对于爬虫设置的爬取范围，设置之后用于过滤要爬取的url，如果爬取的url与允许的域不通则被过滤掉。<br></p><p>示例：</p><pre class=" language-shell"><code class="language-shell">    cd myspider    scrapy genspider xywy  xywy.com</code></pre><h3 id="5-完善爬虫"><a href="#5-完善爬虫" class="headerlink" title="5. 完善爬虫"></a>5. 完善爬虫</h3><h4 id="5-1-在-myspider-myspider-spiders-xywx-py中修改内容如下"><a href="#5-1-在-myspider-myspider-spiders-xywx-py中修改内容如下" class="headerlink" title="5.1 在/myspider/myspider/spiders/xywx.py中修改内容如下:"></a>5.1 在/myspider/myspider/spiders/xywx.py中修改内容如下:</h4><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> scrapy<span class="token keyword">from</span> myspider<span class="token punctuation">.</span>items <span class="token keyword">import</span> MyspiderItem<span class="token keyword">from</span>  copy <span class="token keyword">import</span> deepcopy<span class="token keyword">class</span> <span class="token class-name">XywySpider</span><span class="token punctuation">(</span>scrapy<span class="token punctuation">.</span>Spider<span class="token punctuation">)</span><span class="token punctuation">:</span>    name <span class="token operator">=</span> <span class="token string">'xywy'</span>    allowed_domains <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'xywy.com'</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># 允许爬取的范围</span>    start_urls <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'http://jib.xywy.com/'</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># 开始爬取的url地址</span>    <span class="token keyword">def</span> <span class="token function">parse</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> response<span class="token punctuation">)</span><span class="token punctuation">:</span>        item <span class="token operator">=</span> MyspiderItem<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 实例化后可直接使用</span>        <span class="token keyword">for</span> page <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">11000</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">try</span><span class="token punctuation">:</span>                item<span class="token punctuation">[</span><span class="token string">'id'</span><span class="token punctuation">]</span> <span class="token operator">=</span> page                basic_url <span class="token operator">=</span> <span class="token string">'http://jib.xywy.com/il_sii/gaishu/%s.htm'</span> <span class="token operator">%</span> page  <span class="token comment" spellcheck="true">## 简介</span>                <span class="token keyword">yield</span> scrapy<span class="token punctuation">.</span>Request<span class="token punctuation">(</span>basic_url<span class="token punctuation">,</span> callback<span class="token operator">=</span>self<span class="token punctuation">.</span>parse_basic_url<span class="token punctuation">,</span> meta<span class="token operator">=</span><span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;"item": deepcopy(item)&amp;#125;)</span>            <span class="token keyword">except</span> Exception <span class="token keyword">as</span> e<span class="token punctuation">:</span>                <span class="token keyword">print</span><span class="token punctuation">(</span>e<span class="token punctuation">,</span> page<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">parse_basic_url</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> response<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""        基本信息解析        :param response:        :return:        """</span>        item <span class="token operator">=</span> response<span class="token punctuation">.</span>meta<span class="token punctuation">[</span><span class="token string">'item'</span><span class="token punctuation">]</span>        title <span class="token operator">=</span> response<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">'//strong[@class="db f20 fYaHei fb jib-articl-tit tc pr"]/text()'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract_first<span class="token punctuation">(</span><span class="token punctuation">)</span>        category <span class="token operator">=</span> response<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">'//div[@class="wrap mt10 nav-bar"]/a/text()'</span><span class="token punctuation">)</span>        desc <span class="token operator">=</span> response<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">'//div[@class="jib-articl-con jib-lh-articl"]/p/text()'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract_first<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span>        ps <span class="token operator">=</span> response<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">'//div[@class="mt20 articl-know"]/p'</span><span class="token punctuation">)</span>        infobox <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        <span class="token keyword">for</span> p <span class="token keyword">in</span> ps<span class="token punctuation">:</span>            info <span class="token operator">=</span> p<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">'string(.)'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract_first<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">'\r'</span><span class="token punctuation">,</span> <span class="token string">''</span><span class="token punctuation">)</span><span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">'\n'</span><span class="token punctuation">,</span> <span class="token string">''</span><span class="token punctuation">)</span><span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">'\xa0'</span><span class="token punctuation">,</span> <span class="token string">''</span><span class="token punctuation">)</span><span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">'   '</span><span class="token punctuation">,</span> <span class="token string">''</span><span class="token punctuation">)</span><span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">'\t'</span><span class="token punctuation">,</span> <span class="token string">''</span><span class="token punctuation">)</span>            infobox<span class="token punctuation">.</span>append<span class="token punctuation">(</span>info<span class="token punctuation">)</span>        category_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        <span class="token keyword">for</span> cate <span class="token keyword">in</span> category<span class="token punctuation">:</span> <span class="token comment" spellcheck="true">## 疾病分类, 多分类</span>            category_content <span class="token operator">=</span> cate<span class="token punctuation">.</span>extract<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span>            <span class="token keyword">if</span> category_content <span class="token operator">and</span> category_content <span class="token operator">!=</span> <span class="token string">"疾病百科"</span><span class="token punctuation">:</span> <span class="token comment" spellcheck="true">## 去掉不需要的分类</span>                category_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>cate<span class="token punctuation">.</span>extract<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        basic_data <span class="token operator">=</span> <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;&amp;#125;</span>        basic_data<span class="token punctuation">[</span><span class="token string">'category'</span><span class="token punctuation">]</span> <span class="token operator">=</span> category_list        basic_data<span class="token punctuation">[</span><span class="token string">'name'</span><span class="token punctuation">]</span> <span class="token operator">=</span> title<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">'简介'</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        basic_data<span class="token punctuation">[</span><span class="token string">'desc'</span><span class="token punctuation">]</span> <span class="token operator">=</span> desc        basic_data<span class="token punctuation">[</span><span class="token string">'attributes'</span><span class="token punctuation">]</span> <span class="token operator">=</span> infobox        item<span class="token punctuation">[</span><span class="token string">'basic'</span><span class="token punctuation">]</span> <span class="token operator">=</span> basic_data        cause_url <span class="token operator">=</span> <span class="token string">'http://jib.xywy.com/il_sii/cause/%s.htm'</span> <span class="token operator">%</span> item<span class="token punctuation">[</span><span class="token string">'id'</span><span class="token punctuation">]</span>  <span class="token comment" spellcheck="true">## 原因</span>        <span class="token keyword">yield</span> scrapy<span class="token punctuation">.</span>Request<span class="token punctuation">(</span>cause_url<span class="token punctuation">,</span> callback<span class="token operator">=</span>self<span class="token punctuation">.</span>parse_cause_url<span class="token punctuation">,</span> meta<span class="token operator">=</span><span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;"item": deepcopy(item)&amp;#125;)</span>    <span class="token keyword">def</span> <span class="token function">parse_cause_url</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> response<span class="token punctuation">)</span><span class="token punctuation">:</span>        item <span class="token operator">=</span> response<span class="token punctuation">.</span>meta<span class="token punctuation">[</span><span class="token string">'item'</span><span class="token punctuation">]</span>        ps <span class="token operator">=</span> response<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">'//p'</span><span class="token punctuation">)</span>        infobox <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        <span class="token keyword">for</span> p <span class="token keyword">in</span> ps<span class="token punctuation">:</span>            info <span class="token operator">=</span> p<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">'string(.)'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract_first<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">'\r'</span><span class="token punctuation">,</span> <span class="token string">''</span><span class="token punctuation">)</span><span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">'\n'</span><span class="token punctuation">,</span> <span class="token string">''</span><span class="token punctuation">)</span><span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">'\xa0'</span><span class="token punctuation">,</span> <span class="token string">''</span><span class="token punctuation">)</span><span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">'   '</span><span class="token punctuation">,</span> <span class="token string">''</span><span class="token punctuation">)</span><span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">'\t'</span><span class="token punctuation">,</span> <span class="token string">''</span><span class="token punctuation">)</span>            <span class="token keyword">if</span> info<span class="token punctuation">:</span>                infobox<span class="token punctuation">.</span>append<span class="token punctuation">(</span>info<span class="token punctuation">)</span>        item<span class="token punctuation">[</span><span class="token string">'cause'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">'\n'</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>infobox<span class="token punctuation">)</span>        prevent_url <span class="token operator">=</span> <span class="token string">'http://jib.xywy.com/il_sii/prevent/%s.htm'</span> <span class="token operator">%</span> item<span class="token punctuation">[</span><span class="token string">'id'</span><span class="token punctuation">]</span>  <span class="token comment" spellcheck="true">##治疗</span>        <span class="token keyword">yield</span> scrapy<span class="token punctuation">.</span>Request<span class="token punctuation">(</span>prevent_url<span class="token punctuation">,</span> callback<span class="token operator">=</span>self<span class="token punctuation">.</span>parse_prevent_url<span class="token punctuation">,</span> meta<span class="token operator">=</span><span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;"item": deepcopy(item)&amp;#125;)</span>    <span class="token keyword">def</span> <span class="token function">parse_prevent_url</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> response<span class="token punctuation">)</span><span class="token punctuation">:</span>        item <span class="token operator">=</span> response<span class="token punctuation">.</span>meta<span class="token punctuation">[</span><span class="token string">'item'</span><span class="token punctuation">]</span>        ps <span class="token operator">=</span> response<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">'//p'</span><span class="token punctuation">)</span>        infobox <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        <span class="token keyword">for</span> p <span class="token keyword">in</span> ps<span class="token punctuation">:</span>            info <span class="token operator">=</span> p<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">'string(.)'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract_first<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">'\r'</span><span class="token punctuation">,</span> <span class="token string">''</span><span class="token punctuation">)</span><span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">'\n'</span><span class="token punctuation">,</span> <span class="token string">''</span><span class="token punctuation">)</span><span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">'\xa0'</span><span class="token punctuation">,</span> <span class="token string">''</span><span class="token punctuation">)</span><span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">'   '</span><span class="token punctuation">,</span> <span class="token string">''</span><span class="token punctuation">)</span><span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">'\t'</span><span class="token punctuation">,</span> <span class="token string">''</span><span class="token punctuation">)</span>            <span class="token keyword">if</span> info<span class="token punctuation">:</span>                infobox<span class="token punctuation">.</span>append<span class="token punctuation">(</span>info<span class="token punctuation">)</span>        item<span class="token punctuation">[</span><span class="token string">'prevent'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">'\n'</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>infobox<span class="token punctuation">)</span>        symptom_url <span class="token operator">=</span> <span class="token string">'http://jib.xywy.com/il_sii/symptom/%s.htm'</span> <span class="token operator">%</span> item<span class="token punctuation">[</span><span class="token string">'id'</span><span class="token punctuation">]</span>  <span class="token comment" spellcheck="true">##</span>        <span class="token keyword">yield</span> scrapy<span class="token punctuation">.</span>Request<span class="token punctuation">(</span>symptom_url<span class="token punctuation">,</span> callback<span class="token operator">=</span>self<span class="token punctuation">.</span>parse_symptom_url<span class="token punctuation">,</span> meta<span class="token operator">=</span><span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;"item": deepcopy(item)&amp;#125;)</span>    <span class="token keyword">def</span> <span class="token function">parse_symptom_url</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> response<span class="token punctuation">)</span><span class="token punctuation">:</span>        item <span class="token operator">=</span> response<span class="token punctuation">.</span>meta<span class="token punctuation">[</span><span class="token string">'item'</span><span class="token punctuation">]</span>        symptoms <span class="token operator">=</span> response<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">'//a[@class="gre" ]/text()'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract_first<span class="token punctuation">(</span><span class="token punctuation">)</span>        ps <span class="token operator">=</span> response<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">'//p'</span><span class="token punctuation">)</span>        detail <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        <span class="token keyword">for</span> p <span class="token keyword">in</span> ps<span class="token punctuation">:</span>            info <span class="token operator">=</span> p<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">'string(.)'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract_first<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">'\r'</span><span class="token punctuation">,</span> <span class="token string">''</span><span class="token punctuation">)</span><span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">'\n'</span><span class="token punctuation">,</span> <span class="token string">''</span><span class="token punctuation">)</span><span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">'\xa0'</span><span class="token punctuation">,</span> <span class="token string">''</span><span class="token punctuation">)</span><span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">'   '</span><span class="token punctuation">,</span> <span class="token string">''</span><span class="token punctuation">)</span><span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">'\t'</span><span class="token punctuation">,</span> <span class="token string">''</span><span class="token punctuation">)</span>            detail<span class="token punctuation">.</span>append<span class="token punctuation">(</span>info<span class="token punctuation">)</span>        symptoms_data <span class="token operator">=</span> <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;&amp;#125;</span>        symptoms_data<span class="token punctuation">[</span><span class="token string">'symptoms'</span><span class="token punctuation">]</span> <span class="token operator">=</span> symptoms        symptoms_data<span class="token punctuation">[</span><span class="token string">'symptoms_detail'</span><span class="token punctuation">]</span> <span class="token operator">=</span> detail        item<span class="token punctuation">[</span><span class="token string">'symptom'</span><span class="token punctuation">]</span> <span class="token operator">=</span> symptoms_data        inspect_url <span class="token operator">=</span> <span class="token string">'http://jib.xywy.com/il_sii/inspect/%s.htm'</span> <span class="token operator">%</span> item<span class="token punctuation">[</span><span class="token string">'id'</span><span class="token punctuation">]</span>  <span class="token comment" spellcheck="true">##</span>        <span class="token keyword">yield</span> scrapy<span class="token punctuation">.</span>Request<span class="token punctuation">(</span>inspect_url<span class="token punctuation">,</span> callback<span class="token operator">=</span>self<span class="token punctuation">.</span>parse_inspect_url<span class="token punctuation">,</span> meta<span class="token operator">=</span><span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;"item": deepcopy(item)&amp;#125;)</span>    <span class="token keyword">def</span> <span class="token function">parse_inspect_url</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> response<span class="token punctuation">)</span><span class="token punctuation">:</span>        item <span class="token operator">=</span> response<span class="token punctuation">.</span>meta<span class="token punctuation">[</span><span class="token string">'item'</span><span class="token punctuation">]</span>        inspects <span class="token operator">=</span> response<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">'//li[@class="check-item"]/a/@href'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract_first<span class="token punctuation">(</span><span class="token punctuation">)</span>        item<span class="token punctuation">[</span><span class="token string">'inspect'</span><span class="token punctuation">]</span> <span class="token operator">=</span> inspects        treat_url <span class="token operator">=</span> <span class="token string">'http://jib.xywy.com/il_sii/treat/%s.htm'</span> <span class="token operator">%</span> item<span class="token punctuation">[</span><span class="token string">'id'</span><span class="token punctuation">]</span>  <span class="token comment" spellcheck="true">## 治疗</span>        <span class="token keyword">yield</span> scrapy<span class="token punctuation">.</span>Request<span class="token punctuation">(</span>treat_url<span class="token punctuation">,</span> callback<span class="token operator">=</span>self<span class="token punctuation">.</span>parse_treat_url<span class="token punctuation">,</span> meta<span class="token operator">=</span><span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;"item": deepcopy(item)&amp;#125;)</span>    <span class="token keyword">def</span> <span class="token function">parse_treat_url</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> response<span class="token punctuation">)</span><span class="token punctuation">:</span>        item <span class="token operator">=</span> response<span class="token punctuation">.</span>meta<span class="token punctuation">[</span><span class="token string">'item'</span><span class="token punctuation">]</span>        ps <span class="token operator">=</span> response<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">'//div[starts-with(@class,"mt20 articl-know")]/p'</span><span class="token punctuation">)</span>        infobox <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        <span class="token keyword">for</span> p <span class="token keyword">in</span> ps<span class="token punctuation">:</span>            info <span class="token operator">=</span> p<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">'string(.)'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract_first<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">'\r'</span><span class="token punctuation">,</span> <span class="token string">''</span><span class="token punctuation">)</span><span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">'\n'</span><span class="token punctuation">,</span> <span class="token string">''</span><span class="token punctuation">)</span><span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">'\xa0'</span><span class="token punctuation">,</span> <span class="token string">''</span><span class="token punctuation">)</span><span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">'   '</span><span class="token punctuation">,</span> <span class="token string">''</span><span class="token punctuation">)</span><span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">'\t'</span><span class="token punctuation">,</span> <span class="token string">''</span><span class="token punctuation">)</span>            infobox<span class="token punctuation">.</span>append<span class="token punctuation">(</span>info<span class="token punctuation">)</span>        item<span class="token punctuation">[</span><span class="token string">'treat'</span><span class="token punctuation">]</span> <span class="token operator">=</span> infobox        food_url <span class="token operator">=</span> <span class="token string">'http://jib.xywy.com/il_sii/food/%s.htm'</span> <span class="token operator">%</span> item<span class="token punctuation">[</span><span class="token string">'id'</span><span class="token punctuation">]</span>  <span class="token comment" spellcheck="true">## 食物</span>        <span class="token keyword">yield</span> scrapy<span class="token punctuation">.</span>Request<span class="token punctuation">(</span>food_url<span class="token punctuation">,</span> callback<span class="token operator">=</span>self<span class="token punctuation">.</span>parse_food_url<span class="token punctuation">,</span> meta<span class="token operator">=</span><span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;"item": deepcopy(item)&amp;#125;)</span>    <span class="token keyword">def</span> <span class="token function">parse_food_url</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> response<span class="token punctuation">)</span><span class="token punctuation">:</span>        item <span class="token operator">=</span> response<span class="token punctuation">.</span>meta<span class="token punctuation">[</span><span class="token string">'item'</span><span class="token punctuation">]</span>        divs <span class="token operator">=</span> response<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">'//div[@class="diet-img clearfix mt20"]'</span><span class="token punctuation">)</span>        food_data <span class="token operator">=</span> <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;&amp;#125;</span>        <span class="token keyword">try</span><span class="token punctuation">:</span>            food_data<span class="token punctuation">[</span><span class="token string">'good'</span><span class="token punctuation">]</span> <span class="token operator">=</span> divs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">'./div/p/text()'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract_first<span class="token punctuation">(</span><span class="token punctuation">)</span>            food_data<span class="token punctuation">[</span><span class="token string">'bad'</span><span class="token punctuation">]</span> <span class="token operator">=</span> divs<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">'./div/p/text()'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract_first<span class="token punctuation">(</span><span class="token punctuation">)</span>            food_data<span class="token punctuation">[</span><span class="token string">'recommand'</span><span class="token punctuation">]</span> <span class="token operator">=</span> divs<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">'./div/p/text()'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract_first<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">except</span> Exception <span class="token keyword">as</span> e<span class="token punctuation">:</span>            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"food"</span> <span class="token operator">+</span> str<span class="token punctuation">(</span>e<span class="token punctuation">)</span><span class="token punctuation">)</span>        item<span class="token punctuation">[</span><span class="token string">'food'</span><span class="token punctuation">]</span> <span class="token operator">=</span> food_data        drug_url <span class="token operator">=</span> <span class="token string">'http://jib.xywy.com/il_sii/drug/%s.htm'</span> <span class="token operator">%</span> item<span class="token punctuation">[</span><span class="token string">'id'</span><span class="token punctuation">]</span>  <span class="token comment" spellcheck="true">## 药物</span>        <span class="token keyword">yield</span> scrapy<span class="token punctuation">.</span>Request<span class="token punctuation">(</span>drug_url<span class="token punctuation">,</span> callback<span class="token operator">=</span>self<span class="token punctuation">.</span>parse_drug_url<span class="token punctuation">,</span> meta<span class="token operator">=</span><span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;"item": deepcopy(item)&amp;#125;)</span>    <span class="token keyword">def</span> <span class="token function">parse_drug_url</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> response<span class="token punctuation">)</span><span class="token punctuation">:</span>        item <span class="token operator">=</span> response<span class="token punctuation">.</span>meta<span class="token punctuation">[</span><span class="token string">'item'</span><span class="token punctuation">]</span>        <span class="token keyword">try</span><span class="token punctuation">:</span>            drugs <span class="token operator">=</span> <span class="token punctuation">[</span>i<span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">'\n'</span><span class="token punctuation">,</span> <span class="token string">''</span><span class="token punctuation">)</span><span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">'\t'</span><span class="token punctuation">,</span> <span class="token string">''</span><span class="token punctuation">)</span><span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">' '</span><span class="token punctuation">,</span> <span class="token string">''</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> response<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">'//div[@class="fl drug-pic-rec mr30"]/p/a/text()'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span>        <span class="token keyword">except</span> Exception <span class="token keyword">as</span> e<span class="token punctuation">:</span>            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"drup"</span><span class="token operator">+</span>str<span class="token punctuation">(</span>e<span class="token punctuation">)</span><span class="token punctuation">)</span>        item<span class="token punctuation">[</span><span class="token string">'drug'</span><span class="token punctuation">]</span> <span class="token operator">=</span> drugs        <span class="token keyword">yield</span> item</code></pre><h5 id="注意：-2"><a href="#注意：-2" class="headerlink" title="注意："></a>注意：</h5><ul><li>scrapy.Spider爬虫类中必须有名为parse的解析</li><li>如果网站结构层次比较复杂，也可以自定义其他解析函数</li><li>在解析函数中提取的url地址如果要发送请求，则必须属于allowed_domains范围内，但是start_urls中的url地址不受这个限制，我们会在后续的课程中学习如何在解析函数中构造发送请求</li><li>启动爬虫的时候注意启动的位置，是在项目路径下启动</li><li>parse()函数中使用yield返回数据，<strong>注意：解析函数中的yield能够传递的对象只能是：BaseItem, Request, dict, None</strong></li></ul><h4 id="5-2-定位元素以及提取数据、属性值的方法"><a href="#5-2-定位元素以及提取数据、属性值的方法" class="headerlink" title="5.2 定位元素以及提取数据、属性值的方法"></a>5.2 定位元素以及提取数据、属性值的方法</h4><blockquote><p>解析并获取scrapy爬虫中的数据: 利用xpath规则字符串进行定位和提取</p></blockquote><ol><li>response.xpath方法的返回结果是一个类似list的类型，其中包含的是selector对象，操作和列表一样，但是有一些额外的方法</li><li>额外方法extract()：返回一个包含有字符串的列表</li><li>额外方法extract_first()：返回列表中的第一个字符串，列表为空没有返回None</li></ol><h4 id="5-3-response响应对象的常用属性"><a href="#5-3-response响应对象的常用属性" class="headerlink" title="5.3 response响应对象的常用属性"></a>5.3 response响应对象的常用属性</h4><ul><li>response.url：当前响应的url地址</li><li>response.request.url：当前响应对应的请求的url地址</li><li>response.headers：响应头</li><li>response.requests.headers：当前响应的请求头</li><li>response.body：响应体，也就是html代码，byte类型</li><li>response.status：响应状态码</li></ul><h3 id="6-保存数据"><a href="#6-保存数据" class="headerlink" title="6 保存数据"></a>6 保存数据</h3><blockquote><p>利用管道pipeline来处理(保存)数据</p></blockquote><h4 id="6-1-在pipelines-py文件中定义对数据的操作"><a href="#6-1-在pipelines-py文件中定义对数据的操作" class="headerlink" title="6.1 在pipelines.py文件中定义对数据的操作"></a>6.1 在pipelines.py文件中定义对数据的操作</h4><ol><li>定义一个管道类<br></li><li>重写管道类的process_item方法</li><li>process_item方法处理完item之后必须返回给引擎</li></ol><pre><code>import jsonclass xywxPipeline():    # 爬虫文件中提取数据的方法每yield一次item，就会运行一次    # 该方法为固定名称函数    def process_item(self, item, spider):        print(item)        return item</code></pre><h4 id="6-2-在settings-py配置启用管道"><a href="#6-2-在settings-py配置启用管道" class="headerlink" title="6.2 在settings.py配置启用管道"></a>6.2 在settings.py配置启用管道</h4><pre><code>ITEM_PIPELINES = &amp;#123;    'myspider.pipelines.xywxPipeline': 400&amp;#125;</code></pre><p>配置项中键为使用的管道类，管道类使用.进行分割，第一个为项目目录，第二个为文件，第三个为定义的管道类。<br><br>配置项中值为管道的使用顺序，设置的数值约小越优先执行，该值一般设置为1000以内。<br></p><h3 id="7-运行scrapy"><a href="#7-运行scrapy" class="headerlink" title="7. 运行scrapy"></a>7. 运行scrapy</h3><p>命令：在项目目录下执行scrapy crawl &lt;爬虫名字&gt;</p><p>示例：scrapy crawl xywx</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 爬虫 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据采集 </tag>
            
            <tag> 数据处理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>docker</title>
      <link href="/2020/05/15/docker/"/>
      <url>/2020/05/15/docker/</url>
      
        <content type="html"><![CDATA[<h1 id="docker"><a href="#docker" class="headerlink" title="docker"></a>docker</h1><h2 id="一、安装Docker"><a href="#一、安装Docker" class="headerlink" title="一、安装Docker"></a>一、安装Docker</h2><pre class=" language-shell"><code class="language-shell"># 1、yum 包更新到最新 yum update# 2、安装需要的软件包， yum-util 提供yum-config-manager功能，另外两个是devicemapper驱动依赖的 yum install -y yum-utils device-mapper-persistent-data lvm2# 3、 设置yum源yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo# 4、 安装docker，出现输入的界面都按 y yum install -y docker-ce# 5、 查看docker版本，验证是否验证成功docker -v</code></pre><h2 id="Docker-常见命令"><a href="#Docker-常见命令" class="headerlink" title="Docker 常见命令"></a>Docker 常见命令</h2><pre class=" language-shell"><code class="language-shell">step3：启动docker服务systemclt start dockerstep4：查看是否启动成功systemclt status docker自动启动systemctl&nbsp;&nbsp;enable&nbsp;dockerstep5：使用docker命令&nbsp;docker images备注：查看docker已经安装的镜像 裸机状态下为空三 使用docker安装mysqlstep1：使用docker pull 拉取docker hub仓库中mysql镜像 （注意备注）mysql镜像版本如下：docker pull mysql:8.0备注：docker pull 默认到官方参考拉取 &nbsp;mysql：8.0 &nbsp; 镜像名：镜像tag &nbsp; &nbsp;&nbsp;官方地址为国外地址安装缓慢可进行配置step2：修改镜像文件拉取地址为ustc&nbsp;vi /etc/docker/daemon.json备注：insert 编辑内容如下 &nbsp; &nbsp; &nbsp;并esc &nbsp;输入:wq 保存退出 &nbsp;&#123;"registry-mirrors":["https://docker.mirrors.ustc.edu.cn"]&#125;step3：重启docker &nbsp;&nbsp;重新进行step1 安装速度变快systemclt restart dockerstep4：查看镜像安装情况docker images创建容器命令：docker run-i：表示运行容器-t：表示容器启动后会进入其命令行。加入这两个参数后，容器创建就能登录进去。即分配一个伪终端。--name :为创建的容器命名。-v：表示目录映射关系（前者是宿主机目录，后者是映射到宿主机上的目录），可以使用多个－v做多个目录或文件映射。注意：最好做目录映射，在宿主机上做修改，然后共享到容器上。-d：在run后面加上-d参数,则会创建一个守护式容器在后台运行（这样创建容器后不会自动登录容器，如果只加-i -t两个参数，创建后就会自动进去容器）。-p：表示端口映射，前者是宿主机端口，后者是容器内的映射端口。可以使用多个-p做多个端口映射（1）交互式方式创建容器docker run -it --name=容器名称 镜像名称:标签 /bin/bash这时我们通过ps命令查看，发现可以看到启动的容器，状态为启动状态 退出当前容器exit（2）守护式方式创建容器：docker run -di --name=容器名称 镜像名称:标签登录守护式容器方式：docker exec -it 容器名称 (或者容器ID) /bin/bash如果我们需要将文件拷贝到容器内可以使用cp命令docker cp 需要拷贝的文件或目录 容器名称:容器目录也可以将文件从容器内拷贝出来docker cp 容器名称:容器目录 需要拷贝的文件或目录目录挂载我们可以在创建容器的时候，将宿主机的目录与容器内的目录进行映射，这样我们就可以通过修改宿主机某个目录的文件从而去影响容器。创建容器 添加-v参数 后边为   宿主机目录:容器目录，例如：docker run -di -v /usr/local/myhtml:/usr/local/myhtml --name=mycentos3 centos:7如果你共享的是多级的目录，可能会出现权限不足的提示。这是因为CentOS7中的安全模块selinux把权限禁掉了，我们需要添加参数  --privileged=true  来解决挂载的目录没有权限的问题查看容器IP地址我们可以通过以下命令查看容器运行的各种数据docker inspect 容器名称（容器ID） 也可以直接执行下面的命令直接输出IP地址docker inspect --format='&#123;&#123;.NetworkSettings.IPAddress&#125;&#125;' 容器名称（容器ID）step5：启动docker中镜像docker run --name mysql8.0 -p 3307:3306 -e MYSQL_ROOT_PASSWORD=root -d mysql:8.0或者sudo&nbsp;docker&nbsp;run&nbsp;-d&nbsp;-p&nbsp;3307:3306&nbsp;--restart&nbsp;always&nbsp;-e&nbsp;MYSQL_ROOT_PASSWORD="123456"&nbsp;--name&nbsp;mysql57&nbsp;-v&nbsp;/docker/mysql56/config/my.cnf:/etc/my.cnf&nbsp;-v&nbsp;/docker/mysql57/data:/var/lib/mysql&nbsp;mysql:5.6备注 ：--name 服务启动别名设置 &nbsp;-p端口映射 宿主机端口：镜像运行端口 &nbsp;-d 镜像名：tag 使用守护进程模式启动 -e：设置root帐号密码step6：查看运行的镜像docker ps -astep7：进入镜像 运行mysqldocker exec -it mysql8.0 /bin/bash备注：exec docker进入容器命令 &nbsp; -it 容器中服务别名 /bin/bash &nbsp; 表示命令行模式 &nbsp;与 -d 后台守护进行模式启动 形成两种运行方式 &nbsp;&nbsp;进入容器中如图所示变化如下cd /usr/binmysql -u root -p备注： 在容器中进入用户目录启动mysql 输入密码连接成功四 其他命令step1：删除安装包yum remove -y 安装包名step2：停止/开始服务systemclt stop dockersystemclt start dockerstep2：移除已经安装镜像docker rmi 镜像名：tag   or  镜像idstep3：停止/启动/删除容器服务docker stop 容器服务别名  or 容器iddocker start 容器服务别名  or 容器iddocker rm 容器服务别名  or 容器id</code></pre><h2 id="应用部署"><a href="#应用部署" class="headerlink" title="应用部署"></a>应用部署</h2><h3 id="MySQL部署"><a href="#MySQL部署" class="headerlink" title="MySQL部署"></a>MySQL部署</h3><p>（1）拉取mysql镜像</p><pre><code>docker pull centos/mysql-57-centos7</code></pre><p>（2）创建容器</p><pre><code>docker run -di --name=tensquare_mysql -p 33306:3306 -e MYSQL_ROOT_PASSWORD=123456 mysql</code></pre><p>-p 代表端口映射，格式为 宿主机映射端口:容器运行端口</p><p>-e 代表添加环境变量 MYSQL_ROOT_PASSWORD  是root用户的登陆密码</p><p>（3）远程登录mysql</p><p>连接宿主机的IP  ,指定端口为33306 </p><h3 id="tomcat部署"><a href="#tomcat部署" class="headerlink" title="tomcat部署"></a>tomcat部署</h3><p>（1）拉取镜像</p><pre><code>docker pull tomcat:7-jre7</code></pre><p>（2）创建容器</p><p>创建容器 -p表示地址映射</p><pre><code>docker run -di --name=mytomcat -p 9000:8080 -v /usr/local/webapps:/usr/local/tomcat/webapps tomcat:7-jre7</code></pre><h3 id="Nginx部署"><a href="#Nginx部署" class="headerlink" title="Nginx部署"></a>Nginx部署</h3><p>（1）拉取镜像    </p><pre><code>docker pull nginx</code></pre><p>（2）创建Nginx容器</p><pre><code>docker run -di --name=mynginx -p 80:80 nginx</code></pre><h3 id="Redis部署"><a href="#Redis部署" class="headerlink" title="Redis部署"></a>Redis部署</h3><p>（1）拉取镜像</p><pre><code>docker pull redis</code></pre><p>（2）创建容器</p><pre><code>docker run -di --name=myredis -p 6379:6379 redis</code></pre><h2 id="迁移与备份"><a href="#迁移与备份" class="headerlink" title="迁移与备份"></a>迁移与备份</h2><pre class=" language-shell"><code class="language-shell">容器保存为镜像我们可以通过以下命令将容器保存为镜像docker commit mynginx mynginx_i镜像备份我们可以通过以下命令将镜像保存为tar 文件docker  save -o mynginx.tar mynginx_i镜像恢复与迁移首先我们先删除掉mynginx_img镜像  然后执行此命令进行恢复docker load -i mynginx.tar-i 输入的文件执行后再次查看镜像，可以看到镜像已经恢复</code></pre><h2 id="Dockerfile"><a href="#Dockerfile" class="headerlink" title="Dockerfile"></a>Dockerfile</h2><h3 id="什么是Dockerfile"><a href="#什么是Dockerfile" class="headerlink" title="什么是Dockerfile"></a>什么是Dockerfile</h3><p>Dockerfile是由一系列命令和参数构成的脚本，这些命令应用于基础镜像并最终创建一个新的镜像。</p><p>1、对于开发人员：可以为开发团队提供一个完全一致的开发环境；  2、对于测试人员：可以直接拿开发时所构建的镜像或者通过Dockerfile文件构建一个新的镜像开始工作了；  3、对于运维人员：在部署时，可以实现应用的无缝移植。</p><h3 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h3><table><thead><tr><th align="left">命令</th><th align="left">作用</th></tr></thead><tbody><tr><td align="left">FROM image_name:tag</td><td align="left">定义了使用哪个基础镜像启动构建流程</td></tr><tr><td align="left">MAINTAINER user_name</td><td align="left">声明镜像的创建者</td></tr><tr><td align="left">ENV key value</td><td align="left">设置环境变量 (可以写多条)</td></tr><tr><td align="left">RUN command</td><td align="left">是Dockerfile的核心部分(可以写多条)</td></tr><tr><td align="left">ADD source_dir/file dest_dir/file</td><td align="left">将宿主机的文件复制到容器内，如果是一个压缩文件，将会在复制后自动解压</td></tr><tr><td align="left">COPY source_dir/file dest_dir/file</td><td align="left">和ADD相似，但是如果有压缩文件并不能解压</td></tr><tr><td align="left">WORKDIR path_dir</td><td align="left">设置工作目录</td></tr></tbody></table><h2 id="使用脚本创建镜像"><a href="#使用脚本创建镜像" class="headerlink" title="使用脚本创建镜像"></a>使用脚本创建镜像</h2><p>步骤：</p><p>（1）创建目录</p><pre><code>mkdir –p /usr/local/dockerjdk8</code></pre><p>（2）下载jdk-8u171-linux-x64.tar.gz并上传到服务器（虚拟机）中的/usr/local/dockerjdk8目录</p><p>（3）创建文件Dockerfile  <code>vi Dockerfile</code></p><pre><code>#依赖镜像名称和IDFROM centos:7#指定镜像创建者信息MAINTAINER ITCAST#切换工作目录WORKDIR /usrRUN mkdir  /usr/local/java#ADD 是相对路径jar,把java添加到容器中ADD jdk-8u171-linux-x64.tar.gz /usr/local/java/#配置java环境变量ENV JAVA_HOME /usr/local/java/jdk1.8.0_171ENV JRE_HOME $JAVA_HOME/jreENV CLASSPATH $JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/lib:$CLASSPATHENV PATH $JAVA_HOME/bin:$PATH</code></pre><p>（4）执行命令构建镜像</p><pre><code>docker build -t='jdk1.8' .</code></pre><p>注意后边的空格和点，不要省略 表示当前目录Dockerfile</p><p>（5）查看镜像是否建立完成</p><pre><code>docker images</code></pre><h2 id="Docker私有仓库"><a href="#Docker私有仓库" class="headerlink" title="Docker私有仓库"></a>Docker私有仓库</h2><h3 id="私有仓库搭建与配置"><a href="#私有仓库搭建与配置" class="headerlink" title="私有仓库搭建与配置"></a>私有仓库搭建与配置</h3><p>（1）拉取私有仓库镜像（此步省略）</p><pre><code>docker pull registry</code></pre><p>（2）启动私有仓库容器</p><pre><code>docker run -di --name=registry -p 5000:5000 registry</code></pre><p>（3）打开浏览器 输入地址<a href="http://192.168.184.141:5000/v2/_catalog%E7%9C%8B%E5%88%B0%60%7B&quot;repositories&quot;:[]%7D%60">http://192.168.184.141:5000/v2/_catalog看到`{"repositories":[]}`</a> 表示私有仓库搭建成功并且内容为空</p><p>（4）修改daemon.json</p><pre><code>vi /etc/docker/daemon.json</code></pre><p>添加以下内容，保存退出。</p><pre><code>&amp;#123;"insecure-registries":["192.168.184.141:5000"]&amp;#125; </code></pre><p>此步用于让 docker信任私有仓库地址</p><p>（5）重启docker 服务</p><pre><code>systemctl restart docker</code></pre><h2 id="镜像上传至私有仓库"><a href="#镜像上传至私有仓库" class="headerlink" title="镜像上传至私有仓库"></a>镜像上传至私有仓库</h2><p>（1）标记此镜像为私有仓库的镜像</p><pre><code>docker tag jdk1.8 192.168.184.141:5000/jdk1.8</code></pre><p>（2）再次启动私服容器</p><pre><code>docker start registry</code></pre><p>（3）上传标记的镜像</p><pre><code>docker push 192.168.184.141:5000/jdk1.8</code></pre><h2 id="docker-compose"><a href="#docker-compose" class="headerlink" title="docker-compose"></a>docker-compose</h2><pre class=" language-shell"><code class="language-shell"># 执行docker-compose命令$ docker-composedocker-compose: command not found# 如果找不到命令，下载安装$ sudo curl -L https://github.com/docker/compose/releases/download/1.20.1/docker-compose-$(uname -s)-$(uname -m) -o /usr/local/bin/docker-compose && sudo chmod +x /usr/local/bin/docker-compose$ docker-compose -vdocker-compose version 1.20.1, build 5d8c71b## 在/usr/local 创建docker 目录mkdir dockercd docker## 在docker 创建不同应用的文件夹mkdir tomcatcd tomcat## 创建执行脚本vim docker-compose.yml## 编辑脚本（yml文件不能用tab键打空格）version: '3'services:   tomcat:    restart: always    image: tomcat    container_name: tomcat    ports:      - 88888:8080    volums:      - /usr/local/docker/myshop/ROOT:/usr/local/tomcat/webapps/ROOT## 启动docker-composedocker-compose up## 关闭容器docker-compose down## 带docker-compose.yml路径docker-compose -f /usr/local/docker/tomcat/ up## 守护模式docker-compose -d up#查看日志docker-compose logs tomcat(docker-compose.yml起的名字)#查看日志+监听docker-compose logs -f tomcat(docker-compose.yml起的名字)</code></pre><h3 id="docker-for-tenserflow-serving"><a href="#docker-for-tenserflow-serving" class="headerlink" title="docker for tenserflow serving"></a>docker for tenserflow serving</h3><pre class=" language-bash"><code class="language-bash"><span class="token comment" spellcheck="true">## RESTFUL api 形式</span>docker pull tensorflow/serving<span class="token function">mkdir</span> -p /tmp/tfserving<span class="token function">cd</span> /tmp/tfserving<span class="token function">git</span> clone https://github.com/tensorflow/servingdocker run -dt -p 8501:8501 -v <span class="token string">"/tmp/tfserving/serving/tensorflow_serving/servables/tensorflow/testdata/saved_model_half_plus_two_cpu:/models/half_plus_two"</span> -e MODEL_NAME<span class="token operator">=</span>half_plus_two tensorflow/servingcurl -d <span class="token string">'&amp;#123;"instances": [1.0, 2.0, 5.0]&amp;#125;'</span>  -X POST http://localhost:8501/v1/models/half_plus_two:predict <span class="token comment" spellcheck="true">## </span></code></pre><h2 id="docker-常见错误"><a href="#docker-常见错误" class="headerlink" title="docker 常见错误"></a>docker 常见错误</h2><p>WARNING: IPv4 forwarding is disabled. Networking will not work.<a href="https://www.cnblogs.com/Neeo/articles/12719390.html#2421676247">#</a><a href="https://www.cnblogs.com/Neeo/articles/12719390.html#top">top</a></p><p>docker启动容器时，报<code>WARNING: IPv4 forwarding is disabled. Networking will not work.</code>错误。<br>解决办法：</p><ol><li>编辑<code>vi /etc/sysctl.conf</code>文件或者<code>vi /usr/lib/sysctl.d/00-system.conf</code>文件，添加：</li></ol><pre><code>Copynet.ipv4.ip_forward = 1</code></pre><ol><li>重启network服务，并查看是否修改成功，返回<code>net.ipv4.ip_forward = 1</code>表示修改成功。</li></ol><pre><code>Copy[root@C ~]# systemctl restart network[root@C ~]# sysctl net.ipv4.ip_forwardnet.ipv4.ip_forward = 1</code></pre><ol><li>删除之前的容器，重新创建新容器即可。</li></ol><p>参考：<a href="https://juejin.im/post/5da1897ae51d4578110dc7a1">Docker容器启动报WARNING: IPv4 forwarding is disabled. Networking will not work</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 运维 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>neo4j</title>
      <link href="/2020/05/11/neo4j/"/>
      <url>/2020/05/11/neo4j/</url>
      
        <content type="html"><![CDATA[<h1 id="neo4j"><a href="#neo4j" class="headerlink" title="neo4j"></a>neo4j</h1><h2 id="安装neo4j"><a href="#安装neo4j" class="headerlink" title="安装neo4j"></a>安装neo4j</h2><h2 id="docker-安装neo4j"><a href="#docker-安装neo4j" class="headerlink" title="docker 安装neo4j"></a>docker 安装neo4j</h2><ol><li><code>docker pull neo4j</code></li><li><code>sudo docker run --name neo4j --detach --publish=7474:7474 --publish=7687:7687  --volume=/home/daiyizheng/docker/neo4j/data:/data  --volume=/home/daiyizheng/docker/neo4j/logs:/logs --volume=/home/daiyizheng/docker/neo4j/conf:/conf --volume=/home/daiyizheng/docker/neo4j/import:/import            --env=NEO4J_dbms_memory_pagecache_size=8G  --env=NEO4J_dbms_memory_heap_initial__size=4G --env=NEO4J_dbms_memory_heap_max__size=8G neo4j</code></li></ol><h2 id="非docker-安装"><a href="#非docker-安装" class="headerlink" title="非docker 安装"></a>非docker 安装</h2><p>JDK版本：1.8</p><p>工具：Xshell6、Xftp6</p><p>Neo4j是基于Java的图形数据库，运行Neo4j需要启动JVM进程，因此必须安装JAVA SE的JDK。</p><p>下载安装包 <a href="http://doc.we-yun.com:1008/neo4j/3.5.20/">http://doc.we-yun.com:1008/neo4j/3.5.20/</a></p><p>liunx环境Neo4j下载地址：<a href="http://neo4j.com.cn/topic/5b003eae9662eee704f31cee(%E7%A4%BE%E5%8C%BA%E7%89%88%E5%85%8D%E8%B4%B9)">http://neo4j.com.cn/topic/5b003eae9662eee704f31cee(社区版免费)</a></p><p>或者直接在服务器上使用命令下载：</p><p> curl -O <a href="http://dist.neo4j.org/neo4j-community-3.4.5-unix.tar.gz">http://dist.neo4j.org/neo4j-community-3.4.5-unix.tar.gz</a></p><p>2：解压安装</p><p>tar -axvf neo4j-community-3.4.5-unix.tar.gz</p><p>3:修改配置</p><p>在安装目录下找到conf目录下的neo4j.conf文件</p><p>修改相应配置如下：</p><pre class=" language-shell"><code class="language-shell"># 修改第22行load csv时l路径，在前面加个#，可从任意路径读取文件#dbms.directories.import=import# 修改35行和36行，设置JVM初始堆内存和JVM最大堆内存# 生产环境给的JVM最大堆内存越大越好，但是要小于机器的物理内存dbms.memory.heap.initial_size=5gdbms.memory.heap.max_size=10g# 修改46行，可以认为这个是缓存，如果机器配置高，这个越大越好dbms.memory.pagecache.size=10g# 修改54行，去掉改行的#，可以远程通过ip访问neo4j数据库dbms.connectors.default_listen_address=0.0.0.0# 默认 bolt端口是7687，http端口是7474，https关口是7473，不修改下面3项也可以# 修改71行，去掉#，设置http端口为7687，端口可以自定义，只要不和其他端口冲突就行#dbms.connector.bolt.listen_address=:7687# 修改75行，去掉#，设置http端口为7474，端口可以自定义，只要不和其他端口冲突就行dbms.connector.http.listen_address=:7474# 修改79行，去掉#，设置http端口为7473，端口可以自定义，只要不和其他端口冲突就行dbms.connector.https.listen_address=:7473# 修改227行，去掉#，允许从远程url来load csvdbms.security.allow_csv_import_from_file_urls=true# 修改246行，允许使用neo4j-shell，类似于mysql 命令行之类的dbms.shell.enabled=true# 修改235行，去掉#，设置连接neo4j-shell的端口，一般都是localhost或者127.0.0.1，这样安全，其他地址的话，一般使用https就行dbms.shell.host=127.0.0.1# 修改250行，去掉#，设置neo4j-shell端口，端口可以自定义，只要不和其他端口冲突就行dbms.shell.port=1337# 修改254行，设置neo4j可读可写dbms.read_only=false</code></pre><p>4：启动</p><p> 进入bin目录执行./neo4j start</p><p>进入bin目录执行./neo4j stop</p><p>查看图数据库状态</p><p>进入bin目录执行./neo4j status</p><p>7：客户端访问</p><p>http://服务器ip地址:7474/browser/</p><p>在浏览器访问图数据库所在的机器上的7474端口（第一次访问账号neo4j，密码neo4j，会提示修改初始密码）</p><h2 id="错误解决"><a href="#错误解决" class="headerlink" title="错误解决"></a>错误解决</h2><p>访问Neo4j验证失败（The client is unauthorized due to authentication failure.）</p><p>大概意思就是说服务器验证失败。</p><p>如果你有在浏览器上登录不同的neo4j数据库，很可能是由于缓存没有清理掉导致的。<br>可以试试无痕浏览来访问neo4j的web页面。<br>另外有还有两种解决方案：</p><p>停止neo4j服务，并且删除<code>data/dbms/auth</code>，重新启动<br>修改neo4j.conf配置文件，取消验证机制，修改如下：<br><code>dbms.security.auth_enabled=false</code></p><h2 id="neo4j数据库迁移–Neo4j数据库导入导出的方法"><a href="#neo4j数据库迁移–Neo4j数据库导入导出的方法" class="headerlink" title="neo4j数据库迁移–Neo4j数据库导入导出的方法"></a>neo4j数据库迁移–Neo4j数据库导入导出的方法</h2><p>Neo4j数据进行备份、还原、迁移的操作时，首先要关闭neo4j;</p><pre><code>/usr/share/neo4j/binneo4j stop</code></pre><p>如果出现</p><pre><code>Neo4j not running</code></pre><p>出现这种情况, Neo4j没有运行, 但是浏览器仍然可以访问neo4j数据库的情况, 直接执行导入数据后,是无法看到导入的数据库,</p><p>其实这种情况下Neo4j仍在运行(否则浏览器是无法然访问的),</p><p>这就需要强制杀死Neo4j进程,则执行命令 </p><pre><code>ps -ef|grep neo4j</code></pre><p> <img src="/2020/05/11/neo4j/1.png" alt="img"></p><pre><code>kill -9 &lt;对应的pid&gt;</code></pre><p>关闭Neo4j后, 再次用浏览器访问Neo4j,是无法访问的,说明Neo4j是关闭运行了,在此情况下,是可以执行Neo4j的数据导出与导入的.</p><p>执行数据导出命令</p><pre><code>./neo4j-admin  dump --database=graph.db --to=/home/robot/Neoj_data/graph.db.dump</code></pre><p>执行数据导入命令</p><pre><code>neo4j-admin load --from=/home/robot/Neoj_data/graph.db.dump --database=graph.db --force  #数据导入neo4j start        </code></pre><h2 id="导入csv-文件"><a href="#导入csv-文件" class="headerlink" title="导入csv 文件"></a>导入csv 文件</h2><p>将文件复制到import 文件夹中</p><p>在<code>http://127.0.0.1:7474/browser/</code>窗口中执行脚本（注意，数据过大可能报错）</p><p>可以在浏览器端或者是neo4j的console下进行这种方式的数据导入，但是这种方式只适用于20M以内的数据导入，对于大量数据是不适用的。</p><pre class=" language-shell"><code class="language-shell"># node节点LOAD CSV WITH HEADERS  FROM "file:///node.csv" AS line MERGE (p:Entity&#123;id:line.id,name:line.name&#125;)##查考节点start n = node(*)return n# 关系LOAD CSV WITH HEADERS FROM "file:///relationship.csv" AS line match (from:Entity&#123;id:line.start_id&#125;),(to:Entity&#123;id:line.end_id&#125;)merge (from)-[r:rel&#123;property1:line.property1,property2:line.property2&#125;]->(to)</code></pre><h3 id="命令行导入"><a href="#命令行导入" class="headerlink" title="命令行导入"></a>命令行导入</h3><pre class=" language-shell"><code class="language-shell">./neo4j-admin import --database=pkubase.db --nodes "/home/daiyizheng/文档/NLP-NER-projet/CCKS -2020-ckbqa/data/pre-data/node.csv" --relationships "/home/daiyizheng/文档/NLP-NER-projet/CCKS -2020-ckbqa/data/pre-data/relation.csv" --ignore-extra-columns=true --ignore-missing-nodes=true --ignore-duplicate-nodes=true</code></pre><pre class=" language-shell"><code class="language-shell">import导入csvimport语法neo4j-admin import [--mode=csv] [--database=<name>]                          [--additional-config=<config-file-path>]                          [--report-file=<filename>]                          [--nodes[:Label1:Label2]=<"file1,file2,...">]                          [--relationships[:RELATIONSHIP_TYPE]=<"file1,file2,...">]                          [--id-type=<STRING|INTEGER|ACTUAL>]                          [--input-encoding=<character-set>]                          [--ignore-extra-columns[=<true|false>]]                          [--ignore-duplicate-nodes[=<true|false>]]                          [--ignore-missing-nodes[=<true|false>]]                          [--multiline-fields[=<true|false>]]                          [--delimiter=<delimiter-character>]                          [--array-delimiter=<array-delimiter-character>]                          [--quote=<quotation-character>]                          [--max-memory=<max-memory-that-importer-can-use>]                          [--f=<File containing all arguments to this import>]</code></pre><pre class=" language-python"><code class="language-python">命令：<span class="token punctuation">.</span><span class="token operator">/</span>bin<span class="token operator">/</span>neo4j<span class="token operator">-</span><span class="token keyword">import</span> <span class="token operator">-</span><span class="token operator">-</span>into data<span class="token operator">/</span>databases<span class="token operator">/</span>graph_kg_merge_id<span class="token punctuation">.</span>db <span class="token operator">-</span><span class="token operator">-</span>nodes<span class="token punctuation">:</span>Persons data<span class="token operator">/</span>csv_kg_merge_id<span class="token operator">/</span>persons<span class="token punctuation">.</span>csv <span class="token operator">-</span><span class="token operator">-</span>nodes<span class="token punctuation">:</span>Industry data<span class="token operator">/</span>csv_kg_merge_id<span class="token operator">/</span>industry<span class="token punctuation">.</span>csv <span class="token operator">-</span><span class="token operator">-</span>relationships<span class="token punctuation">:</span>Director data<span class="token operator">/</span>csv_kg_merge_id<span class="token operator">/</span>director<span class="token punctuation">.</span>csv <span class="token operator">-</span><span class="token operator">-</span>multiline<span class="token operator">-</span>fields<span class="token operator">=</span>true</code></pre><pre class=" language-she"><code class="language-she">各文件内容一、节点name:ID 表示该列的属性名为name，ID 表示该属性是唯一标示一个实体的属性（类似关系型数据库中的主码），括号表示一个id-group，即表示该ID唯一表示括号内种类的实体，而不是所有实体；节点Persons.csv：person_name:id,name71927,乜标@1963-01-0171928,冯泽舟@1954-01-0171929,张宇锋@1965-01-0171930,丁涛@1979-01-0171931,周静尧@1963-05-01节点industry.csv：industry_name:id,id,name71917,江苏新日新能源车辆有限公司,江苏新日新能源车辆有限公司71918,北京新泉志和汽车饰件系统有限公司,北京新泉志和汽车饰件系统有限公司71919,芜湖新泉汽车饰件系统有限公司,芜湖新泉汽车饰件系统有限公司71920,宁波新泉志和汽车饰件系统有限公司,宁波新泉志和汽车饰件系统有限公司71921,宁波新泉汽车饰件系统有限公司,宁波新泉汽车饰件系统有限公司二、关系START_ID 和END_ID 表示边的起点和终点的ID，可以加上它们各自的id-group；关系 director.csv:START_ID,:END_ID,position71927,16816,371928,16840,471929,16047,371930,16573,471931,16918,4</code></pre><h2 id="Neo4j多库切换"><a href="#Neo4j多库切换" class="headerlink" title="Neo4j多库切换"></a><strong>Neo4j多库切换</strong></h2><p>方法一：</p><p>因为Neo4j的import导入时，只能导入一个不存的db，这就在想创建多个库时，需要去切换，Neo4j默认的库是graph.db，在./conf/neo4j.conf可以修改</p><pre class=" language-shell"><code class="language-shell"># Neo4j configuration## For more details and a complete list of settings, please see# https://neo4j.com/docs/operations-manual/current/reference/configuration-settings/#*****************************************************************# The name of the database to mount#dbms.active_database=graph.db</code></pre><p>方法二：</p><p>切换多个库的方法，将新库重新连接到默认库graph.db，然后重启Neo4j </p><pre class=" language-shell"><code class="language-shell">//软连接>>>cd ./data/databases/>>>ln -s graph_kg.db graph.db//重启neo4j>>>cd $NEO4j_HOME/bin>>>./neo4j restart// 删除软连接>>>ln-s test_chk  test_chk_ln>>>rm -rf  ./test_chk_ln</code></pre><h2 id="neo4j-基本操作元素"><a href="#neo4j-基本操作元素" class="headerlink" title="neo4j 基本操作元素"></a>neo4j 基本操作元素</h2><ul><li>neo4j可支持语言：**.NET、Java、Spring、JavaScript、Python、Ruby、PHP、R、Go、C / C++、Clojure、Perl、Haskell**</li><li>几个专有名词：<strong>变量（标识符）、节点、关系、实体、标签、属性、索引、约束</strong></li></ul><h2 id="查"><a href="#查" class="headerlink" title="查"></a>查</h2><pre class=" language-sql"><code class="language-sql"><span class="token comment" spellcheck="true">## 查的语句有：WHERE语句、ORDER BY 默认是升序，降序添加DESC、LIMIT 返回靠前的一定数目的数据、SKIP 返回靠后的一定数目的数据、UNION 子查询结果合并</span><span class="token comment" spellcheck="true">## 查询基本结构</span><span class="token keyword">MATCH</span> <span class="token punctuation">(</span>n:NODENAME<span class="token punctuation">)</span> <span class="token keyword">RETURN</span> n <span class="token comment" spellcheck="true"># LIMIT </span><span class="token keyword">MATCH</span> <span class="token punctuation">(</span>n:NODENAME<span class="token punctuation">)</span> <span class="token keyword">RETURN</span> n <span class="token keyword">LIMIT</span> <span class="token number">5</span><span class="token comment" spellcheck="true"># SKIP</span><span class="token keyword">MATCH</span> <span class="token punctuation">(</span>n:NODENAME<span class="token punctuation">)</span> <span class="token keyword">RETURN</span> n SKIP <span class="token number">5</span><span class="token keyword">MATCH</span> <span class="token punctuation">(</span>n:NODENAME<span class="token punctuation">)</span> <span class="token keyword">RETURN</span> n SKIP <span class="token number">1</span> <span class="token keyword">LIMIT</span> <span class="token number">5</span><span class="token comment" spellcheck="true">## union</span><span class="token keyword">MATCH</span> <span class="token punctuation">(</span>pp:Person<span class="token punctuation">)</span><span class="token keyword">RETURN</span> pp<span class="token punctuation">.</span>age<span class="token punctuation">,</span>pp<span class="token punctuation">.</span>name<span class="token keyword">UNION</span><span class="token keyword">MATCH</span> <span class="token punctuation">(</span><span class="token number">cc</span>:Customer<span class="token punctuation">)</span><span class="token keyword">RETURN</span> <span class="token number">cc</span><span class="token punctuation">.</span>age<span class="token punctuation">,</span><span class="token number">cc</span><span class="token punctuation">.</span>name<span class="token comment" spellcheck="true">## 查询节点个数：</span><span class="token keyword">match</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token keyword">return</span> <span class="token function">count</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token comment" spellcheck="true">## 查询所有的关系类型：</span><span class="token keyword">CALL</span> <span class="token number">db</span><span class="token punctuation">.</span>relationshipTypes<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">## 查询所有的节点标签：</span><span class="token keyword">CALL</span> <span class="token number">db</span><span class="token punctuation">.</span>labels<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">## 查询节点关系种类：</span><span class="token keyword">CALL</span> <span class="token number">db</span><span class="token punctuation">.</span><span class="token keyword">schema</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">## 查询N层关系的节点：</span><span class="token keyword">match</span> q<span class="token operator">=</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token operator">-</span><span class="token punctuation">[</span><span class="token operator">*</span><span class="token number">5</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token number">8</span><span class="token punctuation">]</span><span class="token operator">-</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">return</span> q <span class="token keyword">limit</span> <span class="token number">200</span> 这个为查询<span class="token number">5</span>到<span class="token number">8</span>层关系的<span class="token keyword">match</span> q<span class="token operator">=</span><span class="token punctuation">(</span>dh<span class="token punctuation">)</span><span class="token operator">-</span><span class="token punctuation">[</span>r<span class="token punctuation">]</span><span class="token operator">-</span><span class="token punctuation">(</span>jq<span class="token punctuation">)</span><span class="token operator">-</span><span class="token punctuation">[</span>rr<span class="token punctuation">]</span><span class="token operator">-</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">-</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token operator">-</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">-</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token operator">-</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">-</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token operator">-</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">-</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token operator">-</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">-</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token operator">-</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">return</span> q <span class="token keyword">limit</span> <span class="token number">400</span><span class="token comment" spellcheck="true">## 查询节点关系数个数</span><span class="token keyword">match</span><span class="token punctuation">(</span>dh:Person<span class="token punctuation">)</span><span class="token operator">-</span><span class="token punctuation">[</span>r<span class="token punctuation">]</span><span class="token operator">-</span><span class="token punctuation">(</span>jq:Teacher<span class="token punctuation">)</span> <span class="token keyword">with</span> dh<span class="token punctuation">,</span> <span class="token function">count</span><span class="token punctuation">(</span>r<span class="token punctuation">)</span> <span class="token keyword">as</span> dhs <span class="token keyword">where</span> dhs <span class="token operator">></span> <span class="token number">2</span> <span class="token keyword">return</span> dh<span class="token comment" spellcheck="true">## return</span>返回一个节点：<span class="token keyword">match</span> <span class="token punctuation">(</span>n:Node<span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;name:"B"&amp;#125;) return n;</span>返回一个关系：<span class="token keyword">match</span> <span class="token punctuation">(</span>n:Node<span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;name:"A"&amp;#125;)-[r:KNOWS]->(c) return r;</span>返回一个属性：<span class="token keyword">match</span> <span class="token punctuation">(</span>n:Node<span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;name:"A"&amp;#125;) return n.name;</span>返回所有节点：<span class="token keyword">match</span> p<span class="token operator">=</span><span class="token punctuation">(</span>:Node<span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;name:"A"&amp;#125;)-[r]->(b) return *;</span>列别名： <span class="token keyword">match</span> <span class="token punctuation">(</span><span class="token number">a</span>:Node<span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;name:"A"&amp;#125;) return a.age as thisisage;</span>表达式： <span class="token keyword">match</span> <span class="token punctuation">(</span><span class="token number">a</span>:Node:Node<span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;name:"A"&amp;#125;) return a.age >30 ,”literal”,(a)–>();</span>唯一结果：<span class="token keyword">match</span> <span class="token punctuation">(</span><span class="token number">a</span>:Node<span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;name:"A"&amp;#125;)–>(b) return distinct b;</span><span class="token comment" spellcheck="true">## merge-查询</span><span class="token comment" spellcheck="true">## 在merge子句中指定on match子句</span><span class="token comment" spellcheck="true">## 如果节点已经存在于数据库中，那么执行on match子句，修改节点的属性；</span><span class="token keyword">MERGE</span> <span class="token punctuation">(</span>person:Person<span class="token punctuation">)</span><span class="token keyword">ON</span> <span class="token keyword">MATCH</span> <span class="token keyword">SET</span> person<span class="token punctuation">.</span>found <span class="token operator">=</span> <span class="token boolean">TRUE</span> <span class="token punctuation">,</span> person<span class="token punctuation">.</span>lastAccessed <span class="token operator">=</span> <span class="token keyword">timestamp</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">RETURN</span> <span class="token keyword">merge</span>子句用于<span class="token keyword">match</span>或<span class="token keyword">create</span>多个关系<span class="token comment" spellcheck="true">## merge子句用于match或create多个关系</span><span class="token keyword">MATCH</span> <span class="token punctuation">(</span>oliver:Person <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123; name: 'Oliver Stone' &amp;#125;),(reiner:Person &amp;#123; name: 'Rob Reiner' &amp;#125;)</span><span class="token keyword">MERGE</span> <span class="token punctuation">(</span>oliver<span class="token punctuation">)</span><span class="token operator">-</span><span class="token punctuation">[</span>:DIRECTED<span class="token punctuation">]</span><span class="token operator">-</span><span class="token operator">></span><span class="token punctuation">(</span>movie:Movie<span class="token punctuation">)</span><span class="token operator">&lt;</span><span class="token operator">-</span><span class="token punctuation">[</span>:ACTED_IN<span class="token punctuation">]</span><span class="token operator">-</span><span class="token punctuation">(</span>reiner<span class="token punctuation">)</span><span class="token keyword">RETURN</span> movie集合函数查询（<span class="token number">1</span>）通过id函数，返回节点或关系的ID<span class="token keyword">MATCH</span> <span class="token punctuation">(</span>:Person <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123; name: 'Oliver Stone' &amp;#125;)-[r]->(movie)</span><span class="token keyword">RETURN</span> id<span class="token punctuation">(</span>r<span class="token punctuation">)</span><span class="token punctuation">;</span>（<span class="token number">2</span>）通过<span class="token keyword">type</span>函数，查询关系的类型<span class="token keyword">MATCH</span> <span class="token punctuation">(</span>:Person <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123; name: 'Oliver Stone' &amp;#125;)-[r]->(movie)</span><span class="token keyword">RETURN</span> <span class="token keyword">type</span><span class="token punctuation">(</span>r<span class="token punctuation">)</span><span class="token punctuation">;</span>（<span class="token number">3</span>）通过lables函数，查询节点的标签<span class="token keyword">MATCH</span> <span class="token punctuation">(</span>:Person <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123; name: 'Oliver Stone' &amp;#125;)-[r]->(movie)</span><span class="token keyword">RETURN</span> lables<span class="token punctuation">(</span>movie<span class="token punctuation">)</span><span class="token punctuation">;</span>（<span class="token number">4</span>）通过<span class="token keyword">keys</span>函数，查看节点或关系的属性键<span class="token keyword">MATCH</span> <span class="token punctuation">(</span><span class="token number">a</span><span class="token punctuation">)</span><span class="token keyword">WHERE</span> <span class="token number">a</span><span class="token punctuation">.</span>name <span class="token operator">=</span> <span class="token string">'Alice'</span><span class="token keyword">RETURN</span> <span class="token keyword">keys</span><span class="token punctuation">(</span><span class="token number">a</span><span class="token punctuation">)</span>（<span class="token number">5</span>）通过properties<span class="token punctuation">(</span><span class="token punctuation">)</span>函数，查看节点或关系的属性<span class="token keyword">CREATE</span> <span class="token punctuation">(</span>p:Person <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123; name: 'Stefan', city: 'Berlin' &amp;#125;)</span><span class="token keyword">RETURN</span> properties<span class="token punctuation">(</span>p<span class="token punctuation">)</span>（<span class="token number">6</span>）nodes<span class="token punctuation">(</span>path<span class="token punctuation">)</span>：返回path中节点<span class="token keyword">match</span> p<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">a</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">-->(b)-->(c) where a.name='Alice' and c.name='Eskil' return nodes(p)</span>（<span class="token number">7</span>）relationships<span class="token punctuation">(</span>path<span class="token punctuation">)</span>：返回path中的关系<span class="token keyword">match</span> p<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">a</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">-->(b)-->(c) where a.name='Alice' and c.name='Eskil' return relationships(p)</span>路径查询常规路径查询：<span class="token keyword">MATCH</span> <span class="token punctuation">(</span>:Person <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123; name: 'm' &amp;#125;)-->(person)</span><span class="token keyword">RETURN</span> person<span class="token punctuation">;</span>返回的是:name为m的这个节点，指向的节点，不包括m节点本身可变长度路径：<span class="token keyword">match</span> <span class="token punctuation">(</span><span class="token number">a</span>:Product <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;productName:'Chai'&amp;#125; )-[*1..5]-(b:Customer&amp;#123;companyName : 'Frankenversand'&amp;#125;) return a,b</span><span class="token comment" spellcheck="true">//[*1..5]可变长度路径，从a到b的1-5条路径；</span>零长度路径<span class="token keyword">START</span> <span class="token number">a</span><span class="token operator">=</span>node<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token keyword">MATCH</span> p1<span class="token operator">=</span><span class="token number">a</span><span class="token operator">-</span><span class="token punctuation">[</span>:KNOWS<span class="token operator">*</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">-</span><span class="token operator">></span><span class="token number">b</span><span class="token punctuation">,</span> p2<span class="token operator">=</span><span class="token number">b</span><span class="token operator">-</span><span class="token punctuation">[</span>:BLOCKS<span class="token operator">*</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">-</span><span class="token operator">></span><span class="token number">c</span><span class="token keyword">RETURN</span> <span class="token number">a</span><span class="token punctuation">,</span><span class="token number">b</span><span class="token punctuation">,</span><span class="token number">c</span><span class="token punctuation">,</span> length<span class="token punctuation">(</span>p1<span class="token punctuation">)</span><span class="token punctuation">,</span> length<span class="token punctuation">(</span>p2<span class="token punctuation">)</span>这个查询将返回四个路径，其中有些路径长度为<span class="token number">0</span><span class="token punctuation">.</span>最短路径使用shortestPath函数可以找出一条两个节点间的最短路径，如下。查询：<span class="token keyword">START</span> <span class="token number">d</span><span class="token operator">=</span>node<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">e</span><span class="token operator">=</span>node<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token keyword">MATCH</span> p <span class="token operator">=</span> shortestPath<span class="token punctuation">(</span> <span class="token number">d</span><span class="token operator">-</span><span class="token punctuation">[</span><span class="token operator">*</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token number">15</span><span class="token punctuation">]</span><span class="token operator">-</span><span class="token operator">></span><span class="token number">e</span> <span class="token punctuation">)</span><span class="token keyword">RETURN</span> p这意味着：找出两点间的一条最短路径，最大关系长度为<span class="token number">15</span><span class="token punctuation">.</span>圆括号内是一个简单的路径连接，开始节点，连接关系和结束节点。关系的字符描述像关系类型，最大数和方向在寻找最短路径中都将被用到。也可以标识路径为可选。最短路径案例一：<span class="token keyword">MATCH</span> <span class="token punctuation">(</span>p1:Person <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;name:"Jonathan Lipnicki"&amp;#125;),(p2:Person&amp;#123;name:"Joel Silver"&amp;#125;),</span>p<span class="token operator">=</span>shortestpath<span class="token punctuation">(</span><span class="token punctuation">(</span>p1<span class="token punctuation">)</span><span class="token operator">-</span><span class="token punctuation">[</span><span class="token operator">*</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token number">10</span><span class="token punctuation">]</span><span class="token operator">-</span><span class="token punctuation">(</span>p2<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">RETURN</span> p这里<span class="token punctuation">[</span><span class="token operator">*</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token number">10</span><span class="token punctuation">]</span>表示路径深度<span class="token number">10</span>以内查找所有存在的关系中的最短路径关系最短路径案例二：<span class="token keyword">MATCH</span> <span class="token punctuation">(</span>p1:Person <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;name:"Jonathan Lipnicki"&amp;#125;),(p2:Person&amp;#123;name:"Joel Silver"&amp;#125;),</span>p<span class="token operator">=</span>allshortestpaths<span class="token punctuation">(</span><span class="token punctuation">(</span>p1<span class="token punctuation">)</span><span class="token operator">-</span><span class="token punctuation">[</span><span class="token operator">*</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token number">10</span><span class="token punctuation">]</span><span class="token operator">-</span><span class="token punctuation">(</span>p2<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">RETURN</span> p找出所有最短路径查询关系属性<span class="token keyword">MATCH</span> <span class="token punctuation">(</span>:Person <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123; name: 'matt' &amp;#125;)-[r]->( Person) RETURN r,type(r);</span>功能：查看姓名为matt的人，到标签person之间，关系有哪些一些特殊的用法：<span class="token keyword">with</span>用法：<span class="token keyword">with</span>从句可以连接多个查询的结果，即将上一个查询的结果用作下一个查询的开始。collecty用法：代表把内容序列化<span class="token keyword">with</span>用法、匿名变量（具体可以见《图数据库》，40P）（<span class="token number">1</span>）匿名变量<span class="token punctuation">(</span><span class="token number">a</span><span class="token punctuation">)</span><span class="token operator">&lt;</span><span class="token operator">-</span><span class="token punctuation">[</span>:ass<span class="token punctuation">]</span><span class="token operator">-</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">-</span><span class="token punctuation">[</span>:bss<span class="token punctuation">]</span><span class="token operator">-</span><span class="token operator">></span><span class="token punctuation">(</span><span class="token number">b</span><span class="token punctuation">)</span>（<span class="token number">2</span>）<span class="token keyword">with</span>用法：<span class="token keyword">match</span> <span class="token punctuation">(</span><span class="token number">a</span><span class="token punctuation">)</span><span class="token operator">-</span><span class="token punctuation">[</span>:<span class="token keyword">work</span><span class="token punctuation">]</span><span class="token operator">-</span><span class="token operator">></span><span class="token punctuation">(</span><span class="token number">b</span><span class="token punctuation">)</span><span class="token keyword">with</span> <span class="token number">b</span> <span class="token keyword">ORDER</span> <span class="token keyword">BY</span> <span class="token number">b</span><span class="token punctuation">.</span>yeah <span class="token keyword">DESC</span><span class="token keyword">RETURN</span> <span class="token number">a</span><span class="token punctuation">,</span><span class="token number">b</span>过滤聚合函数的结果：<span class="token keyword">MATCH</span> <span class="token punctuation">(</span>david <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123; name: “David” &amp;#125;)–(otherPerson)–>() WITH otherPerson, count(*) AS foaf</span><span class="token keyword">WHERE</span> foaf <span class="token operator">></span> <span class="token number">1</span> <span class="token keyword">RETURN</span> otherPerson<span class="token punctuation">;</span>collect前排序结果：<span class="token keyword">MATCH</span> <span class="token punctuation">(</span>n<span class="token punctuation">)</span>  <span class="token keyword">WITH</span> n   <span class="token keyword">ORDER</span> <span class="token keyword">BY</span> n<span class="token punctuation">.</span>name <span class="token keyword">DESC</span> <span class="token keyword">LIMIT</span> <span class="token number">3</span>  <span class="token keyword">RETURN</span> collect<span class="token punctuation">(</span>n<span class="token punctuation">.</span>name<span class="token punctuation">;</span><span class="token keyword">limit</span>搜索路径的分支：<span class="token keyword">MATCH</span> <span class="token punctuation">(</span>n <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123; name: "Anders" &amp;#125;)--(m)  WITH m  ORDER BY m.name DESC LIMIT 1  MATCH (m)--(o)  RETURN o.name;</span>UNWIND将一个集合展开为一个可选的list，有点像py中的生成器。<span class="token comment" spellcheck="true">//&amp;#123;batch: [&amp;#123;name:"Alice",age:32&amp;#125;,&amp;#123;name:"Bob",age:42&amp;#125;]&amp;#125;</span>UNWIND <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;batch&amp;#125; as row</span><span class="token keyword">CREATE</span> <span class="token punctuation">(</span>n:Label<span class="token punctuation">)</span><span class="token keyword">SET</span> n<span class="token punctuation">.</span>name <span class="token operator">=</span> <span class="token keyword">row</span><span class="token punctuation">.</span>name<span class="token punctuation">,</span> n<span class="token punctuation">.</span>age <span class="token operator">=</span> <span class="token keyword">row</span><span class="token punctuation">.</span>age其中<span class="token keyword">row</span>，就被定义为一个可迭代的List。案例二：<span class="token comment" spellcheck="true">//&amp;#123;batch: [&amp;#123;from:"alice@example.com",to:"bob@example.com",properties:&amp;#123;since:2012&amp;#125;&amp;#125;,&amp;#123;from:"alice@example.com",to:"charlie@example.com",properties:&amp;#123;since:2016&amp;#125;&amp;#125;]&amp;#125;</span>UNWIND <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;batch&amp;#125; as row</span><span class="token keyword">MATCH</span> <span class="token punctuation">(</span><span class="token keyword">from</span>:Label <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;from:row.from&amp;#125;)</span><span class="token keyword">MATCH</span> <span class="token punctuation">(</span><span class="token keyword">to</span>:Label <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;to:row.to&amp;#125;)</span><span class="token keyword">CREATE</span><span class="token operator">/</span><span class="token keyword">MERGE</span> <span class="token punctuation">(</span><span class="token keyword">from</span><span class="token punctuation">)</span><span class="token operator">-</span><span class="token punctuation">[</span>rel:KNOWS<span class="token punctuation">]</span><span class="token operator">-</span><span class="token operator">></span><span class="token punctuation">(</span><span class="token keyword">to</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token keyword">ON</span> <span class="token keyword">CREATE</span><span class="token punctuation">)</span> <span class="token keyword">SET</span> rel<span class="token punctuation">.</span>since <span class="token operator">=</span> <span class="token keyword">row</span><span class="token punctuation">.</span>properties<span class="token punctuation">.</span>since</code></pre><h2 id="创建"><a href="#创建" class="headerlink" title="创建"></a>创建</h2><pre class=" language-sql"><code class="language-sql"><span class="token comment" spellcheck="true">## create-创建节点</span><span class="token keyword">create</span> <span class="token punctuation">(</span>n:Person <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123; name: 'Robert Zemeckis', born: 1951 &amp;#125;) return n;</span><span class="token comment" spellcheck="true">## create-创建节点间关系</span><span class="token keyword">MATCH</span> <span class="token punctuation">(</span><span class="token number">a</span>:Person<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">b</span>:Person<span class="token punctuation">)</span> <span class="token keyword">where</span> <span class="token number">a</span><span class="token punctuation">.</span>name <span class="token operator">=</span> <span class="token string">'m'</span><span class="token operator">and</span> <span class="token number">b</span><span class="token punctuation">.</span>name <span class="token operator">=</span> <span class="token string">'Andres'</span> <span class="token keyword">CREATE</span> <span class="token punctuation">(</span><span class="token number">a</span><span class="token punctuation">)</span><span class="token operator">-</span><span class="token punctuation">[</span>r:girl<span class="token punctuation">]</span><span class="token operator">-</span><span class="token operator">></span><span class="token punctuation">(</span><span class="token number">b</span><span class="token punctuation">)</span> <span class="token keyword">RETURN</span> r<span class="token punctuation">;</span><span class="token comment" spellcheck="true">## create-创建节点间关系 + 关系属性</span><span class="token keyword">MATCH</span> <span class="token punctuation">(</span><span class="token number">a</span>:Person<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">b</span>:Person<span class="token punctuation">)</span><span class="token keyword">WHERE</span> <span class="token number">a</span><span class="token punctuation">.</span>name <span class="token operator">=</span> <span class="token string">'m'</span><span class="token operator">and</span> <span class="token number">b</span><span class="token punctuation">.</span>name <span class="token operator">=</span> <span class="token string">'Andres'</span> <span class="token keyword">CREATE</span> <span class="token punctuation">(</span><span class="token number">a</span><span class="token punctuation">)</span><span class="token operator">-</span><span class="token punctuation">[</span>r:girl <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123; roles:['friend'] &amp;#125;]->(b)</span><span class="token keyword">RETURN</span> r<span class="token punctuation">;</span><span class="token comment" spellcheck="true">##create-创建完整路径path</span><span class="token keyword">CREATE</span> p <span class="token operator">=</span><span class="token punctuation">(</span>vic:Worker:Person<span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123; name:'vic',title:"Developer" &amp;#125;)-[:WORKS_AT]->(neo)&lt;-[:WORKS_AT]-(michael:Worker:Person &amp;#123; name: 'Michael',title:"Manager" &amp;#125;)</span><span class="token keyword">RETURN</span> p逻辑为：创建vic这个人与变量<span class="token punctuation">(</span>neo<span class="token punctuation">)</span>的<span class="token punctuation">[</span>:WORKS_AT<span class="token punctuation">]</span>关系；创建michael这个人与变量<span class="token punctuation">(</span>neo<span class="token punctuation">)</span>的<span class="token punctuation">[</span>:WORKS_AT<span class="token punctuation">]</span>关系<span class="token comment" spellcheck="true">## 创建唯一性节点 CREATE UNIQUE</span><span class="token keyword">MATCH</span> <span class="token punctuation">(</span>root <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123; name: 'root' &amp;#125;) </span><span class="token keyword">CREATE</span> <span class="token keyword">UNIQUE</span> <span class="token punctuation">(</span>root<span class="token punctuation">)</span><span class="token operator">-</span><span class="token punctuation">[</span>:LOVES<span class="token punctuation">]</span><span class="token operator">-</span><span class="token punctuation">(</span>someone<span class="token punctuation">)</span> <span class="token keyword">RETURN</span> someone<span class="token comment" spellcheck="true">##  merge-on create 新增属性</span><span class="token keyword">Merge</span>子句的作用有两个：当模式（Pattern）存在时，匹配该模式；当模式不存在时，创建新的模式（参考）。如果需要创建节点，那么执行<span class="token keyword">on</span> <span class="token keyword">create</span>子句，修改节点的属性<span class="token keyword">MERGE</span> <span class="token punctuation">(</span>keanu:Person <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123; name: 'Keanu Reeves' &amp;#125;)</span><span class="token keyword">ON</span> <span class="token keyword">CREATE</span> <span class="token keyword">SET</span> keanu<span class="token punctuation">.</span>created <span class="token operator">=</span> <span class="token keyword">timestamp</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">RETURN</span> keanu<span class="token punctuation">.</span>name<span class="token punctuation">,</span> keanu<span class="token punctuation">.</span>created注意：<span class="token keyword">ON</span> <span class="token keyword">CREATE</span> <span class="token keyword">SET</span>只在创建使用有用，如果节点已经存在了，那么该命令失效。</code></pre><h2 id="删除"><a href="#删除" class="headerlink" title="删除"></a>删除</h2><pre class=" language-sql"><code class="language-sql">大致有两个：<span class="token keyword">DELETE</span>与REMOVE<span class="token comment" spellcheck="true">## 删除所有节点与关系——delete</span>删除单个节点：<span class="token keyword">MATCH</span> <span class="token punctuation">(</span>n:Useless<span class="token punctuation">)</span> <span class="token keyword">DELETE</span> n<span class="token punctuation">;</span>删除单个节点和连接它的关系：<span class="token keyword">MATCH</span> <span class="token punctuation">(</span>n <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123; name: 'Andres' &amp;#125;)-[r]-() DELETE n, r</span>删除所有节点和关系：<span class="token keyword">MATCH</span> <span class="token punctuation">(</span>n<span class="token punctuation">)</span> OPTIONAL <span class="token keyword">MATCH</span> <span class="token punctuation">(</span>n<span class="token punctuation">)</span><span class="token operator">-</span><span class="token punctuation">[</span>r<span class="token punctuation">]</span><span class="token operator">-</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">DELETE</span> n<span class="token punctuation">,</span>r删除某一类关系：<span class="token keyword">match</span> <span class="token punctuation">(</span>n<span class="token punctuation">)</span><span class="token operator">-</span><span class="token punctuation">[</span>r:created<span class="token punctuation">]</span><span class="token operator">-</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">DELETE</span> <span class="token keyword">delete</span> r<span class="token comment" spellcheck="true">## 删除标签与属性——remove</span>删除属性：<span class="token keyword">MATCH</span> <span class="token punctuation">(</span>andres <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123; name: 'Andres' &amp;#125;) REMOVE andres.age RETURN andres;</span>删除节点的标签：<span class="token keyword">MATCH</span> <span class="token punctuation">(</span>n <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123; name: 'Peter' &amp;#125;) REMOVE n:German RETURN n;</span>删除多重标签：<span class="token keyword">MATCH</span> <span class="token punctuation">(</span>n <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123; name: 'Peter' &amp;#125;) REMOVE n:German:Swedish RETURN n</span><span class="token comment" spellcheck="true">##  重设为NULL——set</span>删除属性：<span class="token keyword">MATCH</span> <span class="token punctuation">(</span>n <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123; name: ‘Andres’ &amp;#125;) SET n.name = NULL RETURN n</span></code></pre><h2 id="改"><a href="#改" class="headerlink" title="改"></a>改</h2><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span>节点额外加入标签与属性<span class="token comment" spellcheck="true">// 加入额外标签</span><span class="token keyword">match</span> <span class="token punctuation">(</span>n<span class="token punctuation">)</span><span class="token keyword">where</span> id<span class="token punctuation">(</span>n<span class="token punctuation">)</span><span class="token operator">=</span><span class="token number">7</span><span class="token keyword">set</span> n:Company<span class="token keyword">return</span> n<span class="token punctuation">;</span><span class="token comment" spellcheck="true">//加入额外属性</span><span class="token keyword">match</span> <span class="token punctuation">(</span>n<span class="token punctuation">)</span><span class="token keyword">where</span> id<span class="token punctuation">(</span>n<span class="token punctuation">)</span><span class="token operator">=</span><span class="token number">100</span><span class="token keyword">set</span> n<span class="token punctuation">.</span>name <span class="token operator">=</span> <span class="token string">'id100'</span><span class="token keyword">return</span> n<span class="token punctuation">;</span><span class="token comment" spellcheck="true">// 设置多个属性</span><span class="token keyword">MATCH</span> <span class="token punctuation">(</span>n <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123; name: 'Andres' &amp;#125;) SET n.position = 'Developer', n.surname = 'Taylor'</span><span class="token comment" spellcheck="true">## 通过set来进行额外加入标签与属性。同时，已有的关系可以通过set赋值上去。</span>在节点和关系之间复制属性：<span class="token keyword">MATCH</span> <span class="token punctuation">(</span>at <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123; name: 'Andres' &amp;#125;),(pn &amp;#123; name: 'Peter' &amp;#125;) </span><span class="token keyword">SET</span> at <span class="token operator">=</span> pn <span class="token keyword">RETURN</span> at<span class="token punctuation">,</span> pn<span class="token punctuation">;</span><span class="token comment" spellcheck="true">## merge-on match</span><span class="token keyword">MERGE</span> <span class="token punctuation">(</span>person:Person<span class="token punctuation">)</span><span class="token keyword">ON</span> <span class="token keyword">MATCH</span> <span class="token keyword">SET</span> person<span class="token punctuation">.</span>found <span class="token operator">=</span> <span class="token boolean">TRUE</span> <span class="token punctuation">,</span> person<span class="token punctuation">.</span>lastAccessed <span class="token operator">=</span> <span class="token keyword">timestamp</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">RETURN</span> person<span class="token punctuation">.</span>name<span class="token punctuation">,</span> person<span class="token punctuation">.</span>found<span class="token punctuation">,</span> person<span class="token punctuation">.</span>lastAccessed</code></pre><h2 id="统计与集合函数"><a href="#统计与集合函数" class="headerlink" title="统计与集合函数"></a>统计与集合函数</h2><pre class=" language-sql"><code class="language-sql">统计函数常见的有：abs<span class="token punctuation">(</span><span class="token punctuation">)</span>，acos<span class="token punctuation">(</span><span class="token punctuation">)</span>，asin<span class="token punctuation">(</span><span class="token punctuation">)</span>，atan<span class="token punctuation">(</span><span class="token punctuation">)</span>，atan2<span class="token punctuation">(</span>x<span class="token punctuation">,</span>y<span class="token punctuation">)</span>，cos<span class="token punctuation">(</span><span class="token punctuation">)</span>，cot<span class="token punctuation">(</span><span class="token punctuation">)</span>，degree<span class="token punctuation">(</span><span class="token punctuation">)</span>，<span class="token number">e</span><span class="token punctuation">(</span><span class="token punctuation">)</span>返回一个常量，exp<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token number">e</span>的二次方，floor<span class="token punctuation">(</span><span class="token number">0.9</span><span class="token punctuation">)</span><span class="token operator">=</span><span class="token number">0.0</span>，haversin<span class="token punctuation">(</span><span class="token punctuation">)</span>，log<span class="token punctuation">(</span><span class="token punctuation">)</span>，log10<span class="token punctuation">(</span><span class="token punctuation">)</span>，pi<span class="token punctuation">(</span><span class="token punctuation">)</span>常量PI，radians<span class="token punctuation">(</span><span class="token number">180</span><span class="token punctuation">)</span>，rand<span class="token punctuation">(</span><span class="token punctuation">)</span>返回<span class="token number">0</span>到<span class="token number">1.0</span>的值，<span class="token function">round</span><span class="token punctuation">(</span><span class="token number">3.14</span><span class="token punctuation">)</span><span class="token operator">=</span><span class="token number">3.0</span>，sign<span class="token punctuation">(</span><span class="token punctuation">)</span>，sin<span class="token punctuation">(</span><span class="token punctuation">)</span>，sqrt<span class="token punctuation">(</span><span class="token punctuation">)</span>，tan<span class="token punctuation">(</span><span class="token punctuation">)</span>count：<span class="token keyword">MATCH</span> <span class="token punctuation">(</span>n <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123; name: ‘A’ &amp;#125;)–>(x) RETURN n, count(*)</span>sum：<span class="token keyword">MATCH</span> <span class="token punctuation">(</span>n:Person<span class="token punctuation">)</span> <span class="token keyword">RETURN</span> <span class="token function">sum</span><span class="token punctuation">(</span>n<span class="token punctuation">.</span>property<span class="token punctuation">)</span>avg：<span class="token keyword">MATCH</span> <span class="token punctuation">(</span>n:Person<span class="token punctuation">)</span> <span class="token keyword">RETURN</span> <span class="token function">avg</span><span class="token punctuation">(</span>n<span class="token punctuation">.</span>property<span class="token punctuation">)</span>percentileDisc：计算百分位。<span class="token keyword">MATCH</span> <span class="token punctuation">(</span>n:Person<span class="token punctuation">)</span> <span class="token keyword">RETURN</span> percentileDisc<span class="token punctuation">(</span>n<span class="token punctuation">.</span>property<span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">)</span>percentileCont：<span class="token keyword">MATCH</span> <span class="token punctuation">(</span>n:Person<span class="token punctuation">)</span> <span class="token keyword">RETURN</span> percentileCont<span class="token punctuation">(</span>n<span class="token punctuation">.</span>property<span class="token punctuation">,</span> <span class="token number">0.4</span><span class="token punctuation">)</span>stdev：计算标准偏差。<span class="token keyword">MATCH</span> <span class="token punctuation">(</span>n<span class="token punctuation">)</span> <span class="token keyword">WHERE</span> n<span class="token punctuation">.</span>name <span class="token operator">IN</span> <span class="token punctuation">[</span>‘A’<span class="token punctuation">,</span> ‘B’<span class="token punctuation">,</span> ‘C’<span class="token punctuation">]</span> <span class="token keyword">RETURN</span> stdev<span class="token punctuation">(</span>n<span class="token punctuation">.</span>property<span class="token punctuation">)</span>stdevp：<span class="token keyword">MATCH</span> <span class="token punctuation">(</span>n<span class="token punctuation">)</span> <span class="token keyword">WHERE</span> n<span class="token punctuation">.</span>name <span class="token operator">IN</span> <span class="token punctuation">[</span>‘A’<span class="token punctuation">,</span> ‘B’<span class="token punctuation">,</span> ‘C’<span class="token punctuation">]</span> <span class="token keyword">RETURN</span> stdevp<span class="token punctuation">(</span>n<span class="token punctuation">.</span>property<span class="token punctuation">)</span>max：<span class="token keyword">MATCH</span> <span class="token punctuation">(</span>n:Person<span class="token punctuation">)</span> <span class="token keyword">RETURN</span> <span class="token function">max</span><span class="token punctuation">(</span>n<span class="token punctuation">.</span>property<span class="token punctuation">)</span>min：<span class="token keyword">MATCH</span> <span class="token punctuation">(</span>n:Person<span class="token punctuation">)</span> <span class="token keyword">RETURN</span> <span class="token function">min</span><span class="token punctuation">(</span>n<span class="token punctuation">.</span>property<span class="token punctuation">)</span>collect：<span class="token keyword">MATCH</span> <span class="token punctuation">(</span>n:Person<span class="token punctuation">)</span> <span class="token keyword">RETURN</span> collect<span class="token punctuation">(</span>n<span class="token punctuation">.</span>property<span class="token punctuation">)</span><span class="token keyword">distinct</span>：<span class="token keyword">MATCH</span> <span class="token punctuation">(</span><span class="token number">a</span>:Person <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123; name: ‘A’ &amp;#125;)–>(b) RETURN</span><span class="token keyword">coalesce</span>：返回第一个<span class="token operator">not</span> <span class="token boolean">null</span>值。<span class="token keyword">match</span> <span class="token punctuation">(</span><span class="token number">a</span><span class="token punctuation">)</span> <span class="token keyword">where</span> <span class="token number">a</span><span class="token punctuation">.</span>name<span class="token operator">=</span>’Alice’ <span class="token keyword">return</span> <span class="token keyword">coalesce</span><span class="token punctuation">(</span><span class="token number">a</span><span class="token punctuation">.</span>hairColor<span class="token punctuation">,</span><span class="token number">a</span><span class="token punctuation">.</span>eyes<span class="token punctuation">)</span>head：返回集合的第一个元素。<span class="token keyword">match</span> <span class="token punctuation">(</span><span class="token number">a</span><span class="token punctuation">)</span> <span class="token keyword">where</span> <span class="token number">a</span><span class="token punctuation">.</span>name<span class="token operator">=</span>’Alic’ <span class="token keyword">return</span> <span class="token number">a</span><span class="token punctuation">.</span>array<span class="token punctuation">,</span>head<span class="token punctuation">(</span><span class="token number">a</span><span class="token punctuation">.</span>array<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">last</span>：返回集合的最后一个元素。<span class="token keyword">match</span> <span class="token punctuation">(</span><span class="token number">a</span><span class="token punctuation">)</span> <span class="token keyword">where</span> <span class="token number">a</span><span class="token punctuation">.</span>name<span class="token operator">=</span>’Alic’ <span class="token keyword">return</span> <span class="token number">a</span><span class="token punctuation">.</span>array<span class="token punctuation">,</span><span class="token function">last</span><span class="token punctuation">(</span><span class="token number">a</span><span class="token punctuation">.</span>array<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">timestamp</span>：返回当前时间的毫秒startNode：返回一个关系的开始节点。<span class="token keyword">match</span> <span class="token punctuation">(</span>x:foo<span class="token punctuation">)</span><span class="token operator">-</span><span class="token punctuation">[</span>r<span class="token punctuation">]</span><span class="token operator">-</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">return</span> startNode<span class="token punctuation">(</span>r<span class="token punctuation">)</span><span class="token punctuation">;</span>endNode：返回一个关系的结束节点。<span class="token keyword">match</span> <span class="token punctuation">(</span>x:foo<span class="token punctuation">)</span><span class="token operator">-</span><span class="token punctuation">[</span>r<span class="token punctuation">]</span><span class="token operator">-</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">return</span> endNode<span class="token punctuation">(</span>r<span class="token punctuation">)</span><span class="token punctuation">;</span>toInt<span class="token punctuation">,</span>toFloat<span class="token punctuation">,</span>toString</code></pre><h2 id="py2neo"><a href="#py2neo" class="headerlink" title="py2neo"></a>py2neo</h2><pre class=" language-python"><code class="language-python"><span class="token keyword">from</span> py2neo <span class="token keyword">import</span> Node<span class="token punctuation">,</span> Relationship<span class="token punctuation">,</span> Graph<span class="token punctuation">,</span> NodeMatcher<span class="token triple-quoted-string string">"""1 创建节点与关系"""</span>a <span class="token operator">=</span> Node<span class="token punctuation">(</span><span class="token string">"Person"</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">"Alice"</span><span class="token punctuation">)</span>b <span class="token operator">=</span> Node<span class="token punctuation">(</span><span class="token string">"Person"</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">"Bob"</span><span class="token punctuation">)</span>ab <span class="token operator">=</span> Relationship<span class="token punctuation">(</span>a<span class="token punctuation">,</span> <span class="token string">"KNOWS"</span><span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token comment" spellcheck="true">## 节点添加属性</span>a<span class="token punctuation">[</span><span class="token string">'age'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">20</span>b<span class="token punctuation">[</span><span class="token string">'age'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">34</span>ab<span class="token punctuation">[</span><span class="token string">'time'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">'2027/08/31'</span><span class="token comment" spellcheck="true">## 通过 setdefault() 方法赋值默认属性</span>a<span class="token punctuation">.</span>setdefault<span class="token punctuation">(</span><span class="token string">'location'</span><span class="token punctuation">,</span> <span class="token string">'beijing'</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">## update批量更新</span>data <span class="token operator">=</span> <span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;</span>    <span class="token string">'name'</span><span class="token punctuation">:</span> <span class="token string">"Amy"</span><span class="token punctuation">,</span>    <span class="token string">"age"</span> <span class="token punctuation">:</span> <span class="token number">22</span><span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#125;</span>a<span class="token punctuation">.</span>update<span class="token punctuation">(</span>data<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># print(a.labels, b, ab)</span><span class="token comment" spellcheck="true">## 节点其他属性</span><span class="token triple-quoted-string string">"""  - hash   - node[key]  - node[key] = value  - del node[key]   - len(node)  - dict(node)  - wake(node)  - labels  - has_label("label")  - add_label("label")  - remove_label('label')  - clear_labels()  - update_labels('iter_label')"""</span><span class="token comment" spellcheck="true">## 连接的属性</span><span class="token triple-quoted-string string">"""  - hash(relationship)  - relationship[key]  - relationship[key] = value  - del relationship[key]  - len(relationship)  - dict(relationship)  - walk(relationship)  - type()"""</span><span class="token triple-quoted-string string">"""2 子图Subgraphs"""</span>s <span class="token operator">=</span> a <span class="token operator">|</span> b <span class="token operator">|</span> abs1 <span class="token operator">=</span> a <span class="token operator">|</span> b <span class="token operator">|</span> abs2 <span class="token operator">=</span> a <span class="token operator">|</span> b<span class="token keyword">print</span><span class="token punctuation">(</span>s1 <span class="token operator">&amp;</span> s2<span class="token punctuation">)</span><span class="token triple-quoted-string string">"""3 Walkable Types"""</span>a <span class="token operator">=</span> Node<span class="token punctuation">(</span><span class="token string">'Person'</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'Alice'</span><span class="token punctuation">)</span>b <span class="token operator">=</span> Node<span class="token punctuation">(</span><span class="token string">'Person'</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'Bob'</span><span class="token punctuation">)</span>c <span class="token operator">=</span> Node<span class="token punctuation">(</span><span class="token string">'Person'</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'Mike'</span><span class="token punctuation">)</span>ab <span class="token operator">=</span> Relationship<span class="token punctuation">(</span>a<span class="token punctuation">,</span> <span class="token string">"KNOWS"</span><span class="token punctuation">,</span> b<span class="token punctuation">)</span>ac <span class="token operator">=</span> Relationship<span class="token punctuation">(</span>a<span class="token punctuation">,</span> <span class="token string">"KNOWS"</span><span class="token punctuation">,</span> c<span class="token punctuation">)</span>w <span class="token operator">=</span> ab <span class="token operator">+</span> Relationship<span class="token punctuation">(</span>b<span class="token punctuation">,</span> <span class="token string">"LIKES"</span><span class="token punctuation">,</span> c<span class="token punctuation">)</span> <span class="token operator">+</span> ac<span class="token comment" spellcheck="true">## 遍历</span><span class="token keyword">for</span> i <span class="token keyword">in</span> w<span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token comment" spellcheck="true">## 其他属性</span><span class="token keyword">print</span><span class="token punctuation">(</span>w<span class="token punctuation">.</span>start_node<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>w<span class="token punctuation">.</span>end_node<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>w<span class="token punctuation">.</span>nodes<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>w<span class="token punctuation">.</span>relationships<span class="token punctuation">)</span><span class="token triple-quoted-string string">"""4 连接已有图数据库 - .Graph()"""</span>graph <span class="token operator">=</span> Graph<span class="token punctuation">(</span><span class="token string">"http://localhost:7474"</span><span class="token punctuation">,</span>  username<span class="token operator">=</span><span class="token string">"neo4j"</span><span class="token punctuation">,</span>  password<span class="token operator">=</span><span class="token string">"123456"</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># test_graph,就连接上了电脑中默认的图数据库，就可以进行查询了。</span><span class="token comment" spellcheck="true">##利用 create() 方法传入 Subgraph 对象来将关系图添加到数据库</span><span class="token comment" spellcheck="true"># graph.create(s)</span><span class="token comment" spellcheck="true">## 单独添加单个 Node 或 Relationship</span><span class="token comment" spellcheck="true"># graph.create(a)</span><span class="token comment" spellcheck="true"># graph.create(b)</span><span class="token comment" spellcheck="true"># graph.create(ab)</span><span class="token triple-quoted-string string">"""其他应用"""</span><span class="token comment" spellcheck="true">## 查找是否存在节点 - exists(subgraph)</span><span class="token keyword">print</span><span class="token punctuation">(</span>graph<span class="token punctuation">.</span>exists<span class="token punctuation">(</span>s<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">"""二、查询方式"""</span><span class="token triple-quoted-string string">"""2.1 结果查询-.run/.data/.match"""</span><span class="token comment" spellcheck="true">## 比较传统的方式：通过nodes的ID进行检索</span><span class="token comment" spellcheck="true"># 其中的数字对应的是节点，ID</span><span class="token comment" spellcheck="true"># 这个ID不按顺序来的，要注意</span>graph<span class="token punctuation">.</span>nodes<span class="token punctuation">[</span><span class="token number">1234</span><span class="token punctuation">]</span>graph<span class="token punctuation">.</span>nodes<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token number">1234</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">## match的方式</span>data <span class="token operator">=</span> graph<span class="token punctuation">.</span>run<span class="token punctuation">(</span><span class="token string">"MATCH (a:Person &amp;#123;name:'Alice'&amp;#125;) return a"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>data<span class="token punctuation">(</span><span class="token punctuation">)</span>data1 <span class="token operator">=</span> graph<span class="token punctuation">.</span>run<span class="token punctuation">(</span><span class="token string">"MATCH (a:Person &amp;#123;name:'Alice'&amp;#125;) -[b:KNOWS]-> (c:Person &amp;#123;name:'Bob'&amp;#125;) return a, b, c"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>data<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">## graph.run()，之中填写的是查询语句。查询的结果也可以转换为dataframe的格式 查询出来的结果是dict/list格式的，并不是graph型，于是不能进行后续查询</span><span class="token keyword">import</span> pandas <span class="token keyword">as</span> pddf <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span>data1<span class="token punctuation">)</span><span class="token comment" spellcheck="true">## 标准化成一些表格的格式</span>df1 <span class="token operator">=</span> graph<span class="token punctuation">.</span>run<span class="token punctuation">(</span><span class="token string">"MATCH (a:Person &amp;#123;name:'Alice'&amp;#125;) return a"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>data<span class="token punctuation">(</span><span class="token punctuation">)</span>  # list型df2 <span class="token operator">=</span> graph<span class="token punctuation">.</span>run<span class="token punctuation">(</span><span class="token string">"MATCH (a:Person &amp;#123;name:'Alice'&amp;#125;) return a"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to_data_frame<span class="token punctuation">(</span><span class="token punctuation">)</span>  # dataframe型df3 <span class="token operator">=</span> graph<span class="token punctuation">.</span>run<span class="token punctuation">(</span><span class="token string">"MATCH (a:Person &amp;#123;name:'Alice'&amp;#125;) return a"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to_table<span class="token punctuation">(</span><span class="token punctuation">)</span>  # table<span class="token triple-quoted-string string">"""更灵活的查询 - NodeMatcher"""</span>selector <span class="token operator">=</span> NodeMatcher<span class="token punctuation">(</span>graph<span class="token punctuation">)</span>nodematch <span class="token operator">=</span> selector<span class="token punctuation">.</span>match<span class="token punctuation">(</span><span class="token string">"Person"</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>list<span class="token punctuation">(</span>nodematch<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#另外也可以使用 where() 进行更复杂的查询，例如查找 name 是 A 开头的 Person Node，实例如下：</span>nodematch1 <span class="token operator">=</span> list<span class="token punctuation">(</span>selector<span class="token punctuation">.</span>match<span class="token punctuation">(</span><span class="token string">"Person"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>where<span class="token punctuation">(</span><span class="token string">"_.name =~ 'B.*'"</span><span class="token punctuation">,</span> <span class="token string">"1960 &lt;= _.born &lt; 1970"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>nodematch1<span class="token punctuation">)</span>persons <span class="token operator">=</span> selector<span class="token punctuation">.</span>match<span class="token punctuation">(</span><span class="token string">'Person'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>where<span class="token punctuation">(</span><span class="token string">"_.name =~ 'B.*'"</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>list<span class="token punctuation">(</span>persons<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">## 另外也可以使用 order_by() 进行排序：</span>persons <span class="token operator">=</span> selector<span class="token punctuation">.</span>match<span class="token punctuation">(</span><span class="token string">'Person'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>order_by<span class="token punctuation">(</span><span class="token string">'_.age'</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>list<span class="token punctuation">(</span>persons<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">"""first()返回单个节点limit(amount)返回底部节点的限值条数skip(amount)返回顶部节点的限值条数order_by(*fields)排序where(*conditions, **properties)筛选条件"""</span><span class="token triple-quoted-string string">"""2.4 match() 或 match_one() 查找Relationshipmatch 匹配关系.match_one，匹配并返回所有满足条件的一条关系"""</span>nodematcher<span class="token operator">=</span>NodeMatcher<span class="token punctuation">(</span>graph<span class="token punctuation">)</span>findnode<span class="token operator">=</span>nodematcher<span class="token punctuation">.</span>match<span class="token punctuation">(</span><span class="token string">'Person'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>first<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># p = graph.match(nodes=(findnode,), r_type='KNOWS')</span><span class="token comment" spellcheck="true"># print(list(p))</span><span class="token comment" spellcheck="true"># graph.match_one(nodes=(findnode,), r_type='KNOWS')</span><span class="token triple-quoted-string string">""" 删除 - .delete()/.delete_all()"""</span>s <span class="token operator">=</span> graph<span class="token punctuation">.</span>match<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># graph.delete(findnode)</span><span class="token comment" spellcheck="true"># graph.delete_all()</span></code></pre><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://blog.csdn.net/sinat_26917383/article/details/79883503">neo4j︱图数据库基本概念、操作罗列与整理（一）</a></p><p><a href="https://blog.csdn.net/sinat_26917383/article/details/79850412">neo4j︱Cypher 查询语言简单案例（二）</a></p><p><a href="https://blog.csdn.net/sinat_26917383/article/details/79852596">neo4j︱Cypher完整案例csv导入、关系联通、高级查询（三）</a></p><p><a href="https://blog.csdn.net/sinat_26917383/article/details/79901207">neo4j︱与python结合的py2neo使用教程（四）</a></p><p><a href="https://www.yuque.com/yahan/mztcmb/lszfiv">Py2neo v4 使用笔记</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 数据库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图数据 </tag>
            
            <tag> 安装 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>准确率、精确率、召回率、F1-score</title>
      <link href="/2020/05/06/zhun-que-lu-jing-que-lu-zhao-hui-lu-f1-score/"/>
      <url>/2020/05/06/zhun-que-lu-jing-que-lu-zhao-hui-lu-f1-score/</url>
      
        <content type="html"><![CDATA[<h1 id="准确率、精确率、召回率、F1-score"><a href="#准确率、精确率、召回率、F1-score" class="headerlink" title="准确率、精确率、召回率、F1-score"></a>准确率、精确率、召回率、F1-score</h1><p>​      机器学习问题之中，通常需要建立模型来解决具体问题，但对于模型的好坏，也就是模型的泛化能力，如何进行评估呢?</p><h2 id="混淆矩阵"><a href="#混淆矩阵" class="headerlink" title="混淆矩阵"></a>混淆矩阵</h2><p><img src="/2020/05/06/zhun-que-lu-jing-que-lu-zhao-hui-lu-f1-score/1.png"></p><p>如上图所示，要了解各个评价指标，混淆矩阵中的 P 表示 Positive，即正例或者阳性，N 表示 Negative，即负例或者阴性。你也可以把 P 和 N 分别理解为二分类中的 1-0</p><ul><li><p>TP：预测为1，实际为1，预测正确。</p></li><li><p>FP：预测为1，实际为0，预测错误。</p></li><li><p>FN：预测为0，实际为1，预测错误。</p></li><li><p>TN：预测为0，实际为0，预测正确。</p></li><li><p>TP+FP：表示所有预测为正的样本数量</p></li><li><p>TN+FN：表示所有预测为负的样本数量</p></li><li><p>TP+FN：表示实际为正的样本数量</p></li><li><p>TN+FP：表示实际为负的样本数量</p></li></ul><h2 id="准确率（accuracy）"><a href="#准确率（accuracy）" class="headerlink" title="准确率（accuracy）"></a>准确率（accuracy）</h2><p>所有的预测正确（正类负类）的占总的比重</p><p>$Accuracy = \frac {TP+TN}{TP+TN+FP+FN}$</p><h2 id="精确率（也叫查准率，precision"><a href="#精确率（也叫查准率，precision" class="headerlink" title="精确率（也叫查准率，precision)"></a>精确率（也叫查准率，precision)</h2><p>即正确预测为正的占全部预测为正的比例，（真正正确的占所有预测为正的比例）</p><p>$Precision = \frac {TP}{TP+FP}$</p><h2 id="召回率（recall）"><a href="#召回率（recall）" class="headerlink" title="召回率（recall）"></a>召回率（recall）</h2><p>即正确预测为正的占全部实际为正的比例（真正正确的占所有实际为正的比例）</p><p>$ Recall = \frac {TP}{TP+ FN}$ </p><h2 id="F1-score"><a href="#F1-score" class="headerlink" title="F1-score"></a>F1-score</h2><p>F1值为算数平均数除以几何平均数，且越大越好，将Precision和Recall的上述公式带入会发现，当F1值小时，True Positive相对增加，而false相对减少，即Precision和Recall都相对增加，即F1对Precision和Recall都进行了加权。</p><p>$\frac {2}{F _1} = \frac{1}{Precision} + \frac {1}{Recall}$</p><p>$F1 = \frac {2TP}{2TP+FP+ FN}$</p><h2 id="各指标优缺点"><a href="#各指标优缺点" class="headerlink" title="各指标优缺点"></a>各指标优缺点</h2><blockquote><p>准确率 样本不均衡的情况下,并不能作为好的指标</p><p>精确率 针对预测结果而言  </p><p>召回率 是针对原样本而言,准确率和召回率互相影响,相互牵制.</p><p>F1  是精确率和召回率的调和平均。引入F1-Score作为综合指标，就是为了平衡准确率和召回率的影响</p></blockquote><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://blog.csdn.net/Joseph__Lagrange/article/details/90813885">参考1</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> nlp </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 评估指标 </tag>
            
            <tag> 统计 </tag>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hexo</title>
      <link href="/2020/05/05/hexo/"/>
      <url>/2020/05/05/hexo/</url>
      
        <content type="html"><![CDATA[<p>转载来自于[<a href="https://github.com/blinkfox/hexo-theme-matery]">https://github.com/blinkfox/hexo-theme-matery]</a></p><h1 id="hexo"><a href="#hexo" class="headerlink" title="hexo"></a>hexo</h1><h2 id="Hexo介绍"><a href="#Hexo介绍" class="headerlink" title="Hexo介绍"></a>Hexo介绍</h2><p><a href="https://hexo.io/zh-cn/">Hexo</a>是一款快速、简洁且高效的基于<code>Node.js</code>的静态博客框架，四大特性：</p><ul><li>超快速度：<code>Node.js</code> 所带来的超快生成速度，让上百个页面在几秒内瞬间完成渲染。</li><li>支持Markdown：<code>Hexo</code> 支持 <code>GitHub Flavored Markdown</code> 的所有功能，甚至可以整合 <code>Octopress</code> 的大多数插件。</li><li>一键部署：只需一条指令即可部署到 <code>GitHub Pages</code>, <code>Heroku</code>或其他平台。</li><li>插件和可扩展性：强大的 <code>API</code> 带来无限的可能，与数种模板引擎<code>（EJS，Pug，Nunjucks）</code>和工具<code>（Babel，PostCSS，Less/Sass）</code>轻易集成。</li></ul><p>这使得很多非编程人员可以很轻松，很自由的定制博客。废话不多说，开始进入搭建环境把。</p><h2 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h2><h3 id="Linux"><a href="#Linux" class="headerlink" title="Linux"></a>Linux</h3><p>直接命令行输入：</p><pre><code>sudo apt-get install nodejssudo apt-get install npm</code></pre><p>或者到<a href="http://nodejs.cn/download/">官网</a>下载：</p><p>下载完成后解压到指定文件夹，然后配置环境变量（目的是为了在终端可以任意位置使用它）：</p><p>首先打开<code>~/.bashrc</code>文件</p><p>在文件的最下端填写如下代码</p><pre><code>export PATH=$&amp;#123;PATH&amp;#125;:$HOME/node-v12.13.0-linux-x64/bin/</code></pre><p>因为我下载的是<code>64</code>位<code>12.13.0</code>版本，并且放到了根目录<code>home</code>下，你可以根据自己的需求进行更改上面的路径。保存退出后，执行命令让修改生效。</p><pre class=" language-shell"><code class="language-shell">source ~/.bashrc</code></pre><p>然后在终端输入<code>npm -v</code>和<code>node -v</code>验证是否安装配置成功</p><pre class=" language-shell"><code class="language-shell">$npm -v6.13.0$node -vv12.13.0</code></pre><h3 id="Windows"><a href="#Windows" class="headerlink" title="Windows"></a>Windows</h3><p>下载稳定版或者最新版都可以<a href="http://nodejs.cn/download/">Node.js</a>，安装选项全部默认，一路点击<code>Next</code>。最后安装好之后，按<code>Win+R</code>打开命令提示符，输入<code>node -v</code>和<code>npm -v</code>，如果出现版本号，那么就安装成功了。</p><h3 id="npm加速"><a href="#npm加速" class="headerlink" title="npm加速"></a>npm加速</h3><p>一般国内通过<code>npm</code>下载东西会比较慢，所以需要添加阿里的源进行加速。</p><pre class=" language-shell"><code class="language-shell">npm config set registry https://registry.npm.taobao.org</code></pre><h3 id="安装Git"><a href="#安装Git" class="headerlink" title="安装Git"></a>安装Git</h3><p>为了把本地的网页文件上传到<code>Github</code>上面去，我们需要用到分布式版本控制工具 <code>git</code>。关于<code>git</code>和<code>Github</code>这里就不多介绍了。同样分为两个版本：</p><h4 id="Linux-1"><a href="#Linux-1" class="headerlink" title="Linux"></a>Linux</h4><p>在Linux平台比较方便，直接使用命令就可以安装：</p><pre class=" language-shell"><code class="language-shell">sudo apt-get install git</code></pre><p>安装完成后即可享用。</p><h4 id="Windows-1"><a href="#Windows-1" class="headerlink" title="Windows"></a>Windows</h4><p>需要去官网下载<a href="https://git-scm.com/download/win">Git</a>，下载完成后按照向导安装即可。</p><blockquote><p>注意：在安装的最后一步添加路径时选择 Use Git from the Windows Command Prompt 。这是把Git添加到了环境变量中，以便可以在cmd中使用。而本人推荐使用下载附带的git bash进行操作，比较方便。</p></blockquote><p>对于git的讲解和使用，大家可以自行到网上查找。<code>Hexo</code>搭建的过程中，已经封装好一个git命令，可以直接使用<code>hexo</code>的命令将生成的静态网站代码同步到<code>github</code>的仓库里。但是如果想要自己同步源码的话，那么就需要掌握一下git命令了。在这里我只列举一下常用的命令：</p><pre class=" language-shell"><code class="language-shell">git init #初始化一个git库，生成.git文件夹，里面保存的是该git库的记录和配置git remote add origin 远程仓库地址 #将本地仓库和远程仓库链接起来git pull #同步代码git status #检查本地仓库修改状态git add 文件名 或者 git add .  #将本地修改的文件加入缓存git commit 文件名 -m "描述" 或者 git commit . -m "描述"  #提交缓存，并描述该提交git push -u origin code # 将本地的提交推送到远程仓库.-u是代表输入账号密码，如果你已经配置了git的公钥，那么可直接push.</code></pre><h3 id="注册Github"><a href="#注册Github" class="headerlink" title="注册Github"></a>注册Github</h3><p><code>Git</code>安装完成之后就可以去<a href="https://github.com/">Github</a>上注册账号并创建仓库， 用来存放我们的网站了。</p><blockquote><p>Github是基于 Git 做版本控制的代码托管平台，同时也是全球最大的代（同）码（性）托（交）管（友）网站。</p></blockquote><p>创建完账户之后新建一个项目仓库<code>New repository</code>，如下所示</p><p>接着输入仓库名，后面一定要加<code>.github.io</code>后缀，README初始化也要勾上。 如下图配置（因为我的已经存在相同的仓库，所以报错）</p><blockquote><p>要创建一个和你用户名相同的仓库，后面加.github.io，只有这样将来要部署到GitHub page的时候，才会被识别，也就是<a href="http://xxxx.github.樛i珟o鋔,曷xxx朒`笋酀g羻ithub摣拣`螟/">http://xxxx.github.io，其中xxx就是你注册<code>GitHub</code>的用户名</a></p></blockquote><p>然后项目就建成了，点击<code>Settings</code>，向下拉到最后有个<code>GitHub Pages</code>，点击<code>Choose a theme</code>选择一个主题。然后等一会儿，再回到<code>GitHub Pages</code>，点击新出来的链接，就会进入到<code>github page</code>的界面。看到这个界面就说明<code>Github</code>的<code>page</code>已经可以使用了，接下来我们进入<code>Hexo</code>的搭建。</p><h2 id="搭建"><a href="#搭建" class="headerlink" title="搭建"></a>搭建</h2><h3 id="安装Hexo"><a href="#安装Hexo" class="headerlink" title="安装Hexo"></a>安装Hexo</h3><p>首先创建一个文件夹，名字自取如<code>YoungBlog</code>，用来存放自己的博客文件，然后<code>cd</code>到这个文件夹下（或者在这个文件夹下直接右键<code>git bash</code>打开）。在该目录下输入如下命令安装<code>Hexo</code>：</p><pre class=" language-shell"><code class="language-shell">npm install -g hexo-cli</code></pre><p>接下来初始化一下<code>hexo</code>,即初始化我们的网站，</p><pre class=" language-shell"><code class="language-shell">hexo init</code></pre><blockquote><p>初始化要求必须是空的目录下进行。</p></blockquote><p>接着输入<code>npm install</code>安装必备的组件。</p><p>初始化完成后会在目下生成几个文件和文件夹，这些就是我们需要编写的网站源码了：</p><ul><li><code>node_modules:</code> 依赖包，npm安装的一些插件存放的文件夹。</li><li><code>public：</code>存放生成的页面，网站正式展示的内容。</li><li><code>scaffolds：</code>生成文章和页面的一些模板。</li><li><code>source：</code>用来存放你的文章和数据。</li><li><code>themes：</code>主题存放文件夹。</li><li><code>_config.yml:</code> 博客的配置文件，非主题的配置。</li><li><code>db.json</code>：博客的版本信息等。</li><li><code>package.json</code>和<code>package-lock.json</code>：依赖包和版本信息。</li></ul><p>这样本地的网站配置也弄好啦，输入<code>hexo g</code>生成静态网页，然后输入<code>hexo s</code>打开本地服务器，然后浏览器打开<a href="http://localhost:4000/">http://localhost:4000</a>就可以看到我们的博客啦，效果如下：</p><p>这里介绍一下<code>Hexo</code>常用的几个命令：</p><pre class=" language-shell"><code class="language-shell">hexo clean #清除db和public文件下的内容，或可写成hexo clhexo g #根据源码生成静态文件hexo s #开启本地的server，这样可在本地通过localhost:4000访问博客。或可写成hexo serverhexo d #部署网站的静态文件到配置好的托管网站，如Github或者Coding，配置在_config中的Deploy。#后续如果安装了一些插件，可能导致缩写无法使用，所以hexo d也可以写成hexo deploy。</code></pre><p>看完展示后，可以按<code>ctrl+c</code>关闭本地服务器。</p><h3 id="部署到Github"><a href="#部署到Github" class="headerlink" title="部署到Github"></a>部署到Github</h3><p>首先要安装一个插件，用于<code>Hexo</code>部署代码的。</p><pre><code>npm i hexo-deployer-git</code></pre><p>安装完成之后，在<code>_config.yml</code>配置文件中加入如下代码，这样我们在使用<code>hexo d</code>的时候就可以直接部署到<code>Github</code>上了，如果你想部署到其他平台（支持<code>Git</code>），也可以添加到这里。</p><pre><code>deploy:  type: git  repository: https://github.com/daiyizheng123/daiyizheng.github.io.git  branch: master</code></pre><blockquote><p>如果不了解git那么请先自行百度学习一下git的相关配置。</p></blockquote><p><code>Git</code>分为无密推送和需要输入账户密码推送。无密码推送就是需要在本地生成公钥，然后添加到代码托管平台如<code>Github</code>，这样在推送时候就不需要输入账户密码了。而反之的话，每次推送就会要求你输入账户密码。下面说一下无密推送的配置过程。</p><p>首先打开<code>Git bash</code>，输入如下内容：</p><pre class=" language-shell"><code class="language-shell">git config --global user.name "你的用户名"git config --global user.email "你的邮箱"</code></pre><p>用户名和邮箱根据你注册<code>github</code>的信息自行修改。</p><p>然后生成密钥SSH key：</p><pre class=" language-shell"><code class="language-shell">ssh-keygen -t rsa -C "你的邮箱"</code></pre><p>这个时候它会告诉你已经生成了<code>.ssh</code>的文件夹。在你的电脑中找到这个文件夹。或者<code>git bash</code>中输入</p><pre class=" language-shell"><code class="language-shell">cat ~/.ssh/id_rsa.pub</code></pre><p>打开<a href="http://github.com/">github</a>，在头像下面点击<code>settings</code>，再点击<code>SSH and GPG keys</code>，新建一个<code>SSH</code>，名字随便取一个都可以，把你的<code>id_rsa.pub</code>里面的信息复制进去。</p><p>这样你的电脑就跟<code>Github</code>建立起的安全联系，以后推送代码就不需要输入密码了。</p><blockquote><p>注意：这里使用hexo d推送代码，推送的是编译完成的静态文件，也就是上面说的public文件夹下的代码，而不是网站的源代码。</p></blockquote><h2 id="插入音乐-视频"><a href="#插入音乐-视频" class="headerlink" title="插入音乐/视频"></a>插入音乐/视频</h2><p>Hexo博客插入视频、音频有三种方法：</p><h3 id="Hexo博客利用iframe标签插入网易音乐方法"><a href="#Hexo博客利用iframe标签插入网易音乐方法" class="headerlink" title="Hexo博客利用iframe标签插入网易音乐方法"></a>Hexo博客利用iframe标签插入网易音乐方法</h3><pre class=" language-xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>iframe</span> <span class="token attr-name">frameborder</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>no<span class="token punctuation">"</span></span> <span class="token attr-name">border</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>0<span class="token punctuation">"</span></span> <span class="token attr-name">marginwidth</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>0<span class="token punctuation">"</span></span> <span class="token attr-name">marginheight</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>0<span class="token punctuation">"</span></span> <span class="token attr-name">width</span><span class="token attr-value"><span class="token punctuation">=</span>430</span> <span class="token attr-name">height</span><span class="token attr-value"><span class="token punctuation">=</span>86</span> <span class="token attr-name">src</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>//music.163.com/outchain/player?type<span class="token punctuation">=</span>2&amp;id<span class="token punctuation">=</span>114389&amp;auto<span class="token punctuation">=</span>0&amp;height<span class="token punctuation">=</span>66<span class="token punctuation">"</span></span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>iframe</span><span class="token punctuation">></span></span></code></pre><h3 id="Hexo博客利用embed-标签插入音乐和视频"><a href="#Hexo博客利用embed-标签插入音乐和视频" class="headerlink" title="Hexo博客利用embed 标签插入音乐和视频"></a>Hexo博客利用embed 标签插入音乐和视频</h3><pre class=" language-xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>embed</span> <span class="token attr-name">height</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>415<span class="token punctuation">"</span></span> <span class="token attr-name">width</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>544<span class="token punctuation">"</span></span> <span class="token attr-name">quality</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>high<span class="token punctuation">"</span></span> <span class="token attr-name">allowfullscreen</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>true<span class="token punctuation">"</span></span> <span class="token attr-name">type</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>application/x-shockwave-flash<span class="token punctuation">"</span></span> <span class="token attr-name">src</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>//static.hdslb.com/miniloader.swf<span class="token punctuation">"</span></span> <span class="token attr-name">flashvars</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>aid<span class="token punctuation">=</span>8506694&amp;page<span class="token punctuation">=</span>1<span class="token punctuation">"</span></span> <span class="token attr-name">pluginspage</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>//www.adobe.com/shockwave/download/download.cgi?P1_Prod_Version<span class="token punctuation">=</span>ShockwaveFlash<span class="token punctuation">"</span></span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>embed</span><span class="token punctuation">></span></span></code></pre><h3 id="Hexo博客利用Hexo插件插入音乐-视频"><a href="#Hexo博客利用Hexo插件插入音乐-视频" class="headerlink" title="Hexo博客利用Hexo插件插入音乐/视频"></a>Hexo博客利用Hexo插件插入音乐/视频</h3><ul><li><p>npm install hexo-tag-aplayer</p><pre class=" language-bash"><code class="language-bash"><span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;% aplayer "她的睫毛" "周杰伦" "http://home.ustc.edu.cn/~mmmwhy/%d6%dc%bd%dc%c2%d7%20-%20%cb%fd%b5%c4%bd%de%c3%ab.mp3"  "http://home.ustc.edu.cn/~mmmwhy/jay.jpg" "autoplay=false" %&amp;#125;</span></code></pre></li><li><p>npm install hexo-tag-dplayer</p><pre class=" language-bash"><code class="language-bash"><span class="token operator">&amp;</span><span class="token comment" spellcheck="true">#123;% dplayer "url=http://home.ustc.edu.cn/~mmmwhy/GEM.mp4"  "pic=http://home.ustc.edu.cn/~mmmwhy/GEM.jpg" "loop=yes" "theme=#FADFA3" "autoplay=false" "token=tokendemo" %&amp;#125;</span></code></pre></li></ul><h3 id="blibi视频嵌入代码"><a href="#blibi视频嵌入代码" class="headerlink" title="blibi视频嵌入代码"></a>blibi视频嵌入代码</h3><pre class=" language-js"><code class="language-js"><span class="token operator">&lt;</span>iframe src<span class="token operator">=</span><span class="token string">"//player.bilibili.com/player.html?aid=73657563&amp;bvid=BV1HE411Y7G9&amp;cid=125986571&amp;page=2"</span> scrolling<span class="token operator">=</span><span class="token string">"no"</span> border<span class="token operator">=</span><span class="token string">"0"</span> frameborder<span class="token operator">=</span><span class="token string">"no"</span> framespacing<span class="token operator">=</span><span class="token string">"0"</span> allowfullscreen<span class="token operator">=</span><span class="token string">"true"</span>  width<span class="token operator">=</span><span class="token string">"680"</span> height<span class="token operator">=</span><span class="token string">"520"</span> align<span class="token operator">=</span><span class="token string">"center"</span><span class="token operator">></span> <span class="token operator">&lt;</span><span class="token operator">/</span>iframe<span class="token operator">></span></code></pre><h3 id="切换主题"><a href="#切换主题" class="headerlink" title="切换主题"></a>切换主题</h3><p>修改 Hexo 根目录下的 <code>_config.yml</code> 的 <code>theme</code> 的值：<code>theme: hexo-theme-matery</code></p><h4 id="config-yml-文件的其它修改建议"><a href="#config-yml-文件的其它修改建议" class="headerlink" title="_config.yml 文件的其它修改建议:"></a><code>_config.yml</code> 文件的其它修改建议:</h4><ul><li>请修改 <code>_config.yml</code> 的 <code>url</code> 的值为你的网站主 <code>URL</code>（如：<code>http://xxx.github.io</code>）。</li><li>建议修改两个 <code>per_page</code> 的分页条数值为 <code>6</code> 的倍数，如：<code>12</code>、<code>18</code> 等，这样文章列表在各个屏幕下都能较好的显示。</li><li>如果你是中文用户，则建议修改 <code>language</code> 的值为 <code>zh-CN</code>。</li></ul><h3 id="新建分类-categories-页"><a href="#新建分类-categories-页" class="headerlink" title="新建分类 categories 页"></a>新建分类 categories 页</h3><p><code>categories</code> 页是用来展示所有分类的页面，如果在你的博客 <code>source</code> 目录下还没有 <code>categories/index.md</code> 文件，那么你就需要新建一个，命令如下：</p><pre><code>hexo new page "categories"</code></pre><p>编辑你刚刚新建的页面文件 <code>/source/categories/index.md</code>，至少需要以下内容：</p><pre><code>---title: categoriesdate: 2018-09-30 17:25:30type: "categories"layout: "categories"---</code></pre><h3 id="新建标签-tags-页"><a href="#新建标签-tags-页" class="headerlink" title="新建标签 tags 页"></a>新建标签 tags 页</h3><p><code>tags</code> 页是用来展示所有标签的页面，如果在你的博客 <code>source</code> 目录下还没有 <code>tags/index.md</code> 文件，那么你就需要新建一个，命令如下：</p><pre><code>hexo new page "tags"</code></pre><p>编辑你刚刚新建的页面文件 <code>/source/tags/index.md</code>，至少需要以下内容：</p><pre><code>---title: tagsdate: 2018-09-30 18:23:38type: "tags"layout: "tags"---</code></pre><h3 id="新建关于我-about-页"><a href="#新建关于我-about-页" class="headerlink" title="新建关于我 about 页"></a>新建关于我 about 页</h3><p><code>about</code> 页是用来展示<strong>关于我和我的博客</strong>信息的页面，如果在你的博客 <code>source</code> 目录下还没有 <code>about/index.md</code> 文件，那么你就需要新建一个，命令如下：</p><pre><code>hexo new page "about"</code></pre><p>编辑你刚刚新建的页面文件 <code>/source/about/index.md</code>，至少需要以下内容：</p><pre><code>---title: aboutdate: 2018-09-30 17:25:30type: "about"layout: "about"---</code></pre><h3 id="新建留言板-contact-页（可选的）"><a href="#新建留言板-contact-页（可选的）" class="headerlink" title="新建留言板 contact 页（可选的）"></a>新建留言板 contact 页（可选的）</h3><p><code>contact</code> 页是用来展示<strong>留言板</strong>信息的页面，如果在你的博客 <code>source</code> 目录下还没有 <code>contact/index.md</code> 文件，那么你就需要新建一个，命令如下：</p><pre><code>hexo new page "contact"</code></pre><p>编辑你刚刚新建的页面文件 <code>/source/contact/index.md</code>，至少需要以下内容：</p><pre><code>---title: contactdate: 2018-09-30 17:25:30type: "contact"layout: "contact"---</code></pre><blockquote><p><strong>注</strong>：本留言板功能依赖于第三方评论系统，请<strong>激活</strong>你的评论系统才有效果。并且在主题的 <code>_config.yml</code> 文件中，第 <code>19</code> 至 <code>21</code> 行的“<strong>菜单</strong>”配置，取消关于留言板的注释即可。</p></blockquote><h3 id="新建友情链接-friends-页（可选的）"><a href="#新建友情链接-friends-页（可选的）" class="headerlink" title="新建友情链接 friends 页（可选的）"></a>新建友情链接 friends 页（可选的）</h3><p><code>friends</code> 页是用来展示<strong>友情链接</strong>信息的页面，如果在你的博客 <code>source</code> 目录下还没有 <code>friends/index.md</code> 文件，那么你就需要新建一个，命令如下：</p><pre><code>hexo new page "friends"</code></pre><p>编辑你刚刚新建的页面文件 <code>/source/friends/index.md</code>，至少需要以下内容：</p><pre><code>---title: friendsdate: 2018-12-12 21:25:30type: "friends"layout: "friends"---</code></pre><p>同时，在你的博客 <code>source</code> 目录下新建 <code>_data</code> 目录，在 <code>_data</code> 目录中新建 <code>friends.json</code> 文件，文件内容如下所示：</p><pre><code>[&amp;#123;    "avatar": "http://image.luokangyuan.com/1_qq_27922023.jpg",    "name": "码酱",    "introduction": "我不是大佬，只是在追寻大佬的脚步",    "url": "http://luokangyuan.com/",    "title": "前去学习"&amp;#125;, &amp;#123;    "avatar": "http://image.luokangyuan.com/4027734.jpeg",    "name": "闪烁之狐",    "introduction": "编程界大佬，技术牛，人还特别好，不懂的都可以请教大佬",    "url": "https://blinkfox.github.io/",    "title": "前去学习"&amp;#125;, &amp;#123;    "avatar": "http://image.luokangyuan.com/avatar.jpg",    "name": "ja_rome",    "introduction": "平凡的脚步也可以走出伟大的行程",    "url": "https://me.csdn.net/jlh912008548",    "title": "前去学习"&amp;#125;]</code></pre><h3 id="新建-404-页"><a href="#新建-404-页" class="headerlink" title="新建 404 页"></a>新建 404 页</h3><p>如果在你的博客 <code>source</code> 目录下还没有 <code>404.md</code> 文件，那么你就需要新建一个</p><p>编辑你刚刚新建的页面文件 <code>/source/404.md</code>，至少需要以下内容：</p><pre><code>---title: 404date: 2018-09-30 17:25:30type: "404"layout: "404"description: "Oops～，我崩溃了！找不到你想要的页面 :("---</code></pre><h3 id="菜单导航配置"><a href="#菜单导航配置" class="headerlink" title="菜单导航配置"></a>菜单导航配置</h3><h4 id="配置基本菜单导航的名称、路径url和图标icon"><a href="#配置基本菜单导航的名称、路径url和图标icon" class="headerlink" title="配置基本菜单导航的名称、路径url和图标icon."></a>配置基本菜单导航的名称、路径url和图标icon.</h4><p>1.菜单导航名称可以是中文也可以是英文(如：<code>Index</code>或<code>主页</code>) 2.图标icon 可以在<a href="https://fontawesome.com/icons">Font Awesome</a> 中查找</p><pre><code>menu:  Index:    url: /    icon: fas fa-home  Tags:    url: /tags    icon: fas fa-tags  Categories:    url: /categories    icon: fas fa-bookmark  Archives:    url: /archives    icon: fas fa-archive  About:    url: /about    icon: fas fa-user-circle  Friends:    url: /friends    icon: fas fa-address-book</code></pre><h4 id="二级菜单配置方法"><a href="#二级菜单配置方法" class="headerlink" title="二级菜单配置方法"></a>二级菜单配置方法</h4><p>如果你需要二级菜单则可以在原基本菜单导航的基础上如下操作<br>1.在需要添加二级菜单的一级菜单下添加<code>children</code>关键字(如:<code>About</code>菜单下添加<code>children</code>)<br>2.在<code>children</code>下创建二级菜单的 名称name,路径url和图标icon.<br>3.注意每个二级菜单模块前要加 <code>-</code>.<br>4.注意缩进格式</p><pre><code>menu:  Index:    url: /    icon: fas fa-home  Tags:    url: /tags    icon: fas fa-tags  Categories:    url: /categories    icon: fas fa-bookmark  Archives:    url: /archives    icon: fas fa-archive  About:    url: /about    icon: fas fa-user-circle-o  Friends:    url: /friends    icon: fas fa-address-book  Medias:    icon: fas fa-list    children:      - name: Musics        url: /musics        icon: fas fa-music      - name: Movies        url: /movies        icon: fas fa-film      - name: Books        url: /books        icon: fas fa-book      - name: Galleries        url: /galleries        icon: fas fa-image</code></pre><p>执行 <code>hexo clean &amp;&amp; hexo g</code> 重新生成博客文件，然后就可以在文章中对应位置看到你用<code>emoji</code>语法写的表情了。</p><h3 id="代码高亮"><a href="#代码高亮" class="headerlink" title="代码高亮"></a>代码高亮</h3><p>由于 Hexo 自带的代码高亮主题显示不好看，所以主题中使用到了 <a href="https://github.com/ele828/hexo-prism-plugin">hexo-prism-plugin</a> 的 Hexo 插件来做代码高亮，安装命令如下：</p><pre><code>npm i -S hexo-prism-plugin</code></pre><p>然后，修改 Hexo 根目录下 <code>_config.yml</code> 文件中 <code>highlight.enable</code> 的值为 <code>false</code>，并新增 <code>prism</code> 插件相关的配置，主要配置如下：</p><pre><code>highlight:  enable: falseprism_plugin:  mode: 'preprocess'    # realtime/preprocess  theme: 'tomorrow'  line_number: false    # default false  custom_css:</code></pre><h3 id="搜索"><a href="#搜索" class="headerlink" title="搜索"></a>搜索</h3><p>本主题中还使用到了 <a href="https://github.com/wzpan/hexo-generator-search">hexo-generator-search</a> 的 Hexo 插件来做内容搜索，安装命令如下：</p><pre><code>npm install hexo-generator-search --save</code></pre><p>在 Hexo 根目录下的 <code>_config.yml</code> 文件中，新增以下的配置项：</p><pre><code>search:  path: search.xml  field: post</code></pre><h3 id="中文链接转拼音（建议安装）"><a href="#中文链接转拼音（建议安装）" class="headerlink" title="中文链接转拼音（建议安装）"></a>中文链接转拼音（建议安装）</h3><p>如果你的文章名称是中文的，那么 Hexo 默认生成的永久链接也会有中文，这样不利于 <code>SEO</code>，且 <code>gitment</code> 评论对中文链接也不支持。我们可以用 <a href="https://github.com/viko16/hexo-permalink-pinyin">hexo-permalink-pinyin</a> Hexo 插件使在生成文章时生成中文拼音的永久链接。</p><p>安装命令如下：</p><pre><code>npm i hexo-permalink-pinyin --save</code></pre><p>在 Hexo 根目录下的 <code>_config.yml</code> 文件中，新增以下的配置项：</p><pre><code>permalink_pinyin:  enable: true  separator: '-' # default: '-'</code></pre><blockquote><p><strong>注</strong>：除了此插件外，<a href="https://github.com/rozbo/hexo-abbrlink">hexo-abbrlink</a> 插件也可以生成非中文的链接。</p></blockquote><h3 id="文章字数统计插件（建议安装）"><a href="#文章字数统计插件（建议安装）" class="headerlink" title="文章字数统计插件（建议安装）"></a>文章字数统计插件（建议安装）</h3><p>如果你想要在文章中显示文章字数、阅读时长信息，可以安装 <a href="https://github.com/willin/hexo-wordcount">hexo-wordcount</a>插件。</p><p>安装命令如下：</p><pre><code>npm i --save hexo-wordcount</code></pre><p>然后只需在本主题下的 <code>_config.yml</code> 文件中，将各个文章字数相关的配置激活即可：</p><pre><code>postInfo:  date: true  update: false  wordCount: false # 设置文章字数统计为 true.  totalCount: false # 设置站点文章总字数统计为 true.  min2read: false # 阅读时长.  readCount: false # 阅读次数.</code></pre><h3 id="添加emoji表情支持（可选的）"><a href="#添加emoji表情支持（可选的）" class="headerlink" title="添加emoji表情支持（可选的）"></a>添加emoji表情支持（可选的）</h3><p>本主题新增了对<code>emoji</code>表情的支持，使用到了 <a href="https://npm.taobao.org/package/hexo-filter-github-emojis">hexo-filter-github-emojis</a> 的 Hexo 插件来支持 <code>emoji</code>表情的生成，把对应的<code>markdown emoji</code>语法（<code>::</code>,例如：<code>:smile:</code>）转变成会跳跃的<code>emoji</code>表情，安装命令如下：</p><pre><code>npm install hexo-filter-github-emojis --save</code></pre><p>在 Hexo 根目录下的 <code>_config.yml</code> 文件中，新增以下的配置项：</p><pre><code>githubEmojis:  enable: true  className: github-emoji  inject: true  styles:  customEmojis:</code></pre><h3 id="添加-RSS-订阅支持（可选的）"><a href="#添加-RSS-订阅支持（可选的）" class="headerlink" title="添加 RSS 订阅支持（可选的）"></a>添加 RSS 订阅支持（可选的）</h3><p>本主题中还使用到了 <a href="https://github.com/hexojs/hexo-generator-feed">hexo-generator-feed</a> 的 Hexo 插件来做 <code>RSS</code>，安装命令如下：</p><pre><code>npm install hexo-generator-feed --save</code></pre><p>在 Hexo 根目录下的 <code>_config.yml</code> 文件中，新增以下的配置项：</p><pre><code>feed:  type: atom  path: atom.xml  limit: 20  hub:  content:  content_limit: 140  content_limit_delim: ' '  order_by: -date</code></pre><p>执行 <code>hexo clean &amp;&amp; hexo g</code> 重新生成博客文件，然后在 <code>public</code> 文件夹中即可看到 <code>atom.xml</code> 文件，说明你已经安装成功了。</p><h3 id="添加-DaoVoice-在线聊天功能（可选的）"><a href="#添加-DaoVoice-在线聊天功能（可选的）" class="headerlink" title="添加 DaoVoice 在线聊天功能（可选的）"></a>添加 <a href="http://www.daovoice.io/">DaoVoice</a> 在线聊天功能（可选的）</h3><p>前往 <a href="http://www.daovoice.io/">DaoVoice</a> 官网注册并且获取 <code>app_id</code>，并将 <code>app_id</code> 填入主题的 <code>_config.yml</code> 文件中。</p><h3 id="添加-Tidio-在线聊天功能（可选的）"><a href="#添加-Tidio-在线聊天功能（可选的）" class="headerlink" title="添加 Tidio 在线聊天功能（可选的）"></a>添加 <a href="https://www.tidio.com/">Tidio</a> 在线聊天功能（可选的）</h3><p>前往 <a href="https://www.tidio.com/">Tidio</a> 官网注册并且获取 <code>Public Key</code>，并将 <code>Public Key</code> 填入主题的 <code>_config.yml</code> 文件中。</p><h3 id="修改页脚"><a href="#修改页脚" class="headerlink" title="修改页脚"></a>修改页脚</h3><p>页脚信息可能需要做定制化修改，而且它不便于做成配置信息，所以可能需要你自己去再修改和加工。修改的地方在主题文件的 <code>/layout/_partial/footer.ejs</code> 文件中，包括站点、使用的主题、访问量等。</p><h3 id="修改社交链接"><a href="#修改社交链接" class="headerlink" title="修改社交链接"></a>修改社交链接</h3><p>在主题的 <code>_config.yml</code> 文件中，默认支持 <code>QQ</code>、<code>GitHub</code> 和邮箱等的配置，你可以在主题文件的 <code>/layout/_partial/social-link.ejs</code> 文件中，新增、修改你需要的社交链接地址，增加链接可参考如下代码：</p><pre><code>&lt;% if (theme.socialLink.github) &amp;#123; %&gt;    &lt;a href="&lt;%= theme.socialLink.github %&gt;" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50"&gt;        &lt;i class="fab fa-github"&gt;&lt;/i&gt;    &lt;/a&gt;&lt;% &amp;#125; %&gt;</code></pre><p>其中，社交图标（如：<code>fa-github</code>）你可以在 <a href="https://fontawesome.com/icons">Font Awesome</a> 中搜索找到。以下是常用社交图标的标识，供你参考：</p><ul><li>Facebook: <code>fab fa-facebook</code></li><li>Twitter: <code>fab fa-twitter</code></li><li>Google-plus: <code>fab fa-google-plus</code></li><li>Linkedin: <code>fab fa-linkedin</code></li><li>Tumblr: <code>fab fa-tumblr</code></li><li>Medium: <code>fab fa-medium</code></li><li>Slack: <code>fab fa-slack</code></li><li>Sina Weibo: <code>fab fa-weibo</code></li><li>Wechat: <code>fab fa-weixin</code></li><li>QQ: <code>fab fa-qq</code></li><li>Zhihu: <code>fab fa-zhihu</code></li></ul><blockquote><p><strong>注意</strong>: 本主题中使用的 <code>Font Awesome</code> 版本为 <code>5.11.0</code>。</p></blockquote><h3 id="修改打赏的二维码图片"><a href="#修改打赏的二维码图片" class="headerlink" title="修改打赏的二维码图片"></a>修改打赏的二维码图片</h3><p>在主题文件的 <code>source/medias/reward</code> 文件中，你可以替换成你的的微信和支付宝的打赏二维码图片。</p><h3 id="配置音乐播放器（可选的）"><a href="#配置音乐播放器（可选的）" class="headerlink" title="配置音乐播放器（可选的）"></a>配置音乐播放器（可选的）</h3><p>要支持音乐播放，在主题的 <code>_config.yml</code> 配置文件中激活music配置即可：</p><pre><code># 是否在首页显示音乐music:  enable: true  title:             # 非吸底模式有效    enable: true    show: 听听音乐  server: netease   # require music platform: netease, tencent, kugou, xiami, baidu  type: playlist    # require song, playlist, album, search, artist  id: 503838841     # require song id / playlist id / album id / search keyword  fixed: false      # 开启吸底模式  autoplay: false   # 是否自动播放  theme: '#42b983'  loop: 'all'       # 音频循环播放, 可选值: 'all', 'one', 'none'  order: 'random'   # 音频循环顺序, 可选值: 'list', 'random'  preload: 'auto'   # 预加载，可选值: 'none', 'metadata', 'auto'  volume: 0.7       # 默认音量，请注意播放器会记忆用户设置，用户手动设置音量后默认音量即失效  listFolded: true  # 列表默认折叠</code></pre><blockquote><p><code>server</code>可选<code>netease</code>（网易云音乐），<code>tencent</code>（QQ音乐），<code>kugou</code>（酷狗音乐），<code>xiami</code>（虾米音乐），</p><p><code>baidu</code>（百度音乐）。</p><p><code>type</code>可选<code>song</code>（歌曲），<code>playlist</code>（歌单），<code>album</code>（专辑），<code>search</code>（搜索关键字），<code>artist</code>（歌手）</p><pre><code>id`获取方法示例: 浏览器打开网易云音乐，点击我喜欢的音乐歌单，浏览器地址栏后面会有一串数字，`playlist`的`id</code></pre><p>即为这串数字。</p></blockquote><h2 id="文章-Front-matter-介绍"><a href="#文章-Front-matter-介绍" class="headerlink" title="文章 Front-matter 介绍"></a>文章 Front-matter 介绍</h2><h3 id="Front-matter-选项详解"><a href="#Front-matter-选项详解" class="headerlink" title="Front-matter 选项详解"></a>Front-matter 选项详解</h3><p><code>Front-matter</code> 选项中的所有内容均为<strong>非必填</strong>的。但我仍然建议至少填写 <code>title</code> 和 <code>date</code> 的值。</p><table><thead><tr><th>配置选项</th><th>默认值</th><th>描述</th></tr></thead><tbody><tr><td>title</td><td><code>Markdown</code> 的文件标题</td><td>文章标题，强烈建议填写此选项</td></tr><tr><td>date</td><td>文件创建时的日期时间</td><td>发布时间，强烈建议填写此选项，且最好保证全局唯一</td></tr><tr><td>author</td><td>根 <code>_config.yml</code> 中的 <code>author</code></td><td>文章作者</td></tr><tr><td>img</td><td><code>featureImages</code> 中的某个值</td><td>文章特征图，推荐使用图床(腾讯云、七牛云、又拍云等)来做图片的路径.如: <code>http://xxx.com/xxx.jpg</code></td></tr><tr><td>top</td><td><code>true</code></td><td>推荐文章（文章是否置顶），如果 <code>top</code> 值为 <code>true</code>，则会作为首页推荐文章</td></tr><tr><td>cover</td><td><code>false</code></td><td><code>v1.0.2</code>版本新增，表示该文章是否需要加入到首页轮播封面中</td></tr><tr><td>coverImg</td><td>无</td><td><code>v1.0.2</code>版本新增，表示该文章在首页轮播封面需要显示的图片路径，如果没有，则默认使用文章的特色图片</td></tr><tr><td>password</td><td>无</td><td>文章阅读密码，如果要对文章设置阅读验证密码的话，就可以设置 <code>password</code> 的值，该值必须是用 <code>SHA256</code> 加密后的密码，防止被他人识破。前提是在主题的 <code>config.yml</code> 中激活了 <code>verifyPassword</code> 选项</td></tr><tr><td>toc</td><td><code>true</code></td><td>是否开启 TOC，可以针对某篇文章单独关闭 TOC 的功能。前提是在主题的 <code>config.yml</code> 中激活了 <code>toc</code> 选项</td></tr><tr><td>mathjax</td><td><code>false</code></td><td>是否开启数学公式支持 ，本文章是否开启 <code>mathjax</code>，且需要在主题的 <code>_config.yml</code> 文件中也需要开启才行</td></tr><tr><td>summary</td><td>无</td><td>文章摘要，自定义的文章摘要内容，如果这个属性有值，文章卡片摘要就显示这段文字，否则程序会自动截取文章的部分内容作为摘要</td></tr><tr><td>categories</td><td>无</td><td>文章分类，本主题的分类表示宏观上大的分类，只建议一篇文章一个分类</td></tr><tr><td>tags</td><td>无</td><td>文章标签，一篇文章可以多个标签</td></tr><tr><td>keywords</td><td>文章标题</td><td>文章关键字，SEO 时需要</td></tr><tr><td>reprintPolicy</td><td>cc_by</td><td>文章转载规则， 可以是 cc_by, cc_by_nd, cc_by_sa, cc_by_nc, cc_by_nc_nd, cc_by_nc_sa, cc0, noreprint 或 pay 中的一个</td></tr></tbody></table><blockquote><p><strong>注意</strong>:</p><ol><li>如果 <code>img</code> 属性不填写的话，文章特色图会根据文章标题的 <code>hashcode</code> 的值取余，然后选取主题中对应的特色图片，从而达到让所有文章都的特色图<strong>各有特色</strong>。</li><li><code>date</code> 的值尽量保证每篇文章是唯一的，因为本主题中 <code>Gitalk</code> 和 <code>Gitment</code> 识别 <code>id</code> 是通过 <code>date</code> 的值来作为唯一标识的。</li><li>如果要对文章设置阅读验证密码的功能，不仅要在 Front-matter 中设置采用了 SHA256 加密的 password 的值，还需要在主题的 <code>_config.yml</code> 中激活了配置。有些在线的 SHA256 加密的地址，可供你使用：<a href="http://tool.oschina.net/encrypt?type=2">开源中国在线工具</a>、<a href="http://encode.chahuo.com/">chahuo</a>、<a href="http://tool.chinaz.com/tools/hash.aspx">站长工具</a>。</li><li>您可以在文章md文件的 front-matter 中指定 reprintPolicy 来给单个文章配置转载规则</li></ol></blockquote><p>以下为文章的 <code>Front-matter</code> 示例。</p><h3 id="最简示例"><a href="#最简示例" class="headerlink" title="最简示例"></a>最简示例</h3><pre><code>---title: typora-vue-theme主题介绍date: 2018-09-07 09:25:00---</code></pre><h3 id="最全示例"><a href="#最全示例" class="headerlink" title="最全示例"></a>最全示例</h3><pre><code>---title: typora-vue-theme主题介绍date: 2018-09-07 09:25:00author: 赵奇img: /source/images/xxx.jpgtop: truecover: truecoverImg: /images/1.jpgpassword: 8d969eef6ecad3c29a3a629280e686cf0c3f5d5a86aff3ca12020c923adc6c92toc: falsemathjax: falsesummary: 这是你自定义的文章摘要内容，如果这个属性有值，文章卡片摘要就显示这段文字，否则程序会自动截取文章的部分内容作为摘要categories: Markdowntags:  - Typora  - Markdown---</code></pre><h2 id="插件"><a href="#插件" class="headerlink" title="插件"></a>插件</h2><h3 id="图片插件"><a href="#图片插件" class="headerlink" title="图片插件"></a>图片插件</h3><h4 id="第一步：安装插件，在hexo根目录打开Git-Bash-执行"><a href="#第一步：安装插件，在hexo根目录打开Git-Bash-执行" class="headerlink" title="第一步：安装插件，在hexo根目录打开Git Bash,执行"></a>第一步：安装插件，在hexo根目录打开Git Bash,执行</h4><pre class=" language-javascript"><code class="language-javascript">npm install hexo<span class="token operator">-</span>asset<span class="token operator">-</span>image <span class="token operator">--</span>save</code></pre><h4 id="第二步：打开hexo的配置文件-config-yml"><a href="#第二步：打开hexo的配置文件-config-yml" class="headerlink" title="第二步：打开hexo的配置文件_config.yml"></a>第二步：打开hexo的配置文件_config.yml</h4><pre class=" language-shell"><code class="language-shell">找到 post_asset_folder，把这个选项从false改成true</code></pre><h4 id="第三步：打开"><a href="#第三步：打开" class="headerlink" title="第三步：打开"></a>第三步：打开</h4><p>修改 /node_modules/hexo-asset-image/index.js</p><pre class=" language-javascript"><code class="language-javascript"><span class="token string">'use strict'</span><span class="token punctuation">;</span><span class="token keyword">var</span> cheerio <span class="token operator">=</span> <span class="token function">require</span><span class="token punctuation">(</span><span class="token string">'cheerio'</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">function</span> <span class="token function">getPosition</span><span class="token punctuation">(</span>str<span class="token punctuation">,</span> m<span class="token punctuation">,</span> i<span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>  <span class="token keyword">return</span> str<span class="token punctuation">.</span><span class="token function">split</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> i<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">join</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span><span class="token punctuation">.</span>length<span class="token punctuation">;</span><span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span><span class="token keyword">var</span> version <span class="token operator">=</span> <span class="token function">String</span><span class="token punctuation">(</span>hexo<span class="token punctuation">.</span>version<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">split</span><span class="token punctuation">(</span><span class="token string">'.'</span><span class="token punctuation">)</span><span class="token punctuation">;</span>hexo<span class="token punctuation">.</span>extend<span class="token punctuation">.</span>filter<span class="token punctuation">.</span><span class="token function">register</span><span class="token punctuation">(</span><span class="token string">'after_post_render'</span><span class="token punctuation">,</span> <span class="token keyword">function</span><span class="token punctuation">(</span>data<span class="token punctuation">)</span><span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>  <span class="token keyword">var</span> config <span class="token operator">=</span> hexo<span class="token punctuation">.</span>config<span class="token punctuation">;</span>  <span class="token keyword">if</span><span class="token punctuation">(</span>config<span class="token punctuation">.</span>post_asset_folder<span class="token punctuation">)</span><span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>        <span class="token keyword">var</span> link <span class="token operator">=</span> data<span class="token punctuation">.</span>permalink<span class="token punctuation">;</span>    <span class="token keyword">if</span><span class="token punctuation">(</span>version<span class="token punctuation">.</span>length <span class="token operator">></span> <span class="token number">0</span> <span class="token operator">&amp;&amp;</span> <span class="token function">Number</span><span class="token punctuation">(</span>version<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">3</span><span class="token punctuation">)</span>       <span class="token keyword">var</span> beginPos <span class="token operator">=</span> <span class="token function">getPosition</span><span class="token punctuation">(</span>link<span class="token punctuation">,</span> <span class="token string">'/'</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">;</span>    <span class="token keyword">else</span>       <span class="token keyword">var</span> beginPos <span class="token operator">=</span> <span class="token function">getPosition</span><span class="token punctuation">(</span>link<span class="token punctuation">,</span> <span class="token string">'/'</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">;</span>    <span class="token keyword">var</span> endPos <span class="token operator">=</span> link<span class="token punctuation">.</span><span class="token function">lastIndexOf</span><span class="token punctuation">(</span><span class="token string">'/'</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">;</span>    link <span class="token operator">=</span> link<span class="token punctuation">.</span><span class="token function">substring</span><span class="token punctuation">(</span>beginPos<span class="token punctuation">,</span> endPos<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">var</span> toprocess <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'excerpt'</span><span class="token punctuation">,</span> <span class="token string">'more'</span><span class="token punctuation">,</span> <span class="token string">'content'</span><span class="token punctuation">]</span><span class="token punctuation">;</span>    <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">var</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> toprocess<span class="token punctuation">.</span>length<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span><span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>      <span class="token keyword">var</span> key <span class="token operator">=</span> toprocess<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>      <span class="token keyword">var</span> $ <span class="token operator">=</span> cheerio<span class="token punctuation">.</span><span class="token function">load</span><span class="token punctuation">(</span>data<span class="token punctuation">[</span>key<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>        ignoreWhitespace<span class="token punctuation">:</span> <span class="token boolean">false</span><span class="token punctuation">,</span>        xmlMode<span class="token punctuation">:</span> <span class="token boolean">false</span><span class="token punctuation">,</span>        lowerCaseTags<span class="token punctuation">:</span> <span class="token boolean">false</span><span class="token punctuation">,</span>        decodeEntities<span class="token punctuation">:</span> <span class="token boolean">false</span>      <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>      <span class="token function">$</span><span class="token punctuation">(</span><span class="token string">'img'</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">each</span><span class="token punctuation">(</span><span class="token keyword">function</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>        <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token function">$</span><span class="token punctuation">(</span><span class="token keyword">this</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">attr</span><span class="token punctuation">(</span><span class="token string">'src'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>            <span class="token keyword">var</span> src <span class="token operator">=</span> <span class="token function">$</span><span class="token punctuation">(</span><span class="token keyword">this</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">attr</span><span class="token punctuation">(</span><span class="token string">'src'</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">replace</span><span class="token punctuation">(</span><span class="token string">'\\'</span><span class="token punctuation">,</span> <span class="token string">'/'</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token keyword">if</span><span class="token punctuation">(</span><span class="token operator">!</span><span class="token regex">/http[s]*.*|\/\/.*/</span><span class="token punctuation">.</span><span class="token function">test</span><span class="token punctuation">(</span>src<span class="token punctuation">)</span> <span class="token operator">&amp;&amp;</span>               <span class="token operator">!</span><span class="token regex">/^\s*\//</span><span class="token punctuation">.</span><span class="token function">test</span><span class="token punctuation">(</span>src<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>              <span class="token keyword">var</span> linkArray <span class="token operator">=</span> link<span class="token punctuation">.</span><span class="token function">split</span><span class="token punctuation">(</span><span class="token string">'/'</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">filter</span><span class="token punctuation">(</span><span class="token keyword">function</span><span class="token punctuation">(</span>elem<span class="token punctuation">)</span><span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>                <span class="token keyword">return</span> elem <span class="token operator">!=</span> <span class="token string">''</span><span class="token punctuation">;</span>              <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>              <span class="token keyword">var</span> srcArray <span class="token operator">=</span> src<span class="token punctuation">.</span><span class="token function">split</span><span class="token punctuation">(</span><span class="token string">'/'</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">filter</span><span class="token punctuation">(</span><span class="token keyword">function</span><span class="token punctuation">(</span>elem<span class="token punctuation">)</span><span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>                <span class="token keyword">return</span> elem <span class="token operator">!=</span> <span class="token string">''</span> <span class="token operator">&amp;&amp;</span> elem <span class="token operator">!=</span> <span class="token string">'.'</span><span class="token punctuation">;</span>              <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>              <span class="token keyword">if</span><span class="token punctuation">(</span>srcArray<span class="token punctuation">.</span>length <span class="token operator">></span> <span class="token number">1</span><span class="token punctuation">)</span>                srcArray<span class="token punctuation">.</span><span class="token function">shift</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>              src <span class="token operator">=</span> srcArray<span class="token punctuation">.</span><span class="token function">join</span><span class="token punctuation">(</span><span class="token string">'/'</span><span class="token punctuation">)</span><span class="token punctuation">;</span>              <span class="token function">$</span><span class="token punctuation">(</span><span class="token keyword">this</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">attr</span><span class="token punctuation">(</span><span class="token string">'src'</span><span class="token punctuation">,</span> config<span class="token punctuation">.</span>root <span class="token operator">+</span> link <span class="token operator">+</span> src<span class="token punctuation">)</span><span class="token punctuation">;</span>              console<span class="token punctuation">.</span>info<span class="token operator">&amp;&amp;</span>console<span class="token punctuation">.</span><span class="token function">info</span><span class="token punctuation">(</span><span class="token string">"update link as:-->"</span><span class="token operator">+</span>config<span class="token punctuation">.</span>root <span class="token operator">+</span> link <span class="token operator">+</span> src<span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>        <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span><span class="token keyword">else</span><span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>            console<span class="token punctuation">.</span>info<span class="token operator">&amp;&amp;</span>console<span class="token punctuation">.</span><span class="token function">info</span><span class="token punctuation">(</span><span class="token string">"no src attr, skipped..."</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            console<span class="token punctuation">.</span>info<span class="token operator">&amp;&amp;</span>console<span class="token punctuation">.</span><span class="token function">info</span><span class="token punctuation">(</span><span class="token function">$</span><span class="token punctuation">(</span><span class="token keyword">this</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>      <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>      data<span class="token punctuation">[</span>key<span class="token punctuation">]</span> <span class="token operator">=</span> $<span class="token punctuation">.</span><span class="token function">html</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>  <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span><span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre><p>修改代码见- 补充文件夹下的images_file.js</p><h4 id="第四步：现在就可以插入图片了"><a href="#第四步：现在就可以插入图片了" class="headerlink" title="第四步：现在就可以插入图片了"></a>第四步：现在就可以插入图片了</h4><p>比如hexo new  photo之后就在source/_posts生成photo.md文件和photo文件夹，我们把要插入的图片复制到photo文件夹内， 在photo.md文件里面按markdown的标准写,（我的文件名是head.jpeg）比如</p><pre class=" language-bash"><code class="language-bash"><span class="token operator">!</span><span class="token punctuation">[</span>这是代替图片的文字，随便写<span class="token punctuation">]</span><span class="token punctuation">(</span>head.jpeg<span class="token punctuation">)</span></code></pre><h3 id="评论系统"><a href="#评论系统" class="headerlink" title="评论系统"></a>评论系统</h3><h4 id="注册"><a href="#注册" class="headerlink" title="注册"></a>注册</h4><p>​       先去注册，valine依附于LeanCloud，先去注册，并完成<em>实名认证</em> <a href="https://links.jianshu.com/go?to=https://leancloud.cn/">LeanCloud注册</a></p><h4 id="实名认证"><a href="#实名认证" class="headerlink" title="实名认证"></a>实名认证</h4><p>​       注册成功切记<strong>实名认证</strong>，反正他会自己提醒的，不完成也做不了下一步</p><p>​       实名认证后创建应用，应用名字啥的随便取。</p><p>####appid</p><p>然后获得appid，和keys，如下图：</p><p><img src="/Users/daiyizheng/Desktop/daiyz/source/_posts/hexo/valine1.webp" alt="appid和appkeys"></p><p>​       然后下一步把你的博客域名写到安全中心。</p><p><img src="/Users/daiyizheng/Desktop/daiyz/source/_posts/hexo/valine2.webp" alt="安全中心"></p><h4 id="valine配置"><a href="#valine配置" class="headerlink" title="valine配置"></a>valine配置</h4><p>​      打开next主题的config文件，找到valine配置，把你的appid和appkey填进去，重新hexo g，hexo d，一下，三分钟后，看看你的文章底部，简约而不简单的评论出来了。</p><pre class=" language-yaml"><code class="language-yaml"><span class="token comment" spellcheck="true"># Valine.</span><span class="token comment" spellcheck="true"># You can get your appid and appkey from https://leancloud.cn</span><span class="token comment" spellcheck="true"># more info please open https://valine.js.org</span><span class="token key atrule">valine</span><span class="token punctuation">:</span>  <span class="token key atrule">enable</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>  <span class="token key atrule">appid</span><span class="token punctuation">:</span>  手动打码  <span class="token key atrule">appkey</span><span class="token punctuation">:</span>  手动打码  <span class="token key atrule">notify</span><span class="token punctuation">:</span> <span class="token boolean important">false </span><span class="token comment" spellcheck="true"># mail notifier , https://github.com/xCss/Valine/wiki</span>  <span class="token key atrule">verify</span><span class="token punctuation">:</span> <span class="token boolean important">false </span><span class="token comment" spellcheck="true"># Verification code</span>  <span class="token key atrule">placeholder</span><span class="token punctuation">:</span> come on baby <span class="token comment" spellcheck="true"># 来啊，快活啊</span>  <span class="token key atrule">avatar</span><span class="token punctuation">:</span> mm <span class="token comment" spellcheck="true"># gravatar style</span>  <span class="token key atrule">guest_info</span><span class="token punctuation">:</span> nick<span class="token punctuation">,</span>mail<span class="token punctuation">,</span>link <span class="token comment" spellcheck="true"># custom comment header</span>  <span class="token key atrule">pageSize</span><span class="token punctuation">:</span> <span class="token number">10 </span><span class="token comment" spellcheck="true"># pagination size</span></code></pre><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><h3 id="数学公式不显示"><a href="#数学公式不显示" class="headerlink" title="数学公式不显示"></a>数学公式不显示</h3><p>在Hexo中渲染MathJax数学公式</p><p>在用markdown写技术文档时，免不了会碰到数学公式。常用的Markdown编辑器都会集成<a href="https://link.jianshu.com/?t=https://www.mathjax.org/">Mathjax</a>，用来渲染文档中的类Latex格式书写的数学公式。基于Hexo搭建的个人博客，默认情况下渲染数学公式却会出现各种各样的问题。</p><h4 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h4><p>Hexo默认使用”hexo-renderer-marked”引擎渲染网页，该引擎会把一些特殊的markdown符号转换为相应的html标签，比如在markdown语法中，下划线’_’代表斜体，会被渲染引擎处理为``标签。</p><p>因为类Latex格式书写的数学公式下划线 ‘_’ 表示下标，有特殊的含义，如果被强制转换为<code>标签，那么MathJax引擎在渲染数学公式的时候就会出错。例如，$x_i$在开始被渲染的时候，处理为$x</code>i``$，这样MathJax引擎就认为该公式有语法错误，因为不会渲染。</p><p>类似的语义冲突的符号还包括’*’, ‘{‘, ‘}’, ‘'等。</p><h4 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h4><p>解决方案有很多，可以网上搜下，为了节省大家的时间，这里只提供亲身测试过的最靠谱的方法。</p><p>更换Hexo的markdown渲染引擎，<a href="https://link.jianshu.com/?t=https://github.com/sun11/hexo-renderer-kramed">hexo-renderer-kramed</a>引擎是在默认的渲染引擎<a href="https://link.jianshu.com/?t=https://github.com/hexojs/hexo-renderer-marked">hexo-renderer-marked</a>的基础上修改了一些bug，两者比较接近，也比较轻量级。</p><pre class=" language-undefined"><code class="language-undefined">npm uninstall hexo-renderer-marked --savenpm install hexo-renderer-kramed --save</code></pre><p>执行上面的命令即可，先卸载原来的渲染引擎，再安装新的。</p><p>然后，跟换引擎后行间公式可以正确渲染了，但是这样还没有完全解决问题，行内公式的渲染还是有问题，因为<a href="https://link.jianshu.com/?t=https://github.com/sun11/hexo-renderer-kramed">hexo-renderer-kramed</a>引擎也有语义冲突的问题。接下来到博客根目录下，找到node_modules\kramed\lib\rules\inline.js，把第11行的escape变量的值做相应的修改：</p><pre class=" language-js"><code class="language-js"><span class="token comment" spellcheck="true">//  escape: /^\\([\\`*&amp;#123;&amp;#125;\[\]()#$+\-.!_>])/,</span>  escape<span class="token punctuation">:</span> <span class="token regex">/^\\([`*\[\]()#$+\-.!_>])/</span></code></pre><p>这一步是在原基础上取消了对,{,}的转义(escape)。<br> 同时把第20行的em变量也要做相应的修改。</p><pre class=" language-ruby"><code class="language-ruby"><span class="token operator">/</span><span class="token operator">/</span>  em<span class="token punctuation">:</span> <span class="token regex">/^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/</span><span class="token punctuation">,</span>  em<span class="token punctuation">:</span> <span class="token regex">/^\*((?:\*\*|[\s\S])+?)\*(?!\*)/</span></code></pre><p>重新启动hexo（先clean再generate）,问题完美解决。哦，如果不幸还没解决的话，看看是不是还需要在使用的主题中配置mathjax开关。</p><h4 id="在主题中开启mathjax开关"><a href="#在主题中开启mathjax开关" class="headerlink" title="在主题中开启mathjax开关"></a>在主题中开启mathjax开关</h4><p>如何使用了主题了，别忘了在主题（Theme）中开启mathjax开关，下面以next主题为例，介绍下如何打开mathjax开关。</p><p>进入到主题目录，找到_config.yml配置问题，把mathjax默认的false修改为true，具体如下：</p><pre class=" language-bash"><code class="language-bash"><span class="token comment" spellcheck="true"># MathJax Support</span>mathjax:  enable: <span class="token boolean">true</span>  per_page: <span class="token boolean">true</span></code></pre><p>别着急，这样还不够，还需要在文章的Front-matter里打开mathjax开关，如下：</p><pre class=" language-css"><code class="language-css"><span class="token property">title</span><span class="token punctuation">:</span> index<span class="token number">.</span>html<span class="token property">date</span><span class="token punctuation">:</span> <span class="token number">2016</span>-<span class="token number">12</span>-<span class="token number">28</span> <span class="token property">21</span><span class="token punctuation">:</span><span class="token property">01</span><span class="token punctuation">:</span><span class="token number">30</span><span class="token property">tags</span><span class="token punctuation">:</span><span class="token property">mathjax</span><span class="token punctuation">:</span> true</code></pre><p>不要嫌麻烦，之所以要在文章头里设置开关，是因为考虑只有在用到公式的页面才加载 Mathjax，这样不需要渲染数学公式的页面的访问速度就不会受到影响了</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 前端 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 静态建站 </tag>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
